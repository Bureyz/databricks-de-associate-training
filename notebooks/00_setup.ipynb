{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da5f3388-d634-4d42-8fe4-80d846028294",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Training Environment Setup\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook **validates your training environment** and exports variables for use in other notebooks.\n",
    "\n",
    "**What it does:**\n",
    "1. Gets your username from Databricks session\n",
    "2. Finds your pre-created catalog (`ecommerce_platform_{username}`)\n",
    "3. Validates schemas and Volume exist\n",
    "4. Exports variables: `CATALOG`, `BRONZE_SCHEMA`, `SILVER_SCHEMA`, `GOLD_SCHEMA`, `DATASET_BASE_PATH`\n",
    "\n",
    "**Prerequisites:**\n",
    "- Trainer must run `00_pre_config.ipynb` before the training session\n",
    "- You must be a member of the training group (e.g., `dp_trn_1`)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0c37e9f-38e8-4fd2-bf40-bdc57dbd0a5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "CATALOG_PREFIX = \"ecommerce_platform\"\n",
    "BRONZE_SCHEMA = \"bronze\"\n",
    "SILVER_SCHEMA = \"silver\"\n",
    "GOLD_SCHEMA = \"gold\"\n",
    "DEFAULT_SCHEMA = \"default\"\n",
    "VOLUME_NAME = \"datasets\"\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: Get Current User\n",
    "# =============================================================================\n",
    "raw_user = spark.sql(\"SELECT current_user()\").first()[0]\n",
    "print(f\"Current user: {raw_user}\")\n",
    "\n",
    "# Create safe catalog name suffix from email\n",
    "user_slug = re.sub(r'[^a-zA-Z0-9]', '_', raw_user.split('@')[0]).lower()\n",
    "user_slug = re.sub(r'_+', '_', user_slug).strip('_')\n",
    "print(f\"User slug: {user_slug}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Find Your Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 2: Find and Validate Catalog\n",
    "# =============================================================================\n",
    "CATALOG = f\"{CATALOG_PREFIX}_{user_slug}\"\n",
    "\n",
    "# Check if catalog exists\n",
    "try:\n",
    "    catalogs = spark.sql(\"SHOW CATALOGS\").collect()\n",
    "    catalog_names = [row[0] for row in catalogs]\n",
    "    \n",
    "    if CATALOG in catalog_names:\n",
    "        print(f\"Catalog found: {CATALOG}\")\n",
    "        spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "    else:\n",
    "        print(f\"ERROR: Catalog '{CATALOG}' not found!\")\n",
    "        print(f\"\\\\nAvailable catalogs matching prefix '{CATALOG_PREFIX}':\")\n",
    "        for c in catalog_names:\n",
    "            if c.startswith(CATALOG_PREFIX):\n",
    "                print(f\"  - {c}\")\n",
    "        print(f\"\\\\nContact trainer to run 00_pre_config.ipynb\\\")\\n\",\n",
    "        raise Exception(f\"Catalog '{CATALOG}' not found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"ERROR: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Validate Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 3: Validate Schemas Exist\n",
    "# =============================================================================\n",
    "schemas = spark.sql(f\"SHOW SCHEMAS IN {CATALOG}\").collect()\n",
    "schema_names = [row[0] for row in schemas]\n",
    "\n",
    "required_schemas = [BRONZE_SCHEMA, SILVER_SCHEMA, GOLD_SCHEMA, DEFAULT_SCHEMA]\n",
    "missing_schemas = [s for s in required_schemas if s not in schema_names]\n",
    "\n",
    "if missing_schemas:\n",
    "    print(f\"ERROR: Missing schemas: {missing_schemas}\")\n",
    "    print(\"Contact trainer to run 00_pre_config.ipynb\")\n",
    "    raise Exception(f\"Missing schemas: {missing_schemas}\")\n",
    "else:\n",
    "    print(f\"All schemas found: {', '.join(required_schemas)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4: Validate Volume and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 4: Validate Volume and Dataset Files\n",
    "# =============================================================================\n",
    "DATASET_BASE_PATH = f\"/Volumes/{CATALOG}/{DEFAULT_SCHEMA}/{VOLUME_NAME}\"\n",
    "\n",
    "try:\n",
    "    files = dbutils.fs.ls(DATASET_BASE_PATH)\n",
    "    print(f\"Volume found: {DATASET_BASE_PATH}\")\n",
    "    print(f\"Dataset folders:\")\n",
    "    for f in files:\n",
    "        print(f\"  - {f.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Cannot access Volume at {DATASET_BASE_PATH}\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\\\nContact trainer to run 00_pre_config.ipynb\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Export Variables\n",
    "\n",
    "These variables are now available in all notebooks that run `%run ../00_setup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 5: Export Variables (Summary)\n",
    "# =============================================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING ENVIRONMENT READY\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(f\"User:              {raw_user}\")\n",
    "print(f\"CATALOG:           {CATALOG}\")\n",
    "print(f\"BRONZE_SCHEMA:     {BRONZE_SCHEMA}\")\n",
    "print(f\"SILVER_SCHEMA:     {SILVER_SCHEMA}\")\n",
    "print(f\"GOLD_SCHEMA:       {GOLD_SCHEMA}\")\n",
    "print(f\"DATASET_BASE_PATH: {DATASET_BASE_PATH}\")\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Example usage in notebooks:\")\n",
    "print(f\"  spark.read.csv('{DATASET_BASE_PATH}/customers/customers.csv')\")\n",
    "print(f\"  spark.sql('SELECT * FROM {CATALOG}.{BRONZE_SCHEMA}.customers')\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
