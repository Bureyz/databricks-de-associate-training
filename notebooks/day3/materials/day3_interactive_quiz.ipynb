{
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Quiz -- Day 3\n",
    "\n",
    "**Modules:** M07 (Medallion & Lakeflow), M08 (Orchestration), M09 (Governance)  \n",
    "**Questions:** 23 (single correct answer A-D)  \n",
    "\n",
    "---\n",
    "\n",
    "### How it works\n",
    "1. **Run Cell 2** to create answer widgets (dropdowns at the top of the notebook)\n",
    "2. Read each question and select your answer from the dropdown\n",
    "3. **Run the last cell** to check your score\n",
    "\n",
    "> All widgets appear at the top of the notebook in Databricks. Scroll up to see them after running Cell 2."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run this cell FIRST to create answer widgets\n",
    "# Dropdowns will appear at the top of the notebook\n",
    "\n",
    "dbutils.widgets.dropdown(\"Q1\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q1\")\n",
    "dbutils.widgets.dropdown(\"Q2\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q2\")\n",
    "dbutils.widgets.dropdown(\"Q3\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q3\")\n",
    "dbutils.widgets.dropdown(\"Q4\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q4\")\n",
    "dbutils.widgets.dropdown(\"Q5\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q5\")\n",
    "dbutils.widgets.dropdown(\"Q6\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q6\")\n",
    "dbutils.widgets.dropdown(\"Q7\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q7\")\n",
    "dbutils.widgets.dropdown(\"Q8\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q8\")\n",
    "dbutils.widgets.dropdown(\"Q9\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q9\")\n",
    "dbutils.widgets.dropdown(\"Q10\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q10\")\n",
    "dbutils.widgets.dropdown(\"Q11\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q11\")\n",
    "dbutils.widgets.dropdown(\"Q12\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q12\")\n",
    "dbutils.widgets.dropdown(\"Q13\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q13\")\n",
    "dbutils.widgets.dropdown(\"Q14\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q14\")\n",
    "dbutils.widgets.dropdown(\"Q15\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q15\")\n",
    "dbutils.widgets.dropdown(\"Q16\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q16\")\n",
    "dbutils.widgets.dropdown(\"Q17\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q17\")\n",
    "dbutils.widgets.dropdown(\"Q18\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q18\")\n",
    "dbutils.widgets.dropdown(\"Q19\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q19\")\n",
    "dbutils.widgets.dropdown(\"Q20\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q20\")\n",
    "dbutils.widgets.dropdown(\"Q21\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q21\")\n",
    "dbutils.widgets.dropdown(\"Q22\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q22\")\n",
    "dbutils.widgets.dropdown(\"Q23\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q23\")\n",
    "\n",
    "print(\"Created 23 widgets -- scroll up to see dropdowns\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "In the Medallion Architecture, which layer contains raw, unvalidated data ingested from source systems?\n\n- A. Silver\n- B. Gold\n- C. Bronze\n- D. Platinum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "In a Lakeflow pipeline, what is the difference between a STREAMING TABLE and a MATERIALIZED VIEW?\n\n- A. STREAMING TABLE supports batch only; MATERIALIZED VIEW supports streaming only\n- B. STREAMING TABLE processes data incrementally (append-only); MATERIALIZED VIEW recomputes the full result set\n- C. They are identical; the names are interchangeable\n- D. STREAMING TABLE stores views; MATERIALIZED VIEW stores tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "In Lakeflow SQL, what does the `LIVE.` prefix reference?\n\n- A. A live-streaming external source\n- B. Another table or view within the same pipeline\n- C. A table in the default catalog\n- D. A real-time dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4\n",
    "\n",
    "What does this Lakeflow declaration do?\n\n```sql\nCONSTRAINT valid_id EXPECT (id IS NOT NULL) ON VIOLATION DROP ROW\n```\n\n- A. Logs a warning for null IDs but keeps all rows\n- B. Drops the entire pipeline run if any null ID is found\n- C. Silently removes rows where `id` is NULL from the output\n- D. Replaces NULL IDs with a default value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5\n",
    "\n",
    "What happens when a Lakeflow expectation uses `ON VIOLATION FAIL UPDATE`?\n\n- A. The failing rows are dropped and the pipeline continues\n- B. The pipeline update fails immediately, and no data is written for that table\n- C. A warning is logged but processing continues\n- D. The failing rows are quarantined to an error table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6\n",
    "\n",
    "What happens when an expectation is declared without an `ON VIOLATION` clause?\n\n```sql\nCONSTRAINT positive_amount EXPECT (amount > 0)\n```\n\n- A. Failing rows are dropped\n- B. The pipeline fails\n- C. All rows are kept; a warning is logged and metrics are recorded\n- D. An error -- `ON VIOLATION` is required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7\n",
    "\n",
    "Which Lakeflow SQL keyword is used to read from an upstream streaming table within the same pipeline?\n\n**A.**\n```sql\nSELECT * FROM STREAM(LIVE.bronze_orders)\n```\n\n**B.**\n```sql\nSELECT * FROM LIVE.bronze_orders\n```\n\n**C.**\n```sql\nSELECT * FROM STREAMING(bronze_orders)\n```\n\n**D.**\n```sql\nSELECT * FROM READ_STREAM(bronze_orders)\n```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8\n",
    "\n",
    "Where can you view data quality metrics (pass/fail counts) for Lakeflow expectations?\n\n**A.**\n```sql\nSELECT * FROM event_log(TABLE(catalog.schema.silver_table))\nWHERE details:flow_progress:data_quality IS NOT NULL\n```\n\n**B.**\n```sql\nDESCRIBE EXPECTATIONS silver_table\n```\n\n**C.**\n```sql\nSELECT * FROM INFORMATION_SCHEMA.EXPECTATIONS\n```\n\n**D.**\n```sql\nSHOW QUALITY METRICS FOR silver_table\n```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9\n",
    "\n",
    "In Databricks Workflows, what is a \"Task\" within a Job?\n\n- A. A separate cluster configuration\n- B. A single unit of work (notebook, SQL, Python script, or pipeline) that the Job orchestrates\n- C. A permission setting\n- D. A notification channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10\n",
    "\n",
    "What does `dbutils.jobs.taskValues.set()` do?\n\nTask 1:\n```python\ndbutils.jobs.taskValues.set(key=\"row_count\", value=1000)\n```\n\nTask 2 (downstream):\n```python\ncount = dbutils.jobs.taskValues.get(taskKey=\"Task_1\", key=\"row_count\")\n```\n\n- A. Sets a job-level environment variable\n- B. Passes a value from one task to a downstream task within the same job run\n- C. Configures cluster parameters\n- D. Sets the task retry count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q11\n",
    "\n",
    "What is a \"Repair Run\" in Databricks Workflows?\n\n- A. A full re-run of the entire job from scratch\n- B. Re-runs only the failed tasks (and their downstream dependents) without re-running successful ones\n- C. A manual fix applied to the job configuration\n- D. Rolling back the job to a previous version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q12\n",
    "\n",
    "Which of the following is a valid task type in a Databricks Workflow Job?\n\n- A. Notebook, SQL, Python script, Lakeflow pipeline, JAR\n- B. Only notebooks\n- C. Only SQL and Python\n- D. Only Lakeflow pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q13\n",
    "\n",
    "Which Unity Catalog feature allows you to restrict which rows a user can see when querying a table?\n\n- A. Column Mask\n- B. Row Filter\n- C. Dynamic View\n- D. Data Profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q14\n",
    "\n",
    "How do you apply a Column Mask to a table in Unity Catalog?\n\n**A.**\n```sql\nGRANT MASK ON COLUMN email TO mask_func\n```\n\n**B.**\n```sql\nCREATE MASK ON TABLE t COLUMN email\n```\n\n**C.**\n```sql\nALTER TABLE t ALTER COLUMN email SET MASK catalog.schema.mask_func\n```\n\n**D.**\n```sql\nUPDATE TABLE t SET COLUMN email MASKED\n```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q15\n",
    "\n",
    "Which INFORMATION_SCHEMA view would you query to list all tables and views in a Unity Catalog catalog?\n\n- A. `INFORMATION_SCHEMA.SCHEMATA`\n- B. `INFORMATION_SCHEMA.COLUMNS`\n- C. `INFORMATION_SCHEMA.TABLES`\n- D. `INFORMATION_SCHEMA.TABLE_PRIVILEGES`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q16\n",
    "\n",
    "Which SQL function is used inside Row Filters and Column Masks to check if the querying user belongs to a specific group?\n\n**A.**\n```sql\nis_account_group_member('admins')\n```\n\n**B.**\n```sql\nuser_in_group('admins')\n```\n\n**C.**\n```sql\nhas_role('admins')\n```\n\n**D.**\n```sql\ncheck_group('admins')\n```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q17\n",
    "\n",
    "What is the purpose of Delta Sharing?\n\n- A. Sharing compute clusters between workspaces\n- B. An open protocol for secure, cross-organization data sharing without copying data\n- C. Sharing notebooks between users\n- D. Synchronizing Git repositories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q18\n",
    "\n",
    "Which statement correctly grants `SELECT` access on all tables in the `silver` schema to the `analysts` group?\n\n**A.**\n```sql\nGRANT SELECT ON SCHEMA my_catalog.silver TO `analysts`\n```\n\n**B.**\n```sql\nGRANT READ ON DATABASE silver TO analysts\n```\n\n**C.**\n```sql\nGRANT ALL ON SCHEMA my_catalog.silver TO `analysts`\n```\n\n**D.**\n```sql\nALLOW SELECT ON my_catalog.silver FOR analysts\n```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q19\n",
    "\n",
    "How do you remove a Row Filter from a table?\n\n- A. `DROP FILTER ON TABLE t`\n- B. `ALTER TABLE t DROP ROW FILTER`\n- C. `REVOKE ROW FILTER FROM t`\n- D. `DELETE ROW FILTER ON t`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q20\n",
    "\n",
    "What does Unity Catalog's lineage feature provide?\n\n- A. Version history of table schema changes\n- B. Automatic tracking of data flow from source to downstream tables, showing column-level dependencies\n- C. A list of users who accessed a table\n- D. Storage cost breakdown per table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q21\n",
    "\n",
    "Which system table would you query to track DBU consumption and costs across your Databricks workspace?\n\n- A. `system.access.audit`\n- B. `system.billing.usage`\n- C. `system.compute.clusters`\n- D. `system.storage.table_storage`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q22\n",
    "\n",
    "A data engineer needs to find all queries that took longer than 60 seconds on a SQL Warehouse in the last 7 days. Which system table should they query?\n\n- A. `system.access.audit`\n- B. `system.compute.warehouse_events`\n- C. `system.query.history`\n- D. `system.lakeflow.pipeline_event_log`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q23\n",
    "\n",
    "Which system table provides an audit trail of who accessed, modified, or granted permissions on Unity Catalog objects?\n\n- A. `INFORMATION_SCHEMA.TABLE_PRIVILEGES`\n- B. `system.access.audit`\n- C. `system.access.table_lineage`\n- D. `system.billing.usage`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run this cell to check your score\n",
    "\n",
    "answers = {\n",
    "    1: ('C', 'Bronze is the raw/landing layer. Silver is cleaned/validated. Gold is business-level aggregations.'),\n",
    "    2: ('B', 'STREAMING TABLE processes data incrementally (append-only). MATERIALIZED VIEW recomputes the full result when updated.'),\n",
    "    3: ('B', '`LIVE.table_name` references other datasets within the same Lakeflow pipeline. It is pipeline-scoped.'),\n",
    "    4: ('C', '`ON VIOLATION DROP ROW` silently filters out rows that fail the expectation.'),\n",
    "    5: ('B', '`FAIL UPDATE` aborts the pipeline immediately, preventing bad data from being written.'),\n",
    "    6: ('C', 'Without `ON VIOLATION`, the expectation is in \"warn\" mode: all rows pass through, but violations are logged as metrics.'),\n",
    "    7: ('A', '`STREAM(LIVE.table)` reads from an upstream dataset as a streaming source. `LIVE.table` (without STREAM) reads as batch.'),\n",
    "    8: ('A', '`event_log(TABLE(...))` is the function to query pipeline event logs, including data quality metrics.'),\n",
    "    9: ('B', 'A Task is the smallest unit of work in a Job. Jobs can have multiple tasks with dependencies forming a DAG.'),\n",
    "    10: ('B', '`taskValues.set(key, value)` stores a value that downstream tasks retrieve with `taskValues.get()`.'),\n",
    "    11: ('B', 'Repair Run re-executes only failed tasks and their downstream dependents, saving time and compute.'),\n",
    "    12: ('A', 'Valid task types include: Notebook, SQL, Python script, Lakeflow pipeline, JAR, dbt, and more.'),\n",
    "    13: ('B', 'Row Filters are SQL UDFs attached to tables that filter rows based on the querying user\\'s identity or group.'),\n",
    "    14: ('C', 'Column Masks are applied via `ALTER TABLE ... ALTER COLUMN ... SET MASK <function>`.'),\n",
    "    15: ('C', '`INFORMATION_SCHEMA.TABLES` lists all tables and views with their schema, name, and type.'),\n",
    "    16: ('A', '`is_account_group_member(\\'group_name\\')` returns true if the current user belongs to the specified group.'),\n",
    "    17: ('B', 'Delta Sharing is an open protocol for cross-org data sharing without copying data.'),\n",
    "    18: ('A', '`GRANT SELECT ON SCHEMA catalog.schema TO principal` grants SELECT on all current and future tables in the schema.'),\n",
    "    19: ('B', '`ALTER TABLE t DROP ROW FILTER` removes the row filter. Similarly, `ALTER TABLE t ALTER COLUMN c DROP MASK` removes a mask.'),\n",
    "    20: ('B', 'Lineage automatically tracks data flow between tables, including column-level dependencies, in the Unity Catalog UI.'),\n",
    "    21: ('B', '`system.billing.usage` tracks DBU consumption by SKU, user, and workspace. `system.access.audit` tracks security events, not costs.'),\n",
    "    22: ('C', '`system.query.history` contains query execution details including duration, user, and statement text for SQL Warehouses.'),\n",
    "    23: ('B', '`system.access.audit` logs all access, modification, and permission events. `INFORMATION_SCHEMA.TABLE_PRIVILEGES` shows current grants, not the audit trail.'),\n",
    "}\n",
    "\n",
    "correct = 0\n",
    "total = len(answers)\n",
    "results = []\n",
    "\n",
    "for qnum in sorted(answers.keys()):\n",
    "    user_ans = dbutils.widgets.get(f\"Q{qnum}\")\n",
    "    correct_ans, explanation = answers[qnum]\n",
    "    if user_ans == correct_ans:\n",
    "        correct += 1\n",
    "        results.append(f\"  Q{qnum}: {user_ans} -- Correct!\")\n",
    "    elif user_ans == \"-\":\n",
    "        results.append(f\"  Q{qnum}: Not answered (correct: {correct_ans})\")\n",
    "    else:\n",
    "        results.append(f\"  Q{qnum}: {user_ans} -- Wrong. Correct: {correct_ans} | {explanation}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  QUIZ RESULTS -- Day 3\")\n",
    "print(\"=\" * 60)\n",
    "for r in results:\n",
    "    print(r)\n",
    "print(\"=\" * 60)\n",
    "pct = round(correct / total * 100)\n",
    "print(f\"\\n  Score: {correct}/{total} ({pct}%)\")\n",
    "if pct >= 90:\n",
    "    print(\"  Excellent! Exam-ready!\")\n",
    "elif pct >= 70:\n",
    "    print(\"  Good job! Review missed topics.\")\n",
    "elif pct >= 50:\n",
    "    print(\"  Keep studying -- revisit the cheatsheet.\")\n",
    "else:\n",
    "    print(\"  Re-read the demo notebooks and try again.\")\n",
    "print(\"=\" * 60)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# (Optional) Remove all widgets\n",
    "# dbutils.widgets.removeAll()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}