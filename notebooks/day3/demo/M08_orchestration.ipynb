{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6438c89-2989-4fec-be2e-47ec390656d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# M08: Orchestration & Lakeflow Jobs\n",
    "\n",
    "Orchestration is the glue connecting pipelines into production workflows. We'll learn to create multi-task Jobs with dependencies (DAG), configure triggers (CRON, File Arrival), set up retries and alerts, pass parameters between tasks (taskValues), and monitor execution via System Tables.\n",
    "\n",
    "| Exam Domain | Weight |\n",
    "|---|---|\n",
    "| Production Pipelines | 13% |\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71db7212-7f11-4069-ab66-c9871e71ad34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Introduction to Lakeflow Jobs\n",
    "\n",
    "**Lakeflow Jobs** (formerly Databricks Jobs) is a managed orchestration service.\n",
    "\n",
    "| Scenario | Solution |\n",
    "|----------|----------|\n",
    "| ETL pipeline with multiple steps | Multi-task Job |\n",
    "| Daily report at fixed time | Scheduled Job |\n",
    "| Reaction to new files | File Arrival Trigger |\n",
    "| ML training pipeline | Job with notebook tasks |\n",
    "| Run Lakeflow Pipeline | Job with Pipeline task |\n",
    "\n",
    "| Feature | Jobs | Lakeflow Pipelines |\n",
    "|---------|------|-----|\n",
    "| Orchestration | General | ETL only |\n",
    "| Dependencies | Manual configuration | Automatic (DAG) |\n",
    "| Data Quality | Custom code | Built-in expectations |\n",
    "| Flexibility | High | Opinionated |\n",
    "\n",
    "**Best Practice**: Use Lakeflow Pipelines for ETL, Jobs for orchestrating Pipelines + other tasks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6207e011-2da6-48ac-a086-dd4dc75039cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task Types in Lakeflow Jobs\n",
    "\n",
    "Overview of all available task types in Lakeflow Jobs and when to use each one.\n",
    "\n",
    "| Task Type | Description | Use Case |\n",
    "|-----------|-------------|----------|\n",
    "| Notebook | Run a Databricks notebook | ETL logic, ML training |\n",
    "| Pipeline | Run a Lakeflow Declarative Pipeline | Streaming/batch pipelines |\n",
    "| Python Script | Run a Python file | Utility scripts |\n",
    "| SQL | Run a SQL query | DDL, reporting queries |\n",
    "| JAR | Run a Java/Scala JAR | Legacy Spark jobs |\n",
    "| Spark Submit | Submit a Spark application | Custom Spark apps |\n",
    "| If/Else Condition | Branch based on condition | Conditional workflows |\n",
    "| For Each | Iterate over a list | Parameterized batch runs |\n",
    "\n",
    "**Repair Runs:** Re-runs only **failed and downstream tasks**, skipping successful ones — saves compute and time.\n",
    "\n",
    "**Exam Note:** Know that repair runs skip already-successful tasks. If/Else and For Each enable conditional and iterative workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5ccfda7-f73d-48db-9f7a-a1949f3b1fd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Preparing Notebooks for Job\n",
    "\n",
    "Below are 3 simple notebooks that we'll use in the demo.\n",
    "\n",
    "**Instructions**: \n",
    "1. Create folder `/Workspace/Users/<your-email>/jobs_demo/`\n",
    "2. Copy each of the following code snippets to a separate notebook\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c19928fd-f9d4-492a-8ebd-795f31842597",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 1: Validate Source\n",
    "\n",
    "Validates row count against `min_rows` threshold. Returns status + count via `dbutils.notebook.exit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8637c35-566c-4699-99ec-ebc5dcbaac45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 1: Validate Source Data\n",
    "# Copy this code to notebook: task_01_validate\n",
    "\n",
    "# Parameters from Job\n",
    "dbutils.widgets.text(\"source_table\", \"samples.nyctaxi.trips\")\n",
    "dbutils.widgets.text(\"min_rows\", \"100\")\n",
    "\n",
    "source_table = dbutils.widgets.get(\"source_table\")\n",
    "min_rows = int(dbutils.widgets.get(\"min_rows\"))\n",
    "\n",
    "# Validation\n",
    "df = spark.table(source_table)\n",
    "row_count = df.count()\n",
    "\n",
    "if row_count < min_rows:\n",
    "    raise Exception(f\"Validation FAILED: {row_count} rows < {min_rows} minimum\")\n",
    "\n",
    "# Return result to next task\n",
    "import json\n",
    "dbutils.notebook.exit(json.dumps({\n",
    "    \"status\": \"SUCCESS\",\n",
    "    \"source_table\": source_table,\n",
    "    \"row_count\": row_count\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dc5bc67-4e8c-49ed-b93a-044b1ac2c002",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 2: Transform Data\n",
    "\n",
    "Reads previous task result via `taskValues`, applies transformations (duration, cost per mile). Returns row count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3101b9b2-13b5-459d-9cd6-e80212a47adc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 2: Transform Data\n",
    "# Copy this code to notebook: task_02_transform\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "import json\n",
    "\n",
    "# Parameters\n",
    "dbutils.widgets.text(\"source_table\", \"samples.nyctaxi.trips\")\n",
    "dbutils.widgets.text(\"run_date\", \"\")\n",
    "\n",
    "source_table = dbutils.widgets.get(\"source_table\")\n",
    "run_date = dbutils.widgets.get(\"run_date\") or str(current_date())\n",
    "\n",
    "# Get result from previous task (optional)\n",
    "try:\n",
    "    prev_result = dbutils.jobs.taskValues.get(\n",
    "        taskKey=\"validate\",\n",
    "        key=\"returnValue\",\n",
    "        default=\"{}\"\n",
    "    )\n",
    "    prev_data = json.loads(prev_result)\n",
    "    print(f\"Previous task result: {prev_data}\")\n",
    "except:\n",
    "    print(\"Running standalone (no previous task)\")\n",
    "\n",
    "# Transformation\n",
    "print(f\"Transforming: {source_table}\")\n",
    "\n",
    "df = spark.table(source_table)\n",
    "\n",
    "df_transformed = (\n",
    "    df\n",
    "    .withColumn(\"trip_duration_minutes\", \n",
    "                round((col(\"tpep_dropoff_datetime\").cast(\"long\") - \n",
    "                       col(\"tpep_pickup_datetime\").cast(\"long\")) / 60, 2))\n",
    "    .withColumn(\"cost_per_mile\", \n",
    "                when(col(\"trip_distance\") > 0, \n",
    "                     round(col(\"fare_amount\") / col(\"trip_distance\"), 2))\n",
    "                .otherwise(0))\n",
    "    .withColumn(\"processing_date\", lit(run_date))\n",
    ")\n",
    "\n",
    "row_count = df_transformed.count()\n",
    "print(f\"Transformed {row_count} rows\")\n",
    "\n",
    "df_transformed.select(\n",
    "    \"trip_distance\", \"fare_amount\", \"trip_duration_minutes\", \"cost_per_mile\"\n",
    ").show(5)\n",
    "\n",
    "# Return result\n",
    "dbutils.notebook.exit(json.dumps({\n",
    "    \"status\": \"SUCCESS\",\n",
    "    \"rows_transformed\": row_count\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dae53bf-dbb5-418a-99cc-8f3c2f4548f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 3: Generate Report\n",
    "\n",
    "Aggregates metrics (total trips, revenue, avg fare/distance). Prints summary report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c0efa18-b0a7-4924-be15-bd9f2f8ac53e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 3: Generate Report\n",
    "# Copy this code to notebook: task_03_report\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Parameters\n",
    "dbutils.widgets.text(\"source_table\", \"samples.nyctaxi.trips\")\n",
    "\n",
    "source_table = dbutils.widgets.get(\"source_table\")\n",
    "\n",
    "# Aggregations\n",
    "df = spark.table(source_table)\n",
    "\n",
    "report = df.agg(\n",
    "    count(\"*\").alias(\"total_trips\"),\n",
    "    round(sum(\"fare_amount\"), 2).alias(\"total_revenue\"),\n",
    "    round(avg(\"fare_amount\"), 2).alias(\"avg_fare\"),\n",
    "    round(avg(\"trip_distance\"), 2).alias(\"avg_distance\"),\n",
    "    round(max(\"fare_amount\"), 2).alias(\"max_fare\")\n",
    ").collect()[0]\n",
    "\n",
    "# Display report\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DAILY REPORT\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Avg Fare:       ${report.avg_fare:.2f}\")\n",
    "print(f\"Avg Distance:   {report.avg_distance:.2f} miles\")\n",
    "print(f\"Max Fare:       ${report.max_fare:.2f}\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Generated at:   {datetime.now()}\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# Return result\n",
    "dbutils.notebook.exit(json.dumps({\n",
    "    \"status\": \"SUCCESS\",\n",
    "    \"total_trips\": report.total_trips,\n",
    "    \"total_revenue\": float(report.total_revenue)\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a5f425a-3559-475a-8d72-d9b6fc6a3ee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [UI DEMO] Creating Multi-task Job\n",
    "\n",
    "**Step 1: Create new Job**\n",
    "- [ ] Workflows → Create Job\n",
    "- [ ] Name: `Demo_ETL_Pipeline`\n",
    "\n",
    "<img src=\"../../../assets/images/93c107ca21a54aab98249cf47db0337d.png\" width=\"800\">\n",
    "\n",
    "\n",
    "- [ ] Cluster Job: Create new cluster job\n",
    "\n",
    "<img src=\"../../../assets/images/a967557a143a40c0ac7ed26ce469866a.png\" width=\"800\">\n",
    "\n",
    "**Step 2: Add Task 1 (Validate)**\n",
    "- [ ] Task name: `validate`\n",
    "- [ ] Type: Notebook\n",
    "- [ ] Path: `/Workspace/.../task_01_validate`\n",
    "- [ ] Cluster: Serverless or new Job Cluster\n",
    "- [ ] Parameters: `source_table` = `samples.nyctaxi.trips`\n",
    "\n",
    "<img src=\"../../../assets/images/214b868309344df3a6e81f3cc2a84c13.png\" width=\"800\">\n",
    "\n",
    "**Step 3: Add Task 2 (Transform)**\n",
    "- [ ] Task name: `transform`\n",
    "- [ ] Depends on: `validate`\n",
    "- [ ] Path: `/Workspace/.../task_02_transform`\n",
    "- [ ] Parameters: `source_table` = `samples.nyctaxi.trips`\n",
    "\n",
    "**Step 4: Add Task 3 (Report)**\n",
    "- [ ] Task name: `report`\n",
    "- [ ] Depends on: `transform`\n",
    "- [ ] Path: `/Workspace/.../task_03_report`\n",
    "\n",
    "<img src=\"../../../assets/images/a3cc387f44de4247bf275bdbd38efb84.png\" width=\"800\">\n",
    "\n",
    "**Step 5: Run Job**\n",
    "- [ ] Run now\n",
    "- [ ] Show: DAG visualization\n",
    "- [ ] Show: Task logs and output\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [UI DEMO 2] Medallion Pipeline Job — Ready-to-Use Config\n",
    "\n",
    "We already have 6 production-ready notebooks in `lab/materials/medallion/` that implement a full Medallion pipeline:\n",
    "\n",
    "| Layer | Notebook | Input | Output |\n",
    "|-------|----------|-------|--------|\n",
    "| **Bronze** | `bronze_customers` | CSV files | `bronze.bronze_customers` |\n",
    "| **Bronze** | `bronze_orders` | JSON files | `bronze.bronze_orders` |\n",
    "| **Silver** | `silver_customers` | bronze_customers | `silver.silver_customers` |\n",
    "| **Silver** | `silver_orders_cleaned` | bronze_orders | `silver.silver_orders_cleaned` |\n",
    "| **Gold** | `gold_customer_orders_summary` | silver_customers + silver_orders | `gold.gold_customer_orders_summary` |\n",
    "| **Gold** | `gold_daily_orders` | silver_orders | `gold.gold_daily_orders` |\n",
    "\n",
    "**DAG Structure:**\n",
    "```\n",
    "bronze_customers ──→ silver_customers ──────→ gold_customer_orders_summary\n",
    "bronze_orders ────→ silver_orders_cleaned ─┤\n",
    "                                           └→ gold_daily_orders\n",
    "```\n",
    "\n",
    "**How to deploy:** Copy the JSON config below → Workflows → Create Job → switch to JSON editor (or use Databricks CLI: `databricks jobs create --json @job.json`).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# ============================================================\n",
    "# Medallion Pipeline Job — JSON Configuration\n",
    "# Copy this JSON → Databricks Workflows → JSON Editor\n",
    "# Or deploy via CLI: databricks jobs create --json @job.json\n",
    "# ============================================================\n",
    "\n",
    "# IMPORTANT: Replace these placeholders before deploying:\n",
    "#   <YOUR_CATALOG>     → your Unity Catalog name (e.g. retailhub_jsmith)\n",
    "#   <NOTEBOOK_ROOT>    → workspace path to medallion notebooks\n",
    "#   <SOURCE_CUSTOMERS> → path to customers CSV (e.g. /Volumes/.../customers.csv)\n",
    "#   <SOURCE_ORDERS>    → path to orders JSON  (e.g. /Volumes/.../orders/)\n",
    "\n",
    "job_config = {\n",
    "    \"name\": \"RetailHub_Medallion_Pipeline\",\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"task_key\": \"bronze_customers\",\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"<NOTEBOOK_ROOT>/bronze_customers\",\n",
    "                \"base_parameters\": {\n",
    "                    \"catalog\": \"<YOUR_CATALOG>\",\n",
    "                    \"schema\": \"bronze\",\n",
    "                    \"source_path\": \"<SOURCE_CUSTOMERS>\"\n",
    "                }\n",
    "            },\n",
    "            \"job_cluster_key\": \"pipeline_cluster\"\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"bronze_orders\",\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"<NOTEBOOK_ROOT>/bronze_orders\",\n",
    "                \"base_parameters\": {\n",
    "                    \"catalog\": \"<YOUR_CATALOG>\",\n",
    "                    \"schema\": \"bronze\",\n",
    "                    \"source_path\": \"<SOURCE_ORDERS>\"\n",
    "                }\n",
    "            },\n",
    "            \"job_cluster_key\": \"pipeline_cluster\"\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"silver_customers\",\n",
    "            \"depends_on\": [{\"task_key\": \"bronze_customers\"}],\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"<NOTEBOOK_ROOT>/silver_customers\",\n",
    "                \"base_parameters\": {\n",
    "                    \"catalog\": \"<YOUR_CATALOG>\",\n",
    "                    \"schema_bronze\": \"bronze\",\n",
    "                    \"schema_silver\": \"silver\"\n",
    "                }\n",
    "            },\n",
    "            \"job_cluster_key\": \"pipeline_cluster\"\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"silver_orders_cleaned\",\n",
    "            \"depends_on\": [{\"task_key\": \"bronze_orders\"}],\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"<NOTEBOOK_ROOT>/silver_orders_cleaned\",\n",
    "                \"base_parameters\": {\n",
    "                    \"catalog\": \"<YOUR_CATALOG>\",\n",
    "                    \"schema_bronze\": \"bronze\",\n",
    "                    \"schema_silver\": \"silver\"\n",
    "                }\n",
    "            },\n",
    "            \"job_cluster_key\": \"pipeline_cluster\"\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"gold_customer_orders_summary\",\n",
    "            \"depends_on\": [\n",
    "                {\"task_key\": \"silver_customers\"},\n",
    "                {\"task_key\": \"silver_orders_cleaned\"}\n",
    "            ],\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"<NOTEBOOK_ROOT>/gold_customer_orders_summary\",\n",
    "                \"base_parameters\": {\n",
    "                    \"catalog\": \"<YOUR_CATALOG>\",\n",
    "                    \"schema_silver\": \"silver\",\n",
    "                    \"schema_gold\": \"gold\"\n",
    "                }\n",
    "            },\n",
    "            \"job_cluster_key\": \"pipeline_cluster\"\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"gold_daily_orders\",\n",
    "            \"depends_on\": [{\"task_key\": \"silver_orders_cleaned\"}],\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"<NOTEBOOK_ROOT>/gold_daily_orders\",\n",
    "                \"base_parameters\": {\n",
    "                    \"catalog\": \"<YOUR_CATALOG>\",\n",
    "                    \"schema_silver\": \"silver\",\n",
    "                    \"schema_gold\": \"gold\"\n",
    "                }\n",
    "            },\n",
    "            \"job_cluster_key\": \"pipeline_cluster\"\n",
    "        }\n",
    "    ],\n",
    "    \"job_clusters\": [\n",
    "        {\n",
    "            \"job_cluster_key\": \"pipeline_cluster\",\n",
    "            \"new_cluster\": {\n",
    "                \"spark_version\": \"16.2.x-scala2.12\",\n",
    "                \"num_workers\": 1,\n",
    "                \"node_type_id\": \"i3.xlarge\",\n",
    "                \"data_security_mode\": \"SINGLE_USER\"\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"trigger\": {\n",
    "        \"pause_status\": \"PAUSED\",\n",
    "        \"periodic\": {\n",
    "            \"interval\": 1,\n",
    "            \"unit\": \"DAYS\"\n",
    "        }\n",
    "    },\n",
    "    \"queue\": {\"enabled\": True},\n",
    "    \"max_concurrent_runs\": 1\n",
    "}\n",
    "\n",
    "print(json.dumps(job_config, indent=2))\n",
    "print(f\"\\n--- Total tasks: {len(job_config['tasks'])}\")\n",
    "print(\"--- DAG: bronze(2) → silver(2) → gold(2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Checklist\n",
    "\n",
    "**Option A — JSON Editor (UI):**\n",
    "1. [ ] Workflows → Create Job\n",
    "2. [ ] Click `⚙ Edit as JSON` (top-right)\n",
    "3. [ ] Paste the JSON above (replace placeholders)\n",
    "4. [ ] Save → Run now\n",
    "\n",
    "**Option B — Databricks CLI:**\n",
    "```bash\n",
    "# Save JSON to file, then:\n",
    "databricks jobs create --json @medallion_pipeline_job.json\n",
    "```\n",
    "\n",
    "**After deployment — show participants:**\n",
    "- [ ] DAG visualization (fan-out at Bronze, fan-in at Gold)\n",
    "- [ ] Task-level parameters (catalog, schema, source_path)\n",
    "- [ ] Trigger: set to PAUSED — run manually for demo\n",
    "- [ ] Run → observe sequential layer execution (Bronze → Silver → Gold)\n",
    "- [ ] Show Repair Run: intentionally fail one task → repair reruns only failed + downstream\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b68b0a63-260e-44b7-8cc6-49e32fe17f8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## [UI DEMO] Triggers and Schedule\n",
    "\n",
    "How to configure different trigger types for Lakeflow Jobs — scheduled (CRON), file arrival, continuous, and manual triggers.\n",
    "\n",
    "---\n",
    "\n",
    "| Trigger Type | Usage | Example |\n",
    "|---|---|---|\n",
    "| **Scheduled** | Fixed schedule (CRON) | `0 0 2 * * ?` — daily at 2:00 |\n",
    "| **File arrival** | Reaction to new files | New file in UC Volume |\n",
    "| **Continuous** | Continuous processing | Streaming-like |\n",
    "| **Manual** | On-demand | Testing |\n",
    "\n",
    "**Exam Note:** Know CRON syntax and File Arrival trigger configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b50e7b34-9d58-4292-ab35-4330254b73c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Trigger Configuration Checklist\n",
    "\n",
    "Step-by-step instructor checklist for demonstrating trigger options in the Lakeflow Jobs UI.\n",
    "\n",
    "**Trigger Options** (Triggers tab):\n",
    "\n",
    "| Trigger Type | Usage | Example |\n",
    "|--------------|-------|---------|\n",
    "| **Scheduled** | Fixed schedule | Daily at 2:00 |\n",
    "| **File arrival** | Reaction to new files | New file in `/landing/` |\n",
    "| **Continuous** | Continuous processing | Streaming-like |\n",
    "| **Manual** | On-demand | Testing |\n",
    "\n",
    "<img src=\"../../../assets/images/6e23746ce063491fa8afb3dea6268a1d.png\" width=\"800\">\n",
    "\n",
    "\n",
    "**Demo: Scheduled Trigger**\n",
    "- [ ] Add trigger → Scheduled\n",
    "- [ ] Cron expression: `0 0 2 * * ?` (daily at 2:00)\n",
    "- [ ] Timezone: `Europe/Warsaw`\n",
    "- [ ] Show: Preview next runs\n",
    "\n",
    "\n",
    "<img src=\"../../../assets/images/cf3cfc85162a466eb77e15e20df5c15c.png\" width=\"800\">\n",
    "\n",
    "**Demo: File Arrival Trigger** (optional)\n",
    "- [ ] Add trigger → File arrival\n",
    "- [ ] URL: Unity Catalog Volume path\n",
    "- [ ] Min files: 1\n",
    "### Useful CRON Expressions\n",
    "\n",
    "Common CRON patterns for scheduling Lakeflow Jobs at various intervals.\n",
    "\n",
    "```\n",
    "0 0 2 * * ?        # Daily at 2:00\n",
    "0 0 * * * ?        # Every hour\n",
    "0 0 9 ? * MON-FRI  # Mon-Fri at 9:00\n",
    "0 0 0 1 * ?        # First day of month\n",
    "0 */15 * * * ?     # Every 15 minutes\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cb23815-b85e-4a39-a390-aa5472c7728c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## [UI DEMO] Options, Retry and Alerting\n",
    "\n",
    "**Task-level:** Timeout, Retries (count + delay)  \n",
    "**Job-level:** Max concurrent runs, Job timeout  \n",
    "**Notifications:** Email (on failure/success), Webhooks (Slack/Teams via Destinations)\n",
    "\n",
    "| Scenario | Retry? | Why |\n",
    "|----------|--------|-----|\n",
    "| Network timeout | Yes | Transient error |\n",
    "| API rate limit | Yes | Transient error |\n",
    "| Data quality issue | No | Retry won't fix data |\n",
    "| Code bug | No | Retry won't fix code |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f4e52de-2c4c-445c-bd0d-e8606726ebc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f9afdbd-f2ff-47a4-8107-f6df54a8cd6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Demo: Use yAML to create job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2983bffb-12a8-40d2-a27c-cd3c4aabc4ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "create medalion architecutre job with dependecies base on that yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae5ea5b3-52de-4d39-a0cb-a712aa2992f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "resources:\n",
    "  jobs:\n",
    "    job_medalion_run:\n",
    "      name: job_medalion_run\n",
    "      tasks:\n",
    "        - task_key: bronze_customer\n",
    "          notebook_task:\n",
    "            notebook_path: /Workspace/Users/krzysztof.burejza@outlook.com/databricks-de-associate-training/notebooks/day3/lab/materials/medallion/bronze_customers\n",
    "            source: WORKSPACE\n",
    "          existing_cluster_id: 0215-230117-3c5px09z\n",
    "        - task_key: bronze_orders\n",
    "          notebook_task:\n",
    "            notebook_path: /Workspace/Users/krzysztof.burejza@outlook.com/databricks-de-associate-training/notebooks/day3/lab/materials/medallion/bronze_orders\n",
    "            source: WORKSPACE\n",
    "          existing_cluster_id: 0215-230117-3c5px09z\n",
    "        - task_key: silver_customers\n",
    "          depends_on:\n",
    "            - task_key: bronze_customer\n",
    "          notebook_task:\n",
    "            notebook_path: /Workspace/Users/krzysztof.burejza@outlook.com/databricks-de-associate-training/notebooks/day3/lab/materials/medallion/silver_customers\n",
    "            source: WORKSPACE\n",
    "          existing_cluster_id: 0215-230117-3c5px09z\n",
    "        - task_key: silver_orders_cleaned\n",
    "          depends_on:\n",
    "            - task_key: bronze_orders\n",
    "          notebook_task:\n",
    "            notebook_path: /Workspace/Users/krzysztof.burejza@outlook.com/databricks-de-associate-training/notebooks/day3/lab/materials/medallion/silver_orders_cleaned\n",
    "            source: WORKSPACE\n",
    "          existing_cluster_id: 0215-230117-3c5px09z\n",
    "        - task_key: gold_customer_order_summary\n",
    "          depends_on:\n",
    "            - task_key: silver_customers\n",
    "            - task_key: silver_orders_cleaned\n",
    "          notebook_task:\n",
    "            notebook_path: /Workspace/Users/krzysztof.burejza@outlook.com/databricks-de-associate-training/notebooks/day3/lab/materials/medallion/gold_customer_orders_summary\n",
    "            source: WORKSPACE\n",
    "          existing_cluster_id: 0215-230117-3c5px09z\n",
    "        - task_key: gold_daily_orders\n",
    "          depends_on:\n",
    "            - task_key: gold_customer_order_summary\n",
    "          notebook_task:\n",
    "            notebook_path: /Workspace/Users/krzysztof.burejza@outlook.com/databricks-de-associate-training/notebooks/day3/lab/materials/medallion/gold_daily_orders\n",
    "            source: WORKSPACE\n",
    "          existing_cluster_id: 0215-230117-3c5px09z\n",
    "      queue:\n",
    "        enabled: true\n",
    "      parameters:\n",
    "        - name: catalog\n",
    "          default: retailhub_trainer\n",
    "        - name: source_path\n",
    "          default: /Volumes/retailhub_trainer/default/datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4705da0b-5173-452b-9faa-e0b72ea180e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Demo: Widgets and Parameters\n",
    "\n",
    "Databricks Widgets allow you to parameterize notebooks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56cd432f-018e-41f9-9a49-6f508bf37262",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Widget types\n",
    "\n",
    "# Text - any text\n",
    "dbutils.widgets.text(\"environment\", \"dev\", \"Environment\")\n",
    "\n",
    "# Dropdown - select from list\n",
    "dbutils.widgets.dropdown(\"region\", \"EU\", [\"EU\", \"US\", \"APAC\"], \"Region\")\n",
    "\n",
    "# Combobox - dropdown with typing option\n",
    "dbutils.widgets.combobox(\"table\", \"orders\", [\"orders\", \"customers\", \"products\"], \"Table\")\n",
    "\n",
    "# Multiselect - multiple selection\n",
    "dbutils.widgets.multiselect(\"columns\", \"id\", [\"id\", \"name\", \"date\", \"amount\"], \"Columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cee2d90-5fea-4b8f-baf9-af5c6ee98e1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Getting values\n",
    "environment = dbutils.widgets.get(\"environment\")\n",
    "region = dbutils.widgets.get(\"region\")\n",
    "table = dbutils.widgets.get(\"table\")\n",
    "columns = dbutils.widgets.get(\"columns\")  # returns comma-separated string\n",
    "\n",
    "print(f\"Environment: {environment}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Table: {table}\")\n",
    "print(f\"Columns: {columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b5a3402-9560-46ae-8012-65a21292d4fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dynamic parameters in Job\n",
    "# These values are available when notebook is run as a task in Job\n",
    "\n",
    "dynamic_params = {\n",
    "    \"{{job.start_time.iso_date}}\": \"Run date (YYYY-MM-DD)\",\n",
    "    \"{{job.start_time}}\": \"Full timestamp\",\n",
    "    \"{{job.id}}\": \"Job ID\",\n",
    "    \"{{run.id}}\": \"Current run ID\",\n",
    "    \"{{task.name}}\": \"Current task name\"\n",
    "}\n",
    "\n",
    "for param, description in dynamic_params.items():\n",
    "    print(f\"{param:35} -> {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70a532f0-4d64-41a4-8190-d6e2b46c6aa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Widget cleanup\n",
    "dbutils.widgets.removeAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e2b2d45-caed-4fb4-b734-32f7d68d3da0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Monitoring via System Tables\n",
    "\n",
    "Key table: `system.lakeflow.job_run_timeline` — contains run history, duration, result state.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c00e9872-1d5c-474d-bc40-4ef736240b43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        *\n",
    "    FROM system.lakeflow.job_run_timeline\n",
    "    ORDER BY 1 DESC\n",
    "    LIMIT 20\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dda9551-edd8-42b2-b8bf-5ceeda5a55b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        DATE(period_start_time) as run_date,\n",
    "        run_name as job_name,\n",
    "        ROUND(\n",
    "            AVG(\n",
    "                (UNIX_TIMESTAMP(period_end_time) - UNIX_TIMESTAMP(period_start_time)) / 60\n",
    "            ), 1\n",
    "        ) as avg_duration_min,\n",
    "        COUNT(*) as runs\n",
    "    FROM system.lakeflow.job_run_timeline\n",
    "    WHERE period_start_time >= current_date() - INTERVAL 14 DAYS\n",
    "        AND result_state = 'SUCCESS'\n",
    "    GROUP BY run_date, job_name\n",
    "    ORDER BY run_date DESC\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70c0aaf9-a51b-4fe2-9d45-6f7659e4487b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "| Topic | Key Concept | Exam Keywords |\n",
    "|---|---|---|\n",
    "| **Multi-task Jobs** | DAG workflow, task dependencies | Task types, Repair Runs |\n",
    "| **Triggers** | Scheduled (CRON), File arrival, Continuous | `0 0 2 * * ?` |\n",
    "| **Options** | Timeout, Retry, Max concurrent runs | Transient vs permanent errors |\n",
    "| **Alerting** | Email, Webhooks (Slack/Teams) | Notification destinations |\n",
    "| **Parameters** | Widgets, dynamic values, taskValues | `dbutils.widgets`, `dbutils.jobs.taskValues` |\n",
    "| **Monitoring** | System tables, success rate, duration | `system.lakeflow.job_run_timeline` |\n",
    "\n",
    "---\n",
    "\n",
    "> **← M07: Medallion & Lakeflow | Day 3 | M09: Governance →**"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "M08_orchestration",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
