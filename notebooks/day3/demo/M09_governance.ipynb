{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c87b8be8-fafa-4b50-b590-e0fafba772ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# M09: Governance & Unity Catalog \n",
    "\n",
    "**Training Objective:** Master Unity Catalog as a governance platform for Databricks Lakehouse, managing access, data masking, lineage, and audit logging\n",
    "\n",
    "**Topics Covered:**\n",
    "- Unity Catalog Architecture: Metastore, Catalog, Schema, Tables/Views/Volumes\n",
    "- Access Management: GRANT/REVOKE privileges\n",
    "- Data Masking and Row-Level Security\n",
    "- Data Lineage and Audit Logging\n",
    "- Delta Sharing - secure data sharing\n",
    "- Best Practices for Data Governance\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33c416ad-7766-4ee9-8df4-0f23ea94a939",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 9.1. Theoretical Introduction\n",
    "\n",
    "**Section Objective:** Understanding Unity Catalog as a unified governance platform for data lakehouse\n",
    "\n",
    "**Basic Concepts:**\n",
    "- **Unity Catalog**: Unified governance solution for all data assets\n",
    "- **Metastore**: Region-level container for catalogs (top-level)\n",
    "- **Three-level namespace**: catalog.schema.table\n",
    "- **Securable objects**: Tables, Views, Functions, Volumes, Models\n",
    "- **Fine-grained access control**: Table, column, row-level security\n",
    "- **Automatic lineage**: End-to-end data flow tracking without instrumentation\n",
    "\n",
    "**Unity Catalog Object Hierarchy:**\n",
    "```\n",
    "Metastore (region-level)\n",
    " ↓\n",
    "Catalog (domain/environment)\n",
    " ↓\n",
    "Schema (namespace/layer)\n",
    " ↓\n",
    "Securable Objects:\n",
    " - Tables / Views (data)\n",
    " - Functions (UDF, stored procedures)\n",
    " - Volumes (file storage)\n",
    " - Models (ML models)\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "- **Unified governance**: Single platform for data, ML, BI\n",
    "- **ACID transactions**: Transactional guarantees at catalog level\n",
    "- **Audit logging**: Who accessed what and when\n",
    "- **Data discovery**: Metadata search and tagging\n",
    "- **Delta Sharing**: Secure cross-organization sharing\n",
    "\n",
    "**Why is this important?**\n",
    "Unity Catalog solves fundamental governance problems in data lake:\n",
    "- Lack of central access control\n",
    "- Difficulty tracking lineage\n",
    "- No data access audit\n",
    "- Compliance issues (GDPR, HIPAA)\n",
    "- Data silos between teams\n",
    "\n",
    "Unity Catalog provides enterprise-grade governance while maintaining data lakehouse flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff63a82f-9adb-4173-919e-f8beb120e997",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 9.2. Per-User Isolation\n",
    "\n",
    "Run the initialization script for per-user catalog and schema isolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d4e64c6-47e9-4e59-a9b5-43f6fd21d666",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../setup/00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e8fb497-6104-41e1-8aa0-dc7276ba1774",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 9.3. Configuration\n",
    "\n",
    "Library imports and user context display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebc7e52b-626b-4f23-bd4a-cf46a845b106",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paths to data directories (subdirectories in DATASET_PATH from 00_setup)\n",
    "CUSTOMERS_PATH = f\"{DATASET_PATH}/customers\"\n",
    "ORDERS_PATH = f\"{DATASET_PATH}/orders\"\n",
    "PRODUCTS_PATH = f\"{DATASET_PATH}/products\"\n",
    "\n",
    "# Paths to specific files\n",
    "CUSTOMERS_CSV = f\"{CUSTOMERS_PATH}/customers.csv\"\n",
    "ORDERS_JSON = f\"{ORDERS_PATH}/orders_batch.json\"\n",
    "PRODUCTS_PARQUET = f\"{PRODUCTS_PATH}/products.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1eff3a5-6717-433c-b7d0-cb994dc0e83d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 9.4. Unity Catalog Architecture\n",
    "\n",
    "**Unity Catalog** is a unified governance solution for Databricks Lakehouse.\n",
    "\n",
    "### Object Hierarchy:\n",
    "\n",
    "```\n",
    "Metastore (region-level)\n",
    " ↓\n",
    "Catalog (database/domain)\n",
    " ↓\n",
    "Schema (namespace)\n",
    " ↓\n",
    "Securable Objects:\n",
    " - Tables / Views\n",
    " - Functions (UDF, stored procedures)\n",
    " - Volumes (files storage)\n",
    " - Models (ML models)\n",
    "```\n",
    "\n",
    "### Three-level namespace:\n",
    "```sql\n",
    "catalog.schema.table\n",
    "```\n",
    "\n",
    "Example:\n",
    "```sql\n",
    "main.sales.orders\n",
    "dev.analytics.customer_metrics\n",
    "prod.gold.daily_revenue\n",
    "```\n",
    "\n",
    "### Key Features:\n",
    "- **Unified governance**: single platform for data, ML, BI\n",
    "- **Fine-grained access control**: table, column, row level\n",
    "- **Automatic lineage**: end-to-end data flow tracking\n",
    "- **Audit logging**: who accessed what and when\n",
    "- **Data discovery**: metadata search and tagging\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unity Catalog Full Architecture\n",
    "\n",
    "![Unity Catalog Architecture - Metastore, Catalogs, Schemas, Objects, External Locations](../../../assets/images/training_2026/m09_unity_catalog_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "396f57c7-82fb-4c15-bd0f-312a511ef9bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 9.4.1. Setup and Basic Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df1e3f88-b58f-43dc-82be-bf0bb92bdb63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 9.4.2. Creating User Groups\n",
    "We create user groups for permission demonstration:\n",
    "- `data_engineers`: Full access to Bronze/Silver schemas\n",
    "- `data_analysts`: Read-only access to Gold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ac033bd-a2e1-43d8-a0df-93b2a39f92d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Active context verification:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "995a5f2f-ac7b-41ae-aa5a-8182f25a2238",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification of created schemas\n",
    "schemas = spark.sql(f\"SHOW SCHEMAS IN {CATALOG}\").select(\"databaseName\").collect()\n",
    "schema_names = [row.databaseName for row in schemas]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a0e04ab-765a-4467-b2fe-45b1dc5eda29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Active catalog and schema set**\n",
    "\n",
    "We set the default working context - all subsequent operations will be executed in this catalog and schema unless a full path is specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b9d223e-dd38-413e-b30b-f6b0d7eee629",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification of created schemas\n",
    "spark.sql(f\"SHOW SCHEMAS IN {CATALOG}\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af32d6fd-2250-4fd0-b23e-9a323c69fe63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 9.5. Data Preparation\n",
    "\n",
    "Before we proceed to access management, we will load real data from the dataset/ directory that we will use in the Unity Catalog examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96cb2d83-f2e9-4b6e-911a-dce64481478e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").json(ORDERS_JSON)\n",
    "orders_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.orders\")\n",
    "\n",
    "display(spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.orders\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "777c5f37-a6ea-4a76-a87c-c88aa176dc35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customers_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(CUSTOMERS_CSV)\n",
    "customers_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.customers\")\n",
    "\n",
    "display(spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69724b78-5d85-4d86-894f-c3453eb8b19a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "products_df = spark.read.parquet(PRODUCTS_PARQUET)\n",
    "products_df.write.mode(\"overwrite\").saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.products\")\n",
    "display(spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.products\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72fba9cc-3ee3-486f-bd34-e4df6721e431",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification of orders record count\n",
    "spark.sql(f\"SELECT COUNT(*) as count FROM {CATALOG}.{BRONZE_SCHEMA}.orders\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3de51af2-1c51-41ea-9d70-36562cf56fab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 9.5.1. Add comments to table and columns\n",
    "\n",
    "You can add descriptive comments to Unity Catalog tables and columns using SQL commands. This improves data discoverability and governance.\n",
    "\n",
    "The cell below demonstrates how to add comments to a table and a specific column using Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "331fc4d0-46ad-4e61-a55d-fc2abe3d0266",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add comments to table and columns\n",
    "spark.sql(f\"\"\"\n",
    "    COMMENT ON TABLE {CATALOG}.{BRONZE_SCHEMA}.orders IS\n",
    "    'Cleaned orders table with data quality validations applied'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    COMMENT ON COLUMN {CATALOG}.{BRONZE_SCHEMA}.orders.customer_id IS\n",
    "    'Customer identifier - PII data, access restricted'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1525f9d0-5e44-4b17-82b6-d369dad8205d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 9.5.2. Add tags to orders table\n",
    "\n",
    "You can classify and manage tables in Unity Catalog using **tags** (key-value pairs). Tags help with data discovery, compliance, and governance (e.g., marking tables as PII, GDPR, or Sensitive).\n",
    "\n",
    "Example:  \n",
    "sql\n",
    "ALTER TABLE retailhub_trainer.bronze.orders\n",
    "  SET TAGS ('pii' = 'false', 'data_classification' = 'transactional', 'retention' = '7_years');\n",
    "\n",
    "- `pii`: Indicates if table contains personally identifiable information.\n",
    "- `data_classification`: Describes the type of data (e.g., transactional, reference).\n",
    "- `retention`: Specifies data retention policy.\n",
    "\n",
    "You need `APPLY TAG` privilege to add tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9734bf3-8696-4326-9ee5-6d7892fd4ed7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add tags to orders table\n",
    "spark.sql(f\"\"\"\n",
    "    ALTER TABLE {CATALOG}.{BRONZE_SCHEMA}.orders \n",
    "    SET TAGS ('sensitivity' = 'high', 'domain' = 'sales')\n",
    "\"\"\")\n",
    "\n",
    "# Add tags to customer_id column\n",
    "spark.sql(f\"\"\"\n",
    "    ALTER TABLE {CATALOG}.{BRONZE_SCHEMA}.orders \n",
    "    ALTER COLUMN customer_id SET TAGS ('pii' = 'true')\n",
    "\"\"\")\n",
    "\n",
    "display(spark.createDataFrame([(\"Status\", \" Tags added to table and column\")], [\"Info\", \"Value\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dd1b493-cea9-4f56-ae10-dc8ad63fba40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 9.6. Querying Metadata and Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd1cdbcd-67a2-425c-b40a-999010099a10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find all columns marked as PII\n",
    "pii_columns = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        catalog_name, \n",
    "        schema_name, \n",
    "        table_name, \n",
    "        column_name, \n",
    "        tag_value \n",
    "    FROM system.information_schema.column_tags\n",
    "    WHERE tag_name = 'pii' AND tag_value = 'true'\n",
    "      AND catalog_name = '{CATALOG}'\n",
    "\"\"\")\n",
    "\n",
    "display(pii_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.7. Monitoring & Observability (System Tables)\n",
    "\n",
    "Unity Catalog provides **System Tables** (`system.*`) for operational monitoring and observability.\n",
    "These tables give insights into costs, job runs, pipeline health, query performance, and storage usage.\n",
    "\n",
    "**Available System Table Categories:**\n",
    "\n",
    "| Category | Schema | Key Tables |\n",
    "|----------|--------|------------|\n",
    "| **Billing** | `system.billing` | `usage`, `list_prices` |\n",
    "| **Compute** | `system.compute` | `clusters`, `warehouse_events` |\n",
    "| **Workflows** | `system.workflow` | `job_run_timeline`, `job_task_run_timeline` |\n",
    "| **Pipelines** | `system.lakeflow` | `pipeline_event_log` |\n",
    "| **Queries** | `system.query` | `history` |\n",
    "| **Storage** | `system.storage` | `predictive_optimization_operations_history` |\n",
    "| **Access** | `system.access` | `audit`, `table_lineage`, `column_lineage` |\n",
    "\n",
    "> **Note**: System tables require **Metastore admin** or specific `MONITOR` privileges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### System Tables Overview\n",
    "\n",
    "![System Tables - audit, billing, compute, information_schema, lineage, storage](../../../assets/images/training_2026/m09_system_tables_overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.7.1. Cost Monitoring (DBU Usage)\n",
    "\n",
    "Track Databricks Unit (DBU) consumption by workspace, SKU, and user. Essential for budget management and chargeback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily DBU cost breakdown by SKU (last 30 days)\n",
    "cost_daily = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        usage_date,\n",
    "        sku_name,\n",
    "        usage_unit,\n",
    "        SUM(usage_quantity) as total_dbus,\n",
    "        ROUND(SUM(usage_quantity * list_prices.pricing.default), 2) as estimated_cost_usd\n",
    "    FROM system.billing.usage\n",
    "    LEFT JOIN system.billing.list_prices \n",
    "        ON usage.sku_name = list_prices.sku_name\n",
    "        AND usage.usage_date BETWEEN list_prices.price_start_time AND COALESCE(list_prices.price_end_time, '2099-12-31')\n",
    "    WHERE usage_date >= current_date() - INTERVAL 30 DAYS\n",
    "    GROUP BY usage_date, sku_name, usage_unit\n",
    "    ORDER BY usage_date DESC, estimated_cost_usd DESC\n",
    "\"\"\")\n",
    "\n",
    "display(cost_daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 most expensive users (last 30 days)\n",
    "cost_by_user = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        identity_metadata.run_as as run_as_user,\n",
    "        sku_name,\n",
    "        ROUND(SUM(usage_quantity), 2) as total_dbus,\n",
    "        COUNT(DISTINCT usage_date) as active_days\n",
    "    FROM system.billing.usage\n",
    "    WHERE usage_date >= current_date() - INTERVAL 30 DAYS\n",
    "        AND identity_metadata.run_as IS NOT NULL\n",
    "    GROUP BY identity_metadata.run_as, sku_name\n",
    "    ORDER BY total_dbus DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "display(cost_by_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost trend: weekly aggregation with week-over-week change\n",
    "cost_trend = spark.sql(\"\"\"\n",
    "    WITH weekly AS (\n",
    "        SELECT\n",
    "            DATE_TRUNC('week', usage_date) as week_start,\n",
    "            ROUND(SUM(usage_quantity), 2) as total_dbus\n",
    "        FROM system.billing.usage\n",
    "        WHERE usage_date >= current_date() - INTERVAL 90 DAYS\n",
    "        GROUP BY DATE_TRUNC('week', usage_date)\n",
    "    )\n",
    "    SELECT\n",
    "        week_start,\n",
    "        total_dbus,\n",
    "        LAG(total_dbus) OVER (ORDER BY week_start) as prev_week_dbus,\n",
    "        ROUND(\n",
    "            (total_dbus - LAG(total_dbus) OVER (ORDER BY week_start)) \n",
    "            / LAG(total_dbus) OVER (ORDER BY week_start) * 100, 1\n",
    "        ) as wow_change_pct\n",
    "    FROM weekly\n",
    "    ORDER BY week_start DESC\n",
    "\"\"\")\n",
    "\n",
    "display(cost_trend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.7.2. Job & Workflow Monitoring\n",
    "\n",
    "Monitor Lakeflow Jobs execution: success rates, durations, failures. Critical for SLA compliance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Job run history with success/failure rates (last 7 days)\n",
    "job_runs = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        job_id,\n",
    "        job_name,\n",
    "        COUNT(*) as total_runs,\n",
    "        SUM(CASE WHEN result_state = 'SUCCESS' THEN 1 ELSE 0 END) as success_count,\n",
    "        SUM(CASE WHEN result_state IN ('FAILED', 'TIMEDOUT') THEN 1 ELSE 0 END) as failure_count,\n",
    "        ROUND(SUM(CASE WHEN result_state = 'SUCCESS' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 1) as success_rate_pct,\n",
    "        ROUND(AVG(TIMESTAMPDIFF(MINUTE, period.start_time, period.end_time)), 1) as avg_duration_min\n",
    "    FROM system.workflow.job_run_timeline\n",
    "    WHERE period.start_time >= current_date() - INTERVAL 7 DAYS\n",
    "    GROUP BY job_id, job_name\n",
    "    ORDER BY failure_count DESC, total_runs DESC\n",
    "\"\"\")\n",
    "\n",
    "display(job_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failed jobs with error details (last 24 hours)\n",
    "failed_jobs = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        job_name,\n",
    "        run_id,\n",
    "        result_state,\n",
    "        period.start_time as start_time,\n",
    "        period.end_time as end_time,\n",
    "        TIMESTAMPDIFF(MINUTE, period.start_time, period.end_time) as duration_min,\n",
    "        triggered_by\n",
    "    FROM system.workflow.job_run_timeline\n",
    "    WHERE result_state IN ('FAILED', 'TIMEDOUT', 'CANCELED')\n",
    "        AND period.start_time >= current_timestamp() - INTERVAL 24 HOURS\n",
    "    ORDER BY period.start_time DESC\n",
    "\"\"\")\n",
    "\n",
    "display(failed_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.7.3. Lakeflow Pipeline Monitoring\n",
    "\n",
    "Track Lakeflow Declarative Pipeline health, update durations, and data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lakeflow pipeline events (last 7 days)\n",
    "pipeline_events = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        pipeline_id,\n",
    "        pipeline_name,\n",
    "        event_type,\n",
    "        maturity_level,\n",
    "        message,\n",
    "        timestamp\n",
    "    FROM system.lakeflow.pipeline_event_log\n",
    "    WHERE timestamp >= current_date() - INTERVAL 7 DAYS\n",
    "        AND event_type IN ('flow_progress', 'update_progress', 'maintenance_progress')\n",
    "    ORDER BY timestamp DESC\n",
    "    LIMIT 50\n",
    "\"\"\")\n",
    "\n",
    "display(pipeline_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline update durations and success rates\n",
    "pipeline_health = spark.sql(\"\"\"\n",
    "    WITH updates AS (\n",
    "        SELECT\n",
    "            pipeline_id,\n",
    "            pipeline_name,\n",
    "            origin.update_id,\n",
    "            MIN(timestamp) as start_time,\n",
    "            MAX(timestamp) as end_time,\n",
    "            MAX(CASE WHEN message LIKE '%completed successfully%' THEN 'SUCCESS'\n",
    "                      WHEN message LIKE '%FAILED%' OR message LIKE '%error%' THEN 'FAILED'\n",
    "                      ELSE 'RUNNING' END) as status\n",
    "        FROM system.lakeflow.pipeline_event_log\n",
    "        WHERE timestamp >= current_date() - INTERVAL 30 DAYS\n",
    "        GROUP BY pipeline_id, pipeline_name, origin.update_id\n",
    "    )\n",
    "    SELECT\n",
    "        pipeline_name,\n",
    "        COUNT(*) as total_updates,\n",
    "        SUM(CASE WHEN status = 'SUCCESS' THEN 1 ELSE 0 END) as successes,\n",
    "        SUM(CASE WHEN status = 'FAILED' THEN 1 ELSE 0 END) as failures,\n",
    "        ROUND(AVG(TIMESTAMPDIFF(MINUTE, start_time, end_time)), 1) as avg_duration_min\n",
    "    FROM updates\n",
    "    GROUP BY pipeline_name\n",
    "    ORDER BY failures DESC\n",
    "\"\"\")\n",
    "\n",
    "display(pipeline_health)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.7.4. Query Performance Monitoring\n",
    "\n",
    "Identify slow queries, heavy users, and optimization opportunities using SQL Warehouse query history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slowest queries (last 7 days, > 60s)\n",
    "slow_queries = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        statement_id,\n",
    "        executed_by as user,\n",
    "        SUBSTRING(statement_text, 1, 120) as query_preview,\n",
    "        execution_status,\n",
    "        total_duration_ms / 1000 as duration_sec,\n",
    "        rows_produced,\n",
    "        start_time\n",
    "    FROM system.query.history\n",
    "    WHERE start_time >= current_date() - INTERVAL 7 DAYS\n",
    "        AND total_duration_ms > 60000\n",
    "        AND statement_type IN ('SELECT', 'MERGE', 'INSERT', 'CREATE_TABLE_AS_SELECT')\n",
    "    ORDER BY total_duration_ms DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "display(slow_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query volume and performance by user (last 7 days)\n",
    "query_by_user = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        executed_by as user,\n",
    "        COUNT(*) as total_queries,\n",
    "        ROUND(AVG(total_duration_ms / 1000), 2) as avg_duration_sec,\n",
    "        ROUND(MAX(total_duration_ms / 1000), 2) as max_duration_sec,\n",
    "        SUM(rows_produced) as total_rows_produced,\n",
    "        COUNT(CASE WHEN execution_status = 'FAILED' THEN 1 END) as failed_queries\n",
    "    FROM system.query.history\n",
    "    WHERE start_time >= current_date() - INTERVAL 7 DAYS\n",
    "    GROUP BY executed_by\n",
    "    ORDER BY total_queries DESC\n",
    "    LIMIT 15\n",
    "\"\"\")\n",
    "\n",
    "display(query_by_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.7.5. Compute & Cluster Monitoring\n",
    "\n",
    "Track cluster utilization, uptime, and idle time to optimize compute costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Active clusters with uptime and DBU usage\n",
    "cluster_usage = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        cluster_id,\n",
    "        cluster_name,\n",
    "        cluster_source,\n",
    "        driver_node_type,\n",
    "        worker_count,\n",
    "        change_time,\n",
    "        state\n",
    "    FROM system.compute.clusters\n",
    "    WHERE change_time >= current_date() - INTERVAL 7 DAYS\n",
    "    ORDER BY change_time DESC\n",
    "    LIMIT 50\n",
    "\"\"\")\n",
    "\n",
    "display(cluster_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Warehouse usage patterns\n",
    "warehouse_usage = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        warehouse_id,\n",
    "        event_type,\n",
    "        event_time,\n",
    "        cluster_count\n",
    "    FROM system.compute.warehouse_events\n",
    "    WHERE event_time >= current_date() - INTERVAL 7 DAYS\n",
    "    ORDER BY event_time DESC\n",
    "    LIMIT 50\n",
    "\"\"\")\n",
    "\n",
    "display(warehouse_usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.7.6. Storage & Table Size Monitoring\n",
    "\n",
    "Monitor table sizes, growth trends, and identify tables that need optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Largest tables in the catalog\n",
    "largest_tables = spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        catalog_name,\n",
    "        schema_name,\n",
    "        table_name,\n",
    "        ROUND(total_size_in_bytes / (1024*1024*1024), 3) as size_gb,\n",
    "        row_count,\n",
    "        active_files_count,\n",
    "        last_updated\n",
    "    FROM system.storage.table_storage\n",
    "    WHERE catalog_name = '{CATALOG}'\n",
    "    ORDER BY total_size_in_bytes DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "display(largest_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictive Optimization history\n",
    "pred_opt = spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        catalog_name,\n",
    "        schema_name,\n",
    "        table_name,\n",
    "        operation_type,\n",
    "        operation_status,\n",
    "        usage_quantity as dbus_used,\n",
    "        start_time,\n",
    "        end_time\n",
    "    FROM system.storage.predictive_optimization_operations_history\n",
    "    WHERE catalog_name = '{CATALOG}'\n",
    "        AND start_time >= current_date() - INTERVAL 30 DAYS\n",
    "    ORDER BY start_time DESC\n",
    "    LIMIT 30\n",
    "\"\"\")\n",
    "\n",
    "display(pred_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.7.7. Governance Health Dashboard\n",
    "\n",
    "Combined governance health check: tables without comments, untagged PII, permissions audit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables without comments (governance gap)\n",
    "undocumented = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        table_catalog,\n",
    "        table_schema,\n",
    "        table_name,\n",
    "        table_type\n",
    "    FROM {CATALOG}.information_schema.tables\n",
    "    WHERE comment IS NULL OR comment = ''\n",
    "    ORDER BY table_schema, table_name\n",
    "\"\"\")\n",
    "\n",
    "display(undocumented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All tags across catalog (governance inventory)\n",
    "all_tags = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        catalog_name,\n",
    "        schema_name,\n",
    "        table_name,\n",
    "        column_name,\n",
    "        tag_name,\n",
    "        tag_value\n",
    "    FROM system.information_schema.column_tags\n",
    "    WHERE catalog_name = '{CATALOG}'\n",
    "    ORDER BY schema_name, table_name, column_name\n",
    "\"\"\")\n",
    "\n",
    "display(all_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permission audit: all grants in catalog\n",
    "perm_audit = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        grantor,\n",
    "        grantee,\n",
    "        table_catalog,\n",
    "        table_schema,\n",
    "        table_name,\n",
    "        privilege_type,\n",
    "        is_grantable\n",
    "    FROM {CATALOG}.information_schema.table_privileges\n",
    "    ORDER BY grantee, table_schema, table_name\n",
    "\"\"\")\n",
    "\n",
    "display(perm_audit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Tip**: Create a **Lakeflow Job** that runs these monitoring queries daily and sends alerts via email/Slack on anomalies (e.g., cost spike > 20%, job failure rate > 5%, tables without comments)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c89b8775-bdb0-411c-81e4-347bdae848f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 9.8. Unity Catalog Functions (UDF)\n",
    "\n",
    "**Functions** in Unity Catalog allow:\n",
    "- Creating reusable SQL/Python functions\n",
    "- Centralized management of business logic\n",
    "- Access control through GRANT/REVOKE\n",
    "- Lineage tracking for functions\n",
    "\n",
    "**Function types**:\n",
    "- **Scalar Functions**: return a single value\n",
    "- **Table Functions**: return a table\n",
    "- **SQL Functions**: written in SQL\n",
    "- **Python Functions**: written in Python (UDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bbd307c-979a-40e2-aa91-adaa53c3efce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 9.8.1. Data Classification (Tagging)\n",
    "\n",
    "> *Note: This section covers data tagging for classification, related to governance.*\n",
    "\n",
    "**Tagging** allows data classification (e.g., PII, Sensitive, GDPR) at the table or column level.\n",
    "This facilitates data discovery and governance (e.g., reporting all tables containing personal data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd62bc15-95be-43bd-a1c9-6b4fa60e3e8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL Function - masking customer_id\n",
    "spark.sql(f\"\"\"\n",
    "  CREATE OR REPLACE FUNCTION {CATALOG}.{SILVER_SCHEMA}.mask_customer_id(customer_id STRING)\n",
    "  RETURNS STRING\n",
    "  LANGUAGE SQL\n",
    "  COMMENT 'Masks customer_id, showing only last 3 digits'\n",
    "  RETURN CONCAT('****', SUBSTRING(CAST(customer_id AS STRING), -3))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfe02d38-a2af-46ea-b1e8-088e37a14f3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test mask_customer_id function\n",
    "result_df = spark.sql(f\"\"\"\n",
    "  SELECT \n",
    "    customer_id,\n",
    "    {CATALOG}.{SILVER_SCHEMA}.mask_customer_id(customer_id) as masked_id,\n",
    "    first_name,\n",
    "    last_name\n",
    "  FROM {CATALOG}.{BRONZE_SCHEMA}.customers\n",
    "  LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc075b2b-8291-45a3-9e58-d81946d3928b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Creating categorize_price function**\n",
    "\n",
    "Python UDF function categorizes product prices:\n",
    "- **Low**: < 50\n",
    "- **Medium**: 50-200 \n",
    "- **High**: > 200\n",
    "\n",
    "Python UDF can contain any Python logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19331a50-fd21-4867-b069-9b58fbbd11c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Python UDF - price categorization\n",
    "spark.sql(f\"\"\"\n",
    "  CREATE OR REPLACE FUNCTION {CATALOG}.{SILVER_SCHEMA}.categorize_price(price DOUBLE)\n",
    "  RETURNS STRING\n",
    "  LANGUAGE PYTHON\n",
    "  COMMENT 'Categorizes prices: Low, Medium, High'\n",
    "  AS $$\n",
    "    if price < 50:\n",
    "        return \"Low\"\n",
    "    elif price < 200:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"High\"\n",
    "  $$\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "818a03cf-113c-46e5-9725-d28b1b3bda1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test categorize_price function\n",
    "result_df = spark.sql(f\"\"\"\n",
    "  SELECT \n",
    "    product_name,\n",
    "    unit_cost,\n",
    "    {CATALOG}.{SILVER_SCHEMA}.categorize_price(unit_cost) as price_category\n",
    "  FROM {CATALOG}.{BRONZE_SCHEMA}.products\n",
    "  ORDER BY unit_cost\n",
    "  LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01a27238-ef5f-4339-9eba-e4bb87653298",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Setting permissions for data-analysts**\n",
    "\n",
    "The `data-analysts` group received:\n",
    "- **USE CATALOG**: Access to catalog\n",
    "- **USE SCHEMA**: Access to Silver schema \n",
    "- **SELECT**: Read data from Silver schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3549e0a-2c5a-4658-87c5-9a1d5d981b1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Setup:** Create groups for demonstration purposes\n",
    "> **Note:** This requires account admin privileges. If you don't have them, ensure these groups exist.\n",
    "\n",
    "* TO DO IN GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c745663-efad-432f-9874-e4e88856c9d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Grant catalog access to data analysts\n",
    "spark.sql(f\"\"\"\n",
    "    GRANT USE CATALOG ON CATALOG {CATALOG} TO `data-analysts`\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    GRANT USE SCHEMA ON SCHEMA {CATALOG}.{SILVER_SCHEMA} TO `data-analysts`\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    GRANT SELECT ON SCHEMA {CATALOG}.{SILVER_SCHEMA} TO `data-analysts`\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b544a92-ddc2-4ccf-8f19-930fe31d1079",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Grant full access to data engineers\n",
    "spark.sql(f\"\"\"\n",
    "    GRANT USE CATALOG, CREATE SCHEMA ON CATALOG {CATALOG} TO `data-engineers`\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8db82ca-c5a1-4798-b90b-859b2cbbb522",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Permissions for Data Analysts (Gold Layer):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b66a07a-1b06-403c-9c60-1ad26c5d2d1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# GRANT for data-analysts on Gold schema\n",
    "spark.sql(f\"\"\"\n",
    "  GRANT USE SCHEMA ON SCHEMA {CATALOG}.{GOLD_SCHEMA} TO `data-analysts`\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "  GRANT SELECT ON SCHEMA {CATALOG}.{GOLD_SCHEMA} TO `data-analysts`\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46f3fc5b-4ae2-408f-803d-8473453ec372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Table-specific access control**\n",
    "\n",
    "Fine-grained permissions:\n",
    "- **finance-team**: Access to fact_sales (revenue analysis)\n",
    "- **marketing-team**: Access to customers_masked (customer insights with PII masking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59eb2938-35f6-4187-a8be-c68268599697",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# GRANT EXECUTE na Functions\n",
    "spark.sql(f\"\"\"\n",
    "  GRANT EXECUTE ON FUNCTION {CATALOG}.{SILVER_SCHEMA}.mask_customer_id TO `data-analysts`\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "  GRANT EXECUTE ON FUNCTION {CATALOG}.{SILVER_SCHEMA}.categorize_price TO `data-analysts`\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7cb10ca-a279-45cc-afa1-107d7c8ac590",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify permissions on table\n",
    "spark.sql(f\"\"\"\n",
    "    SHOW GRANTS ON TABLE {CATALOG}.{BRONZE_SCHEMA}.customers\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82fa36fc-b952-45c5-8a91-10935481d886",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 9.9. Data Masking and Row-Level Security\n",
    "\n",
    "### 9.9.1. Column-level masking (Dynamic Views):\n",
    "\n",
    "Use `current_user()` and `is_account_group_member()` functions for conditional masking:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column Masking & Row-Level Security Flow\n",
    "\n",
    "![Column Masking & Row-Level Security - dynamiczna kontrola dostepu do danych](../../../assets/images/training_2026/m09_masking_rls_flow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5da86732-d508-4ca7-bb50-6297f2ab0ab4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create masked view for PII data\n",
    "spark.sql(f\"\"\"\n",
    "  CREATE OR REPLACE VIEW {CATALOG}.{GOLD_SCHEMA}.customers_masked AS\n",
    "  SELECT \n",
    "    customer_id,\n",
    "    CASE \n",
    "      WHEN is_account_group_member('alt_test') THEN first_name\n",
    "      ELSE CONCAT(LEFT(first_name, 1), '***')\n",
    "    END as first_name,\n",
    "    CASE \n",
    "      WHEN is_account_group_member('alt_test') THEN last_name\n",
    "      ELSE CONCAT(LEFT(last_name, 1), '***')\n",
    "    END as last_name,\n",
    "    city,\n",
    "    country,\n",
    "    registration_date\n",
    "  FROM {CATALOG}.{BRONZE_SCHEMA}.customers\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24f33aa2-6c41-485b-a70a-4e04dd227f69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(f\"{CATALOG}.{GOLD_SCHEMA}.customers_masked\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "271580a9-88ed-4e64-961f-b819ca112d97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**View customers_masked created**\n",
    "\n",
    "View with dynamic PII data masking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47c97299-afdd-425d-9568-fc9652e26c62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test View z maskowaniem\n",
    "result_df = spark.sql(f\"\"\"\n",
    "  SELECT * FROM {CATALOG}.{GOLD_SCHEMA}.customers_masked LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdc4547c-c7dc-48fe-9ab3-2e01173fbfd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{SILVER_SCHEMA}.customers_masked;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34e14df5-bd59-417d-a6ed-44933dc46fe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE FUNCTION customer_mask_test (customer_id STRING)\n",
    "  RETURN CASE WHEN is_account_group_member('alt_test') THEN customer_id ELSE 'CUST-**-****' END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "838ecee4-e20c-46d4-83f9-d804ce3e13ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the customers_masked table schema without CTAS\n",
    "spark.sql(f\"\"\"\n",
    "  CREATE OR REPLACE TABLE {CATALOG}.{SILVER_SCHEMA}.customers_masked (\n",
    "    customer_id STRING,\n",
    "    customer_id_masked STRING MASK customer_mask_test,\n",
    "    first_name STRING,\n",
    "    last_name STRING,\n",
    "    email STRING,\n",
    "    country STRING\n",
    "  )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5df0444a-9c01-4450-80f5-eb9be08c8adf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "  INSERT INTO {CATALOG}.{SILVER_SCHEMA}.customers_masked (customer_id, customer_id_masked, first_name, last_name, email, country)\n",
    "  VALUES\n",
    "    ('CUST001', 'CUST001', 'Alice', 'Smith', 'alice.smith@example.com', 'USA'),\n",
    "    ('CUST002', 'CUST002', 'Bob', 'Johnson', 'bob.johnson@example.com', 'Canada'),\n",
    "    ('CUST003', 'CUST003', 'Carol', 'Williams', 'carol.williams@example.com', 'UK')\n",
    "\"\"\")\n",
    "display(spark.sql(f\"SELECT * FROM {CATALOG}.{SILVER_SCHEMA}.customers_masked\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1594b6b5-8622-4a03-916d-0cb319a9d658",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {CATALOG}.{GOLD_SCHEMA}.orders_hashed AS\n",
    "    SELECT \n",
    "        order_id,\n",
    "        SHA2(CAST(customer_id AS STRING), 256) as customer_id_hash,\n",
    "        product_id,\n",
    "        quantity,\n",
    "        total_amount,\n",
    "        order_datetime\n",
    "    FROM {CATALOG}.{BRONZE_SCHEMA}.orders\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "display(\n",
    "    spark.createDataFrame(\n",
    "        [\n",
    "            (\"View\", f\"{CATALOG}.{GOLD_SCHEMA}.orders_hashed\"),\n",
    "            (\"Masking\", \"customer_id → SHA2-256 hash\"),\n",
    "            (\"Purpose\", \"Analysts can aggregate without revealing customer_id\")\n",
    "        ],\n",
    "        [\"Parameter\", \"Value\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93c5534e-0356-43aa-b3d2-667f5d5ea295",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**View orders_hashed created**\n",
    "\n",
    "Customer_id is hashed using SHA2-256. This enables:\n",
    "- **Analysts**: Data aggregation without revealing customer_id\n",
    "- **Privacy**: Maintaining anonymity while preserving grouping capability\n",
    "- **Compliance**: Meeting GDPR/privacy regulations requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edf3d92c-436e-4205-9d7b-5b05e1f28c64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT * FROM {CATALOG}.{GOLD_SCHEMA}.orders_hashed\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4efc52b-0943-44ae-be4f-5707eb209fc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 9.9.2. Row-Level Security (RLS):\n",
    "\n",
    "Restrict which rows users can see based on their identity or group membership:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60493ed6-482f-4560-821f-f687fe020adc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**RLS View customers_rls created**\n",
    "\n",
    "Row-Level Security filters data based on group membership:\n",
    "- **global-access**: Sees all customers\n",
    "- **east-coast-team**: Only customers from NY, NJ, NC, GA \n",
    "- **alt_test**: Only customers from CA\n",
    "- **midwest-team**: Only customers from FL, IL, TX, MI\n",
    "- **Other groups**: No access (FALSE)\n",
    "\n",
    "Automatic row filtering without data duplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fbb6a7e-f46d-4124-8de5-5addb6749965",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating RLS view - access per region (state)\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {CATALOG}.{GOLD_SCHEMA}.customers_rls AS\n",
    "    SELECT *\n",
    "    FROM {CATALOG}.{BRONZE_SCHEMA}.customers\n",
    "    WHERE \n",
    "        CASE \n",
    "            WHEN is_account_group_member('global-access') THEN TRUE\n",
    "            WHEN is_account_group_member('east-coast-team') THEN UPPER(state) IN ('NY', 'NJ', 'NC', 'GA')\n",
    "            WHEN is_account_group_member('alt_test') THEN UPPER(state) = 'CA'\n",
    "            WHEN is_account_group_member('midwest-team') THEN UPPER(state) IN ('FL', 'IL', 'TX', 'MI')\n",
    "            ELSE FALSE\n",
    "        END\n",
    "\"\"\")\n",
    "display(spark.table(f\"{CATALOG}.{GOLD_SCHEMA}.customers_rls\"))\n",
    "display(spark.createDataFrame([\n",
    "    (\"RLS View\", f\"{CATALOG}.{GOLD_SCHEMA}.customers_rls\"),\n",
    "    (\"Mechanism\", \"Filtering per state based on group membership\"),\n",
    "    (\"global-access\", \"All customers\"),\n",
    "    (\"east-coast-team\", \"NY, NJ, NC, GA\"),\n",
    "    (\"alt_test\", \"CA\"),\n",
    "    (\"midwest-team\", \"FL, IL, TX, MI\")\n",
    "], [\"Group\", \"Visibility\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76d2d51c-143b-498d-84f9-d20362319c57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Granting permissions to RLS Views:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.9.3. Attribute-Based Access Control (ABAC)\n",
    "\n",
    "ABAC is a **security pattern** in Unity Catalog that combines multiple governance features to control access based on **data attributes** rather than just user identity.\n",
    "\n",
    "**ABAC in Databricks = Tags + Column Masks + Row Filters**\n",
    "\n",
    "| Component | Purpose | Mechanism |\n",
    "|---|---|---|\n",
    "| **Tags** (Data Classification) | Mark sensitive data with attributes | `ALTER TABLE ... SET TAGS ('pii' = 'true')` |\n",
    "| **Column Masks** | Hide/transform column values based on user group | `CREATE FUNCTION mask_fn(...)` + Dynamic Views |\n",
    "| **Row Filters** | Restrict which rows a user can see | RLS Views with `IS_ACCOUNT_GROUP_MEMBER()` |\n",
    "\n",
    "**How ABAC works in practice:**\n",
    "\n",
    "1. **Classify** -- Tag tables and columns with sensitivity levels (e.g., `pii`, `data_classification`)\n",
    "2. **Define policies** -- Create masking functions and RLS views that enforce access rules\n",
    "3. **Assign** -- Grant access to groups; the tags + functions automatically enforce attribute-based filtering\n",
    "4. **Audit** -- Use `system.information_schema` to track tags and access patterns\n",
    "\n",
    "**Key difference from RBAC:**\n",
    "- **RBAC** (Role-Based): Access determined by user's *role* (e.g., `data-analysts` group gets SELECT)\n",
    "- **ABAC** (Attribute-Based): Access determined by *data attributes* (e.g., columns tagged `pii=true` are automatically masked)\n",
    "\n",
    "> **Exam note**: Unity Catalog implements ABAC through the combination of Tags, Column Masks, and Row Filters. Know that tags provide metadata for governance, while masks and filters enforce data-level security policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f75488c4-b440-4005-a397-14951b15f528",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 9.10. Access Management: GRANT / REVOKE\n",
    "\n",
    "### Privileges Hierarchy in Unity Catalog:\n",
    "\n",
    "**Privilege levels**:\n",
    "1. **Metastore-level**: CREATE CATALOG, USE CATALOG\n",
    "2. **Catalog-level**: USE CATALOG, CREATE SCHEMA\n",
    "3. **Schema-level**: USE SCHEMA, CREATE TABLE, CREATE FUNCTION, CREATE VOLUME\n",
    "4. **Object-level**: SELECT, MODIFY (INSERT/UPDATE/DELETE/MERGE), EXECUTE\n",
    "\n",
    "**Securable Objects - Inheritance**:\n",
    "- Privileges inherit down the hierarchy\n",
    "- GRANT on Catalog → inherits to all Schemas and Tables\n",
    "- GRANT on Schema → inherits to all Tables in that Schema\n",
    "- You can grant privileges at specific level for fine-grained control\n",
    "\n",
    "### GRANT/REVOKE Examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Permission Inheritance Diagram\n",
    "\n",
    "![Permission Inheritance - dziedziczenie uprawnien GRANT w hierarchii UC](../../../assets/images/training_2026/m09_grant_inheritance.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75695bdc-c3ea-48a2-8e5a-5539cef40a3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# GRANT access to customers_rls\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    GRANT SELECT ON VIEW {CATALOG}.{GOLD_SCHEMA}.customers_rls TO `account users`\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a27e4c39-de6e-47b1-b743-551400975d51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Granting permissions to orders_hashed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d9dcddb-474e-4811-ac13-c00f5385846d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# GRANT access to orders_hashed\n",
    "spark.sql(f\"\"\"\n",
    "  GRANT SELECT ON VIEW {CATALOG}.{GOLD_SCHEMA}.orders_hashed TO `account users`\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7341651f-1314-491c-90a9-f43f9ab8f846",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Revoking access to base tables (Enforcement):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f58970c-42ee-45ca-94ec-346b6135a82b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Revoke direct access to base table\n",
    "spark.sql(f\"\"\"\n",
    "    REVOKE SELECT ON TABLE {CATALOG}.{BRONZE_SCHEMA}.orders FROM `account users`\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "888754e3-2e00-4708-af2d-9592314e7060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**RLS Views - Access control setup**\n",
    "\n",
    "Security pattern:\n",
    "1. **GRANT SELECT** on RLS Views for `all-users`\n",
    "2. **REVOKE SELECT** on base tables (force Views usage)\n",
    "3. **Automatic filtering** based on group membership\n",
    "\n",
    "Users can SELECT from Views, but not from base tables - enforcing RLS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9015b0f-f49b-4680-a612-ece6be91a8cf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 76"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 9.11. Data Lineage and Audit Logging\n",
    "\n",
    "### 9.11.1. Querying Data Lineage:\n",
    "\n",
    "Unity Catalog automatically tracks lineage for:\n",
    "- Table → Table (ETL transformations)\n",
    "- Notebook → Table (data writes)\n",
    "- Dashboard → Table (BI queries)\n",
    "- ML Model → Table (training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d14b784-26de-4670-873b-2d12c7a14147",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**General Table Lineage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e608a406-0e3e-4354-b9f7-30713188898d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Query table lineage from system tables\n",
    "lineage_df = spark.sql(f\"\"\"\n",
    "  SELECT \n",
    "    source_table_full_name,\n",
    "    source_type,\n",
    "    target_table_full_name,\n",
    "    target_type,\n",
    "    event_date,\n",
    "    created_by\n",
    "  FROM system.access.table_lineage\n",
    "  WHERE target_table_full_name LIKE '{CATALOG}.%'\n",
    "  ORDER BY event_date DESC\n",
    "  LIMIT 50\n",
    "\"\"\")\n",
    "\n",
    "display(lineage_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2f7d5e5-7cba-4b96-adf1-cc907a315f75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Lineage for tables in catalog displayed**\n",
    "\n",
    "The system automatically tracks lineage for:\n",
    "- **Table → Table**: ETL transformations\n",
    "- **Notebook → Table**: Data writes \n",
    "- **Dashboard → Table**: BI queries\n",
    "- **ML Model → Table**: Training data\n",
    "\n",
    "Lineage is available through `system.access.table_lineage` without additional instrumentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02e0ccfd-98df-42c2-9a42-3f581df1ee87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1. Upstream Lineage (Sources)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64f26fbd-93a1-4873-96b9-058ec4f3406a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find upstream dependencies (sources) for a table\n",
    "upstream_df = spark.sql(f\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        source_table_full_name,\n",
    "        source_type\n",
    "    FROM system.access.table_lineage\n",
    "    WHERE target_table_full_name = '{CATALOG}.{SILVER_SCHEMA}.fact_sales'\n",
    "\"\"\")\n",
    "\n",
    "display(upstream_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5016821e-536d-4c07-8b74-bacb4f6b23a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⬆ Upstream: Source tables for fact_sales**\n",
    "\n",
    "Shows all tables used as data sources in the `fact_sales` View. Helpful for impact analysis when making changes to upstream tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac48c859-0121-42bd-afb2-9a3a8707e5c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**2. Downstream Lineage (Consumers)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10555c9b-7f04-4f41-ad38-4cb68fe9ef08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find downstream dependencies (consumers) of a table\n",
    "downstream_df = spark.sql(f\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        target_table_full_name,\n",
    "        target_type\n",
    "    FROM system.access.table_lineage\n",
    "    WHERE source_table_full_name = '{CATALOG}.{BRONZE_SCHEMA}.customers'\n",
    "\"\"\")\n",
    "\n",
    "display(downstream_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fcf3c74-55cc-4e01-bc6d-c94312b94ba6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Downstream: Views/Tables consuming customers**\n",
    "\n",
    "Shows all Views and tables that consume data from the `customers` table. Critical for understanding impact of changes and data governance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd4f25c0-fc8b-42ad-8a18-8eda485dca9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**3. Column-Level Lineage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b5aca0f-df4f-4d7b-a188-cc376246f58c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Column-level lineage (if available)\n",
    "column_lineage = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        source_table_full_name,\n",
    "        source_column_name,\n",
    "        target_table_full_name,\n",
    "        target_column_name,\n",
    "        event_date\n",
    "    FROM system.access.column_lineage\n",
    "    WHERE target_table_full_name = '{CATALOG}.{SILVER_SCHEMA}.fact_sales'\n",
    "    ORDER BY target_column_name\n",
    "\"\"\")\n",
    "\n",
    "display(column_lineage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aced943-3bc5-4c83-802a-eae402761fd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Column-level lineage for fact_sales**\n",
    "\n",
    "Unity Catalog tracks lineage at column level - which columns in source tables affect which columns in the target table. Detailed information for data governance and impact analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12b33a89-b132-4fc5-82df-1698baa4c131",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 9.11.2. Audit Logging:\n",
    "\n",
    "Unity Catalog logs all access and operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "244f20ee-3722-4a8f-84a3-1d2ac076ceec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1. General Audit Logs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9113046-41d8-4b12-aad4-c8d83e556873",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Query audit logs\n",
    "audit_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        event_time,\n",
    "        user_identity.email as user_email,\n",
    "        service_name,\n",
    "        action_name,\n",
    "        request_params.full_name_arg as table_name,\n",
    "        response.status_code,\n",
    "        request_id\n",
    "    FROM system.access.audit\n",
    "    WHERE action_name IN ('getTable', 'createTable', 'deleteTable', 'updateTable')\n",
    "        AND event_date >= current_date() - INTERVAL 7 DAYS\n",
    "    ORDER BY event_time DESC\n",
    "    LIMIT 100\n",
    "\"\"\")\n",
    "audit_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d1123c7-5a47-4832-bef5-2e6ba29c5796",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**2. Sensitive Data Access**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b806c038-6edb-4ebb-8935-5dcfde7264a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Track who accessed sensitive tables\n",
    "sensitive_access = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        event_time,\n",
    "        user_identity.email as user,\n",
    "        action_name,\n",
    "        request_params.full_name_arg as table_accessed,\n",
    "        source_ip_address\n",
    "    FROM system.access.audit\n",
    "    WHERE request_params.full_name_arg LIKE '{CATALOG}.%.customers%'\n",
    "        AND action_name = 'getTable'\n",
    "        AND event_date >= current_date() - INTERVAL 7 DAYS\n",
    "    ORDER BY event_time DESC\n",
    "    LIMIT 100\n",
    "\"\"\")\n",
    "\n",
    "display(sensitive_access)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d507fe4-badb-4d17-853c-b85f69098823",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "** Audit logs: Access to customers table (last 7 days)**\n",
    "\n",
    "Monitoring access to sensitive tables with PII data:\n",
    "- **Who**: User email\n",
    "- **When**: Event time \n",
    "- **What**: Table name\n",
    "- **From where**: Source IP address\n",
    "\n",
    "Critical for compliance (GDPR, HIPAA) and security monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a61ebf6a-d043-4533-b631-a0559d51fb34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**3. Privilege Changes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06126332-4b85-4675-9a7f-dd4cb8e61390",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Grant/Revoke audit trail\n",
    "grant_audit = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        event_time,\n",
    "        user_identity.email as admin_user,\n",
    "        action_name,\n",
    "        request_params.privilege as privilege_granted,\n",
    "        request_params.securable_full_name as object_name,\n",
    "        request_params.principal as grantee\n",
    "    FROM system.access.audit\n",
    "    WHERE action_name IN ('grantPrivilege', 'revokePrivilege')\n",
    "        AND event_date >= current_date() - INTERVAL 30 DAYS\n",
    "    ORDER BY event_time DESC\n",
    "\"\"\")\n",
    "\n",
    "display(grant_audit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7f3b325-7c05-497b-ac11-4946c80d8781",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Audit trail of privilege changes**\n",
    "\n",
    "Complete audit trail of permission changes:\n",
    "- **Admin user**: Who executed GRANT/REVOKE\n",
    "- **Action**: grantPrivilege or revokePrivilege\n",
    "- **Privilege**: Which permission (SELECT, MODIFY, etc.)\n",
    "- **Object**: On which object (table, schema, catalog)\n",
    "- **Grantee**: To whom permissions were granted/revoked\n",
    "\n",
    "Essential for governance and compliance audits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b751ae4b-f602-4972-af65-60f6dcd86d7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 9.12. Delta Sharing\n",
    "\n",
    "**Delta Sharing** = Secure data sharing protocol (cross-org, cross-cloud)\n",
    "\n",
    "### 9.12.1. Components:\n",
    "- **Share**: collection of tables to share\n",
    "- **Recipient**: organization/user receiving data\n",
    "- **Provider**: data owner (you)\n",
    "\n",
    "### 9.12.2. Create Share:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delta Sharing Architecture\n",
    "\n",
    "![Delta Sharing - Provider, Recipient, Open Protocol, Cross-Cloud](../../../assets/images/training_2026/m09_delta_sharing.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f34fabe-7227-457f-b3ff-faa5fe5b1289",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating Share for external partners\n",
    "share_name = f\"{CATALOG}_partner_share\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "  CREATE SHARE IF NOT EXISTS {share_name}\n",
    "  COMMENT 'Data sharing for business partners'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "026f1f86-a594-41a4-9d52-e059417e6e26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Share '{share_name}' created**\n",
    "\n",
    "Delta Sharing Share is a collection of tables for secure sharing with external partners:\n",
    "- **Cross-org**: Between different Databricks organizations\n",
    "- **Cross-cloud**: AWS ↔ Azure ↔ GCP \n",
    "- **Open protocol**: Open-source standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff224eb5-4785-4e3d-9a7e-837cf18ac456",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add table to Share (Gold layer only - aggregated data)\n",
    "spark.sql(f\"\"\"\n",
    "  ALTER SHARE {share_name}\n",
    "  ADD TABLE {CATALOG}.{GOLD_SCHEMA}.fact_sales\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23565bdf-1fa2-4be6-8371-0b5fd5bc600a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "  ALTER SHARE {share_name}\n",
    "  ADD SCHEMA {CATALOG}.{SILVER_SCHEMA}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d64c367-72d4-4e3f-8e83-4d711770a610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Table fact_sales added to Share**\n",
    "\n",
    "Best practice: Share only Gold layer (aggregated data):\n",
    "- **Security**: No access to raw data\n",
    "- **Privacy**: Aggregations hide individual records\n",
    "- **Stability**: Gold layer has stable schema and structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4a56e44-bc47-4529-91c4-e1ee3885ba4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Tables in Share verified**\n",
    "\n",
    "Share currently contains the added tables and can be shared with recipients. Recipients will receive an activation link to consume shared data via Delta Sharing protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86f70a75-19f2-42c9-b5a0-41c7f218102f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify Share contents\n",
    "spark.sql(f\"SHOW ALL IN SHARE {share_name}\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8ac64c7-35c6-4c64-8916-7a2d03831b5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 9.12.3. Create Recipient:\n",
    "\n",
    "> **[UI DEMO]** Create a recipient in the Databricks UI: Catalog -> Delta Sharing -> New Recipient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "238613e8-7975-4546-8523-e5251ce7ede7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 9.12.4. Consuming shared data (as recipient):\n",
    "\n",
    "> **[UI DEMO]** As a recipient, use the activation link to access shared data via Open Sharing protocol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e58171ca-3a08-4a63-837a-e1e4b9dfa894",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 9.12.5. Best practices for Delta Sharing:\n",
    "\n",
    "1. **Share only aggregated/gold data**: don't share raw/bronze layers\n",
    "2. **Use views for masking**: create view with masked PII before sharing\n",
    "3. **Monitor access**: track who accesses shared data\n",
    "4. **Version control**: use table versions for stable APIs\n",
    "5. **Documentation**: clear documentation for recipients\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.13. Information Schema\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Unity Catalog provides an `INFORMATION_SCHEMA` in every catalog for metadata queries.\n",
    "\n",
    "```sql\n",
    "-- List all tables in a schema\n",
    "SELECT table_name, table_type, created\n",
    "FROM my_catalog.information_schema.tables\n",
    "WHERE table_schema = 'my_schema';\n",
    "\n",
    "-- List all columns for a table\n",
    "SELECT column_name, data_type, is_nullable\n",
    "FROM my_catalog.information_schema.columns\n",
    "WHERE table_name = 'customers';\n",
    "\n",
    "-- Check grants on a table\n",
    "SELECT grantee, privilege_type\n",
    "FROM my_catalog.information_schema.table_privileges\n",
    "WHERE table_name = 'customers';\n",
    "```\n",
    "\n",
    "**Available views in `information_schema`:**\n",
    "\n",
    "| View | Content |\n",
    "|------|--------|\n",
    "| `tables` | All tables and views |\n",
    "| `columns` | Column definitions |\n",
    "| `table_privileges` | Granted permissions |\n",
    "| `schemata` | Schema metadata |\n",
    "| `catalogs` | Catalog information |\n",
    "| `views` | View definitions |\n",
    "\n",
    "**Exam Note:** `INFORMATION_SCHEMA` is the standard SQL way to query metadata. It is available per catalog in Unity Catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e7d8c59-1915-4d02-ab86-a358325e0b0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 9.14. Summary\n",
    "\n",
    "### 9.14.1. You learned:\n",
    "\n",
    " **Unity Catalog Architecture**: Metastore → Catalog → Schema → Tables \n",
    " **Access Control**: GRANT/REVOKE privileges at multiple levels \n",
    " **Data Masking**: Column-level masking with dynamic views \n",
    " **Row-Level Security**: Filter data based on user identity \n",
    " **Data Lineage**: Track data flow through system tables \n",
    " **Audit Logging**: Monitor who accessed what and when \n",
    " **Monitoring & Observability**: Cost, job, query, and storage monitoring via System Tables \n",
    " **Delta Sharing**: Secure cross-organization data sharing \n",
    "\n",
    "### 9.14.2. Key Takeaways:\n",
    "\n",
    "1. **Unified Governance**: Single platform for all data assets\n",
    "2. **Fine-grained Control**: Table, column, row-level security\n",
    "3. **Automatic Lineage**: No extra instrumentation needed\n",
    "4. **Compliance-ready**: Audit logs for regulatory requirements\n",
    "5. **Secure Sharing**: Delta Sharing for external collaboration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0186fd5b-7c86-4c43-ae2d-50ce2dcf833f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 9.15. Troubleshooting\n",
    "\n",
    "### Problem 1: \"Table or view not found\"\n",
    "**Cause**: Missing USE CATALOG or USE SCHEMA permissions \n",
    "**Solution**:\n",
    "```sql\n",
    "GRANT USE CATALOG ON CATALOG <catalog_name> TO <principal>;\n",
    "GRANT USE SCHEMA ON SCHEMA <catalog>.<schema> TO <principal>;\n",
    "```\n",
    "\n",
    "### Problem 2: \"Permission denied\" on SELECT\n",
    "**Cause**: Missing SELECT permissions on table \n",
    "**Solution**:\n",
    "```sql\n",
    "GRANT SELECT ON TABLE <catalog>.<schema>.<table> TO <principal>;\n",
    "-- or on entire schema:\n",
    "GRANT SELECT ON SCHEMA <catalog>.<schema> TO <principal>;\n",
    "```\n",
    "\n",
    "### Problem 3: \"Cannot execute function\"\n",
    "**Cause**: Missing EXECUTE permission on function \n",
    "**Solution**:\n",
    "```sql\n",
    "GRANT EXECUTE ON FUNCTION <catalog>.<schema>.<function_name> TO <principal>;\n",
    "```\n",
    "\n",
    "### Problem 4: \"Volume not accessible\"\n",
    "**Cause**: Missing READ VOLUME / WRITE VOLUME permissions \n",
    "**Solution**:\n",
    "```sql\n",
    "GRANT READ VOLUME ON VOLUME <catalog>.<schema>.<volume> TO <principal>;\n",
    "GRANT WRITE VOLUME ON VOLUME <catalog>.<schema>.<volume> TO <principal>;\n",
    "```\n",
    "\n",
    "### Problem 5: RLS View not filtering data\n",
    "**Cause**: User doesn't belong to any group defined in CASE WHEN \n",
    "**Solution**: Add user to appropriate group or add default fallback in View\n",
    "\n",
    "### Problem 6: Lineage not showing dependencies\n",
    "**Cause**: Lineage is automatic but may be delayed by a few minutes \n",
    "**Solution**: Wait 5-10 minutes and query system.access.table_lineage again\n",
    "\n",
    "### Problem 7: Share not visible to recipient\n",
    "**Cause**: Recipient hasn't activated the activation link \n",
    "**Solution**: Send activation link from DESCRIBE RECIPIENT\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be5a402a-626e-444b-9a81-a0fec3d967ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 9.16. Best Practices Summary\n",
    "\n",
    "### 1. **Catalog Organization**\n",
    "- Use environment-based catalogs: `dev`, `test`, `prod`\n",
    "- Organize schemas by layers: `bronze`, `silver`, `gold`\n",
    "- Apply naming conventions: `<catalog>.<schema>.<object>`\n",
    "\n",
    "### 2. **Access Control**\n",
    "- **Principle of Least Privilege**: Grant minimum required permissions\n",
    "- Use groups, not individual users\n",
    "- Inheritance: GRANT on Catalog → inherits to Schema → inherits to Tables\n",
    "- Regularly audit permissions (SHOW GRANTS)\n",
    "\n",
    "### 3. **Data Masking & RLS**\n",
    "- Mask PII in Views for users without pii-access-group\n",
    "- Use RLS for multi-tenant scenarios\n",
    "- Always test masking with different group memberships\n",
    "\n",
    "### 4. **Lineage & Audit**\n",
    "- Leverage automatic lineage to track data flow\n",
    "- Regularly check audit logs for sensitive tables\n",
    "- Monitor lineage after pipeline changes\n",
    "\n",
    "### 5. **Delta Sharing**\n",
    "- Share only Gold layer (aggregated data)\n",
    "- Use masked Views in Share\n",
    "- Document Share contracts for recipients\n",
    "\n",
    "### 6. **Documentation & Governance**\n",
    "- Add COMMENT to all tables, views, functions\n",
    "- Use Table Properties for metadata (owner, PII, retention)\n",
    "- Regularly check governance health checks\n",
    "\n",
    "### 7. **Monitoring & Observability**\n",
    "- Use `system.billing.usage` for cost tracking and chargeback\n",
    "- Monitor job/pipeline SLAs with `system.workflow.job_run_timeline`\n",
    "- Track slow queries in `system.query.history`\n",
    "- Set up daily governance health checks (undocumented tables, untagged PII)\n",
    "- Create alerts for cost spikes and job failures\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4873620425682615,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "07_governance",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
