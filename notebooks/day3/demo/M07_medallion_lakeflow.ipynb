{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb6d571c-ebfe-44a0-908e-8079cd0e7cf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# M07: Medallion Architecture & Lakeflow\n",
    "\n",
    "\n",
    "The Medallion architecture (Bronze → Silver → Gold) is the standard for organizing data in the Lakehouse. We'll explore declarative Lakeflow pipelines: STREAMING TABLE, MATERIALIZED VIEW, constraints (Expectations), and the FLOW mechanism. We'll build a complete pipeline in SQL and PySpark, including SCD Type 1 and Type 2 handling via AUTO CDC.\n",
    "\n",
    "| Exam Domain | Weight |\n",
    "|---|---|\n",
    "| Production Pipelines | 13% |\n",
    "| Incremental Data Processing | 20% |\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "296acbc8-e708-4d93-9829-fcf46c0946da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SCD Type 1 & Type 2\n",
    "\n",
    "Slowly Changing Dimensions (SCD) define strategies for handling changes in dimensional data over time. Type 1 overwrites old values, while Type 2 preserves full history with validity timestamps.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "596e282f-a7a2-4264-95ea-74aee988ae44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### What is SCD?\n",
    "\n",
    "**Slowly Changing Dimension (SCD)** — how to handle changes in dimensional data.\n",
    "\n",
    "| Type | Strategy | Result |\n",
    "|-----|-----------|----------|\n",
    "| **SCD Type 0** | Retain original | Always Warsaw |\n",
    "| **SCD Type 1** | Overwrite | Only Krakow |\n",
    "| **SCD Type 2** | Track history | Both records with dates |\n",
    "| **SCD Type 3** | Add column | `current_city=Krakow`, `previous_city=Warsaw` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "596b86c9-d330-46c2-982e-323abdcfb945",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SCD Type 1 — Overwrite\n",
    "\n",
    "- **No history** — old values are overwritten\n",
    "- **Use cases:** Error corrections, non-historical data\n",
    "- **Implementation:** `MERGE INTO ... WHEN MATCHED THEN UPDATE SET *`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d72500b8-444e-497e-9a31-7a29e8ac4d5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- ### SCD Type 1 in Lakeflow:\n",
    "CREATE FLOW silver_products_scd1_flow\n",
    "AS AUTO CDC INTO silver_products\n",
    "FROM bronze_products\n",
    "KEYS (product_id)\n",
    "SEQUENCE BY ingestion_ts\n",
    "STORED AS SCD TYPE 1;  -- Overwrite without history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "deb12ba3-d37c-45d3-9c03-5a65b5788862",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "MERGE INTO dim_customer t\n",
    "USING source_customers s\n",
    "ON t.customer_id = s.customer_id\n",
    "WHEN MATCHED THEN UPDATE SET *\n",
    "WHEN NOT MATCHED THEN INSERT *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f5130fa-a9b8-4dbd-8448-37c164914045",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SCD Type 2 — Track History\n",
    "\n",
    "- **Full history** of changes with `__START_AT`, `__END_AT`\n",
    "- **Use cases:** Audit, historical analysis, compliance\n",
    "- **Current record:** `__END_AT IS NULL`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3524e678-9156-48dc-83ca-1fff6a9fc8f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Demo: SCD Type 2 with AUTO CDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f67f5414-9576-4975-ba6a-f7a8391bb8e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Creating target table SCD2\n",
    "CREATE OR REFRESH STREAMING TABLE silver_customers (\n",
    "  customer_id        STRING,\n",
    "  first_name         STRING,\n",
    "  last_name          STRING,\n",
    "  city               STRING,\n",
    "  -- SCD2 columns added automatically:\n",
    "  __START_AT         TIMESTAMP,\n",
    "  __END_AT           TIMESTAMP\n",
    ");\n",
    "\n",
    "-- Flow with AUTO CDC for SCD2\n",
    "CREATE FLOW silver_customers_scd2_flow\n",
    "AS AUTO CDC INTO silver_customers\n",
    "FROM STREAM bronze_customers\n",
    "KEYS (customer_id)         -- Business key\n",
    "SEQUENCE BY ingestion_ts   -- Column determining the order\n",
    "STORED AS SCD TYPE 2;      -- SCD Type\n",
    "\n",
    "-- ### Key elements:\n",
    "-- - **KEYS** - columns identifying the business record\n",
    "-- - **SEQUENCE BY** - column determining the order of changes\n",
    "-- - **STORED AS SCD TYPE 1|2** - SCD type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d56c2e21-d8cd-4edf-a0a0-0d7ec1746326",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Lakeflow Pipelines — Declarations\n",
    "\n",
    "Lakeflow (formerly Delta Live Tables) enables a declarative approach to building data pipelines — you define the desired outcome rather than step-by-step logic. This section covers table types, expectations, FLOW declarations, and both SQL and PySpark syntax.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b77af01-673a-4b01-9259-fb31aef1d7f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### What is Lakeflow?\n",
    "\n",
    "**Lakeflow** (formerly Delta Live Tables) — declarative framework for data pipelines.\n",
    "\n",
    "| Approach | Example | You describe |\n",
    "|-----------|----------|-------------|\n",
    "| **Imperative** | `df.write.mode(\"overwrite\")...` | HOW |\n",
    "| **Declarative** | `CREATE TABLE AS SELECT...` | WHAT |\n",
    "\n",
    "Key benefits: automatic dependencies, built-in data quality, unified batch/streaming, automatic recovery, lineage & monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "243cc8f2-5eb7-47ec-a7e1-95c6d16b796a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Table Types in Lakeflow\n",
    "\n",
    "| Type | Usage | Processing |\n",
    "|-----|--------|-----------|\n",
    "| **STREAMING TABLE** | Append-only ingestion | Incremental (new rows only) |\n",
    "| **MATERIALIZED VIEW** | Aggregations, Gold layer | Full recomputation |\n",
    "| **VIEW** | Intermediate logic | Not materialized |\n",
    "\n",
    "> **Exam:** STREAMING TABLE = incremental (new data only). MATERIALIZED VIEW = full recompute each refresh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f397598e-c0e6-4011-86b3-976ed92ff981",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**STREAMING TABLE vs MATERIALIZED VIEW (Exam Topic):**\n",
    "\n",
    "| Feature | STREAMING TABLE | MATERIALIZED VIEW |\n",
    "|---------|----------------|-------------------|\n",
    "| Data Source | Streaming (append-only) | Batch or streaming |\n",
    "| Processing | Incremental (new rows only) | Full recomputation |\n",
    "| Updates | Append-only | Full refresh |\n",
    "| Use Case | Bronze/Silver layers | Gold aggregations |\n",
    "| Supports AUTO CDC | Yes | No |\n",
    "| Query Pattern | `STREAM(source)` | Regular `SELECT` |\n",
    "| Idempotent | Yes (checkpoints) | Yes (full refresh) |\n",
    "\n",
    "**Key Exam Point:** STREAMING TABLEs process only NEW data (incremental). MATERIALIZED VIEWs recompute the FULL result on each refresh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16099f51-b3e4-47c9-9bcd-a32b97238e0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Demo: STREAMING TABLE with Constraints\n",
    "\n",
    "Constraint actions: **`DROP ROW`** (remove invalid), **`FAIL UPDATE`** (fail pipeline), or no action (log only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22f8a620-c55d-4661-9ac9-5c1ade95be15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REFRESH STREAMING TABLE silver_orders\n",
    "(\n",
    "  CONSTRAINT valid_order_id EXPECT (order_id IS NOT NULL)\n",
    "    ON VIOLATION DROP ROW,\n",
    "  CONSTRAINT valid_customer EXPECT (customer_id IS NOT NULL)\n",
    "    ON VIOLATION DROP ROW,\n",
    "  CONSTRAINT valid_quantity EXPECT (quantity > 0)\n",
    "    ON VIOLATION DROP ROW,\n",
    "  CONSTRAINT valid_price EXPECT (unit_price >= 0)\n",
    "    ON VIOLATION FAIL UPDATE\n",
    ")\n",
    "AS\n",
    "SELECT\n",
    "  order_id,\n",
    "  customer_id,\n",
    "  product_id,\n",
    "  CAST(order_datetime AS TIMESTAMP) AS order_ts,\n",
    "  quantity,\n",
    "  unit_price,\n",
    "  (quantity * unit_price) AS gross_amount\n",
    "FROM STREAM(bronze_orders);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0112da2d-1319-408b-93b8-bf5a40bac0a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Demo: MATERIALIZED VIEW (Gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d23d8381-a889-4ce4-8515-7d4fdda034c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Dimension - current snapshot from SCD2\n",
    "CREATE OR REFRESH MATERIALIZED VIEW dim_customer\n",
    "AS\n",
    "SELECT\n",
    "  customer_id,\n",
    "  first_name,\n",
    "  last_name,\n",
    "  email,\n",
    "  city,\n",
    "  customer_segment\n",
    "FROM silver_customers\n",
    "WHERE __END_AT IS NULL;\n",
    "\n",
    "-- Date Dimension\n",
    "CREATE OR REFRESH MATERIALIZED VIEW dim_date\n",
    "AS\n",
    "SELECT DISTINCT\n",
    "  CAST(date_format(order_date, 'yyyyMMdd') AS INT) AS date_key,\n",
    "  order_date AS date,\n",
    "  year(order_date) AS year,\n",
    "  quarter(order_date) AS quarter,\n",
    "  month(order_date) AS month\n",
    "FROM silver_orders;\n",
    "\n",
    "-- Fact - streaming from Silver\n",
    "CREATE OR REFRESH STREAMING TABLE fact_sales\n",
    "AS\n",
    "SELECT\n",
    "  order_id,\n",
    "  customer_id,\n",
    "  product_id,\n",
    "  order_date_key,\n",
    "  quantity,\n",
    "  gross_amount,\n",
    "  net_amount\n",
    "FROM STREAM(silver_orders);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9dbc1a2-5469-4598-955a-101fa2d08e5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### What is FLOW?\n",
    "\n",
    "**FLOW** separates table definition from data source. Key capabilities:\n",
    "1. **Multiple sources → one table** (e.g. backfill + streaming)\n",
    "2. **CDC** with automatic SCD via `AUTO CDC`\n",
    "3. **`INSERT INTO ONCE`** — one-time backfill\n",
    "4. **`INSERT INTO`** — continuous streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b97c1532-2504-4233-a6c6-b62a6bbe638a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 1. We define empty target table\n",
    "CREATE OR REFRESH STREAMING TABLE bronze_orders;\n",
    "\n",
    "-- 2. We define FLOW(s) which populate it\n",
    "CREATE FLOW flow_name\n",
    "AS INSERT INTO target_table BY NAME\n",
    "SELECT ... FROM source;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32e2f228-f78a-4713-b92d-ac50e8691c4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Demo: Backfill + Streaming Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c49acc6-2d57-487a-8cee-4c458753db40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Target table\n",
    "CREATE OR REFRESH STREAMING TABLE bronze_orders;\n",
    "\n",
    "-- FLOW 1: One-time backfill\n",
    "CREATE FLOW bronze_orders_backfill\n",
    "AS \n",
    "INSERT INTO ONCE bronze_orders BY NAME\n",
    "SELECT\n",
    "  order_id,\n",
    "  customer_id,\n",
    "  product_id,\n",
    "  order_datetime,\n",
    "  'batch' AS source_system,\n",
    "  _metadata.file_path AS source_file_path,\n",
    "  current_timestamp() AS load_ts\n",
    "FROM read_files(\n",
    "  '${order_path}/orders_batch.json',\n",
    "  format => 'json'\n",
    ");\n",
    "\n",
    "-- FLOW 2: Continuous streaming\n",
    "CREATE FLOW bronze_orders_stream\n",
    "AS \n",
    "INSERT INTO bronze_orders BY NAME\n",
    "SELECT\n",
    "  order_id,\n",
    "  customer_id,\n",
    "  'stream' AS source_system,\n",
    "  _metadata.file_path AS source_file_path,\n",
    "  current_timestamp() AS load_ts\n",
    "FROM STREAM read_files(\n",
    "  '${order_path}/stream/orders_stream_*.json',\n",
    "  format => 'json'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cde124b7-99df-48b1-9235-7c3725d0641c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Demo: AUTO CDC for SCD Type 2\n",
    "\n",
    "AUTO CDC: compares new records by `KEYS`, detects changes, closes old record (`__END_AT`), inserts new (SCD2) or overwrites (SCD1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "317bf3c8-8130-4651-82ab-f7ba4f4f7379",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SCD2 Table with schema\n",
    "CREATE OR REFRESH STREAMING TABLE silver_customers (\n",
    "  customer_id        STRING,\n",
    "  first_name         STRING,\n",
    "  last_name          STRING,\n",
    "  email              STRING,\n",
    "  city               STRING,\n",
    "  __START_AT         TIMESTAMP,\n",
    "  __END_AT           TIMESTAMP\n",
    ");\n",
    "\n",
    "-- AUTO CDC Flow\n",
    "CREATE FLOW silver_customers_scd2_flow\n",
    "AS AUTO CDC INTO silver_customers\n",
    "FROM STREAM bronze_customers\n",
    "KEYS (customer_id)\n",
    "SEQUENCE BY ingestion_ts\n",
    "STORED AS SCD TYPE 2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de9928ec-7050-4503-b1d7-f37494f2194c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PySpark Declarations\n",
    "\n",
    "Lakeflow pipelines can also be defined in Python using the `pyspark.pipelines` decorators instead of SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5957fcd-37c0-48f6-a851-eee161b8d785",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import pipelines as dp\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# STREAMING TABLE\n",
    "@dp.table(\n",
    "    name=\"bronze_customers\",\n",
    "    comment=\"Raw customers from CSV\"\n",
    ")\n",
    "def bronze_customers():\n",
    "    return (\n",
    "        spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"csv\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .load(spark.conf.get(\"customer_path\"))\n",
    "            .select(\n",
    "                \"*\",\n",
    "                \"_metadata.file_path\".alias(\"source_file_path\"),\n",
    "                current_timestamp().alias(\"load_ts\")\n",
    "            )\n",
    "    )\n",
    "\n",
    "# MATERIALIZED VIEW\n",
    "@dp.table(name=\"dim_customer\")\n",
    "def dim_customer():\n",
    "    return (\n",
    "        spark.read.table(\"silver_customers\")\n",
    "            .filter(col(\"__END_AT\").isNull())\n",
    "            .select(\"customer_id\", \"first_name\", \"last_name\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb2aa8c3-d7ee-44c3-aaba-81ffb2402759",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PySpark: Expectations\n",
    "\n",
    "Decorators: **`@dp.expect`** (log only), **`@dp.expect_or_drop`** (drop record), **`@dp.expect_or_fail`** (fail pipeline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd54decf-bcee-4c97-9b0d-edfcb3432ff1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dp.table(name=\"silver_orders\")\n",
    "@dp.expect_or_drop(\"valid_order_id\", \"order_id IS NOT NULL\")\n",
    "@dp.expect_or_drop(\"valid_customer\", \"customer_id IS NOT NULL\")\n",
    "@dp.expect_or_drop(\"valid_quantity\", \"quantity > 0\")\n",
    "@dp.expect(\"valid_price\", \"unit_price >= 0\")\n",
    "def silver_orders():\n",
    "    return (\n",
    "        spark.readStream.table(\"bronze_orders\")\n",
    "            .select(\n",
    "                \"order_id\",\n",
    "                \"customer_id\",\n",
    "                col(\"order_datetime\").cast(\"timestamp\").alias(\"order_ts\"),\n",
    "                (col(\"quantity\") * col(\"unit_price\")).alias(\"gross_amount\")\n",
    "            )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d849523-8a35-48ba-bef5-cbc10078d4fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PySpark: AUTO CDC\n",
    "\n",
    "Python equivalent of SQL AUTO CDC — uses `dp.create_auto_cdc_flow()` to define SCD logic programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7613e54a-6dcd-49e7-b8d6-fb9887ba6891",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import pipelines as dp\n",
    "\n",
    "# Define the target table\n",
    "dp.create_streaming_table(\n",
    "    name=\"silver_customers\",\n",
    "    schema=\"\"\"\n",
    "        customer_id STRING,\n",
    "        first_name STRING,\n",
    "        city STRING,\n",
    "        __START_AT TIMESTAMP,\n",
    "        __END_AT TIMESTAMP\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Define the CDC flow\n",
    "dp.create_auto_cdc_flow(\n",
    "    target=\"silver_customers\",\n",
    "    source=\"bronze_customers\",\n",
    "    keys=[\"customer_id\"],\n",
    "    sequence_by=\"ingestion_ts\",\n",
    "    stored_as_scd_type=2  # or 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b52359f-9cfd-4993-a8c3-c491bafc4d1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Workshop — Building the Pipeline\n",
    "\n",
    "In this hands-on demo, we create a complete Lakeflow pipeline from SQL files — uploading source code, configuring the pipeline in the Databricks UI, running it, and validating the results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66b10034-118d-4460-b55f-a33b067c2bac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SQL Files Overview\n",
    "\n",
    "The pipeline source code is organized by medallion layer — each SQL file declares one table or view.\n",
    "\n",
    "![image_1771356717743.png](./image_1771356717743.png \"image_1771356717743.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "194b80f2-d0de-4495-b709-2089fede1524",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1: Upload SQL Files\n",
    "\n",
    "**Option A: Via UI** — Workspace → Users → create `lakeflow_pipeline` folder → upload SQL files\n",
    "\n",
    "**Option B: Via Git Folders** — Git Folders → Add Git Folder → clone training repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70bd1cb7-c0cd-48ed-a893-3d47f5dc268a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 2: Create Pipeline\n",
    "\n",
    "1. **Workflows** → **Jobs & Pipelines** → **Create ETL Pipeline**\n",
    "2. **Catalog:** `retailhub_<your_name>`\n",
    "3. **Pipeline Name:** `lakeflow_pipeline_<your_name>`\n",
    "4. **Target Schema:** `<your_name>_lakeflow`\n",
    "5. **Source Code:** Add existing assets → choose pipeline root folder and source code folder\n",
    "\n",
    "<img src=\"../../../assets/images/fab4fef8e72d4d5786ba818e6c2f73c5.png\" width=\"800\">\n",
    "\n",
    "<img src=\"../../../assets/images/a4e21cb35d3b45f2ba184877139cdc48.png\" width=\"800\">\n",
    "\n",
    "<img src=\"../../../assets/images/4e75e96544f142508c3bd8b0b4dbf446.png\" width=\"800\">\n",
    "\n",
    "![image_1771357001640.png](./image_1771357001640.png \"image_1771357001640.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a842dccd-5f60-4892-b9e7-248b430e7e6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 3: Configure Variables\n",
    "\n",
    "**Configuration** → **Add configuration**:\n",
    "\n",
    "| Key | Value |\n",
    "|-----|-------|\n",
    "| `customer_path` | `/Volumes/<your catalog>/default/datasets/customers` |\n",
    "| `order_path` | `/Volumes/<your catalog>/default/datasets/orders` |\n",
    "| `product_path` | `/Volumes/<your catalog>/default/datasets/products/products.parquet/` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1977ee54-0ca9-4e05-a98b-b0a615bf63a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Open settings and go to Pipeline Configuration \n",
    "\n",
    "<img src=\"../../../assets/images/b2fa841a52004939ac2679f9edef8dc3.png\" width=\"800\">\n",
    "\n",
    "Add configuration : \n",
    "\n",
    "<img src=\"../../../assets/images/dc31d5dec7444c1d959b8e1f220c10ce.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6635c2c-a642-4b5f-b4b1-93d4d0d66421",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "<img src=\"../../../assets/images/6f16ad4f7fd941ebbb4fb286dbb8fbfd.png\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38f4e402-9f53-4dd1-94ed-16135277d17d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You should also see a DAG diagram built based on Spark Declarative Pipelines definition\n",
    "\n",
    "![image_1771357068422.png](./image_1771357068422.png \"image_1771357068422.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a9e5731-e98c-4f7e-8263-4386341e933e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 4: Run the Pipeline\n",
    "\n",
    "Start the pipeline and test incremental processing by adding new data files.\n",
    "\n",
    "![image_1771357652994.png](./image_1771357652994.png \"image_1771357652994.png\")\n",
    "\n",
    "1. Add new file to folder orders/stream/\n",
    "2. Run pipeline again\n",
    "3. Check Event Log - should process only new files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "969c5e75-20a0-47bd-9372-9228129b1ab6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 5: Verify Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "630db1d1-fcdd-4fe8-a81c-a6f95d2378e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Provide your user_schema\n",
    "# user_schema is set via 00_setup (BRONZE_SCHEMA, SILVER_SCHEMA, GOLD_SCHEMA)\n",
    "%run ../../setup/00_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2204a4ef-e2c3-411f-8cf0-de96477f8cb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check fact_sales with joins to dimensions\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        f.order_id,\n",
    "        c.first_name || ' ' || c.last_name AS customer_name,\n",
    "        p.product_name,\n",
    "        d.date,\n",
    "        f.quantity,\n",
    "        f.net_amount\n",
    "    FROM {CATALOG}.{user_schema}.fact_sales f\n",
    "    LEFT JOIN {CATALOG}.{user_schema}.dim_customer c ON f.customer_id = c.customer_id\n",
    "    LEFT JOIN {CATALOG}.{user_schema}.dim_product p ON f.product_id = p.product_id\n",
    "    LEFT JOIN {CATALOG}.{user_schema}.dim_date d ON f.order_date_key = d.date_key\n",
    "    LIMIT 10\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8add225-5e83-4e63-a617-23a6a1ab2c97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find customers with change history\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        customer_id, first_name, city,\n",
    "        __START_AT, __END_AT,\n",
    "        CASE WHEN __END_AT IS NULL THEN 'Current' ELSE 'Historical' END AS status\n",
    "    FROM {CATALOG}.{user_schema}.silver_customers\n",
    "    WHERE customer_id IN (\n",
    "        SELECT customer_id \n",
    "        FROM {CATALOG}.{user_schema}.silver_customers \n",
    "        GROUP BY customer_id HAVING COUNT(*) > 1\n",
    "    )\n",
    "    ORDER BY customer_id, __START_AT\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a137f3c-5e4a-41d2-9910-6c4e4d3ec597",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Monitoring and Troubleshooting\n",
    "\n",
    "Common issues encountered when running Lakeflow pipelines and how to resolve them.\n",
    "\n",
    "| Issue | Cause | Solution |\n",
    "|---------|-----------|-------------|\n",
    "| Pipeline hangs | Cluster too small | Increase min workers |\n",
    "\n",
    "| Missing data | Constraint DROP ROW | Check Data Quality tab || Schema mismatch | Schema change | Full refresh |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a03a4637-569a-4bb2-8891-664f737ef98c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploying as a Lakeflow Job\n",
    "\n",
    "The medallion pipeline can be deployed as a **multi-task Lakeflow Job** with individual notebook tasks.\n",
    "\n",
    "**Simplified medallion notebooks** are in: `materials/medallion/`\n",
    "\n",
    "| Layer | Notebook | Purpose |\n",
    "|-------|----------|---------|\n",
    "| Bronze | `bronze_customers.ipynb` | Batch CSV → Delta |\n",
    "| Bronze | `bronze_orders.ipynb` | Batch JSON → Delta |\n",
    "| Silver | `silver_customers.ipynb` | Dedup + normalize |\n",
    "| Silver | `silver_orders_cleaned.ipynb` | Quality filters + computed columns |\n",
    "| Gold | `gold_customer_orders_summary.ipynb` | Join + aggregate metrics |\n",
    "| Gold | `gold_daily_orders.ipynb` | Daily order aggregation |\n",
    "| Validate | `materials/orchestration/task_validate_pipeline.py` | Check all tables + event_log |\n",
    "\n",
    "**DAG Structure:**\n",
    "```\n",
    "bronze_customers ──→ silver_customers ──────→ gold_customer_orders_summary ──→ validate\n",
    "bronze_orders ────→ silver_orders_cleaned ──→ gold_daily_orders ─────────────↗\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d261b9a-b9c9-4f1d-b18f-512787ee8908",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Job JSON Configuration — Medallion Pipeline\n",
    "# Use this with Databricks Jobs REST API or Databricks CLI\n",
    "\n",
    "job_config = {\n",
    "    \"name\": \"Medallion_Pipeline_Job\",\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"task_key\": \"bronze_customers\",\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/Workspace/Users/<email>/materials/medallion/bronze_customers\",\n",
    "                \"base_parameters\": {\n",
    "                    \"catalog\": \"{{job.parameters.catalog}}\",\n",
    "                    \"schema\": \"bronze\",\n",
    "                    \"source_path\": \"/Volumes/{{job.parameters.catalog}}/default/landing/customers/\"\n",
    "                }\n",
    "            },\n",
    "            \"new_cluster\": {\"spark_version\": \"15.4.x-scala2.12\", \"num_workers\": 1}\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"bronze_orders\",\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/Workspace/Users/<email>/materials/medallion/bronze_orders\",\n",
    "                \"base_parameters\": {\n",
    "                    \"catalog\": \"{{job.parameters.catalog}}\",\n",
    "                    \"schema\": \"bronze\",\n",
    "                    \"source_path\": \"/Volumes/{{job.parameters.catalog}}/default/landing/orders/\"\n",
    "                }\n",
    "            },\n",
    "            \"new_cluster\": {\"spark_version\": \"15.4.x-scala2.12\", \"num_workers\": 1}\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"silver_customers\",\n",
    "            \"depends_on\": [{\"task_key\": \"bronze_customers\"}],\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/Workspace/Users/<email>/materials/medallion/silver_customers\",\n",
    "                \"base_parameters\": {\n",
    "                    \"catalog\": \"{{job.parameters.catalog}}\",\n",
    "                    \"schema_bronze\": \"bronze\",\n",
    "                    \"schema_silver\": \"silver\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"silver_orders_cleaned\",\n",
    "            \"depends_on\": [{\"task_key\": \"bronze_orders\"}],\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/Workspace/Users/<email>/materials/medallion/silver_orders_cleaned\",\n",
    "                \"base_parameters\": {\n",
    "                    \"catalog\": \"{{job.parameters.catalog}}\",\n",
    "                    \"schema_bronze\": \"bronze\",\n",
    "                    \"schema_silver\": \"silver\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"gold_customer_orders_summary\",\n",
    "            \"depends_on\": [\n",
    "                {\"task_key\": \"silver_customers\"},\n",
    "                {\"task_key\": \"silver_orders_cleaned\"}\n",
    "            ],\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/Workspace/Users/<email>/materials/medallion/gold_customer_orders_summary\",\n",
    "                \"base_parameters\": {\n",
    "                    \"catalog\": \"{{job.parameters.catalog}}\",\n",
    "                    \"schema_silver\": \"silver\",\n",
    "                    \"schema_gold\": \"gold\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"gold_daily_orders\",\n",
    "            \"depends_on\": [{\"task_key\": \"silver_orders_cleaned\"}],\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/Workspace/Users/<email>/materials/medallion/gold_daily_orders\",\n",
    "                \"base_parameters\": {\n",
    "                    \"catalog\": \"{{job.parameters.catalog}}\",\n",
    "                    \"schema_silver\": \"silver\",\n",
    "                    \"schema_gold\": \"gold\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"validate_pipeline\",\n",
    "            \"depends_on\": [\n",
    "                {\"task_key\": \"gold_customer_orders_summary\"},\n",
    "                {\"task_key\": \"gold_daily_orders\"}\n",
    "            ],\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/Workspace/Users/<email>/materials/orchestration/task_validate_pipeline\",\n",
    "                \"base_parameters\": {\n",
    "                    \"catalog\": \"{{job.parameters.catalog}}\",\n",
    "                    \"schema_bronze\": \"bronze\",\n",
    "                    \"schema_silver\": \"silver\",\n",
    "                    \"schema_gold\": \"gold\",\n",
    "                    \"job_run_id\": \"{{run.id}}\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"parameters\": [\n",
    "        {\"name\": \"catalog\", \"default\": \"<your_catalog>\"}\n",
    "    ],\n",
    "    \"trigger\": {\n",
    "        \"file_arrival\": {\n",
    "            \"url\": \"/Volumes/<catalog>/default/landing_zone/trigger\",\n",
    "            \"min_time_between_triggers_seconds\": 60,\n",
    "            \"wait_after_last_change_seconds\": 15\n",
    "        }\n",
    "    },\n",
    "    \"max_concurrent_runs\": 1,\n",
    "    \"timeout_seconds\": 3600\n",
    "}\n",
    "\n",
    "import json\n",
    "print(json.dumps(job_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f2ef2ab-93d1-4bbd-9550-dfe346cf2f3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Event Log Table\n",
    "\n",
    "The validation task (`task_validate_pipeline`) logs results to a **pipeline_event_log** table for monitoring and auditing.\n",
    "\n",
    "```sql\n",
    "CREATE TABLE IF NOT EXISTS <catalog>.default.pipeline_event_log (\n",
    "    event_id        STRING,\n",
    "    event_timestamp TIMESTAMP,\n",
    "    job_run_id      STRING,\n",
    "    event_type      STRING,      -- e.g. 'PIPELINE_VALIDATION'\n",
    "    status          STRING,      -- 'SUCCESS' or 'FAILURE'\n",
    "    details         STRING       -- JSON with per-table check results\n",
    ");\n",
    "```\n",
    "\n",
    "**Validation task behavior:**\n",
    "- Checks every table (bronze → silver → gold) has rows\n",
    "- Logs `SUCCESS` or `FAILURE` to `pipeline_event_log`\n",
    "- If any table is empty/missing → task raises exception → Job marked as failed\n",
    "- Dynamic `job_run_id` via `{{run.id}}` parameter\n",
    "\n",
    "**Query event log:**\n",
    "```sql\n",
    "SELECT event_id, event_timestamp, status, details\n",
    "FROM <catalog>.default.pipeline_event_log\n",
    "WHERE event_type = 'PIPELINE_VALIDATION'\n",
    "ORDER BY event_timestamp DESC\n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e44767ef-1141-44f4-907d-b6f8f7f7faf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "Key concepts and exam-relevant keywords covered in this module.\n",
    "\n",
    "---\n",
    "\n",
    "| Topic | Key Concept | Exam Keywords |\n",
    "|---|---|---|\n",
    "| **Medallion** | Bronze → Silver → Gold | Raw, Validated, Business-ready |\n",
    "| **SCD Type 1** | Overwrite, no history | `MERGE INTO ... UPDATE SET *` |\n",
    "| **SCD Type 2** | Track history | `__START_AT`, `__END_AT`, `AUTO CDC` |\n",
    "| **STREAMING TABLE** | Append-only, incremental | `CREATE STREAMING TABLE`, `STREAM()` |\n",
    "| **MATERIALIZED VIEW** | Full recomputation | `CREATE MATERIALIZED VIEW` |\n",
    "| **FLOW** | Separate source from table | `INSERT INTO ONCE` (backfill) |\n",
    "| **Expectations** | Data quality constraints | `DROP ROW`, `FAIL UPDATE`, warn-only |\n",
    "\n",
    "> **← M06: Advanced Transforms | Day 3 | M08: Orchestration →**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7d5a02d-5a0d-462e-84b5-72851dff60b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Resources\n",
    "\n",
    "- [Databricks Lakeflow Docs](https://docs.databricks.com/en/delta-live-tables/index.html)\n",
    "- [Medallion Architecture](https://www.databricks.com/glossary/medallion-architecture)\n",
    "- [SCD with Lakeflow](https://docs.databricks.com/en/delta-live-tables/cdc.html)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "M07_medallion_lakeflow",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
