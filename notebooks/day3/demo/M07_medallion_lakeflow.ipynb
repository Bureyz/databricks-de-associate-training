{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb6d571c-ebfe-44a0-908e-8079cd0e7cf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# M07: Medallion Architecture & Lakeflow\n",
    "\n",
    "## 7.1. The Story: Building the Data Factory\n",
    "\n",
    "Your e-commerce platform now has Delta Lake running. But there's chaos:\n",
    "- 15 different notebooks, each loading data differently\n",
    "- No naming conventions - `customers_v2_final_FINAL.csv` anyone?\n",
    "- Data Scientists use Bronze data directly (with all the garbage)\n",
    "- Finance dashboard shows different numbers than Marketing\n",
    "\n",
    "**The CTO asks:** \"Can we standardize this before we hire more engineers?\"\n",
    "\n",
    "**The Answer:** Medallion Architecture - a proven pattern used by Netflix, Uber, and thousands of companies.\n",
    "\n",
    "---\n",
    "\n",
    "## 7.2. Why Medallion? (The Engineering Case)\n",
    "\n",
    "### The Pattern\n",
    "\n",
    "![Diagram](../../../assets/images/vm6wbIWLjFG0fvwwftsOu.png)\n",
    "\n",
    "\n",
    "### Layer Responsibilities\n",
    "\n",
    "| Layer | Owner | SLA | Access |\n",
    "|-------|-------|-----|--------|\n",
    "| **Bronze** | Data Engineering | 15 min from source | Engineers only |\n",
    "| **Silver** | Data Engineering | 1 hour from Bronze | Engineers + DS |\n",
    "| **Gold** | Analytics Engineering | 4 hours from Silver | Everyone |\n",
    "\n",
    "\n",
    "**Recommendation:** \n",
    "- Bronze: 90 days retention (or regulatory requirement)\n",
    "- Silver: Indefinite (your master data)\n",
    "- Gold: Depends on use case (often regenerated)\n",
    "\n",
    "---\n",
    "\n",
    "## 7.3. What You'll Build\n",
    "\n",
    "| Layer | Tables | Transformations |\n",
    "|-------|--------|-----------------|\n",
    "| **Bronze** | customers, orders, products | Raw ingestion, metadata |\n",
    "| **Silver** | customers, orders, products | Validation, dedup, typing |\n",
    "| **Gold** | fact_sales, dim_customer, dim_product, dim_date | Star schema, SCD |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "296acbc8-e708-4d93-9829-fcf46c0946da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 7.4. SCD Type 1 & Type 2\n",
    "\n",
    "### 7.4.1. Goal\n",
    "\n",
    "Understanding **Slowly Changing Dimensions** - how to track changes in dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "596e282f-a7a2-4264-95ea-74aee988ae44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7.4.2. What is SCD (Slowly Changing Dimension)?\n",
    "\n",
    "**Slowly Changing Dimension (SCD)** is a data warehousing concept describing how to handle changes in dimensional data.\n",
    "\n",
    "### Example: Customer address change\n",
    "\n",
    "Customer John Smith moved from Warsaw to Krakow. What do we do?\n",
    "\n",
    "| Type | Strategy | Result |\n",
    "|-----|-----------|----------|\n",
    "| **SCD Type 0** | Retain original | Always Warsaw |\n",
    "| **SCD Type 1** | Overwrite | Only Krakow |\n",
    "| **SCD Type 2** | Track history | Both records with dates |\n",
    "| **SCD Type 3** | Add column | `current_city=Krakow`, `previous_city=Warsaw` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "596b86c9-d330-46c2-982e-323abdcfb945",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7.4.3. SCD Type 1 - Overwrite\n",
    "\n",
    "### Characteristics:\n",
    "- **Simplest** approach\n",
    "- **No history** - old values are overwritten\n",
    "- **Use cases:** Error corrections, data that don't require history\n",
    "\n",
    "\n",
    "### Implementation in SQL (MERGE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f2c8937-ad69-49bb-8945-27b3de76f224",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# Setup - load shared variables\n",
    "%run ../../setup/00_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d72500b8-444e-497e-9a31-7a29e8ac4d5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- ### SCD Type 1 in Lakeflow:\n",
    "CREATE FLOW silver_products_scd1_flow\n",
    "AS AUTO CDC INTO silver_products\n",
    "FROM bronze_products\n",
    "KEYS (product_id)\n",
    "SEQUENCE BY ingestion_ts\n",
    "STORED AS SCD TYPE 1;  -- Overwrite without history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "deb12ba3-d37c-45d3-9c03-5a65b5788862",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "MERGE INTO dim_customer t\n",
    "USING source_customers s\n",
    "ON t.customer_id = s.customer_id\n",
    "WHEN MATCHED THEN UPDATE SET *\n",
    "WHEN NOT MATCHED THEN INSERT *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f5130fa-a9b8-4dbd-8448-37c164914045",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7.4.4. SCD Type 2 - Track History\n",
    "\n",
    "### Characteristics:\n",
    "- **Full history** of changes\n",
    "- **Additional columns:** `__START_AT`, `__END_AT` (or `valid_from`, `valid_to`)\n",
    "- **Use cases:** Audit, historical analysis, compliance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3524e678-9156-48dc-83ca-1fff6a9fc8f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7.4.5. SCD Type 2 in Lakeflow - AUTO CDC\n",
    "\n",
    "Databricks Lakeflow offers built-in support for SCD Type 2 through `AUTO CDC INTO`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f67f5414-9576-4975-ba6a-f7a8391bb8e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Creating target table SCD2\n",
    "CREATE OR REFRESH STREAMING TABLE silver_customers (\n",
    "  customer_id        STRING,\n",
    "  first_name         STRING,\n",
    "  last_name          STRING,\n",
    "  city               STRING,\n",
    "  -- SCD2 columns added automatically:\n",
    "  __START_AT         TIMESTAMP,\n",
    "  __END_AT           TIMESTAMP\n",
    ");\n",
    "\n",
    "-- Flow with AUTO CDC for SCD2\n",
    "CREATE FLOW silver_customers_scd2_flow\n",
    "AS AUTO CDC INTO silver_customers\n",
    "FROM STREAM bronze_customers\n",
    "KEYS (customer_id)         -- Business key\n",
    "SEQUENCE BY ingestion_ts   -- Column determining the order\n",
    "STORED AS SCD TYPE 2;      -- SCD Type\n",
    "\n",
    "-- ### Key elements:\n",
    "-- - **KEYS** - columns identifying the business record\n",
    "-- - **SEQUENCE BY** - column determining the order of changes\n",
    "-- - **STORED AS SCD TYPE 1|2** - SCD type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d56c2e21-d8cd-4edf-a0a0-0d7ec1746326",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 7.5. Lakeflow Pipelines - Declarations\n",
    "\n",
    "### 7.5.1. Goal\n",
    "\n",
    "Learning the syntax **Spark Declarative Pipelines** in SQL and PySpark - declarative way of defining data pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b77af01-673a-4b01-9259-fb31aef1d7f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7.5.2. What is Lakeflow?\n",
    "\n",
    "**Lakeflow** (formerly Delta Live Tables - DLT) is a Databricks framework for building declarative data pipelines.\n",
    "\n",
    "### Declarative vs Imperative\n",
    "\n",
    "| Approach | Example | Characteristics |\n",
    "|-----------|----------|----------------|\n",
    "| **Imperative** | `df.write.mode(\"overwrite\")...` | You describe HOW |\n",
    "| **Declarative** | `CREATE TABLE AS SELECT...` | You describe WHAT |\n",
    "\n",
    "### Lakeflow Benefits:\n",
    "- **Automatic dependency management** between tables\n",
    "- **Built-in data quality** - constraints with actions\n",
    "- **Unified batch/streaming** - same code\n",
    "- **Automatic recovery** after failures\n",
    "- **Lineage and monitoring** out-of-the-box\n",
    "- **Incremental processing** - only new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "243cc8f2-5eb7-47ec-a7e1-95c6d16b796a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7.5.3. Table Types in Lakeflow\n",
    "\n",
    "| Type | Usage | Characteristics |\n",
    "|-----|--------|----------------|\n",
    "| **STREAMING TABLE** | Data source, append-only | Processes only new data |\n",
    "| **MATERIALIZED VIEW** | Aggregations, transformations | Always full recomputation |\n",
    "| **VIEW** | Intermediate logic | Not materialized |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f397598e-c0e6-4011-86b3-976ed92ff981",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**STREAMING TABLE vs MATERIALIZED VIEW (Exam Topic):**\n",
    "\n",
    "| Feature | STREAMING TABLE | MATERIALIZED VIEW |\n",
    "|---------|----------------|-------------------|\n",
    "| Data Source | Streaming (append-only) | Batch or streaming |\n",
    "| Processing | Incremental (new rows only) | Full recomputation |\n",
    "| Updates | Append-only | Full refresh |\n",
    "| Use Case | Bronze/Silver layers | Gold aggregations |\n",
    "| Supports AUTO CDC | Yes | No |\n",
    "| Query Pattern | `STREAM(source)` | Regular `SELECT` |\n",
    "| Idempotent | Yes (checkpoints) | Yes (full refresh) |\n",
    "\n",
    "**Key Exam Point:** STREAMING TABLEs process only NEW data (incremental). MATERIALIZED VIEWs recompute the FULL result on each refresh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16099f51-b3e4-47c9-9bcd-a32b97238e0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7.5.4. SQL: STREAMING TABLE with CONSTRAINTS (Silver)\n",
    "\n",
    "### Actions for CONSTRAINTS:\n",
    "- **`DROP ROW`** - remove invalid record\n",
    "- **`FAIL UPDATE`** - fail the pipeline\n",
    "- No action - log only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22f8a620-c55d-4661-9ac9-5c1ade95be15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REFRESH STREAMING TABLE silver_orders\n",
    "(\n",
    "  CONSTRAINT valid_order_id EXPECT (order_id IS NOT NULL)\n",
    "    ON VIOLATION DROP ROW,\n",
    "  CONSTRAINT valid_customer EXPECT (customer_id IS NOT NULL)\n",
    "    ON VIOLATION DROP ROW,\n",
    "  CONSTRAINT valid_quantity EXPECT (quantity > 0)\n",
    "    ON VIOLATION DROP ROW,\n",
    "  CONSTRAINT valid_price EXPECT (unit_price >= 0)\n",
    "    ON VIOLATION FAIL UPDATE\n",
    ")\n",
    "AS\n",
    "SELECT\n",
    "  order_id,\n",
    "  customer_id,\n",
    "  product_id,\n",
    "  CAST(order_datetime AS TIMESTAMP) AS order_ts,\n",
    "  quantity,\n",
    "  unit_price,\n",
    "  (quantity * unit_price) AS gross_amount\n",
    "FROM STREAM(bronze_orders);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0112da2d-1319-408b-93b8-bf5a40bac0a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7.5.5. SQL: MATERIALIZED VIEW (Gold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d23d8381-a889-4ce4-8515-7d4fdda034c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Dimension - current snapshot from SCD2\n",
    "CREATE OR REFRESH MATERIALIZED VIEW dim_customer\n",
    "AS\n",
    "SELECT\n",
    "  customer_id,\n",
    "  first_name,\n",
    "  last_name,\n",
    "  email,\n",
    "  city,\n",
    "  customer_segment\n",
    "FROM silver_customers\n",
    "WHERE __END_AT IS NULL;\n",
    "\n",
    "-- Date Dimension\n",
    "CREATE OR REFRESH MATERIALIZED VIEW dim_date\n",
    "AS\n",
    "SELECT DISTINCT\n",
    "  CAST(date_format(order_date, 'yyyyMMdd') AS INT) AS date_key,\n",
    "  order_date AS date,\n",
    "  year(order_date) AS year,\n",
    "  quarter(order_date) AS quarter,\n",
    "  month(order_date) AS month\n",
    "FROM silver_orders;\n",
    "\n",
    "-- Fact - streaming from Silver\n",
    "CREATE OR REFRESH STREAMING TABLE fact_sales\n",
    "AS\n",
    "SELECT\n",
    "  order_id,\n",
    "  customer_id,\n",
    "  product_id,\n",
    "  order_date_key,\n",
    "  quantity,\n",
    "  gross_amount,\n",
    "  net_amount\n",
    "FROM STREAM(silver_orders);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9dbc1a2-5469-4598-955a-101fa2d08e5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7.5.6. What is FLOW?\n",
    "\n",
    "**FLOW** is a Lakeflow mechanism that allows:\n",
    "1. **Separation of table definition from data source**\n",
    "2. **Multiple sources to one table** (e.g. backfill + streaming)\n",
    "3. **CDC (Change Data Capture)** with automatic SCD\n",
    "\n",
    "### FLOW Anatomy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b97c1532-2504-4233-a6c6-b62a6bbe638a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 1. We define empty target table\n",
    "CREATE OR REFRESH STREAMING TABLE bronze_orders;\n",
    "\n",
    "-- 2. We define FLOW(s) which populate it\n",
    "CREATE FLOW flow_name\n",
    "AS INSERT INTO target_table BY NAME\n",
    "SELECT ... FROM source;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32e2f228-f78a-4713-b92d-ac50e8691c4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7.5.7. FLOW: Backfill + Streaming Pattern\n",
    "\n",
    "### Key elements:\n",
    "- **`INSERT INTO ONCE`** - execute once (backfill)\n",
    "- **`INSERT INTO`** - continuous insertion (streaming)\n",
    "- **`BY NAME`** - column mapping by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c49acc6-2d57-487a-8cee-4c458753db40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Target table\n",
    "CREATE OR REFRESH STREAMING TABLE bronze_orders;\n",
    "\n",
    "-- FLOW 1: One-time backfill\n",
    "CREATE FLOW bronze_orders_backfill\n",
    "AS \n",
    "INSERT INTO ONCE bronze_orders BY NAME\n",
    "SELECT\n",
    "  order_id,\n",
    "  customer_id,\n",
    "  product_id,\n",
    "  order_datetime,\n",
    "  'batch' AS source_system,\n",
    "  _metadata.file_path AS source_file_path,\n",
    "  current_timestamp() AS load_ts\n",
    "FROM read_files(\n",
    "  '${order_path}/orders_batch.json',\n",
    "  format => 'json'\n",
    ");\n",
    "\n",
    "-- FLOW 2: Continuous streaming\n",
    "CREATE FLOW bronze_orders_stream\n",
    "AS \n",
    "INSERT INTO bronze_orders BY NAME\n",
    "SELECT\n",
    "  order_id,\n",
    "  customer_id,\n",
    "  'stream' AS source_system,\n",
    "  _metadata.file_path AS source_file_path,\n",
    "  current_timestamp() AS load_ts\n",
    "FROM STREAM read_files(\n",
    "  '${order_path}/stream/orders_stream_*.json',\n",
    "  format => 'json'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cde124b7-99df-48b1-9235-7c3725d0641c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7.5.8. FLOW: AUTO CDC for SCD Type 2\n",
    "\n",
    "\n",
    "### What does AUTO CDC do?\n",
    "1. Compares new records with existing by `KEYS`\n",
    "2. Detects changes in columns\n",
    "3. For SCD2: closes old record (`__END_AT`), inserts new\n",
    "4. For SCD1: overwrites existing record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "317bf3c8-8130-4651-82ab-f7ba4f4f7379",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SCD2 Table with schema\n",
    "CREATE OR REFRESH STREAMING TABLE silver_customers (\n",
    "  customer_id        STRING,\n",
    "  first_name         STRING,\n",
    "  last_name          STRING,\n",
    "  email              STRING,\n",
    "  city               STRING,\n",
    "  __START_AT         TIMESTAMP,\n",
    "  __END_AT           TIMESTAMP\n",
    ");\n",
    "\n",
    "-- AUTO CDC Flow\n",
    "CREATE FLOW silver_customers_scd2_flow\n",
    "AS AUTO CDC INTO silver_customers\n",
    "FROM STREAM bronze_customers\n",
    "KEYS (customer_id)\n",
    "SEQUENCE BY ingestion_ts\n",
    "STORED AS SCD TYPE 2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de9928ec-7050-4503-b1d7-f37494f2194c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7.5.9. PySpark Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5957fcd-37c0-48f6-a851-eee161b8d785",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import pipelines as dp\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# STREAMING TABLE\n",
    "@dp.table(\n",
    "    name=\"bronze_customers\",\n",
    "    comment=\"Raw customers from CSV\"\n",
    ")\n",
    "def bronze_customers():\n",
    "    return (\n",
    "        spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"csv\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .load(spark.conf.get(\"customer_path\"))\n",
    "            .select(\n",
    "                \"*\",\n",
    "                \"_metadata.file_path\".alias(\"source_file_path\"),\n",
    "                current_timestamp().alias(\"load_ts\")\n",
    "            )\n",
    "    )\n",
    "\n",
    "# MATERIALIZED VIEW\n",
    "@dp.table(name=\"dim_customer\")\n",
    "def dim_customer():\n",
    "    return (\n",
    "        spark.read.table(\"silver_customers\")\n",
    "            .filter(col(\"__END_AT\").isNull())\n",
    "            .select(\"customer_id\", \"first_name\", \"last_name\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb2aa8c3-d7ee-44c3-aaba-81ffb2402759",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7.5.10. PySpark: Expectations (Constraints)\n",
    "\n",
    "\n",
    "### Expectations Decorators:\n",
    "- **`@dp.expect`** - log only\n",
    "- **`@dp.expect_or_drop`** - drop the record\n",
    "- **`@dp.expect_or_fail`** - fail the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd54decf-bcee-4c97-9b0d-edfcb3432ff1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dp.table(name=\"silver_orders\")\n",
    "@dp.expect_or_drop(\"valid_order_id\", \"order_id IS NOT NULL\")\n",
    "@dp.expect_or_drop(\"valid_customer\", \"customer_id IS NOT NULL\")\n",
    "@dp.expect_or_drop(\"valid_quantity\", \"quantity > 0\")\n",
    "@dp.expect(\"valid_price\", \"unit_price >= 0\")\n",
    "def silver_orders():\n",
    "    return (\n",
    "        spark.readStream.table(\"bronze_orders\")\n",
    "            .select(\n",
    "                \"order_id\",\n",
    "                \"customer_id\",\n",
    "                col(\"order_datetime\").cast(\"timestamp\").alias(\"order_ts\"),\n",
    "                (col(\"quantity\") * col(\"unit_price\")).alias(\"gross_amount\")\n",
    "            )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d849523-8a35-48ba-bef5-cbc10078d4fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7.5.11. PySpark: AUTO CDC (create_auto_cdc_flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7613e54a-6dcd-49e7-b8d6-fb9887ba6891",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import pipelines as dp\n",
    "\n",
    "# Define the target table\n",
    "dp.create_streaming_table(\n",
    "    name=\"silver_customers\",\n",
    "    schema=\"\"\"\n",
    "        customer_id STRING,\n",
    "        first_name STRING,\n",
    "        city STRING,\n",
    "        __START_AT TIMESTAMP,\n",
    "        __END_AT TIMESTAMP\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Define the CDC flow\n",
    "dp.create_auto_cdc_flow(\n",
    "    target=\"silver_customers\",\n",
    "    source=\"bronze_customers\",\n",
    "    keys=[\"customer_id\"],\n",
    "    sequence_by=\"ingestion_ts\",\n",
    "    stored_as_scd_type=2  # or 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b52359f-9cfd-4993-a8c3-c491bafc4d1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 7.6. DEMO - Building the Pipeline\n",
    "\n",
    "### 7.6.1. Goal\n",
    "\n",
    "Practical construction of a Lakeflow pipeline step-by-step in Databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66b10034-118d-4460-b55f-a33b067c2bac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7.6.2. SQL Files Overview\n",
    "\n",
    "Our pipeline consists of SQL files in folder `lakeflow/`:\n",
    "\n",
    "```\n",
    "lakeflow/\n",
    "├── 01_bronze/\n",
    "│   ├── bronze_customers.sql\n",
    "│   ├── bronze_orders.sql\n",
    "│   └── bronze_products.sql\n",
    "├── 02_silver/\n",
    "│   ├── silver_customers.sql\n",
    "│   ├── silver_orders.sql\n",
    "│   └── silver_products.sql\n",
    "└── 03_gold/\n",
    "    ├── dim_customer.sql\n",
    "    ├── dim_date.sql\n",
    "    ├── dim_payment_method.sql\n",
    "    ├── dim_product.sql\n",
    "    ├── dim_store.sql\n",
    "    └── fact_sales.sql\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91087335-6585-4817-a88d-4414fe99fc88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![alt text](../../../assets/images/nw_SjXPxbrKCuOkckmVuU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "194b80f2-d0de-4495-b709-2089fede1524",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7.6.3. Step 1: Upload SQL Files\n",
    "\n",
    "### Option A: Via UI\n",
    "1. Databricks → Workspace → Users → (your folder)\n",
    "2. Create folder `lakeflow_pipeline`\n",
    "3. Upload SQL files from folder `lakeflow/`\n",
    "\n",
    "### Option B: Via Git Folders\n",
    "1. Databricks → Git Folders → Add Git Folder\n",
    "2. Clone training repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70bd1cb7-c0cd-48ed-a893-3d47f5dc268a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7.6.4. Step 2: Create Pipeline\n",
    "\n",
    "1. Go to **Workflows** → **Jobs & Pipelines** (1)\n",
    "2. Click **Create ETL Pipeline** (2)\n",
    "\n",
    "![Diagram](<../../../assets/images/fab4fef8e72d4d5786ba818e6c2f73c5.png>)\n",
    "\n",
    "\n",
    "3. Fill in details:\n",
    "- **Choose your CATALOG** retailhub_<your_name>\n",
    "- **Pipeline Name:** `lakeflow_pipeline_<your_name>`\n",
    "\n",
    "![Diagram](<../../../assets/images/Zrzut ekranu 2025-12-8 o 20.43.57.png>)](<../../../assets/images/a4e21cb35d3b45f2ba184877139cdc48.png>)\n",
    "\n",
    "\n",
    "- **Target Schema:** `<your_name>_lakeflow`\n",
    "\n",
    "![Diagram](<../../../assets/images/4e75e96544f142508c3bd8b0b4dbf446.png>)\n",
    "\n",
    "\n",
    "\n",
    "- **Source Code:**  Choose Add existing assets \n",
    "\n",
    "\n",
    "- **Choose pipeline root folder** /Workspace/User<your_name>/DatabricksDataEngineerOne/lakeflow/lakeflow_trn_pipeline\n",
    "\n",
    "- **Choose source code folder** /Workspace/User<your_name>/DatabricksDataEngineerOne/lakeflow/lakeflow_trn_pipeline/transformations\n",
    "\n",
    "\n",
    "![Diagram](<../../../assets/images/edc3ce795e504c67ab127fbf0c5f58a6.png>)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4503aa43-99ba-4f69-aef2-990e42cd1135",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Pipeline Source Configuration View](<../../../assets/images/88121d8488a242c494faa0a9a9443b70.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a842dccd-5f60-4892-b9e7-248b430e7e6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7.6.5. Step 3: Configure Variables\n",
    "\n",
    " **Configuration** → **Add configuration**:\n",
    "\n",
    "| Key | Value |\n",
    "|-----|-------|\n",
    "| `customer_path` | `/Volumes/<your catalog>/default/datasets/customers` |\n",
    "| `order_path` | `/Volumes/<your catalog>/default/datasets/orders` |\n",
    "| `product_path` | `/Volumes/<your catalog>/default/datasets/products/products.parquet/` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1977ee54-0ca9-4e05-a98b-b0a615bf63a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Open settings and go to Pipeline Configuration \n",
    "\n",
    "![Opening Pipeline Settings](<../../../assets/images/b2fa841a52004939ac2679f9edef8dc3.png>)\n",
    "\n",
    "Add configuration : \n",
    "\n",
    "![Adding Pipeline Configuration Parameters](<../../../assets/images/dc31d5dec7444c1d959b8e1f220c10ce.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c111baf8-e679-47ab-ba68-c525ce05f091",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Add cluster configuration\n",
    "\n",
    "![Cluster Configuration](<../../../assets/images/954b77029bb046b38f2b8296f02dd4eb.png>)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9dc74425-1c41-420a-8988-fb9225ca6081",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Click **Dry Run** - check syntax and DAG\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6635c2c-a642-4b5f-b4b1-93d4d0d66421",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "![Validating Pipeline (Dry Run)](<../../../assets/images/6f16ad4f7fd941ebbb4fb286dbb8fbfd.png>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38f4e402-9f53-4dd1-94ed-16135277d17d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You should also see a DAG diagram built based on Spark Declarative Pipelines definition\n",
    "\n",
    "![Pipeline DAG Diagram](<../../../assets/images/fb073c114c78410ab4770d600c25a47c.png>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a9e5731-e98c-4f7e-8263-4386341e933e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Pipeline Execution Progress](<../../../assets/images/bab6cbc64db54b02a9efe77480ea64cc.png>)\n",
    "\n",
    "1. Add new file to folder orders/stream/\n",
    "2. Run pipeline again\n",
    "3. Check Event Log - should process only new files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "414e740a-52fc-47bb-8824-ec024969b895",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "If you click schedule button on top right corner then you will be able to schedule that job based on trigger mode:\n",
    "\n",
    "![Scheduling the Pipeline](<../../../assets/images/0a5021a24ce14da1874fd24422905fb1.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "969c5e75-20a0-47bd-9372-9228129b1ab6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7.6.6. Step 5: Verify Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "630db1d1-fcdd-4fe8-a81c-a6f95d2378e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Provide your user_schema\n",
    "# user_schema is set via 00_setup (BRONZE_SCHEMA, SILVER_SCHEMA, GOLD_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2204a4ef-e2c3-411f-8cf0-de96477f8cb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check fact_sales with joins to dimensions\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        f.order_id,\n",
    "        c.first_name || ' ' || c.last_name AS customer_name,\n",
    "        p.product_name,\n",
    "        d.date,\n",
    "        f.quantity,\n",
    "        f.net_amount\n",
    "    FROM {CATALOG}.{user_schema}.fact_sales f\n",
    "    LEFT JOIN {CATALOG}.{user_schema}.dim_customer c ON f.customer_id = c.customer_id\n",
    "    LEFT JOIN {CATALOG}.{user_schema}.dim_product p ON f.product_id = p.product_id\n",
    "    LEFT JOIN {CATALOG}.{user_schema}.dim_date d ON f.order_date_key = d.date_key\n",
    "    LIMIT 10\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8add225-5e83-4e63-a617-23a6a1ab2c97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find customers with change historyź klientów z historią zmian\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        customer_id, first_name, city,\n",
    "        __START_AT, __END_AT,\n",
    "        CASE WHEN __END_AT IS NULL THEN 'Current' ELSE 'Historical' END AS status\n",
    "    FROM {CATALOG}.{user_schema}.silver_customers\n",
    "    WHERE customer_id IN (\n",
    "        SELECT customer_id \n",
    "        FROM {CATALOG}.{user_schema}.silver_customers \n",
    "        GROUP BY customer_id HAVING COUNT(*) > 1\n",
    "    )\n",
    "    ORDER BY customer_id, __START_AT\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a137f3c-5e4a-41d2-9910-6c4e4d3ec597",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7.6.7. Monitoring and Troubleshooting\n",
    "\n",
    "### Event Log\n",
    "- Click on table in DAG → processing details\n",
    "- Metrics: records processed, dropped rows, duration\n",
    "\n",
    "### Common issues:\n",
    "\n",
    "| Issue | Cause | Solution |\n",
    "|---------|-----------|-------------|\n",
    "| Pipeline hangs | Cluster too small | Increase min workers |\n",
    "| Missing data | Constraint DROP ROW | Check Data Quality tab |\n",
    "| Schema mismatch | Schema change | Full refresh |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e44767ef-1141-44f4-907d-b6f8f7f7faf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 7.7. Summary\n",
    "\n",
    "### 7.7.1. Key Takeaways\n",
    "\n",
    "### Medallion Architecture\n",
    "- **Bronze:** Raw data + metadata\n",
    "- **Silver:** Cleaned, validated\n",
    "- **Gold:** Business-ready, star schema\n",
    "\n",
    "### SCD Types\n",
    "- **SCD Type 1:** Overwrite - no history\n",
    "- **SCD Type 2:** Track history - `__START_AT`, `__END_AT`\n",
    "\n",
    "### Lakeflow Declarations\n",
    "- **STREAMING TABLE:** Data append-only\n",
    "- **MATERIALIZED VIEW:** Aggregations\n",
    "- **FLOW:** CDC, backfill+streaming\n",
    "\n",
    "### Best Practices\n",
    "1. Constraints with `DROP ROW` for quality\n",
    "2. Backfill (ONCE) + streaming FLOWs\n",
    "3. SCD Type 2 for dimensions with history\n",
    "4. STREAMING TABLE for facts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7d5a02d-5a0d-462e-84b5-72851dff60b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7.7.2. Resources\n",
    "\n",
    "- [Databricks Lakeflow Docs](https://docs.databricks.com/en/delta-live-tables/index.html)\n",
    "- [Medallion Architecture](https://www.databricks.com/glossary/medallion-architecture)\n",
    "- [SCD with Lakeflow](https://docs.databricks.com/en/delta-live-tables/cdc.html)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "M07_medallion_lakeflow",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
