{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb6d571c-ebfe-44a0-908e-8079cd0e7cf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# M07: Medallion Architecture & Lakeflow\n",
    "\n",
    "\n",
    "The Medallion architecture (Bronze → Silver → Gold) is the standard for organizing data in the Lakehouse. We'll explore declarative Lakeflow pipelines: STREAMING TABLE, MATERIALIZED VIEW, constraints (Expectations), and the FLOW mechanism. We'll build a complete pipeline in SQL and PySpark, including SCD Type 1 and Type 2 handling via AUTO CDC.\n",
    "\n",
    "| Exam Domain | Weight |\n",
    "|---|---|\n",
    "| Production Pipelines | 13% |\n",
    "| Incremental Data Processing | 20% |\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Theory — SCD & Lakeflow Declarations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "296acbc8-e708-4d93-9829-fcf46c0946da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SCD Type 1 & Type 2\n",
    "\n",
    "Slowly Changing Dimensions (SCD) define strategies for handling changes in dimensional data over time. Type 1 overwrites old values, while Type 2 preserves full history with validity timestamps.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "596e282f-a7a2-4264-95ea-74aee988ae44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### What is SCD?\n",
    "\n",
    "**Slowly Changing Dimension (SCD)** — how to handle changes in dimensional data.\n",
    "\n",
    "| Type | Strategy | Result |\n",
    "|-----|-----------|----------|\n",
    "| **SCD Type 0** | Retain original | Always Warsaw |\n",
    "| **SCD Type 1** | Overwrite | Only Krakow |\n",
    "| **SCD Type 2** | Track history | Both records with dates |\n",
    "| **SCD Type 3** | Add column | `current_city=Krakow`, `previous_city=Warsaw` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "596b86c9-d330-46c2-982e-323abdcfb945",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SCD Type 1 — Overwrite\n",
    "\n",
    "- **No history** — old values are overwritten\n",
    "- **Use cases:** Error corrections, non-historical data\n",
    "- **Implementation:** `MERGE INTO ... WHEN MATCHED THEN UPDATE SET *`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d72500b8-444e-497e-9a31-7a29e8ac4d5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- ### SCD Type 1 in Lakeflow:\n",
    "CREATE FLOW silver_products_scd1_flow\n",
    "AS AUTO CDC INTO silver_products\n",
    "FROM bronze_products\n",
    "KEYS (product_id)\n",
    "SEQUENCE BY ingestion_ts\n",
    "STORED AS SCD TYPE 1;  -- Overwrite without history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "deb12ba3-d37c-45d3-9c03-5a65b5788862",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "MERGE INTO dim_customer t\n",
    "USING source_customers s\n",
    "ON t.customer_id = s.customer_id\n",
    "WHEN MATCHED THEN UPDATE SET *\n",
    "WHEN NOT MATCHED THEN INSERT *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f5130fa-a9b8-4dbd-8448-37c164914045",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SCD Type 2 — Track History\n",
    "\n",
    "- **Full history** of changes with `__START_AT`, `__END_AT`\n",
    "- **Use cases:** Audit, historical analysis, compliance\n",
    "- **Current record:** `__END_AT IS NULL`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3524e678-9156-48dc-83ca-1fff6a9fc8f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Demo: SCD Type 2 with AUTO CDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f67f5414-9576-4975-ba6a-f7a8391bb8e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Creating target table SCD2\n",
    "CREATE OR REFRESH STREAMING TABLE silver_customers (\n",
    "  customer_id        STRING,\n",
    "  first_name         STRING,\n",
    "  last_name          STRING,\n",
    "  city               STRING,\n",
    "  -- SCD2 columns added automatically:\n",
    "  __START_AT         TIMESTAMP,\n",
    "  __END_AT           TIMESTAMP\n",
    ");\n",
    "\n",
    "-- Flow with AUTO CDC for SCD2\n",
    "CREATE FLOW silver_customers_scd2_flow\n",
    "AS AUTO CDC INTO silver_customers\n",
    "FROM STREAM bronze_customers\n",
    "KEYS (customer_id)         -- Business key\n",
    "SEQUENCE BY ingestion_ts   -- Column determining the order\n",
    "STORED AS SCD TYPE 2;      -- SCD Type\n",
    "\n",
    "-- ### Key elements:\n",
    "-- - **KEYS** - columns identifying the business record\n",
    "-- - **SEQUENCE BY** - column determining the order of changes\n",
    "-- - **STORED AS SCD TYPE 1|2** - SCD type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d56c2e21-d8cd-4edf-a0a0-0d7ec1746326",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Lakeflow Pipelines — Declarations\n",
    "\n",
    "Lakeflow (formerly Delta Live Tables) enables a declarative approach to building data pipelines — you define the desired outcome rather than step-by-step logic. This section covers table types, expectations, FLOW declarations, and both SQL and PySpark syntax.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b77af01-673a-4b01-9259-fb31aef1d7f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### What is Lakeflow?\n",
    "\n",
    "**Lakeflow** (formerly Delta Live Tables) — declarative framework for data pipelines.\n",
    "\n",
    "| Approach | Example | You describe |\n",
    "|-----------|----------|-------------|\n",
    "| **Imperative** | `df.write.mode(\"overwrite\")...` | HOW |\n",
    "| **Declarative** | `CREATE TABLE AS SELECT...` | WHAT |\n",
    "\n",
    "Key benefits: automatic dependencies, built-in data quality, unified batch/streaming, automatic recovery, lineage & monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "243cc8f2-5eb7-47ec-a7e1-95c6d16b796a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Table Types in Lakeflow\n",
    "\n",
    "| Type | Usage | Processing |\n",
    "|-----|--------|-----------|\n",
    "| **STREAMING TABLE** | Append-only ingestion | Incremental (new rows only) |\n",
    "| **MATERIALIZED VIEW** | Aggregations, Gold layer | Full recomputation |\n",
    "| **VIEW** | Intermediate logic | Not materialized |\n",
    "\n",
    "> **Exam:** STREAMING TABLE = incremental (new data only). MATERIALIZED VIEW = full recompute each refresh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f397598e-c0e6-4011-86b3-976ed92ff981",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**STREAMING TABLE vs MATERIALIZED VIEW (Exam Topic):**\n",
    "\n",
    "| Feature | STREAMING TABLE | MATERIALIZED VIEW |\n",
    "|---------|----------------|-------------------|\n",
    "| Data Source | Streaming (append-only) | Batch or streaming |\n",
    "| Processing | Incremental (new rows only) | Full recomputation |\n",
    "| Updates | Append-only | Full refresh |\n",
    "| Use Case | Bronze/Silver layers | Gold aggregations |\n",
    "| Supports AUTO CDC | Yes | No |\n",
    "| Query Pattern | `STREAM(source)` | Regular `SELECT` |\n",
    "| Idempotent | Yes (checkpoints) | Yes (full refresh) |\n",
    "\n",
    "**Key Exam Point:** STREAMING TABLEs process only NEW data (incremental). MATERIALIZED VIEWs recompute the FULL result on each refresh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16099f51-b3e4-47c9-9bcd-a32b97238e0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Demo: STREAMING TABLE with Constraints\n",
    "\n",
    "Constraint actions: **`DROP ROW`** (remove invalid), **`FAIL UPDATE`** (fail pipeline), or no action (log only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22f8a620-c55d-4661-9ac9-5c1ade95be15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REFRESH STREAMING TABLE silver_orders\n",
    "(\n",
    "  CONSTRAINT valid_order_id EXPECT (order_id IS NOT NULL)\n",
    "    ON VIOLATION DROP ROW,\n",
    "  CONSTRAINT valid_customer EXPECT (customer_id IS NOT NULL)\n",
    "    ON VIOLATION DROP ROW,\n",
    "  CONSTRAINT valid_quantity EXPECT (quantity > 0)\n",
    "    ON VIOLATION DROP ROW,\n",
    "  CONSTRAINT valid_price EXPECT (unit_price >= 0)\n",
    "    ON VIOLATION FAIL UPDATE\n",
    ")\n",
    "AS\n",
    "SELECT\n",
    "  order_id,\n",
    "  customer_id,\n",
    "  product_id,\n",
    "  CAST(order_datetime AS TIMESTAMP) AS order_ts,\n",
    "  quantity,\n",
    "  unit_price,\n",
    "  (quantity * unit_price) AS gross_amount\n",
    "FROM STREAM(bronze_orders);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0112da2d-1319-408b-93b8-bf5a40bac0a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Demo: MATERIALIZED VIEW (Gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d23d8381-a889-4ce4-8515-7d4fdda034c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Dimension - current snapshot from SCD2\n",
    "CREATE OR REFRESH MATERIALIZED VIEW dim_customer\n",
    "AS\n",
    "SELECT\n",
    "  customer_id,\n",
    "  first_name,\n",
    "  last_name,\n",
    "  email,\n",
    "  city,\n",
    "  customer_segment\n",
    "FROM silver_customers\n",
    "WHERE __END_AT IS NULL;\n",
    "\n",
    "-- Date Dimension\n",
    "CREATE OR REFRESH MATERIALIZED VIEW dim_date\n",
    "AS\n",
    "SELECT DISTINCT\n",
    "  CAST(date_format(order_date, 'yyyyMMdd') AS INT) AS date_key,\n",
    "  order_date AS date,\n",
    "  year(order_date) AS year,\n",
    "  quarter(order_date) AS quarter,\n",
    "  month(order_date) AS month\n",
    "FROM silver_orders;\n",
    "\n",
    "-- Fact - streaming from Silver\n",
    "CREATE OR REFRESH STREAMING TABLE fact_sales\n",
    "AS\n",
    "SELECT\n",
    "  order_id,\n",
    "  customer_id,\n",
    "  product_id,\n",
    "  order_date_key,\n",
    "  quantity,\n",
    "  gross_amount,\n",
    "  net_amount\n",
    "FROM STREAM(silver_orders);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9dbc1a2-5469-4598-955a-101fa2d08e5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### What is FLOW?\n",
    "\n",
    "**FLOW** separates table definition from data source. Key capabilities:\n",
    "1. **Multiple sources → one table** (e.g. backfill + streaming)\n",
    "2. **CDC** with automatic SCD via `AUTO CDC`\n",
    "3. **`INSERT INTO ONCE`** — one-time backfill\n",
    "4. **`INSERT INTO`** — continuous streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b97c1532-2504-4233-a6c6-b62a6bbe638a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 1. We define empty target table\n",
    "CREATE OR REFRESH STREAMING TABLE bronze_orders;\n",
    "\n",
    "-- 2. We define FLOW(s) which populate it\n",
    "CREATE FLOW flow_name\n",
    "AS INSERT INTO target_table BY NAME\n",
    "SELECT ... FROM source;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32e2f228-f78a-4713-b92d-ac50e8691c4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Demo: Backfill + Streaming Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c49acc6-2d57-487a-8cee-4c458753db40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Target table\n",
    "CREATE OR REFRESH STREAMING TABLE bronze_orders;\n",
    "\n",
    "-- FLOW 1: One-time backfill\n",
    "CREATE FLOW bronze_orders_backfill\n",
    "AS \n",
    "INSERT INTO ONCE bronze_orders BY NAME\n",
    "SELECT\n",
    "  order_id,\n",
    "  customer_id,\n",
    "  product_id,\n",
    "  order_datetime,\n",
    "  'batch' AS source_system,\n",
    "  _metadata.file_path AS source_file_path,\n",
    "  current_timestamp() AS load_ts\n",
    "FROM read_files(\n",
    "  '${order_path}/orders_batch.json',\n",
    "  format => 'json'\n",
    ");\n",
    "\n",
    "-- FLOW 2: Continuous streaming\n",
    "CREATE FLOW bronze_orders_stream\n",
    "AS \n",
    "INSERT INTO bronze_orders BY NAME\n",
    "SELECT\n",
    "  order_id,\n",
    "  customer_id,\n",
    "  'stream' AS source_system,\n",
    "  _metadata.file_path AS source_file_path,\n",
    "  current_timestamp() AS load_ts\n",
    "FROM STREAM read_files(\n",
    "  '${order_path}/stream/orders_stream_*.json',\n",
    "  format => 'json'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cde124b7-99df-48b1-9235-7c3725d0641c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Demo: AUTO CDC for SCD Type 2\n",
    "\n",
    "AUTO CDC: compares new records by `KEYS`, detects changes, closes old record (`__END_AT`), inserts new (SCD2) or overwrites (SCD1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "317bf3c8-8130-4651-82ab-f7ba4f4f7379",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SCD2 Table with schema\n",
    "CREATE OR REFRESH STREAMING TABLE silver_customers (\n",
    "  customer_id        STRING,\n",
    "  first_name         STRING,\n",
    "  last_name          STRING,\n",
    "  email              STRING,\n",
    "  city               STRING,\n",
    "  __START_AT         TIMESTAMP,\n",
    "  __END_AT           TIMESTAMP\n",
    ");\n",
    "\n",
    "-- AUTO CDC Flow\n",
    "CREATE FLOW silver_customers_scd2_flow\n",
    "AS AUTO CDC INTO silver_customers\n",
    "FROM STREAM bronze_customers\n",
    "KEYS (customer_id)\n",
    "SEQUENCE BY ingestion_ts\n",
    "STORED AS SCD TYPE 2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de9928ec-7050-4503-b1d7-f37494f2194c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PySpark Declarations\n",
    "\n",
    "Lakeflow pipelines can also be defined in Python using the `pyspark.pipelines` decorators instead of SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5957fcd-37c0-48f6-a851-eee161b8d785",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import pipelines as dp\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# STREAMING TABLE\n",
    "@dp.table(\n",
    "    name=\"bronze_customers\",\n",
    "    comment=\"Raw customers from CSV\"\n",
    ")\n",
    "def bronze_customers():\n",
    "    return (\n",
    "        spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"csv\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .load(spark.conf.get(\"customer_path\"))\n",
    "            .select(\n",
    "                \"*\",\n",
    "                col(\"_metadata.file_path\").alias(\"source_file_path\"),\n",
    "                current_timestamp().alias(\"load_ts\")\n",
    "            )\n",
    "    )\n",
    "\n",
    "# MATERIALIZED VIEW\n",
    "@dp.table(name=\"dim_customer\")\n",
    "def dim_customer():\n",
    "    return (\n",
    "        spark.read.table(\"silver_customers\")\n",
    "            .filter(col(\"__END_AT\").isNull())\n",
    "            .select(\"customer_id\", \"first_name\", \"last_name\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb2aa8c3-d7ee-44c3-aaba-81ffb2402759",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PySpark: Expectations\n",
    "\n",
    "Decorators: **`@dp.expect`** (log only), **`@dp.expect_or_drop`** (drop record), **`@dp.expect_or_fail`** (fail pipeline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd54decf-bcee-4c97-9b0d-edfcb3432ff1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dp.table(name=\"silver_orders\")\n",
    "@dp.expect_or_drop(\"valid_order_id\", \"order_id IS NOT NULL\")\n",
    "@dp.expect_or_drop(\"valid_customer\", \"customer_id IS NOT NULL\")\n",
    "@dp.expect_or_drop(\"valid_quantity\", \"quantity > 0\")\n",
    "@dp.expect(\"valid_price\", \"unit_price >= 0\")\n",
    "def silver_orders():\n",
    "    return (\n",
    "        spark.readStream.table(\"bronze_orders\")\n",
    "            .select(\n",
    "                \"order_id\",\n",
    "                \"customer_id\",\n",
    "                col(\"order_datetime\").cast(\"timestamp\").alias(\"order_ts\"),\n",
    "                (col(\"quantity\") * col(\"unit_price\")).alias(\"gross_amount\")\n",
    "            )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d849523-8a35-48ba-bef5-cbc10078d4fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PySpark: AUTO CDC\n",
    "\n",
    "Python equivalent of SQL AUTO CDC — uses `dp.create_auto_cdc_flow()` to define SCD logic programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7613e54a-6dcd-49e7-b8d6-fb9887ba6891",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import pipelines as dp\n",
    "\n",
    "# Define the target table\n",
    "dp.create_streaming_table(\n",
    "    name=\"silver_customers\",\n",
    "    schema=\"\"\"\n",
    "        customer_id STRING,\n",
    "        first_name STRING,\n",
    "        city STRING,\n",
    "        __START_AT TIMESTAMP,\n",
    "        __END_AT TIMESTAMP\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Define the CDC flow\n",
    "dp.create_auto_cdc_flow(\n",
    "    target=\"silver_customers\",\n",
    "    source=\"bronze_customers\",\n",
    "    keys=[\"customer_id\"],\n",
    "    sequence_by=\"ingestion_ts\",\n",
    "    stored_as_scd_type=2  # or 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3a15580-c942-4533-8629-31c128548705",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Section 2: UI Demo — Building a Lakeflow Pipeline\n",
    "\n",
    "Step-by-step walkthrough of creating, configuring, and executing a Lakeflow pipeline in the Databricks workspace. This demo covers the full lifecycle: pipeline creation, asset configuration, compute provisioning, validation (dry run), and production execution.\n",
    "\n",
    "> **Estimated time:** ~15 min (including 5-8 min for initial cluster provisioning)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create Pipeline\n",
    "\n",
    "Navigate to **Jobs & Pipelines** in the left sidebar and select **ETL Pipeline**.\n",
    "\n",
    "<img src=\"../../../assets/images/training_2026/day3/fd6ad124b1764ec0a2867cb7c612c75d.webp\" width=\"800\">\n",
    "\n",
    "Assign a unique, descriptive name to your pipeline (e.g., `<your_name>_lakeflow_demo`).\n",
    "\n",
    "<img src=\"../../../assets/images/training_2026/day3/eb57b69aa1494a759d1f69fc1de84189.webp\" width=\"800\">\n",
    "\n",
    "### Step 2: Configure Target — Catalog & Schema\n",
    "\n",
    "Select the target **Unity Catalog** catalog and schema for pipeline output tables. In a shared training environment, create a dedicated schema with a unique name (e.g., `<your_name>_lakeflow_demo`) to avoid namespace conflicts with other participants.\n",
    "\n",
    "<img src=\"../../../assets/images/training_2026/day3/bef9f61c293743f190f68d188265f635.webp\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Add Source Assets\n",
    "\n",
    "Click **Add existing assets** to link the pipeline to repository-managed source code.\n",
    "\n",
    "<img src=\"../../../assets/images/training_2026/day3/198cba2b98dd4bafbceb4d178597cfc6.webp\" width=\"800\">\n",
    "\n",
    "Configure the following paths:\n",
    "- **Pipeline root folder** → `materials/lakeflow/lakeflow_ny_demo`\n",
    "- **Source code path** → `transformations` subdirectory within `lakeflow_ny_demo`\n",
    "\n",
    "<img src=\"../../../assets/images/training_2026/day3/8b6b591b1b3843f6a39fbe400448c311.webp\" width=\"800\">\n",
    "\n",
    "Verify that the final configuration matches the expected setup before proceeding:\n",
    "\n",
    "<img src=\"../../../assets/images/training_2026/day3/8300f3f7570043ceaf1411a8be91e9a7.webp\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Review Pipeline Structure\n",
    "\n",
    "The left panel displays the auto-discovered pipeline graph — all tables, views, and dependencies extracted automatically from the source code repository.\n",
    "\n",
    "<img src=\"../../../assets/images/training_2026/day3/1e476ee5b1f94c649511adddc93a2bfd.webp\" width=\"800\">\n",
    "\n",
    "Navigate to the **Settings** tab to configure compute resources.\n",
    "\n",
    "<img src=\"../../../assets/images/training_2026/day3/5c2bfc315a2148268f2c9bf313bbc928.webp\" width=\"800\">\n",
    "\n",
    "### Step 5: Configure Compute Resources\n",
    "\n",
    "Open **Compute** → **Edit compute configuration** to adjust cluster settings.\n",
    "\n",
    "<img src=\"../../../assets/images/training_2026/day3/7674419b96ca494d8c9e5c0ed9bc45fe.webp\" width=\"800\">\n",
    "\n",
    "Set **Cluster mode** to `Fixed size` with **Workers = 1** to minimize resource consumption within training quota limits.\n",
    "\n",
    "<img src=\"../../../assets/images/training_2026/day3/7980e9740c6b4ab99183520e23684aeb.webp\" width=\"800\">\n",
    "\n",
    "Under **Advanced settings**, select worker type **D4ds_v5** — optimized for the training environment's vCPU quota constraints.\n",
    "\n",
    "<img src=\"../../../assets/images/training_2026/day3/c410d04e1ddd4be99b52fbb35f124bac.webp\" width=\"800\">\n",
    "\n",
    "> **Note:** Training environments have strict vCPU quotas. Using a minimal fixed-size cluster ensures reliable execution without exceeding resource limits.\n",
    "\n",
    "Click **Save** and close the compute configuration dialog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Validate Pipeline — Dry Run\n",
    "\n",
    "Click **Dry Run** to validate the pipeline without processing data. This verifies all SQL/Python declarations, resolves table dependencies, and confirms resource availability.\n",
    "\n",
    "<img src=\"../../../assets/images/training_2026/day3/b06e4093070d4febababc2678f560ead.webp\" width=\"800\">\n",
    "\n",
    "> **Tip:** Initial cluster provisioning may take 5-8 minutes. This is expected for the first run — use this time for a break.\n",
    "\n",
    "After successful validation, the pipeline DAG is generated — showing all tables, dependencies, and data flow paths from source to gold layer.\n",
    "\n",
    "<img src=\"../../../assets/images/training_2026/day3/0ebbedde030b43e7bed35295000c42a9.webp\" width=\"800\">\n",
    "\n",
    "### Step 7: Execute Pipeline — Full Refresh\n",
    "\n",
    "With validation confirmed, click **Run Pipeline** → select **Full table refresh** to execute the complete pipeline end-to-end.\n",
    "\n",
    "<img src=\"../../../assets/images/training_2026/day3/075c2f87b48d4e38ab018495c631aaa7.webp\" width=\"800\">\n",
    "\n",
    "A successful execution shows all tables with **green status indicators** and a fully green DAG — confirming that data has been processed through the complete **Bronze → Silver → Gold** medallion architecture.\n",
    "\n",
    "<img src=\"../../../assets/images/training_2026/day3/044431e4088049faa10fb2aee4e9d84e.webp\" width=\"800\">\n",
    "\n",
    "> **Result:** The Lakeflow pipeline has been executed end-to-end — from raw source ingestion (Bronze) through validated transformations (Silver) to business-ready aggregations (Gold) — using the declarative framework with automatic dependency management and data quality enforcement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e44767ef-1141-44f4-907d-b6f8f7f7faf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "Key concepts and exam-relevant keywords covered in this module.\n",
    "\n",
    "---\n",
    "\n",
    "| Topic | Key Concept | Exam Keywords |\n",
    "|---|---|---|\n",
    "| **Medallion** | Bronze → Silver → Gold | Raw, Validated, Business-ready |\n",
    "| **SCD Type 1** | Overwrite, no history | `MERGE INTO ... UPDATE SET *` |\n",
    "| **SCD Type 2** | Track history | `__START_AT`, `__END_AT`, `AUTO CDC` |\n",
    "| **STREAMING TABLE** | Append-only, incremental | `CREATE STREAMING TABLE`, `STREAM()` |\n",
    "| **MATERIALIZED VIEW** | Full recomputation | `CREATE MATERIALIZED VIEW` |\n",
    "| **FLOW** | Separate source from table | `INSERT INTO ONCE` (backfill) |\n",
    "| **Expectations** | Data quality constraints | `DROP ROW`, `FAIL UPDATE`, warn-only |\n",
    "\n",
    "> **← M06: Advanced Transforms | Day 3 | M08: Orchestration →**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7d5a02d-5a0d-462e-84b5-72851dff60b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Resources\n",
    "\n",
    "- [Databricks Lakeflow Docs](https://docs.databricks.com/en/delta-live-tables/index.html)\n",
    "- [Medallion Architecture](https://www.databricks.com/glossary/medallion-architecture)\n",
    "- [SCD with Lakeflow](https://docs.databricks.com/en/delta-live-tables/cdc.html)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "M07_medallion_lakeflow",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
