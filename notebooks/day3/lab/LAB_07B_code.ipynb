{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23314a57",
   "metadata": {},
   "source": [
    "# LAB 07B: Orchestration — Multi-task Jobs with Triggers\n",
    "\n",
    "**Duration:** ~45 min | **Day:** 3 | **Difficulty:** Advanced  \n",
    "**After module:** M08: Orchestration & Lakeflow Jobs\n",
    "\n",
    "> *\"Build two Lakeflow Jobs that orchestrate medallion pipeline notebooks:  \n",
    "> Job A uses File Arrival trigger, Job B uses Table Update trigger.\"*\n",
    "\n",
    "**What you'll do:**\n",
    "1. Import medallion notebooks to your workspace\n",
    "2. Create **Job A**: orders pipeline with File Arrival trigger\n",
    "3. Create **Job B**: customer pipeline with Table Update trigger\n",
    "4. Trigger both jobs and verify results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da43cdd",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d969a96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../setup/00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b5fa6a",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1: Prepare Workspace\n",
    "\n",
    "Import the medallion notebooks from `materials/medallion/` into your Databricks workspace.\n",
    "\n",
    "**Steps:**\n",
    "1. In Databricks Workspace, create folder: `/Workspace/Users/<your-email>/medallion/`\n",
    "2. Import (or copy) the following notebooks:\n",
    "\n",
    "| Notebook | Layer | Purpose |\n",
    "|----------|-------|---------|\n",
    "| `bronze_orders.py` | Bronze | Batch JSON → Delta |\n",
    "| `silver_orders_cleaned.py` | Silver | Quality filters + computed columns |\n",
    "| `gold_daily_orders.py` | Gold | Daily aggregation |\n",
    "| `bronze_customers.py` | Bronze | Batch CSV → Delta |\n",
    "| `silver_customers.py` | Silver | Dedup + normalize |\n",
    "| `gold_customer_orders_summary.py` | Gold | Join + aggregate metrics |\n",
    "\n",
    "3. Also import: `materials/orchestration/task_validate_pipeline.py`\n",
    "\n",
    "<!-- INSTRUCTOR: Screenshot placeholder - workspace with imported notebooks -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0984d055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset files exist\n",
    "print(\"Orders files:\")\n",
    "for f in dbutils.fs.ls(f\"{DATASET_PATH}/orders/stream/\"):\n",
    "    print(f\"  {f.name} ({f.size} bytes)\")\n",
    "\n",
    "print(\"\\nCustomers files:\")\n",
    "for f in dbutils.fs.ls(f\"{DATASET_PATH}/customers/\"):\n",
    "    print(f\"  {f.name} ({f.size} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219a3e64",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Create Job A — Orders Pipeline (File Arrival)\n",
    "\n",
    "Create a multi-task Job that processes orders through the medallion layers.\n",
    "\n",
    "**Job configuration:**\n",
    "\n",
    "| Setting | Value |\n",
    "|---------|-------|\n",
    "| Job name | `LAB_Orders_Pipeline` |\n",
    "| Cluster | Serverless or new Job cluster |\n",
    "\n",
    "**Tasks (in order):**\n",
    "\n",
    "| # | Task name | Notebook | Depends on | Parameters |\n",
    "|---|-----------|----------|------------|------------|\n",
    "| 1 | `bronze_orders` | `medallion/bronze_orders` | — | `catalog`, `schema=bronze`, `source_path` |\n",
    "| 2 | `silver_orders` | `medallion/silver_orders_cleaned` | `bronze_orders` | `catalog`, `schema_bronze=bronze`, `schema_silver=silver` |\n",
    "| 3 | `gold_daily` | `medallion/gold_daily_orders` | `silver_orders` | `catalog`, `schema_silver=silver`, `schema_gold=gold` |\n",
    "\n",
    "**Trigger:**\n",
    "- Type: **File Arrival**\n",
    "- URL: `/Volumes/<catalog>/default/landing_zone/trigger`\n",
    "- Min time between triggers: `60s`\n",
    "- Wait after last change: `15s`\n",
    "\n",
    "<!-- INSTRUCTOR: Screenshot placeholder - Job A DAG view -->\n",
    "<!-- INSTRUCTOR: Screenshot placeholder - File Arrival trigger config -->\n",
    "\n",
    "**Steps:**\n",
    "- [ ] Workflows → Create Job\n",
    "- [ ] Add 3 tasks with dependencies (DAG)\n",
    "- [ ] Set parameters for each task\n",
    "- [ ] Add File Arrival trigger\n",
    "- [ ] **Do NOT run yet** — we'll trigger it in Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a5f264",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3: Trigger Job A — Create Signal File\n",
    "\n",
    "Job A is configured with File Arrival trigger. To start it, create a file in the monitored Volume path.\n",
    "\n",
    "**Fill in** the Volume path and run the cell to create a signal file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc812fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid, json\n",
    "from datetime import datetime\n",
    "\n",
    "# TODO: Set your Volume path (same as configured in Job A trigger)\n",
    "volume_path = f\"/Volumes/{CATALOG}/________/landing_zone/trigger\"  # Fill schema\n",
    "\n",
    "# Create signal file\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "file_id = str(uuid.uuid4())[:8]\n",
    "file_path = f\"{volume_path}/signal_{timestamp}_{file_id}.json\"\n",
    "\n",
    "signal = {\"event\": \"data_ready\", \"timestamp\": timestamp, \"id\": file_id}\n",
    "\n",
    "dbutils.fs.mkdirs(volume_path)\n",
    "dbutils.fs.put(file_path, json.dumps(signal), overwrite=True)\n",
    "\n",
    "print(f\"Signal file created: {file_path}\")\n",
    "print(f\"Job A should trigger within ~60 seconds!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d59a9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation: file was created\n",
    "files = dbutils.fs.ls(volume_path)\n",
    "assert len(files) > 0, \"No files found in trigger path\"\n",
    "print(f\"Task 3 OK: {len(files)} file(s) in {volume_path}\")\n",
    "for f in files:\n",
    "    print(f\"  {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5363eb",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 4: Create Job B — Customer Pipeline (Table Update)\n",
    "\n",
    "Create a second Job that processes customers and joins with orders.\n",
    "This Job triggers when **silver_orders_cleaned** table is updated (by Job A).\n",
    "\n",
    "**Job configuration:**\n",
    "\n",
    "| Setting | Value |\n",
    "|---------|-------|\n",
    "| Job name | `LAB_Customer_Pipeline` |\n",
    "| Cluster | Serverless or new Job cluster |\n",
    "\n",
    "**Tasks (in order):**\n",
    "\n",
    "| # | Task name | Notebook | Depends on | Parameters |\n",
    "|---|-----------|----------|------------|------------|\n",
    "| 1 | `bronze_customers` | `medallion/bronze_customers` | — | `catalog`, `schema=bronze`, `source_path` |\n",
    "| 2 | `silver_customers` | `medallion/silver_customers` | `bronze_customers` | `catalog`, `schema_bronze=bronze`, `schema_silver=silver` |\n",
    "| 3 | `gold_summary` | `medallion/gold_customer_orders_summary` | `silver_customers` | `catalog`, `schema_silver=silver`, `schema_gold=gold` |\n",
    "| 4 | `validate` | `orchestration/task_validate_pipeline` | `gold_summary` | `catalog`, all schemas, `job_run_id={{run.id}}` |\n",
    "\n",
    "**Trigger:**\n",
    "- Type: **Table updated**\n",
    "- Table: `<catalog>.<schema>.silver_orders_cleaned`\n",
    "- Condition: Any new rows\n",
    "- Min time between triggers: `60s`\n",
    "\n",
    "<!-- INSTRUCTOR: Screenshot placeholder - Job B DAG view -->\n",
    "<!-- INSTRUCTOR: Screenshot placeholder - Table Update trigger config -->\n",
    "\n",
    "**Steps:**\n",
    "- [ ] Workflows → Create Job\n",
    "- [ ] Add 4 tasks with dependencies (DAG)\n",
    "- [ ] Set parameters — use `{{job.parameters.catalog}}` for dynamic catalog\n",
    "- [ ] Add **Table updated** trigger → point to `silver_orders_cleaned`\n",
    "- [ ] Save Job (it will trigger automatically when Job A updates silver_orders_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65070a7f",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 5: Verify Pipeline Execution\n",
    "\n",
    "After both jobs complete, verify the results.\n",
    "\n",
    "**Wait for Job A to complete** (check Workflows UI), then check tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f831a861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uncomment and run after jobs complete\n",
    "\n",
    "# -- Check bronze layer\n",
    "# print(\"Bronze Orders:\", spark.table(f\"{CATALOG}.bronze.bronze_orders\").count())\n",
    "# print(\"Bronze Customers:\", spark.table(f\"{CATALOG}.bronze.bronze_customers\").count())\n",
    "\n",
    "# -- Check silver layer\n",
    "# print(\"Silver Orders:\", spark.table(f\"{CATALOG}.silver.silver_orders_cleaned\").count())\n",
    "# print(\"Silver Customers:\", spark.table(f\"{CATALOG}.silver.silver_customers\").count())\n",
    "\n",
    "# -- Check gold layer\n",
    "# print(\"Gold Daily Orders:\", spark.table(f\"{CATALOG}.gold.gold_daily_orders\").count())\n",
    "# print(\"Gold Customer Summary:\", spark.table(f\"{CATALOG}.gold.gold_customer_orders_summary\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61345b59",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 6: Check Event Log\n",
    "\n",
    "The validation task logs results to `pipeline_event_log`. Check if it recorded success:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e625f1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uncomment and run after validation task completes\n",
    "\n",
    "# display(spark.sql(f\"\"\"\n",
    "#     SELECT event_id, event_timestamp, job_run_id, status, details\n",
    "#     FROM {CATALOG}.default.pipeline_event_log\n",
    "#     WHERE event_type = 'PIPELINE_VALIDATION'\n",
    "#     ORDER BY event_timestamp DESC\n",
    "#     LIMIT 5\n",
    "# \"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecba5a4e",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 7: Cross-Job Orchestration Pattern\n",
    "\n",
    "Fill in the blanks to describe the trigger chain:\n",
    "\n",
    "```\n",
    "Signal file → ________ trigger → Job A runs → writes silver_orders_cleaned\n",
    "                                                         ↓\n",
    "                                              ________ trigger → Job B runs → validates all tables\n",
    "```\n",
    "\n",
    "**Questions:**\n",
    "1. What happens if Job A fails at `silver_orders` task? Does Job B trigger?\n",
    "2. What is the advantage of Repair Runs when a task fails?\n",
    "3. How would you add an email alert for Job B failures?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6d38b8",
   "metadata": {},
   "source": [
    "---\n",
    "## Lab Complete!\n",
    "\n",
    "### What you've learned:\n",
    "- Creating multi-task Jobs with task dependencies (DAG)\n",
    "- Configuring **File Arrival** triggers on UC Volumes\n",
    "- Configuring **Table Update** triggers for cross-job orchestration\n",
    "- Using a validation task with `pipeline_event_log` for monitoring\n",
    "- Passing parameters with Job-level variables and `{{run.id}}`\n",
    "\n",
    "### Exam Tips:\n",
    "- **File Arrival trigger** monitors a cloud storage path or UC Volume for new files\n",
    "- **Table Update trigger** fires when a Delta table receives new rows (`inserted_count > 0`)\n",
    "- **Repair Runs** re-execute only failed and downstream tasks, skipping successful ones\n",
    "- **`dbutils.notebook.exit()`** returns a JSON result that can be read by downstream tasks via `taskValues`\n",
    "- **`max_concurrent_runs: 1`** prevents duplicate runs when triggers fire rapidly\n",
    "- Jobs can use **Serverless compute** or dedicated **Job clusters** (not All-Purpose clusters)\n",
    "\n",
    "### Trigger Comparison:\n",
    "\n",
    "| Trigger | Fires when | Best for | Config key |\n",
    "|---------|-----------|----------|------------|\n",
    "| **Scheduled** | CRON time reached | Regular ETL | `cron_expression` |\n",
    "| **File Arrival** | New file in path | Event-driven ingestion | `file_arrival.url` |\n",
    "| **Table Update** | Table DML detected | Cross-pipeline chains | `table.table_name` |\n",
    "| **Continuous** | Always running | Near-real-time | `continuous` |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9a5965",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad41929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete trigger files\n",
    "# dbutils.fs.rm(f\"/Volumes/{CATALOG}/default/landing_zone/trigger\", recurse=True)\n",
    "\n",
    "# Delete both Jobs from Workflows UI\n",
    "print(\"LAB 07B complete. Delete Jobs from Workflows UI when done.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
