{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "893a2944-bafa-4238-8278-1b640a10563f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# LAB 07B: Orchestration — Multi-task Jobs with Triggers\n",
    "\n",
    "**Duration:** ~45 min | **Day:** 3 | **Difficulty:** Advanced  \n",
    "**After module:** M08: Orchestration & Lakeflow Jobs\n",
    "\n",
    "> *\"Build two Lakeflow Jobs that orchestrate medallion pipeline notebooks:  \n",
    "> Job A uses File Arrival trigger, Job B uses Table Update trigger.\"*\n",
    "\n",
    "**What you'll do:**\n",
    "1. Import medallion notebooks to your workspace\n",
    "2. Create **Job A**: orders pipeline with File Arrival trigger\n",
    "3. Create **Job B**: customer pipeline with Table Update trigger\n",
    "4. Trigger both jobs and verify results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0db01a24-db32-445a-b740-13a7d28e827b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "384a320f-18df-43c4-bd89-5e79535dc7bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../setup/00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67119128-6afd-4735-b917-d3094773b2ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Task 1: Prepare Workspace\n",
    "\n",
    "Import the medallion notebooks from `lab/materials/medallion/` into your Databricks workspace.\n",
    "\n",
    "**Steps:**\n",
    "1. In Databricks Workspace, create folder: `/Workspace/Users/<your-email>/medallion/`\n",
    "2. Import (or copy) the following notebooks:\n",
    "\n",
    "| Notebook | Layer | Purpose |\n",
    "|----------|-------|---------|\n",
    "| `bronze_orders.ipynb` | Bronze | Batch JSON → Delta |\n",
    "| `silver_orders_cleaned.ipynb` | Silver | Quality filters + computed columns |\n",
    "| `gold_daily_orders.ipynb` | Gold | Daily aggregation |\n",
    "| `bronze_customers.ipynb` | Bronze | Batch CSV → Delta |\n",
    "| `silver_customers.ipynb` | Silver | Dedup + normalize |\n",
    "| `gold_customer_orders_summary.ipynb` | Gold | Join + aggregate metrics |\n",
    "\n",
    "3. Also import: `lab/materials/orchestration/task_validate_pipeline.py`\n",
    "\n",
    "<!-- INSTRUCTOR: Screenshot placeholder - workspace with imported notebooks -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d2e8314-f949-48ab-a993-7768af0aba67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify dataset files exist\n",
    "print(\"Orders files:\")\n",
    "for f in dbutils.fs.ls(f\"{DATASET_PATH}/orders/stream/\"):\n",
    "    print(f\"  {f.name} ({f.size} bytes)\")\n",
    "\n",
    "print(\"\\nCustomers files:\")\n",
    "for f in dbutils.fs.ls(f\"{DATASET_PATH}/customers/\"):\n",
    "    print(f\"  {f.name} ({f.size} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4843f78b-6dd0-4204-a62e-9a802ae19f58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Task 2: Create Job A — Orders Pipeline (File Arrival)\n",
    "\n",
    "Create a multi-task Job that processes orders through the medallion layers.\n",
    "\n",
    "**Job configuration:**\n",
    "\n",
    "| Setting | Value |\n",
    "|---------|-------|\n",
    "| Job name | `LAB_Orders_Pipeline` |\n",
    "| Cluster | Serverless or new Job cluster |\n",
    "\n",
    "**Tasks (in order):**\n",
    "\n",
    "| # | Task name | Notebook | Depends on | Parameters |\n",
    "|---|-----------|----------|------------|------------|\n",
    "| 1 | `bronze_orders` | `medallion/bronze_orders` | — | `catalog`, `schema=bronze`, `source_path` |\n",
    "| 2 | `silver_orders` | `medallion/silver_orders_cleaned` | `bronze_orders` | `catalog`, `schema_bronze=bronze`, `schema_silver=silver` |\n",
    "| 3 | `gold_daily` | `medallion/gold_daily_orders` | `silver_orders` | `catalog`, `schema_silver=silver`, `schema_gold=gold` |\n",
    "\n",
    "**Trigger:**\n",
    "- Type: **File Arrival**\n",
    "- URL: `/Volumes/<catalog>/default/landing_zone/trigger`\n",
    "- Min time between triggers: `60s`\n",
    "- Wait after last change: `15s`\n",
    "\n",
    "<!-- INSTRUCTOR: Screenshot placeholder - Job A DAG view -->\n",
    "<!-- INSTRUCTOR: Screenshot placeholder - File Arrival trigger config -->\n",
    "\n",
    "**Steps:**\n",
    "- [ ] Workflows → Create Job\n",
    "- [ ] Add 3 tasks with dependencies (DAG)\n",
    "- [ ] Set parameters for each task\n",
    "- [ ] Add File Arrival trigger\n",
    "- [ ] **Do NOT run yet** — we'll trigger it in Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a319843b-d895-4dbe-92af-1c2223f9fc6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Task 3: Trigger Job A — Create Signal File\n",
    "\n",
    "Job A is configured with File Arrival trigger. To start it, create a file in the monitored Volume path.\n",
    "\n",
    "**Fill in** the Volume path and run the cell to create a signal file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97187b03-4486-4f8a-a425-4271a00a0665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid, json\n",
    "from datetime import datetime\n",
    "\n",
    "# TODO: Set your Volume path (same as configured in Job A trigger)\n",
    "volume_path = f\"/Volumes/{CATALOG}/________/landing_zone/trigger\"  # Fill schema\n",
    "\n",
    "# Create signal file\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "file_id = str(uuid.uuid4())[:8]\n",
    "file_path = f\"{volume_path}/signal_{timestamp}_{file_id}.json\"\n",
    "\n",
    "signal = {\"event\": \"data_ready\", \"timestamp\": timestamp, \"id\": file_id}\n",
    "\n",
    "dbutils.fs.mkdirs(volume_path)\n",
    "dbutils.fs.put(file_path, json.dumps(signal), overwrite=True)\n",
    "\n",
    "print(f\"Signal file created: {file_path}\")\n",
    "print(f\"Job A should trigger within ~60 seconds!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc5731df-c03d-40d8-858e-1b53365418bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validation: file was created\n",
    "files = dbutils.fs.ls(volume_path)\n",
    "assert len(files) > 0, \"No files found in trigger path\"\n",
    "print(f\"Task 3 OK: {len(files)} file(s) in {volume_path}\")\n",
    "for f in files:\n",
    "    print(f\"  {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65088726-1611-4f23-b976-68907191dede",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Task 4: Create Job B — Customer Pipeline (Table Update)\n",
    "\n",
    "Create a second Job that processes customers and joins with orders.\n",
    "This Job triggers when **silver_orders_cleaned** table is updated (by Job A).\n",
    "\n",
    "**Job configuration:**\n",
    "\n",
    "| Setting | Value |\n",
    "|---------|-------|\n",
    "| Job name | `LAB_Customer_Pipeline` |\n",
    "| Cluster | Serverless or new Job cluster |\n",
    "\n",
    "**Tasks (in order):**\n",
    "\n",
    "| # | Task name | Notebook | Depends on | Parameters |\n",
    "|---|-----------|----------|------------|------------|\n",
    "| 1 | `bronze_customers` | `medallion/bronze_customers` | — | `catalog`, `schema=bronze`, `source_path` |\n",
    "| 2 | `silver_customers` | `medallion/silver_customers` | `bronze_customers` | `catalog`, `schema_bronze=bronze`, `schema_silver=silver` |\n",
    "| 3 | `gold_summary` | `medallion/gold_customer_orders_summary` | `silver_customers` | `catalog`, `schema_silver=silver`, `schema_gold=gold` |\n",
    "| 4 | `validate` | `orchestration/task_validate_pipeline` | `gold_summary` | `catalog`, all schemas, `job_run_id={{run.id}}` |\n",
    "\n",
    "**Trigger:**\n",
    "- Type: **Table updated**\n",
    "- Table: `<catalog>.<schema>.silver_orders_cleaned`\n",
    "- Condition: Any new rows\n",
    "- Min time between triggers: `60s`\n",
    "\n",
    "<!-- INSTRUCTOR: Screenshot placeholder - Job B DAG view -->\n",
    "<!-- INSTRUCTOR: Screenshot placeholder - Table Update trigger config -->\n",
    "\n",
    "**Steps:**\n",
    "- [ ] Workflows → Create Job\n",
    "- [ ] Add 4 tasks with dependencies (DAG)\n",
    "- [ ] Set parameters — use `{{job.parameters.catalog}}` for dynamic catalog\n",
    "- [ ] Add **Table updated** trigger → point to `silver_orders_cleaned`\n",
    "- [ ] Save Job (it will trigger automatically when Job A updates silver_orders_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "382e3259-8c79-42a0-b0e8-ec5ffbebef11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Task 5: Verify Pipeline Execution\n",
    "\n",
    "After both jobs complete, verify the results.\n",
    "\n",
    "**Wait for Job A to complete** (check Workflows UI), then check tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6ceb208-07a3-4185-b31a-7f29a4255273",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment and run after jobs complete\n",
    "\n",
    "# -- Check bronze layer\n",
    "# print(\"Bronze Orders:\", spark.table(f\"{CATALOG}.bronze.bronze_orders\").count())\n",
    "# print(\"Bronze Customers:\", spark.table(f\"{CATALOG}.bronze.bronze_customers\").count())\n",
    "\n",
    "# -- Check silver layer\n",
    "# print(\"Silver Orders:\", spark.table(f\"{CATALOG}.silver.silver_orders_cleaned\").count())\n",
    "# print(\"Silver Customers:\", spark.table(f\"{CATALOG}.silver.silver_customers\").count())\n",
    "\n",
    "# -- Check gold layer\n",
    "# print(\"Gold Daily Orders:\", spark.table(f\"{CATALOG}.gold.gold_daily_orders\").count())\n",
    "# print(\"Gold Customer Summary:\", spark.table(f\"{CATALOG}.gold.gold_customer_orders_summary\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c74a8592-c896-4c60-b7a4-6fb5c2b883a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Task 6: Check Event Log\n",
    "\n",
    "The validation task logs results to `pipeline_event_log`. Check if it recorded success:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "beed8d65-25ac-4f4e-aa82-c8ac900527b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Uncomment and run after validation task completes\n",
    "\n",
    "# display(spark.sql(f\"\"\"\n",
    "#     SELECT event_id, event_timestamp, job_run_id, status, details\n",
    "#     FROM {CATALOG}.default.pipeline_event_log\n",
    "#     WHERE event_type = 'PIPELINE_VALIDATION'\n",
    "#     ORDER BY event_timestamp DESC\n",
    "#     LIMIT 5\n",
    "# \"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23684884-dedc-42a7-9846-124f7466c6f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Task 7: Cross-Job Orchestration Pattern\n",
    "\n",
    "Fill in the blanks to describe the trigger chain:\n",
    "\n",
    "```\n",
    "Signal file → ________ trigger → Job A runs → writes silver_orders_cleaned\n",
    "                                                         ↓\n",
    "                                              ________ trigger → Job B runs → validates all tables\n",
    "```\n",
    "\n",
    "**Questions:**\n",
    "1. What happens if Job A fails at `silver_orders` task? Does Job B trigger?\n",
    "2. What is the advantage of Repair Runs when a task fails?\n",
    "3. How would you add an email alert for Job B failures?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "380a65ec-96e2-4f2c-834b-c03a0c87a83d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Lab Complete!\n",
    "\n",
    "### What you've learned:\n",
    "- Creating multi-task Jobs with task dependencies (DAG)\n",
    "- Configuring **File Arrival** triggers on UC Volumes\n",
    "- Configuring **Table Update** triggers for cross-job orchestration\n",
    "- Using a validation task with `pipeline_event_log` for monitoring\n",
    "- Passing parameters with Job-level variables and `{{run.id}}`\n",
    "\n",
    "### Exam Tips:\n",
    "- **File Arrival trigger** monitors a cloud storage path or UC Volume for new files\n",
    "- **Table Update trigger** fires when a Delta table receives new rows (`inserted_count > 0`)\n",
    "- **Repair Runs** re-execute only failed and downstream tasks, skipping successful ones\n",
    "- **`dbutils.notebook.exit()`** returns a JSON result that can be read by downstream tasks via `taskValues`\n",
    "- **`max_concurrent_runs: 1`** prevents duplicate runs when triggers fire rapidly\n",
    "- Jobs can use **Serverless compute** or dedicated **Job clusters** (not All-Purpose clusters)\n",
    "\n",
    "### Trigger Comparison:\n",
    "\n",
    "| Trigger | Fires when | Best for | Config key |\n",
    "|---------|-----------|----------|------------|\n",
    "| **Scheduled** | CRON time reached | Regular ETL | `cron_expression` |\n",
    "| **File Arrival** | New file in path | Event-driven ingestion | `file_arrival.url` |\n",
    "| **Table Update** | Table DML detected | Cross-pipeline chains | `table.table_name` |\n",
    "| **Continuous** | Always running | Near-real-time | `continuous` |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b257b67d-c1df-4c9c-9d1b-4ffd1372049a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd01dcb4-ec88-40d2-8c96-fe854b823c13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Delete trigger files\n",
    "# dbutils.fs.rm(f\"/Volumes/{CATALOG}/default/landing_zone/trigger\", recurse=True)\n",
    "\n",
    "# Delete both Jobs from Workflows UI\n",
    "print(\"LAB 07B complete. Delete Jobs from Workflows UI when done.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LAB_07B_code",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
