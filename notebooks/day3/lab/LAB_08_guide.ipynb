{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 08: Lakeflow Jobs -- Triggers, Dependencies & Orchestration\n\n**Duration:** ~30 min  \n**Day:** 3  \n**After module:** M08: Lakeflow Jobs & Orchestration  \n**Difficulty:** Intermediate\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario\n\n> *\"The RetailHub Medallion pipeline (LAB 07) is running successfully on-demand. Now it's time to automate it. The business requires: (1) daily scheduled refresh of the pipeline, (2) a validation task that runs AFTER the pipeline completes, (3) an email alert if any task fails. You will configure a multi-task Job with triggers and dependencies in the Databricks UI, then explore the configuration via notebook code.\"*\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n\nAfter completing this lab you will be able to:\n- Create a multi-task Lakeflow Job with task dependencies\n- Configure different trigger types (scheduled/cron, file arrival, continuous)\n- Define task dependency chains (linear and fan-out/fan-in patterns)\n- Set up retry policies and failure notifications\n- Use `dbutils.jobs` to pass parameters between tasks\n- Query system tables to monitor job runs programmatically\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n\n- Completed LAB 07 (Lakeflow Pipeline is created in workspace)\n- Access to Databricks workspace with Job creation permissions\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Job Architecture (~5 min)\n\n### Task 1: Review Job Components\n\nBefore creating a Job in the UI, review these key components:\n\n| Component | Description | Exam Relevance |\n|-----------|-------------|----------------|\n| **Task** | A single unit of work (notebook, DLT pipeline, SQL, JAR, Python script) | Know all task types |\n| **Dependency** | Defines execution order between tasks (task B runs after task A) | DAG structure |\n| **Trigger** | When the job starts (manual, scheduled, file arrival, continuous) | Trigger types and use cases |\n| **Cluster** | Each task can use its own cluster or share one (job cluster vs all-purpose) | Cost optimization |\n| **Retry Policy** | How many times to retry a failed task before marking it as failed | Resilience |\n| **Timeout** | Maximum duration before a task is killed | Resource protection |\n\n### Job Dependency Patterns\n\n```\nLINEAR:                    FAN-OUT / FAN-IN:\n                          \n  ┌──────────┐               ┌──────────┐\n  │ Validate │               │ Ingest   │\n  └────┬─────┘               └──┬───┬───┘\n       │                        │   │\n  ┌────▼─────┐            ┌────▼┐ ┌▼────┐\n  │Transform │            │ DIM │ │FACT │\n  └────┬─────┘            └──┬──┘ └──┬──┘\n       │                     │       │\n  ┌────▼─────┐            ┌──▼──────▼──┐\n  │  Report  │            │   Report   │\n  └──────────┘            └────────────┘\n```\n\n> **Exam Tip:** In fan-out/fan-in pattern, the Report task has **two dependencies** (DIM and FACT). It starts only when **both** complete successfully. This is how Databricks handles parallel execution within jobs.\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Create Multi-Task Job in UI (~10 min)\n\n### Task 2: Build the RetailHub Daily Job\n\n**Open the Databricks UI and follow these steps:**\n\n1. **Navigate:** Workflows > Jobs > Create Job\n\n2. **Task 1 -- Pipeline Refresh:**\n   - Task name: `refresh_pipeline`\n   - Type: **Lakeflow pipeline task**\n   - Pipeline: Select your RetailHub pipeline from LAB 07\n   - Cluster: Job cluster (Single Node, smallest available)\n\n3. **Task 2 -- Validate Results:**\n   - Task name: `validate_results`\n   - Type: **Notebook task**\n   - Notebook: `task_01_validate.ipynb` (from the training setup)\n   - **Depends on:** `refresh_pipeline`\n   - Cluster: Same job cluster\n\n4. **Task 3 -- Generate Report:**\n   - Task name: `generate_report`\n   - Type: **Notebook task**\n   - Notebook: `task_03_report.ipynb`\n   - **Depends on:** `validate_results`\n   - Cluster: Same job cluster\n\n> **Exam Tip:** A **Job cluster** is created when the Job starts and terminated when it ends. It's cheaper than an all-purpose cluster for scheduled jobs. Multiple tasks can share the same Job cluster.\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Configure Triggers (~5 min)\n\n### Task 3: Set Up Scheduled Trigger\n\nConfigure a CRON-based trigger for daily execution:\n\n1. In Job settings, click **Add trigger**\n2. Select **Scheduled**\n3. Set the CRON expression: `0 6 * * *` (daily at 06:00 UTC)\n4. Timezone: Select your timezone\n\n**Common CRON patterns for reference:**\n\n| Pattern | CRON Expression | Use Case |\n|---------|----------------|----------|\n| Every day at 6 AM | `0 6 * * *` | Daily batch refresh |\n| Every hour | `0 * * * *` | Near-real-time updates |\n| Mon-Fri at 8 AM | `0 8 * * 1-5` | Business day reports |\n| Every 15 min | `*/15 * * * *` | Frequent incremental loads |\n| First day of month | `0 0 1 * *` | Monthly aggregations |\n\n### Task 4: Explore Other Trigger Types\n\nReview (do NOT configure) these additional trigger types:\n\n| Trigger Type | When to Use | Configuration |\n|-------------|------------|---------------|\n| **Manual** | Ad-hoc runs, testing | Default, no trigger needed |\n| **Scheduled (CRON)** | Regular batch processing | CRON expression + timezone |\n| **File arrival** | Event-driven: run when new files land in a Volume/cloud path | Path to monitor + wait time |\n| **Continuous** | Always-on processing, minimal latency | Restart on completion, with optional pause |\n\n> **Exam Tip:** **File arrival trigger** monitors a cloud storage path. When new files appear, the job starts. This is ideal for event-driven architectures. The trigger checks for files at configurable intervals (default: 5 min). Continuous trigger restarts the job immediately after completion -- used for streaming-like behavior with notebook tasks.\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Dependencies & Error Handling (~5 min)\n\n### Task 5: Configure Retry and Alerting\n\nIn the Job settings:\n\n1. **Retry Policy:**\n   - For `validate_results` task: Set max retries to **2**, retry interval **60 seconds**\n   - Rationale: Validation may fail due to transient Delta table issues\n\n2. **Timeout:**\n   - Set `refresh_pipeline` timeout to **30 minutes**\n   - If the pipeline doesn't finish in 30 min, something is wrong\n\n3. **Notifications (Email/Webhook):**\n   - Add notification on **Failure**: Your email\n   - Add notification on **Success**: Optional (for monitoring)\n\n### Task 6: Test Run and Repair\n\n1. **Run the Job manually** (click \"Run now\")\n2. **Observe the DAG** in the Runs tab -- watch tasks execute in dependency order\n3. If a task fails:\n   - Click the failed run\n   - Click **Repair run** -- this re-runs ONLY the failed task and downstream dependencies\n   - Saves time vs. re-running the entire job\n\n> **Exam Tip:** **Repair run** is a key exam concept. It re-executes only failed and downstream tasks, preserving successful results. This is critical for long-running pipelines where re-running everything would be wasteful.\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Programmatic Job Monitoring (~5 min)\n\n### Task 7: Query System Tables\n\nOpen `LAB_08_code.ipynb` and complete the tasks there. You will:\n- Query `system.lakeflow.job_run_timeline` to see job execution history\n- Query `system.lakeflow.job_task_run_timeline` for task-level details\n- Analyze job duration trends and failure rates\n- Use `dbutils.jobs.taskValues` to pass data between tasks\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n\nAfter completing this lab you have:\n- Created a multi-task Job with linear task dependencies\n- Configured a CRON scheduled trigger for daily execution\n- Understood file arrival and continuous trigger types\n- Set up retry policies and email notifications\n- Used Repair Run to re-execute failed tasks efficiently\n- Queried system tables for job monitoring\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next: LAB 09\n\nNext you will apply governance and security controls: permissions, row filters, column masking, and Delta Sharing.\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n\nOpen **`LAB_08_code.ipynb`** and complete all `# TODO` cells.  \nEach task has an `assert` cell to verify your solution."
   ]
  }
 ]
}