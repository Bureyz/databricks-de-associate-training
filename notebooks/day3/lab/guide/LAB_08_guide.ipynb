{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 08: Lakeflow Jobs — Triggers, Dependencies & Orchestration\n",
    "\n",
    "**Duration:** ~30 min  \n",
    "**Day:** 3  \n",
    "**After module:** M08: Lakeflow Jobs & Orchestration  \n",
    "**Difficulty:** Intermediate\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario\n",
    "\n",
    "> *\"Orchestrate your Medallion pipeline by creating multi-task Databricks Jobs with dependencies, triggers, and monitoring. First build a pipeline in the UI (Workshop), then solve orchestration concept questions (Practice).\"*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "- Create and configure multi-task Jobs in Databricks UI\n",
    "- Set task dependencies (DAG structure)\n",
    "- Configure Table Update Triggers for event-driven orchestration\n",
    "- Understand Job configuration JSON\n",
    "- Write CRON expressions for scheduling\n",
    "- Design DAGs with fan-out/fan-in patterns\n",
    "- Use Repair Runs for partial re-execution\n",
    "- Choose between Job Clusters and All-Purpose Clusters\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Cluster running and attached to notebook\n",
    "- Medallion notebooks from `materials/medallion/` available\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Workshop — Creating & Running Jobs in Databricks UI\n",
    "\n",
    "Follow the trainer's instructions step by step.\n",
    "\n",
    "### Step 1: Create a New Job\n",
    "- Go to **Workflows → Jobs → Create Job**\n",
    "\n",
    "### Step 2: Set the Job Name\n",
    "- Name it `<your_name>_customer_pipeline`\n",
    "\n",
    "### Step 3: Configure Job Parameters\n",
    "- `catalog` = `retailhub_<your_name>`\n",
    "- `source_path` = `/Volumes/retailhub_<your_name>/default/datasets`\n",
    "\n",
    "### Step 4: Add First Task — `bronze_customer`\n",
    "- Type: **Notebook** task\n",
    "- Source: `materials/medallion/bronze_customers`\n",
    "- No dependencies (first task)\n",
    "\n",
    "### Step 5: Add Second Task — `silver_customer`\n",
    "- Type: **Notebook** task\n",
    "- Source: `materials/medallion/silver_customers`\n",
    "- **Depends on:** `bronze_customer`\n",
    "\n",
    "### Step 6: Create Second Job — `orders_pipeline`\n",
    "- 4 tasks with dependencies:\n",
    "  - `bronze_orders` → `silver_orders` → `gold_daily_orders` → `gold_summary`\n",
    "\n",
    "### Step 7: Configure Table Update Trigger\n",
    "- Set `orders_pipeline` to trigger automatically when `silver_customers` table is updated\n",
    "\n",
    "### Step 8: Run the Pipeline\n",
    "1. Run `customer_pipeline` first\n",
    "2. `orders_pipeline` should trigger automatically via Table Update Trigger\n",
    "3. Monitor execution in the **Runs** tab\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Practice — Orchestration Concepts\n",
    "\n",
    "Open **`LAB_08_code.ipynb`** and complete the `# TODO` cells.\n",
    "\n",
    "| Task | What to do | Key concept |\n",
    "|------|-----------|-------------|\n",
    "| **Task 1** | Job Configuration JSON | Read config and extract: num_tasks, first_task, timeout, retries |\n",
    "| **Task 2** | Trigger Types | Match scenarios to triggers: scheduled, file_arrival, continuous, manual |\n",
    "| **Task 3** | CRON Expressions | Write CRON for: daily 6AM, hourly, weekdays 8AM, every 15min |\n",
    "| **Task 4** | DAG Design | Define task dependencies (fan-out / fan-in pattern) |\n",
    "| **Task 5** | Repair Runs | Identify which tasks re-run after `build_fact_tables` failure |\n",
    "| **Task 6** | Task Values | `dbutils.jobs.taskValues` — set and get values between tasks |\n",
    "| **Task 7** | System Tables SQL | Query `system.lakeflow.job_run_timeline` for FAILED runs |\n",
    "| **Task 8** | Cluster Selection | Choose job_cluster vs all_purpose for different scenarios |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Hints\n",
    "\n",
    "### Task 1: Job Config JSON\n",
    "- Count the tasks in the `tasks` array\n",
    "- First task = task with empty `depends_on` list\n",
    "- Timeout and retries are in the task-level configuration\n",
    "\n",
    "### Task 2: Trigger Types\n",
    "- `scheduled` — runs on a CRON schedule\n",
    "- `file_arrival` — triggers when new files land in a path\n",
    "- `continuous` — runs continuously (streaming)\n",
    "- `manual` — ad hoc, run by user\n",
    "\n",
    "### Task 3: CRON\n",
    "- Format: `minute hour day_of_month month day_of_week`\n",
    "- `0 6 * * *` = daily at 6:00 AM\n",
    "- `0 * * * *` = every hour at :00\n",
    "- `1-5` = Monday through Friday\n",
    "- `*/15` = every 15 minutes\n",
    "\n",
    "### Task 4: DAG Dependencies\n",
    "- `ingest` has no dependencies (empty list)\n",
    "- `build_dim_tables` and `build_fact_tables` both depend on `ingest` (fan-out)\n",
    "- `generate_report` depends on both dim and fact (fan-in)\n",
    "- `send_notification` depends on `generate_report`\n",
    "\n",
    "### Task 5: Repair Runs\n",
    "- Repair Run re-executes the failed task and all downstream tasks\n",
    "- Successful upstream tasks (ingest, dim) are NOT re-run → saves cost\n",
    "\n",
    "### Task 6: Task Values\n",
    "- `dbutils.jobs.taskValues.set(key, value)` in source task\n",
    "- `dbutils.jobs.taskValues.get(taskName, key)` in downstream task\n",
    "\n",
    "### Task 7: System Tables\n",
    "- Table: `system.lakeflow.job_run_timeline`\n",
    "- Filter: `result_state = 'FAILED'`\n",
    "- Date filter: `start_time >= current_date() - INTERVAL 7 DAYS`\n",
    "\n",
    "### Task 8: Cluster Selection\n",
    "- **Job cluster**: cheaper, auto-terminates, for production ETL + ML training\n",
    "- **All-purpose cluster**: interactive, for development + ad-hoc analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab you:\n",
    "- Built multi-task Jobs with dependencies in the Databricks UI\n",
    "- Configured Table Update Triggers for event-driven orchestration\n",
    "- Practiced CRON expressions and trigger types\n",
    "- Designed DAGs with fan-out/fan-in patterns\n",
    "- Explored Repair Runs for cost-efficient re-execution\n",
    "- Used Task Values for inter-task communication\n",
    "- Queried System Tables for job monitoring\n",
    "\n",
    "> **Exam Tip:** Repair Runs only re-execute the failed task and its downstream dependencies — upstream tasks are skipped. Job Clusters are cheaper for production. `system.lakeflow.job_run_timeline` is the key system table for job monitoring.\n",
    "\n",
    "> **What's next:** In LAB 09 you will implement governance controls — column masks, row filters, and privilege management with Unity Catalog."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}