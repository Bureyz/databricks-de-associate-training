{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 07: Lakeflow Declarative Pipeline\n",
    "\n",
    "**Duration:** ~45 min  \n",
    "**Day:** 3  \n",
    "**After module:** M07: Medallion & Lakeflow Pipelines  \n",
    "**Difficulty:** Advanced\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario\n",
    "\n",
    "> *\"Time to formalize the RetailHub data flow into a production-grade Medallion pipeline using Lakeflow Declarative Pipelines. You'll define Bronze (raw ingestion), Silver (cleaned/validated), and Gold (aggregates) layers with data quality expectations and SCD Type 2 for customer dimension tracking.\"*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "- Write `CREATE STREAMING TABLE` declarations for Bronze layer\n",
    "- Write `CREATE MATERIALIZED VIEW` declarations for Gold layer\n",
    "- Define data quality expectations (CONSTRAINT ... EXPECT)\n",
    "- Configure a Lakeflow pipeline via the UI\n",
    "- Run and monitor the pipeline DAG\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Write Pipeline SQL Files (~20 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Bronze Layer -- Streaming Tables\n",
    "\n",
    "Create SQL declarations for Bronze layer ingestion:\n",
    "\n",
    "```sql\n",
    "CREATE OR REFRESH STREAMING TABLE bronze_orders\n",
    "AS SELECT * FROM STREAM read_files('/path/to/orders', format => 'json');\n",
    "```\n",
    "\n",
    "> **Exam Tip:** `STREAMING TABLE` processes data incrementally. Each run picks up only new files. Use `STREAM read_files()` for file-based sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Silver Layer -- Validated Tables\n",
    "\n",
    "Create Silver layer with data quality expectations:\n",
    "\n",
    "```sql\n",
    "CREATE OR REFRESH STREAMING TABLE silver_orders (\n",
    "    CONSTRAINT valid_order_id EXPECT (order_id IS NOT NULL) ON VIOLATION DROP ROW,\n",
    "    CONSTRAINT valid_amount EXPECT (total_price > 0) ON VIOLATION DROP ROW\n",
    ")\n",
    "AS SELECT * FROM STREAM(bronze_orders);\n",
    "```\n",
    "\n",
    "> **Exam Tip:** Know the three expectation actions:\n",
    "> - `ON VIOLATION DROP ROW` -- drops invalid rows\n",
    "> - `ON VIOLATION FAIL UPDATE` -- fails the pipeline\n",
    "> - (no action) -- records metric only, rows pass through"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Gold Layer -- Materialized Views\n",
    "\n",
    "Create Gold aggregation tables:\n",
    "\n",
    "```sql\n",
    "CREATE OR REFRESH MATERIALIZED VIEW gold_daily_revenue\n",
    "AS SELECT order_date, SUM(total_price) AS revenue, COUNT(*) AS orders\n",
    "   FROM silver_orders\n",
    "   GROUP BY order_date;\n",
    "```\n",
    "\n",
    "> **Exam Tip:** `MATERIALIZED VIEW` recomputes fully on each run (not incremental). Use for aggregations and joins on Silver data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Configure Pipeline in UI (~10 min)\n",
    "\n",
    "### Task 4: Create the Pipeline\n",
    "\n",
    "1. Go to **Lakeflow Pipelines** (left sidebar > Pipelines)\n",
    "2. Click **Create pipeline**\n",
    "3. Configure:\n",
    "\n",
    "| Setting | Value |\n",
    "|---------|-------|\n",
    "| **Pipeline name** | `retailhub_{your_name}_pipeline` |\n",
    "| **Source code** | Select the SQL files from your workspace |\n",
    "| **Target catalog** | Your catalog (`retailhub_{your_name}`) |\n",
    "| **Target schema** | `bronze` / `silver` / `gold` (multi-schema) |\n",
    "\n",
    "### Task 5: Add Pipeline Variables\n",
    "\n",
    "Add configuration variables used in your SQL files:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Run & Monitor (~15 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Run the Pipeline\n",
    "\n",
    "1. Click **Start** to trigger a Development run\n",
    "2. Watch the DAG visualization\n",
    "3. Check the data quality metrics in the UI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Verify Results\n",
    "\n",
    "Open the notebook and query the pipeline results:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8: Check Expectations\n",
    "\n",
    "Review the data quality metrics:\n",
    "- How many rows passed each expectation?\n",
    "- How many were dropped?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab you:\n",
    "- Wrote SQL declarations for a full Medallion pipeline (Bronze/Silver/Gold)\n",
    "- Added data quality expectations (DROP ROW, FAIL UPDATE)\n",
    "- Configured and ran a Lakeflow pipeline via the UI\n",
    "- Monitored the DAG and verified data quality metrics\n",
    "\n",
    "> **Exam Tip:** Lakeflow (formerly DLT) pipelines are **declarative** â€” you define WHAT, not HOW. The engine handles orchestration, checkpointing, error handling, and data lineage.\n",
    "\n",
    "| Feature | STREAMING TABLE | MATERIALIZED VIEW |\n",
    "|---------|----------------|-------------------|\n",
    "| Processing | Incremental | Full recompute |\n",
    "| Use case | Raw ingestion, append-only | Aggregations, joins |\n",
    "| Reference | `STREAM(table_name)` | `table_name` |\n",
    "\n",
    "> **Next:** LAB 08 - Lakeflow Jobs: Triggers, Dependencies & Orchestration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
