{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 07: Lakeflow Declarative Pipeline\n",
    "\n",
    "**Duration:** ~45 min  \n",
    "**Day:** 3  \n",
    "**After module:** M07: Medallion & Lakeflow Pipelines  \n",
    "**Difficulty:** Advanced\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario\n",
    "\n",
    "> *\"Build a complete Medallion Pipeline using Lakeflow Declarative Pipelines. First you'll create a pipeline via the Databricks UI (Workshop), then practice writing SQL declarations for Bronze, Silver, and Gold layers (Practice).\"*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "- Create and configure a Lakeflow Declarative Pipeline in the Databricks UI\n",
    "- Upload SQL source files and set pipeline variables\n",
    "- Write `STREAMING TABLE` declarations for Bronze\n",
    "- Add data quality expectations (`ON VIOLATION DROP ROW`)\n",
    "- Create `MATERIALIZED VIEW` declarations for Gold\n",
    "- Verify pipeline results and query the Event Log\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Cluster running and attached to notebook\n",
    "- SQL source files available in `materials/lakeflow/`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Workshop — Building the Pipeline in UI\n",
    "\n",
    "Follow the trainer's instructions step by step.\n",
    "\n",
    "### Step 1: Upload SQL Files\n",
    "- Upload the SQL transformation files from `materials/lakeflow/` via Databricks UI or Git Folders\n",
    "\n",
    "### Step 2: Create Pipeline\n",
    "1. Go to **Workflows → Pipelines → Create Pipeline**\n",
    "2. Set **Pipeline name**: `lakeflow_pipeline_<your_name>`\n",
    "3. Set **Catalog**: `retailhub_<your_name>`\n",
    "4. Set **Target schema**: `<your_name>_lakeflow`\n",
    "5. Add the uploaded SQL files as source code\n",
    "\n",
    "### Step 3: Configure Variables\n",
    "- Add pipeline configuration keys:\n",
    "  - `customer_path` → path to customer data\n",
    "  - `order_path` → path to order data\n",
    "  - `product_path` → path to product data\n",
    "\n",
    "### Step 4: Run the Pipeline\n",
    "1. Click **Start** to run the pipeline\n",
    "2. After first run completes, add a new file to `orders/stream/`\n",
    "3. Run the pipeline again — observe incremental processing\n",
    "4. Check the **Event Log** tab for processing metrics\n",
    "\n",
    "### Step 5: Verify Results\n",
    "- Query `fact_sales` with joins to `dim_customer`, `dim_product`, `dim_date`\n",
    "- Check SCD Type 2 history in `silver_customers`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Practice — Lakeflow SQL Declarations\n",
    "\n",
    "Open **`LAB_07_code.ipynb`** and complete the `# TODO` cells.\n",
    "\n",
    "| Task | What to do | Key concept |\n",
    "|------|-----------|-------------|\n",
    "| **Task 1** | Write Bronze Declaration | `CREATE OR REFRESH STREAMING TABLE` + `read_files()` |\n",
    "| **Task 2** | Write Silver with Expectations | `ON VIOLATION DROP ROW` for constraints |\n",
    "| **Task 3** | Write Gold Declaration | `CREATE OR REFRESH MATERIALIZED VIEW` |\n",
    "| **Task 4** | Compare ST vs MV | Fill comparison table (Streaming Table vs Materialized View) |\n",
    "| **Task 5** | Verify Pipeline Results | Query bronze, silver, gold tables |\n",
    "| **Task 6** | Check Pipeline Event Log | `event_log(TABLE(...))` for data quality metrics |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Hints\n",
    "\n",
    "### Task 1: Bronze — STREAMING TABLE\n",
    "- `CREATE OR REFRESH STREAMING TABLE bronze_orders`\n",
    "- Source: `STREAM read_files('{path}', format => 'json')`\n",
    "\n",
    "### Task 2: Silver — Expectations\n",
    "- Constraints use `EXPECT (condition) ON VIOLATION DROP ROW`\n",
    "- First constraint: `order_id IS NOT NULL`\n",
    "- Second constraint: `total_price > 0`\n",
    "- Source: `STREAM(bronze_orders)`\n",
    "\n",
    "### Task 3: Gold — MATERIALIZED VIEW\n",
    "- `CREATE OR REFRESH MATERIALIZED VIEW gold_daily_revenue`\n",
    "- Materialized Views are recalculated from scratch each run\n",
    "\n",
    "### Task 4: ST vs MV\n",
    "\n",
    "| Feature | Streaming Table | Materialized View |\n",
    "|---------|----------------|-------------------|\n",
    "| Processing mode | Incremental (append) | Full recompute |\n",
    "| Best for | Raw/Bronze ingestion | Aggregations/Gold |\n",
    "\n",
    "### Task 6: Event Log\n",
    "- `SELECT * FROM event_log(TABLE(catalog.schema.table))`\n",
    "- Filter by `event_type = 'flow_progress'` for data quality metrics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab you:\n",
    "- Created a complete Lakeflow Pipeline via Databricks UI\n",
    "- Wrote SQL declarations for Bronze (STREAMING TABLE), Silver (with expectations), and Gold (MATERIALIZED VIEW)\n",
    "- Verified incremental processing with streaming sources\n",
    "- Queried the Event Log for pipeline monitoring\n",
    "\n",
    "> **Exam Tip:** `STREAMING TABLE` processes data incrementally (append-only). `MATERIALIZED VIEW` recalculates fully each run. Use `ON VIOLATION DROP ROW` to silently filter invalid rows. `FAIL` stops the pipeline. `EXPECT` without action only logs warnings.\n",
    "\n",
    "> **What's next:** In LAB 08 you will create multi-task Jobs with dependencies and triggers for orchestrating pipelines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}