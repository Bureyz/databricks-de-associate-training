{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbadc771",
   "metadata": {},
   "source": [
    "# LAB 07: Lakeflow Declarative Pipeline\n",
    "\n",
    "**Duration:** ~45 min | **Day:** 3 | **Difficulty:** Advanced\n",
    "**After module:** M07: Medallion & Lakeflow Pipelines\n",
    "\n",
    "> *\"Build a Medallion pipeline: Bronze (streaming), Silver (validated), Gold (aggregated) using Lakeflow SQL declarations.\"*\n",
    "\n",
    "This lab has two sections:\n",
    "1. **Section 1: Workshop** — Build & run a complete pipeline in the Databricks UI\n",
    "2. **Section 2: Practice** — Write and verify Lakeflow SQL declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05f3fd8",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e8fd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../setup/00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e563c3d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Workshop — Building the Pipeline\n",
    "\n",
    "In this hands-on workshop, we create a complete Lakeflow pipeline from SQL files — uploading source code, configuring the pipeline in the Databricks UI, running it, and validating the results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d395c3",
   "metadata": {},
   "source": [
    "### SQL Files Overview\n",
    "\n",
    "The pipeline source code is organized by medallion layer — each SQL file declares one table or view.\n",
    "\n",
    "![../../../assets/images/training_2026/day3/57bea06e969c45dab4de8bea9ec980b1.webp](../../../assets/images/training_2026/day3/57bea06e969c45dab4de8bea9ec980b1.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47245a37",
   "metadata": {},
   "source": [
    "### Step 1: Upload SQL Files\n",
    "\n",
    "**Option A: Via UI** — Workspace → Users → create `lakeflow_pipeline` folder → upload SQL files\n",
    "\n",
    "**Option B: Via Git Folders** — Git Folders → Add Git Folder → clone training repository"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07786989",
   "metadata": {},
   "source": [
    "### Step 2: Create Pipeline\n",
    "\n",
    "1. **Workflows** → **Jobs & Pipelines** → **Create ETL Pipeline**\n",
    "2. **Catalog:** `retailhub_<your_name>`\n",
    "3. **Pipeline Name:** `lakeflow_pipeline_<your_name>`\n",
    "4. **Target Schema:** `<your_name>_lakeflow`\n",
    "5. **Source Code:** Add existing assets → choose pipeline root folder and source code folder\n",
    "\n",
    "<img src=\"../../../assets/images/fab4fef8e72d4d5786ba818e6c2f73c5.png\" width=\"800\">\n",
    "\n",
    "<img src=\"../../../assets/images/a4e21cb35d3b45f2ba184877139cdc48.png\" width=\"800\">\n",
    "\n",
    "<img src=\"../../../assets/images/4e75e96544f142508c3bd8b0b4dbf446.png\" width=\"800\">\n",
    "\n",
    "![../../../assets/images/training_2026/day3/dc9c3eea154e4cb29aee62e6edd5bcca.webp](../../../assets/images/training_2026/day3/dc9c3eea154e4cb29aee62e6edd5bcca.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33efb126",
   "metadata": {},
   "source": [
    "### Step 3: Configure Variables\n",
    "\n",
    "**Configuration** → **Add configuration**:\n",
    "\n",
    "| Key | Value |\n",
    "|-----|-------|\n",
    "| `customer_path` | `/Volumes/<your catalog>/default/datasets/customers` |\n",
    "| `order_path` | `/Volumes/<your catalog>/default/datasets/orders` |\n",
    "| `product_path` | `/Volumes/<your catalog>/default/datasets/products/products.parquet/` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1036f37d",
   "metadata": {},
   "source": [
    "Open settings and go to Pipeline Configuration \n",
    "\n",
    "<img src=\"../../../assets/images/b2fa841a52004939ac2679f9edef8dc3.png\" width=\"800\">\n",
    "\n",
    "Add configuration : \n",
    "\n",
    "<img src=\"../../../assets/images/dc31d5dec7444c1d959b8e1f220c10ce.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e6ae42",
   "metadata": {},
   "source": [
    "<img src=\"../../../assets/images/6f16ad4f7fd941ebbb4fb286dbb8fbfd.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1545e2b0",
   "metadata": {},
   "source": [
    "You should also see a DAG diagram built based on Spark Declarative Pipelines definition\n",
    "\n",
    "![../../../assets/images/training_2026/day3/05f00c9fedb54b0b81ec65bf182a92af.webp](../../../assets/images/training_2026/day3/05f00c9fedb54b0b81ec65bf182a92af.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab976fd7",
   "metadata": {},
   "source": [
    "### Step 4: Run the Pipeline\n",
    "\n",
    "Start the pipeline and test incremental processing by adding new data files.\n",
    "\n",
    "![../../../assets/images/training_2026/day3/8d06de8a2a674cc1bc119b5d91b2d1ce.webp](../../../assets/images/training_2026/day3/8d06de8a2a674cc1bc119b5d91b2d1ce.webp)\n",
    "\n",
    "1. Add new file to folder orders/stream/\n",
    "2. Run pipeline again\n",
    "3. Check Event Log - should process only new files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec7f0f6",
   "metadata": {},
   "source": [
    "### Step 5: Verify Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cd6ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check fact_sales with joins to dimensions\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        f.order_id,\n",
    "        c.first_name || ' ' || c.last_name AS customer_name,\n",
    "        p.product_name,\n",
    "        d.date,\n",
    "        f.quantity,\n",
    "        f.net_amount\n",
    "    FROM {CATALOG}.{user_schema}.fact_sales f\n",
    "    LEFT JOIN {CATALOG}.{user_schema}.dim_customer c ON f.customer_id = c.customer_id\n",
    "    LEFT JOIN {CATALOG}.{user_schema}.dim_product p ON f.product_id = p.product_id\n",
    "    LEFT JOIN {CATALOG}.{user_schema}.dim_date d ON f.order_date_key = d.date_key\n",
    "    LIMIT 10\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f2180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find customers with change history\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        customer_id, first_name, city,\n",
    "        __START_AT, __END_AT,\n",
    "        CASE WHEN __END_AT IS NULL THEN 'Current' ELSE 'Historical' END AS status\n",
    "    FROM {CATALOG}.{user_schema}.silver_customers\n",
    "    WHERE customer_id IN (\n",
    "        SELECT customer_id \n",
    "        FROM {CATALOG}.{user_schema}.silver_customers \n",
    "        GROUP BY customer_id HAVING COUNT(*) > 1\n",
    "    )\n",
    "    ORDER BY customer_id, __START_AT\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08130817",
   "metadata": {},
   "source": [
    "### Monitoring and Troubleshooting\n",
    "\n",
    "Common issues encountered when running Lakeflow pipelines and how to resolve them.\n",
    "\n",
    "| Issue | Cause | Solution |\n",
    "|---------|-----------|-------------|\n",
    "| Pipeline hangs | Cluster too small | Increase min workers |\n",
    "| Missing data | Constraint DROP ROW | Check Data Quality tab |\n",
    "| Schema mismatch | Schema change | Full refresh |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3584bb4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Practice — Lakeflow SQL Declarations\n",
    "\n",
    "Write and verify Lakeflow SQL syntax for each medallion layer.\n",
    "\n",
    "---\n",
    "## Task 1: Write Bronze Declaration\n",
    "\n",
    "Complete the SQL below to create a Bronze streaming table from JSON files.\n",
    "\n",
    "**This SQL would go in a pipeline SQL file.** Here we practice the syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b36a739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice: what the Bronze SQL declaration looks like\n",
    "# (This won't execute outside of a Lakeflow pipeline, but verify the syntax)\n",
    "\n",
    "bronze_sql = f\"\"\"\n",
    "-- TODO: Complete the Bronze declaration\n",
    "CREATE OR REFRESH ________ TABLE bronze_orders\n",
    "AS SELECT * \n",
    "FROM STREAM ________('{DATASET_PATH}/orders/stream', format => 'json');\n",
    "\"\"\"\n",
    "\n",
    "print(\"Bronze SQL declaration:\")\n",
    "print(bronze_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160cfb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert \"STREAMING\" in bronze_sql.upper(), \"Should use STREAMING TABLE\"\n",
    "assert \"read_files\" in bronze_sql.lower(), \"Should use read_files()\"\n",
    "print(\"Task 1 OK: Bronze declaration syntax correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6faeae",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Write Silver Declaration with Expectations\n",
    "\n",
    "Complete the Silver layer with data quality constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073203dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete Silver SQL with expectations\n",
    "silver_sql = \"\"\"\n",
    "CREATE OR REFRESH STREAMING TABLE silver_orders (\n",
    "    CONSTRAINT valid_id EXPECT (order_id IS NOT NULL) ON VIOLATION ________,\n",
    "    CONSTRAINT positive_amount EXPECT (total_price > 0) ON VIOLATION ________\n",
    ")\n",
    "AS SELECT \n",
    "    order_id,\n",
    "    customer_id,\n",
    "    product_id,\n",
    "    CAST(quantity AS INT) AS quantity,\n",
    "    CAST(total_price AS DOUBLE) AS total_price,\n",
    "    CAST(order_date AS DATE) AS order_date,\n",
    "    payment_method,\n",
    "    store_id,\n",
    "    current_timestamp() AS processed_at\n",
    "FROM STREAM(bronze_orders);\n",
    "\"\"\"\n",
    "\n",
    "print(\"Silver SQL declaration:\")\n",
    "print(silver_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22df1211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert \"CONSTRAINT\" in silver_sql.upper(), \"Should have CONSTRAINT declarations\"\n",
    "assert \"DROP ROW\" in silver_sql.upper(), \"Should use ON VIOLATION DROP ROW\"\n",
    "assert \"bronze\" in silver_sql.lower(), \"Should reference bronze_orders\"\n",
    "print(\"Task 2 OK: Silver declaration with expectations correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812f05a6",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3: Write Gold Declaration\n",
    "\n",
    "Create a Materialized View for daily revenue summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63d5158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete Gold declaration\n",
    "gold_sql = \"\"\"\n",
    "CREATE OR REFRESH ________ ________ gold_daily_revenue\n",
    "AS SELECT \n",
    "    order_date,\n",
    "    SUM(total_price) AS total_revenue,\n",
    "    COUNT(*) AS total_orders,\n",
    "    AVG(total_price) AS avg_order_value\n",
    "FROM silver_orders\n",
    "GROUP BY order_date\n",
    "ORDER BY order_date;\n",
    "\"\"\"\n",
    "\n",
    "print(\"Gold SQL declaration:\")\n",
    "print(gold_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7095b627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert \"MATERIALIZED VIEW\" in gold_sql.upper(), \"Gold should use MATERIALIZED VIEW\"\n",
    "assert \"silver\" in gold_sql.lower(), \"Should reference silver_orders\"\n",
    "print(\"Task 3 OK: Gold Materialized View declaration correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb5355e",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 4: Compare STREAMING TABLE vs MATERIALIZED VIEW\n",
    "\n",
    "Fill in the comparison table (markdown exercise).\n",
    "\n",
    "| Feature | STREAMING TABLE | MATERIALIZED VIEW |\n",
    "|---------|----------------|-------------------|\n",
    "| Processing mode | ________ | ________ |\n",
    "| Best for | ________ | ________ |\n",
    "| Read from source | `STREAM(table_name)` | `table_name` |\n",
    "| Supports expectations | Yes | Yes |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70adb81",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 5: Verify Pipeline Results (after running the pipeline)\n",
    "\n",
    "After creating and running the pipeline via the UI, query the results here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d53e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: After running the pipeline, uncomment and verify:\n",
    "\n",
    "# display(spark.sql(f\"SELECT COUNT(*) as cnt FROM {CATALOG}.bronze.bronze_orders\"))\n",
    "# display(spark.sql(f\"SELECT COUNT(*) as cnt FROM {CATALOG}.silver.silver_orders\"))\n",
    "# display(spark.sql(f\"SELECT * FROM {CATALOG}.gold.gold_daily_revenue ORDER BY order_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb283e9a",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 6: Check Pipeline Event Log\n",
    "\n",
    "Query the pipeline event log to see data quality metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0212c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: After running the pipeline, uncomment to check event log:\n",
    "\n",
    "# display(spark.sql(\"\"\"\n",
    "#     SELECT timestamp, details:flow_progress:data_quality:expectations\n",
    "#     FROM event_log(TABLE({CATALOG}.silver.silver_orders))\n",
    "#     WHERE details:flow_progress:data_quality IS NOT NULL\n",
    "# \"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4492422d",
   "metadata": {},
   "source": [
    "---\n",
    "## Lab Complete!\n",
    "\n",
    "You have:\n",
    "- Written Bronze STREAMING TABLE declarations\n",
    "- Written Silver declarations with data quality expectations\n",
    "- Written Gold MATERIALIZED VIEW declarations\n",
    "- Understood ST vs MV differences\n",
    "- (If pipeline ran) Verified results and checked data quality metrics\n",
    "\n",
    "> **Exam Tip:** In Spark Declarative Pipelines, tables within the same pipeline reference each other directly by name — no prefix needed. Use `STREAM(table_name)` for streaming reads and just `table_name` for batch reads.\n",
    "\n",
    "> **Next:** LAB 08 - Lakeflow Jobs & Orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline cleanup is done via Lakeflow UI (delete the pipeline)\n",
    "print(\"LAB 07 complete. Delete the pipeline from Lakeflow UI when done.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
