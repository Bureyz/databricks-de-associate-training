{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6856f4ad-5e7f-4a19-a603-7e48522e5295",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# LAB 07: Lakeflow Declarative Pipeline\n",
    "\n",
    "**Duration:** ~45 min | **Day:** 3 | **Difficulty:** Advanced\n",
    "**After module:** M07: Medallion & Lakeflow Pipelines\n",
    "\n",
    "> *\"Build a Medallion pipeline: Bronze (streaming), Silver (validated), Gold (aggregated) using Lakeflow SQL declarations.\"*\n",
    "\n",
    "This lab has two sections:\n",
    "1. **Section 1: Workshop** — Build & run a complete pipeline in the Databricks UI\n",
    "2. **Section 2: Practice** — Write and verify Lakeflow SQL declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e810add3-338a-4e71-9bc1-86557ad6adca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "259c9950-15d1-42a0-a325-7712ed96310d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../setup/00_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f42226b5-6713-4552-baaf-9454369144d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lakeflow pipeline target schema (matches Step 2 config)\n",
    "user_name = CATALOG.replace(f\"{CATALOG_PREFIX}_\", \"\")\n",
    "user_schema = f\"lakeflow_workshop\"\n",
    "print(f\"Pipeline target schema: {user_schema}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bc3cfe5-b120-4d62-871a-036dd464408b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Section 1: Workshop — Building the Pipeline\n",
    "\n",
    "In this hands-on workshop, we create a complete Lakeflow pipeline from SQL files — uploading source code, configuring the pipeline in the Databricks UI, running it, and validating the results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d23ea8cd-24ba-4a40-8ffb-bbaf38ec5ec5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SQL Files Overview\n",
    "\n",
    "The pipeline source code is organized by medallion layer — each SQL file declares one table or view.\n",
    "\n",
    "![../../../assets/images/training_2026/day3/57bea06e969c45dab4de8bea9ec980b1.webp](../../../assets/images/training_2026/day3/57bea06e969c45dab4de8bea9ec980b1.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90e5f8f6-1e8b-4746-b4a1-e759c0b40cf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1: Upload SQL Files\n",
    "\n",
    "**Option A: Via UI** — Workspace → Users → create `lakeflow_pipeline` folder → upload SQL files\n",
    "\n",
    "**Option B: Via Git Folders** — Git Folders → Add Git Folder → clone training repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46835c34-7832-464b-a918-87e3399c08b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 2: Create Pipeline\n",
    "\n",
    "1. **Workflows** → **Jobs & Pipelines** → **Create ETL Pipeline**\n",
    "2. **Catalog:** `retailhub_<your_name>`\n",
    "3. **Pipeline Name:** `lakeflow_pipeline_<your_name>`\n",
    "4. **Target Schema:** `<your_name>_lakeflow`\n",
    "5. **Source Code:** Add existing assets → choose pipeline root folder and source code folder\n",
    "\n",
    "<img src=\"../../../assets/images/fab4fef8e72d4d5786ba818e6c2f73c5.png\" width=\"800\">\n",
    "\n",
    "<img src=\"../../../assets/images/a4e21cb35d3b45f2ba184877139cdc48.png\" width=\"800\">\n",
    "\n",
    "<img src=\"../../../assets/images/4e75e96544f142508c3bd8b0b4dbf446.png\" width=\"800\">\n",
    "\n",
    "![../../../assets/images/training_2026/day3/dc9c3eea154e4cb29aee62e6edd5bcca.webp](../../../assets/images/training_2026/day3/dc9c3eea154e4cb29aee62e6edd5bcca.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "732262ba-116e-4e54-bd82-d659215b63a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 3: Configure Variables\n",
    "\n",
    "**Configuration** → **Add configuration**:\n",
    "\n",
    "| Key | Value |\n",
    "|-----|-------|\n",
    "| `customer_path` | `/Volumes/<your catalog>/default/datasets/customers` |\n",
    "| `order_path` | `/Volumes/<your catalog>/default/datasets/orders` |\n",
    "| `product_path` | `/Volumes/<your catalog>/default/datasets/products/products.parquet/` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e85506cd-38c1-4ff2-8e4a-a08d12104961",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Open settings and go to Pipeline Configuration \n",
    "\n",
    "<img src=\"../../../assets/images/b2fa841a52004939ac2679f9edef8dc3.png\" width=\"800\">\n",
    "\n",
    "Add configuration : \n",
    "\n",
    "<img src=\"../../../assets/images/dc31d5dec7444c1d959b8e1f220c10ce.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8112b4ff-ba8f-4f85-959f-fe1a00dfa61a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<img src=\"../../../assets/images/6f16ad4f7fd941ebbb4fb286dbb8fbfd.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99733ebb-d2a7-43c2-b539-61a3b3c4e5ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You should also see a DAG diagram built based on Spark Declarative Pipelines definition\n",
    "\n",
    "![../../../assets/images/training_2026/day3/05f00c9fedb54b0b81ec65bf182a92af.webp](../../../assets/images/training_2026/day3/05f00c9fedb54b0b81ec65bf182a92af.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "778b341d-0365-4311-bad0-a3d6928c7d11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 4: Run the Pipeline\n",
    "\n",
    "Start the pipeline and test incremental processing by adding new data files.\n",
    "\n",
    "![../../../assets/images/training_2026/day3/8d06de8a2a674cc1bc119b5d91b2d1ce.webp](../../../assets/images/training_2026/day3/8d06de8a2a674cc1bc119b5d91b2d1ce.webp)\n",
    "\n",
    "1. Add new file to folder orders/stream/ from /Volumes/retailhub_trener/default/datasets/demo/ingestion/orders/stream/\n",
    "2. Run pipeline again\n",
    "3. Check Event Log - should process only new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48e4dfea-c2cc-4239-8e69-771011888fcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mv(\"/Volumes/retailhub_trener/default/datasets/demo/ingestion/orders/stream/\", \"/Volumes/retailhub_trener/default/datasets/orders/stream/\", recurse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fae45efc-8fd0-45bc-b68f-2116cc0eb5e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 5: Verify Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cd5969f-2039-4d44-b752-0354ed80f705",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 15"
    }
   },
   "outputs": [],
   "source": [
    "# Check fact_sales with joins to dimensions\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        f.order_id,\n",
    "        c.first_name || ' ' || c.last_name AS customer_name,\n",
    "        p.product_name,\n",
    "        d.date,\n",
    "        f.quantity,\n",
    "        f.net_amount\n",
    "    FROM {CATALOG}.{user_schema}.fact_sales f\n",
    "    LEFT JOIN {CATALOG}.{user_schema}.dim_customer c ON f.customer_key = c.customer_key\n",
    "    LEFT JOIN {CATALOG}.{user_schema}.dim_product p ON f.product_key = p.product_key\n",
    "    LEFT JOIN {CATALOG}.{user_schema}.dim_date d ON f.order_date_key = d.date_key\n",
    "    LIMIT 10\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04dab3ce-7d3b-4aab-a6cf-ee75065d030c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find customers with change history\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        customer_id, first_name, city,\n",
    "        __START_AT, __END_AT,\n",
    "        CASE WHEN __END_AT IS NULL THEN 'Current' ELSE 'Historical' END AS status\n",
    "    FROM {CATALOG}.{user_schema}.silver_customers\n",
    "    WHERE customer_id IN (\n",
    "        SELECT customer_id \n",
    "        FROM {CATALOG}.{user_schema}.silver_customers \n",
    "        GROUP BY customer_id HAVING COUNT(*) > 1\n",
    "    )\n",
    "    ORDER BY customer_id, __START_AT\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f31ad71c-a590-4547-a573-0e9594dbe926",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Monitoring and Troubleshooting\n",
    "\n",
    "Common issues encountered when running Lakeflow pipelines and how to resolve them.\n",
    "\n",
    "| Issue | Cause | Solution |\n",
    "|---------|-----------|-------------|\n",
    "| Pipeline hangs | Cluster too small | Increase min workers |\n",
    "| Missing data | Constraint DROP ROW | Check Data Quality tab |\n",
    "| Schema mismatch | Schema change | Full refresh |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a972482c-c570-4afc-bc44-a4998ac3171c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Section 2: Practice — Lakeflow SQL Declarations\n",
    "\n",
    "Write and verify Lakeflow SQL syntax for each medallion layer.\n",
    "\n",
    "---\n",
    "## Task 1: Write Bronze Declaration\n",
    "\n",
    "Complete the SQL below to create a Bronze streaming table from JSON files.\n",
    "\n",
    "**This SQL would go in a pipeline SQL file.** Here we practice the syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cfb18f8-c499-4726-87ac-7a5508675694",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Practice: what the Bronze SQL declaration looks like\n",
    "# (This won't execute outside of a Lakeflow pipeline, but verify the syntax)\n",
    "\n",
    "bronze_sql = f\"\"\"\n",
    "-- TODO: Complete the Bronze declaration\n",
    "CREATE OR REFRESH ________ TABLE bronze_orders\n",
    "AS SELECT * \n",
    "FROM STREAM ________('{DATASET_PATH}/orders/stream', format => 'json');\n",
    "\"\"\"\n",
    "\n",
    "print(\"Bronze SQL declaration:\")\n",
    "print(bronze_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "515ea16d-319c-467a-9766-9bd2dfb077f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert \"STREAMING\" in bronze_sql.upper(), \"Should use STREAMING TABLE\"\n",
    "assert \"read_files\" in bronze_sql.lower(), \"Should use read_files()\"\n",
    "print(\"Task 1 OK: Bronze declaration syntax correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67a18778-d4f3-4726-bb7c-211315600225",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Task 2: Write Silver Declaration with Expectations\n",
    "\n",
    "Complete the Silver layer with data quality constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b3ce2a2-abcc-4b3b-a979-0e5b1fc44d57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Complete Silver SQL with expectations\n",
    "silver_sql = \"\"\"\n",
    "CREATE OR REFRESH STREAMING TABLE silver_orders (\n",
    "    CONSTRAINT valid_id EXPECT (order_id IS NOT NULL) ON VIOLATION ________,\n",
    "    CONSTRAINT positive_amount EXPECT (total_price > 0) ON VIOLATION ________\n",
    ")\n",
    "AS SELECT \n",
    "    order_id,\n",
    "    customer_id,\n",
    "    product_id,\n",
    "    CAST(quantity AS INT) AS quantity,\n",
    "    CAST(total_price AS DOUBLE) AS total_price,\n",
    "    CAST(order_date AS DATE) AS order_date,\n",
    "    payment_method,\n",
    "    store_id,\n",
    "    current_timestamp() AS processed_at\n",
    "FROM STREAM(bronze_orders);\n",
    "\"\"\"\n",
    "\n",
    "print(\"Silver SQL declaration:\")\n",
    "print(silver_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "758b4e1e-01a3-46b3-a779-320791c7a4cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert \"CONSTRAINT\" in silver_sql.upper(), \"Should have CONSTRAINT declarations\"\n",
    "assert \"DROP ROW\" in silver_sql.upper(), \"Should use ON VIOLATION DROP ROW\"\n",
    "assert \"bronze\" in silver_sql.lower(), \"Should reference bronze_orders\"\n",
    "print(\"Task 2 OK: Silver declaration with expectations correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f706a450-1c91-47c7-997d-68b7227f996e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Task 3: Write Gold Declaration\n",
    "\n",
    "Create a Materialized View for daily revenue summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bbe3499-527e-4c9b-9641-b1d43f41d1eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Complete Gold declaration\n",
    "gold_sql = \"\"\"\n",
    "CREATE OR REFRESH ________ ________ gold_daily_revenue\n",
    "AS SELECT \n",
    "    order_date,\n",
    "    SUM(total_price) AS total_revenue,\n",
    "    COUNT(*) AS total_orders,\n",
    "    AVG(total_price) AS avg_order_value\n",
    "FROM silver_orders\n",
    "GROUP BY order_date\n",
    "ORDER BY order_date;\n",
    "\"\"\"\n",
    "\n",
    "print(\"Gold SQL declaration:\")\n",
    "print(gold_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9fa83a9-e80f-49b2-9995-c985e8a6f939",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert \"MATERIALIZED VIEW\" in gold_sql.upper(), \"Gold should use MATERIALIZED VIEW\"\n",
    "assert \"silver\" in gold_sql.lower(), \"Should reference silver_orders\"\n",
    "print(\"Task 3 OK: Gold Materialized View declaration correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7ae73fc-5e87-40b2-a20c-5e52ce94a984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Task 4: Compare STREAMING TABLE vs MATERIALIZED VIEW\n",
    "\n",
    "Fill in the comparison table (markdown exercise).\n",
    "\n",
    "| Feature | STREAMING TABLE | MATERIALIZED VIEW |\n",
    "|---------|----------------|-------------------|\n",
    "| Processing mode | ________ | ________ |\n",
    "| Best for | ________ | ________ |\n",
    "| Read from source | `STREAM(table_name)` | `table_name` |\n",
    "| Supports expectations | Yes | Yes |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be4c15a5-646e-4975-8d12-89388fd5c2d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Task 5: Verify Pipeline Results (after running the pipeline)\n",
    "\n",
    "After creating and running the pipeline via the UI, query the results here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de28ae3f-c5d0-48e8-aa53-6bf474ddd167",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: After running the pipeline, uncomment and verify:\n",
    "\n",
    "# display(spark.sql(f\"SELECT COUNT(*) as cnt FROM {CATALOG}.bronze.bronze_orders\"))\n",
    "# display(spark.sql(f\"SELECT COUNT(*) as cnt FROM {CATALOG}.silver.silver_orders\"))\n",
    "# display(spark.sql(f\"SELECT * FROM {CATALOG}.gold.gold_daily_revenue ORDER BY order_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24a3e422-758d-4299-8fe9-d49b12b255b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Task 6: Check Pipeline Event Log\n",
    "\n",
    "Query the pipeline event log to see data quality metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6054a1f-794a-432d-8fb0-8d14979cbe78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: After running the pipeline, uncomment to check event log:\n",
    "\n",
    "# display(spark.sql(\"\"\"\n",
    "#     SELECT timestamp, details:flow_progress:data_quality:expectations\n",
    "#     FROM event_log(TABLE({CATALOG}.silver.silver_orders))\n",
    "#     WHERE details:flow_progress:data_quality IS NOT NULL\n",
    "# \"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c107d01b-f56c-44eb-b072-2a75c6ce4d28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Lab Complete!\n",
    "\n",
    "You have:\n",
    "- Written Bronze STREAMING TABLE declarations\n",
    "- Written Silver declarations with data quality expectations\n",
    "- Written Gold MATERIALIZED VIEW declarations\n",
    "- Understood ST vs MV differences\n",
    "- (If pipeline ran) Verified results and checked data quality metrics\n",
    "\n",
    "> **Exam Tip:** In Spark Declarative Pipelines, tables within the same pipeline reference each other directly by name — no prefix needed. Use `STREAM(table_name)` for streaming reads and just `table_name` for batch reads.\n",
    "\n",
    "> **Next:** LAB 08 - Lakeflow Jobs & Orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63dca28a-88c8-4690-8107-994fb3a948d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a4412db-ebea-4f7d-ac9d-e7bc83b66b95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pipeline cleanup is done via Lakeflow UI (delete the pipeline)\n",
    "print(\"LAB 07 complete. Delete the pipeline from Lakeflow UI when done.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LAB_07_code",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
