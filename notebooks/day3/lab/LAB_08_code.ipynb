{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41699978-84ad-4b47-a921-54997813609c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# LAB 08: Lakeflow Jobs — Triggers, Dependencies & Orchestration\n",
    "\n",
    "**Duration:** ~30 min | **Day:** 3 | **Difficulty:** Intermediate\n",
    "**After module:** M08: Lakeflow Jobs & Orchestration\n",
    "\n",
    "> *\"The RetailHub pipeline works — now automate it. Configure triggers, define task dependencies, handle failures with repair runs, and monitor via system tables.\"*\n",
    "\n",
    "### Lab Structure\n",
    "\n",
    "| Section | Focus | Format |\n",
    "|---------|-------|--------|\n",
    "| **Section 1: Workshop** | Hands-on job creation in Databricks UI | Guided walkthrough with screenshots |\n",
    "| **Section 2: Practice** | Orchestration concepts & configuration | Fill-in-the-blank exercises with verification |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "885a5382-9fb1-4177-a26f-4a04a1ef9d2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76ea348f-e058-4144-a29f-ecbf80b581cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../setup/00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c126c89-4a1a-465c-9da0-ba7d4c3448b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Section 1: Workshop — Creating & Running Jobs in Databricks UI\n",
    "\n",
    "In this section you will create two Lakeflow Jobs through the Databricks UI:\n",
    "\n",
    "| Job | Tasks | Purpose |\n",
    "|-----|-------|---------|\n",
    "| **customer_pipeline** | `bronze_customer` → `silver_customer` | Ingest and cleanse customer data |\n",
    "| **orders_pipeline** | `bronze_orders` → `silver_orders` → `gold_daily_orders` → `gold_summary` | Full orders medallion pipeline, triggered by customer updates |\n",
    "\n",
    "> **Goal:** Create both jobs, configure task dependencies, set up a **Table Update trigger** so the orders pipeline runs automatically when `silver_customers` is updated, then execute the pipeline end-to-end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0405ea48-2910-4744-aeff-a59996890974",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1: Create a New Job — `customer_pipeline`\n",
    "\n",
    "Navigate to **Jobs & Pipelines** in the left sidebar, then click **Create Job**.\n",
    "\n",
    "![Create Job](../../../assets/images/training_2026/day3/6f524625f8464c29a2572b0a324333a5.webp)\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Set the Job Name\n",
    "\n",
    "Enter a **unique name** for your job (e.g., `<your_name>_customer_pipeline`).\n",
    "\n",
    "![Job Name](../../../assets/images/training_2026/day3/ae8ab8cafdcf4690b5d9665cbf058798.webp)\n",
    "\n",
    "> **Note:** The job name does not need to be globally unique — each job is identified by a unique **Job ID** assigned automatically by Databricks.\n",
    "\n",
    "![Job Details — ID & Creator](../../../assets/images/training_2026/day3/460925af2b214f93a2252aed0ea86936.webp)\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Configure Job Parameters\n",
    "\n",
    "Add the following **job parameters** so that all tasks share the same catalog and source path:\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| `catalog` | `retailhub_<your_name>` |\n",
    "| `source_path` | `/Volumes/retailhub_<your_name>/default/datasets` |\n",
    "\n",
    "![Job Parameters](../../../assets/images/training_2026/day3/e24f8fbe50f642e0a28054972140dcb1.webp)\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Add the First Task — `bronze_customer`\n",
    "\n",
    "Click **Add task** and configure:\n",
    "\n",
    "| Field | Value |\n",
    "|-------|-------|\n",
    "| **Task name** | `bronze_customer` |\n",
    "| **Type** | Notebook |\n",
    "| **Source** | Workspace |\n",
    "| **Path** | Path to your `bronze_customers` notebook |\n",
    "| **Compute** | Shared training cluster *(for workshop only — in production, use a dedicated Job cluster)* |\n",
    "\n",
    "![Add Task](../../../assets/images/training_2026/day3/8d2245e891b849e5966aae38a0e33c5f.webp)\n",
    "\n",
    "![Task Configuration](../../../assets/images/training_2026/day3/c89c69190d40463faae2b5186c78c271.webp)\n",
    "\n",
    "Click **Create task** when done.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Add the Second Task — `silver_customer`\n",
    "\n",
    "Repeat the same process for `silver_customer`:\n",
    "- Set **Depends on** → `bronze_customer` (so it runs only after bronze succeeds)\n",
    "- Point the notebook path to your `silver_customers` notebook\n",
    "\n",
    "Your completed job should look like this:\n",
    "\n",
    "![Customer Pipeline — 2 Tasks](../../../assets/images/training_2026/day3/8bdeb6d46c284e3c85bf1c04f4b27657.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b4e5408-1e93-42b9-9b3e-2ef6c0057ca4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 6: Create the Second Job — `orders_pipeline`\n",
    "\n",
    "Create a **new job** following the same steps as above. Name it `<your_name>_orders_pipeline` and add all four medallion tasks with proper dependencies.\n",
    "\n",
    "Your completed pipeline should look like this:\n",
    "\n",
    "![Orders Pipeline — 4 Tasks](../../../assets/images/training_2026/day3/119b8804bea74938aff23d816140bf4b.webp)\n",
    "\n",
    "---\n",
    "\n",
    "### Step 7: Configure a Table Update Trigger\n",
    "\n",
    "Navigate to the **Triggers** tab of the `orders_pipeline` job and add a **Table Update** trigger on `silver_customers`.\n",
    "\n",
    "This means the orders pipeline will **start automatically** whenever the `silver_customers` table is updated by the customer pipeline.\n",
    "\n",
    "![Table Update Trigger Configuration](../../../assets/images/training_2026/day3/d4dd6298fc9b41e9bc784878ccd8dc3a.webp)\n",
    "\n",
    "---\n",
    "\n",
    "### Step 8: Run the Pipeline\n",
    "\n",
    "Click **Run now** on the `customer_pipeline` job first.\n",
    "\n",
    "![Run Job](../../../assets/images/training_2026/day3/73685979cbb243b3af4e01fdcadb1a32.webp)\n",
    "\n",
    "> **Expected result:** The customer pipeline completes successfully, which updates `silver_customers`. This triggers the orders pipeline automatically — both jobs should show a successful run in the **Run History** tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference: YAML Definition — `orders_pipeline`\n",
    "\n",
    "> The YAML below shows the **Databricks Asset Bundle** definition for the orders pipeline. This is the programmatic equivalent of what you configured in the UI above.\n",
    "\n",
    "```yaml\n",
    "resources:\n",
    "  jobs:\n",
    "    demo_orders_pipeline:\n",
    "      name: demo_orders_pipeline\n",
    "      trigger:\n",
    "        pause_status: UNPAUSED\n",
    "        table_update:\n",
    "          table_names:\n",
    "            - retailhub_trainer.silver.silver_customers\n",
    "      tasks:\n",
    "        - task_key: bronze_orders\n",
    "          notebook_task:\n",
    "            notebook_path: /Workspace/.../bronze_orders\n",
    "            source: WORKSPACE\n",
    "          existing_cluster_id: <CLUSTER_ID>\n",
    "        - task_key: silver_orders\n",
    "          depends_on:\n",
    "            - task_key: bronze_orders\n",
    "          notebook_task:\n",
    "            notebook_path: /Workspace/.../silver_orders\n",
    "            source: WORKSPACE\n",
    "          existing_cluster_id: <CLUSTER_ID>\n",
    "        - task_key: gold_daily_orders\n",
    "          depends_on:\n",
    "            - task_key: silver_orders\n",
    "          notebook_task:\n",
    "            notebook_path: /Workspace/.../gold_daily_orders\n",
    "            source: WORKSPACE\n",
    "          existing_cluster_id: <CLUSTER_ID>\n",
    "        - task_key: gold_customer_orders_summary\n",
    "          depends_on:\n",
    "            - task_key: gold_daily_orders\n",
    "          notebook_task:\n",
    "            notebook_path: /Workspace/.../gold_customer_orders_summary\n",
    "            source: WORKSPACE\n",
    "          existing_cluster_id: <CLUSTER_ID>\n",
    "      queue:\n",
    "        enabled: true\n",
    "      parameters:\n",
    "        - name: catalog\n",
    "          default: retailhub_trainer\n",
    "        - name: source_path\n",
    "          default: /Volumes/retailhub_trainer/default/datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference: YAML Definition — `customer_pipeline`\n",
    "\n",
    "```yaml\n",
    "resources:\n",
    "  jobs:\n",
    "    demo_customer_pipeline:\n",
    "      name: demo_customer_pipeline\n",
    "      tasks:\n",
    "        - task_key: bronze_customer\n",
    "          notebook_task:\n",
    "            notebook_path: /Workspace/.../bronze_customers\n",
    "            source: WORKSPACE\n",
    "          existing_cluster_id: <CLUSTER_ID>\n",
    "        - task_key: silver_customer\n",
    "          depends_on:\n",
    "            - task_key: bronze_customer\n",
    "          notebook_task:\n",
    "            notebook_path: /Workspace/.../silver_customers\n",
    "            source: WORKSPACE\n",
    "          existing_cluster_id: <CLUSTER_ID>\n",
    "      queue:\n",
    "        enabled: true\n",
    "      parameters:\n",
    "        - name: catalog\n",
    "          default: retailhub_trainer\n",
    "        - name: source_path\n",
    "          default: /Volumes/retailhub_trainer/default/datasets\n",
    "```\n",
    "\n",
    "> **Key differences:** The customer pipeline has **no trigger** (runs manually or on-demand), while the orders pipeline uses a **table_update trigger** on `silver_customers`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abde1d6d-237d-4cdd-8655-9969c8f62421",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Section 2: Practice — Orchestration Concepts & Configuration\n",
    "\n",
    "Complete the exercises below to reinforce your understanding of Lakeflow Jobs orchestration.\n",
    "\n",
    "| Task | Topic | Skills Tested |\n",
    "|------|-------|---------------|\n",
    "| 1 | Job Configuration (JSON) | Reading job definitions, timeouts, retries |\n",
    "| 2 | Trigger Types | Matching scenarios to trigger types |\n",
    "| 3 | CRON Expressions | Writing schedule expressions |\n",
    "| 4 | Task Dependencies (DAG) | Fan-out / fan-in patterns |\n",
    "| 5 | Repair Runs | Understanding re-execution scope |\n",
    "| 6 | Task Values | Passing parameters between tasks |\n",
    "| 7 | System Tables | SQL queries for job monitoring |\n",
    "| 8 | Cluster Selection | Job cluster vs. All-purpose cluster |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6a2c3ad-5d41-47d2-b086-114d364e1b4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "### Task 1: Understanding Job Configuration (JSON)\n",
    "\n",
    "Databricks Jobs can be configured via UI or programmatically via **REST API / JSON / YAML**.\n",
    "Examine the job configuration below and answer the questions.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"name\": \"RetailHub_Daily_Refresh\",\n",
    "  \"tasks\": [\n",
    "    {\n",
    "      \"task_key\": \"refresh_pipeline\",\n",
    "      \"pipeline_task\": { \"pipeline_id\": \"<PIPELINE_ID>\" },\n",
    "      \"timeout_seconds\": 1800\n",
    "    },\n",
    "    {\n",
    "      \"task_key\": \"validate_results\",\n",
    "      \"depends_on\": [{ \"task_key\": \"refresh_pipeline\" }],\n",
    "      \"notebook_task\": { \"notebook_path\": \"/Workspace/.../task_01_validate\" },\n",
    "      \"max_retries\": 2,\n",
    "      \"retry_on_timeout\": false\n",
    "    },\n",
    "    {\n",
    "      \"task_key\": \"generate_report\",\n",
    "      \"depends_on\": [{ \"task_key\": \"validate_results\" }],\n",
    "      \"notebook_task\": { \"notebook_path\": \"/Workspace/.../task_03_report\" }\n",
    "    }\n",
    "  ],\n",
    "  \"trigger\": {\n",
    "    \"periodic\": { \"interval\": 1, \"unit\": \"DAYS\" }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "> **Hint:** Pay attention to `timeout_seconds`, `max_retries`, and the `depends_on` chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1deb91d5-bc08-44bf-82e2-6f442f90ae79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Answer the questions about the job configuration above\n",
    "\n",
    "# Q1: How many tasks does this job have?\n",
    "num_tasks = ______  # int\n",
    "\n",
    "# Q2: Which task runs first (has no dependencies)?\n",
    "first_task = \"______\"  # str\n",
    "\n",
    "# Q3: What is the maximum time (in minutes) the pipeline task can run before timeout?\n",
    "timeout_minutes = ______  # int\n",
    "\n",
    "# Q4: How many times will validate_results retry on failure?\n",
    "max_retries = ______  # int\n",
    "\n",
    "# Q5: If refresh_pipeline fails, will validate_results run?\n",
    "validate_runs_on_failure = ______  # bool (True/False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3436de64-98e0-419c-aba5-af363a2e67f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification\n",
    "assert num_tasks == 3, f\"Expected 3 tasks, got {num_tasks}\"\n",
    "assert first_task == \"refresh_pipeline\", f\"First task should be refresh_pipeline, got {first_task}\"\n",
    "assert timeout_minutes == 30, f\"1800 seconds = 30 minutes, got {timeout_minutes}\"\n",
    "assert max_retries == 2, f\"Expected 2 retries, got {max_retries}\"\n",
    "assert validate_runs_on_failure == False, \"Dependent tasks do NOT run if their dependency fails\"\n",
    "print(\"Task 1 PASSED: Job configuration understood correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7c114ec-5acb-4170-8fd2-312d794f38c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "### Task 2: Trigger Types\n",
    "\n",
    "Match each business scenario to the correct **trigger type**.\n",
    "\n",
    "| Scenario | Trigger Type |\n",
    "|----------|:------------:|\n",
    "| Run ETL pipeline every day at 6 AM | ? |\n",
    "| Process files as soon as they land in a Volume | ? |\n",
    "| Continuously process streaming data with minimal latency | ? |\n",
    "| Run only when manually triggered by a data engineer | ? |\n",
    "\n",
    "> **Options:** `scheduled`, `file_arrival`, `continuous`, `manual`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "958dc714-e55f-4e5e-a8ce-da34b43b11d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Fill in the correct trigger type for each scenario\n",
    "# Options: \"scheduled\", \"file_arrival\", \"continuous\", \"manual\"\n",
    "\n",
    "trigger_daily_6am = \"______\"\n",
    "trigger_new_files = \"______\"\n",
    "trigger_streaming = \"______\"\n",
    "trigger_adhoc = \"______\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2287c95b-18fd-420c-842c-374a37f967a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification\n",
    "assert trigger_daily_6am == \"scheduled\", f\"Daily at 6 AM = scheduled trigger, got {trigger_daily_6am}\"\n",
    "assert trigger_new_files == \"file_arrival\", f\"Process on file landing = file_arrival, got {trigger_new_files}\"\n",
    "assert trigger_streaming == \"continuous\", f\"Minimal latency streaming = continuous, got {trigger_streaming}\"\n",
    "assert trigger_adhoc == \"manual\", f\"Ad-hoc = manual, got {trigger_adhoc}\"\n",
    "print(\"Task 2 PASSED: Trigger types matched correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e49f6492-3185-4691-92fb-337d63975421",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "### Task 3: CRON Expressions\n",
    "\n",
    "Write the CRON expression for each schedule using the standard 5-field format.\n",
    "\n",
    "**Format:** `minute hour day_of_month month day_of_week`\n",
    "\n",
    "| Field | Allowed Values |\n",
    "|-------|---------------|\n",
    "| Minute | `0–59` |\n",
    "| Hour | `0–23` |\n",
    "| Day of month | `1–31` |\n",
    "| Month | `1–12` |\n",
    "| Day of week | `0–6` (0 = Sunday) or `1-5` for Mon–Fri |\n",
    "\n",
    "> **Tip:** Use `*` for \"every\" and `*/N` for \"every N units\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05dd2cdf-4586-4ba2-8d31-ef0cec777bda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Write CRON expressions\n",
    "\n",
    "# Every day at 6:00 AM\n",
    "cron_daily_6am = \"______\"\n",
    "\n",
    "# Every hour (at minute 0)\n",
    "cron_hourly = \"______\"\n",
    "\n",
    "# Monday to Friday at 8:00 AM\n",
    "cron_weekdays_8am = \"______\"\n",
    "\n",
    "# Every 15 minutes\n",
    "cron_every_15min = \"______\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a209f67-2b37-4325-ae6c-6e428d473ea6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification\n",
    "assert cron_daily_6am == \"0 6 * * *\", f\"Expected '0 6 * * *', got '{cron_daily_6am}'\"\n",
    "assert cron_hourly == \"0 * * * *\", f\"Expected '0 * * * *', got '{cron_hourly}'\"\n",
    "assert cron_weekdays_8am == \"0 8 * * 1-5\", f\"Expected '0 8 * * 1-5', got '{cron_weekdays_8am}'\"\n",
    "assert cron_every_15min == \"*/15 * * * *\", f\"Expected '*/15 * * * *', got '{cron_every_15min}'\"\n",
    "print(\"Task 3 PASSED: CRON expressions correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81184dd8-5990-45aa-8e66-9b73f3da3f38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "### Task 4: Task Dependencies — DAG Design\n",
    "\n",
    "Design a task dependency graph (DAG) for the following requirements:\n",
    "\n",
    "1. **`ingest`** — runs first (no dependencies)\n",
    "2. **`build_dim_tables`** and **`build_fact_tables`** — run **in parallel** after `ingest` completes *(fan-out)*\n",
    "3. **`generate_report`** — runs only after **both** DIM and FACT tasks complete *(fan-in)*\n",
    "4. **`send_notification`** — runs after `generate_report`\n",
    "\n",
    "```\n",
    "  ingest\n",
    "    ├── build_dim_tables ──┐\n",
    "    └── build_fact_tables ─┤\n",
    "                           └── generate_report\n",
    "                                 └── send_notification\n",
    "```\n",
    "\n",
    "Define the dependencies for each task as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77bccd3d-6dad-4ad6-99ab-f095df688c5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Define task dependencies\n",
    "# Use a list of task names that each task depends on.\n",
    "# An empty list [] means no dependencies (runs first).\n",
    "\n",
    "task_dependencies = {\n",
    "    \"ingest\":            ______,  # list of dependencies\n",
    "    \"build_dim_tables\":  ______,  # list of dependencies\n",
    "    \"build_fact_tables\": ______,  # list of dependencies\n",
    "    \"generate_report\":   ______,  # list of dependencies\n",
    "    \"send_notification\":  ______,  # list of dependencies\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb1eee3a-46db-40cd-ae7b-d7bba8ebedce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification\n",
    "assert task_dependencies[\"ingest\"] == [], \"Ingest has no dependencies\"\n",
    "assert task_dependencies[\"build_dim_tables\"] == [\"ingest\"], \"DIM depends on ingest\"\n",
    "assert task_dependencies[\"build_fact_tables\"] == [\"ingest\"], \"FACT depends on ingest\"\n",
    "assert sorted(task_dependencies[\"generate_report\"]) == [\"build_dim_tables\", \"build_fact_tables\"], \\\n",
    "    \"Report depends on BOTH dim and fact (fan-in)\"\n",
    "assert task_dependencies[\"send_notification\"] == [\"generate_report\"], \"Notification depends on report\"\n",
    "print(\"Task 4 PASSED: DAG dependencies correct\")\n",
    "print()\n",
    "print(\"DAG structure:\")\n",
    "print(\"  ingest\")\n",
    "print(\"    +-- build_dim_tables\")\n",
    "print(\"    +-- build_fact_tables\")\n",
    "print(\"          +-- generate_report (waits for both)\")\n",
    "print(\"                +-- send_notification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95f0187d-0fb8-421b-82c2-b815eb065562",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "### Task 5: Repair Run Scenarios\n",
    "\n",
    "The job from Task 4 ran, but **`build_fact_tables` failed**. All other completed tasks succeeded.\n",
    "\n",
    "Answer the following questions about **Repair Run** behavior.\n",
    "\n",
    "> **Reminder:** A Repair Run re-executes only the **failed task** and all its **downstream dependents** — it skips tasks that already succeeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "853b72b3-e3cf-4a9d-9733-2656ebfb1db6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Answer Repair Run questions\n",
    "\n",
    "# Q1: Which tasks will be RE-EXECUTED during a Repair Run?\n",
    "# Options: list the task names that will run again\n",
    "repair_rerun_tasks = [______]  # list of str\n",
    "\n",
    "# Q2: Will 'ingest' run again during repair?\n",
    "ingest_reruns = ______  # bool\n",
    "\n",
    "# Q3: Will 'build_dim_tables' run again during repair?\n",
    "dim_reruns = ______  # bool\n",
    "\n",
    "# Q4: Is Repair Run cheaper than a full re-run? (less compute used)\n",
    "repair_is_cheaper = ______  # bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36e061dd-66a1-452d-b527-42be1d7306ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification\n",
    "expected_repair = sorted([\"build_fact_tables\", \"generate_report\", \"send_notification\"])\n",
    "assert sorted(repair_rerun_tasks) == expected_repair, \\\n",
    "    f\"Repair re-runs the failed task + all downstream. Expected {expected_repair}, got {sorted(repair_rerun_tasks)}\"\n",
    "assert ingest_reruns == False, \"Ingest succeeded -- NOT re-executed in repair\"\n",
    "assert dim_reruns == False, \"DIM succeeded -- NOT re-executed in repair\"\n",
    "assert repair_is_cheaper == True, \"Repair skips successful tasks, using less compute\"\n",
    "print(\"Task 5 PASSED: Repair Run behavior understood correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76d9e128-7cf2-4595-b86f-01d792a4dc3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "### Task 6: Passing Parameters Between Tasks (`dbutils.jobs.taskValues`)\n",
    "\n",
    "Tasks within a Lakeflow Job can share data using **`dbutils.jobs.taskValues`**.\n",
    "\n",
    "**Setting a value (in Task A):**\n",
    "```python\n",
    "dbutils.jobs.taskValues.set(key=\"row_count\", value=42)\n",
    "```\n",
    "\n",
    "**Getting a value (in Task B, which depends on Task A):**\n",
    "```python\n",
    "count = dbutils.jobs.taskValues.get(taskKey=\"task_a\", key=\"row_count\")\n",
    "```\n",
    "\n",
    "> Since we are not running inside a Job, the code below **simulates** this mechanism using a dictionary. Complete the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5536e587-ae7c-492e-b265-4cf9b9bb2cd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Complete the task value operations\n",
    "# Since we are not inside a Job, we will simulate with a dictionary\n",
    "\n",
    "# Simulated task values store\n",
    "task_values = {}\n",
    "\n",
    "# Task A: \"refresh_pipeline\" -- sets the number of rows processed\n",
    "rows_processed = 15420\n",
    "task_values[\"refresh_pipeline\"] = {\"rows_processed\": ______}  # TODO: set the value\n",
    "\n",
    "# Task A also sets the processing timestamp\n",
    "from datetime import datetime\n",
    "processing_time = datetime.now().isoformat()\n",
    "task_values[\"refresh_pipeline\"][\"processing_time\"] = ______  # TODO: set the value\n",
    "\n",
    "# Task B: \"validate_results\" -- reads values from Task A\n",
    "retrieved_rows = task_values[______][______]  # TODO: get rows_processed from refresh_pipeline\n",
    "retrieved_time = task_values[\"refresh_pipeline\"][\"processing_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8b00d8f-4fe3-413f-b80f-0c43dc7f595f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification\n",
    "assert task_values[\"refresh_pipeline\"][\"rows_processed\"] == 15420, \"rows_processed should be 15420\"\n",
    "assert task_values[\"refresh_pipeline\"][\"processing_time\"] == processing_time, \"processing_time should match\"\n",
    "assert retrieved_rows == 15420, \"Should retrieve 15420 from refresh_pipeline\"\n",
    "print(\"Task 6 PASSED: Task value passing works correctly\")\n",
    "print(f\"  rows_processed: {retrieved_rows}\")\n",
    "print(f\"  processing_time: {retrieved_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b77dceab-16fd-4523-901a-c63bf25f3919",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "### Task 7: Job Monitoring via System Tables\n",
    "\n",
    "Databricks **system tables** provide metadata about job executions for monitoring and auditing.\n",
    "\n",
    "| System Table | Content |\n",
    "|-------------|---------|\n",
    "| `system.lakeflow.job_run_timeline` | Job-level run history (status, duration, result) |\n",
    "| `system.lakeflow.job_task_run_timeline` | Task-level run details (per-task status, timing) |\n",
    "\n",
    "Write a SQL query to find all **failed** job runs in the last **7 days**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c61b9ac-8b79-4e0f-869b-dc5769847391",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Write a SQL query to find all FAILED job runs in the last 7 days\n",
    "# Columns available: job_id, run_id, result_state, start_time, end_time\n",
    "\n",
    "query_failed_runs = \"\"\"\n",
    "SELECT job_id, run_id, result_state, start_time, end_time\n",
    "FROM system.lakeflow.job_run_timeline\n",
    "WHERE result_state = '______'\n",
    "  AND start_time >= current_date() - INTERVAL ______ DAYS\n",
    "ORDER BY start_time DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d35c872f-22fa-4258-a91e-af40a14426e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification\n",
    "assert \"FAILED\" in query_failed_runs.upper(), \"Should filter for FAILED result_state\"\n",
    "assert \"7\" in query_failed_runs, \"Should look back 7 days\"\n",
    "assert \"ORDER BY\" in query_failed_runs.upper(), \"Should order results\"\n",
    "print(\"Task 7 PASSED: System table query is correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42349e1f-aa72-473d-8825-5a13a2ed1e57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "### Task 8: Job Cluster vs. All-Purpose Cluster\n",
    "\n",
    "Choose the correct **cluster type** for each scenario.\n",
    "\n",
    "| Scenario | Cluster Type |\n",
    "|----------|:------------:|\n",
    "| Scheduled nightly ETL job (runs 2 hours) | ? |\n",
    "| Interactive notebook development | ? |\n",
    "| Ad-hoc data investigation by an analyst | ? |\n",
    "| Production ML model training (weekly) | ? |\n",
    "\n",
    "> **Rule of thumb:** Use **Job clusters** for automated/scheduled workloads (cost-optimized, auto-terminates). Use **All-purpose clusters** for interactive/development work (long-running, shared)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "157cfeb4-860d-45d9-9d51-5a696a3e1105",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Assign the correct cluster type\n",
    "# Options: \"job_cluster\" or \"all_purpose\"\n",
    "\n",
    "# Scenario 1: Scheduled nightly ETL job that runs for 2 hours\n",
    "nightly_etl = \"______\"\n",
    "\n",
    "# Scenario 2: Interactive notebook development and exploration\n",
    "interactive_dev = \"______\"\n",
    "\n",
    "# Scenario 3: Ad-hoc data investigation by an analyst\n",
    "adhoc_analysis = \"______\"\n",
    "\n",
    "# Scenario 4: Production ML model training triggered weekly\n",
    "ml_training = \"______\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a252f758-15db-4b25-9351-a9e8443fa4e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification\n",
    "assert nightly_etl == \"job_cluster\", \"Scheduled ETL -> job cluster (cheaper, auto-terminates)\"\n",
    "assert interactive_dev == \"all_purpose\", \"Interactive work -> all-purpose (stays running)\"\n",
    "assert adhoc_analysis == \"all_purpose\", \"Ad-hoc analysis -> all-purpose (interactive)\"\n",
    "assert ml_training == \"job_cluster\", \"Scheduled ML training -> job cluster (cost-effective)\"\n",
    "print(\"Task 8 PASSED: Cluster type selection correct\")\n",
    "print()\n",
    "print(\"Key insight:\")\n",
    "print(\"  job_cluster -> automated/scheduled workloads (cost-optimized)\")\n",
    "print(\"  all_purpose -> interactive/development work (long-running)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c04b7a26-4cdb-4eea-96c2-109a6f65994e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Section 1 — Workshop\n",
    "You created two Lakeflow Jobs through the Databricks UI, configured task dependencies, set up a **Table Update trigger**, and executed the full pipeline end-to-end.\n",
    "\n",
    "### Section 2 — Practice\n",
    "You reinforced key orchestration concepts:\n",
    "\n",
    "| Topic | Key Takeaway |\n",
    "|-------|-------------|\n",
    "| **Job Configuration** | Jobs are defined as JSON/YAML with tasks, dependencies, triggers, and compute settings |\n",
    "| **Trigger Types** | `scheduled`, `file_arrival`, `continuous`, `manual` — choose based on latency requirements |\n",
    "| **CRON Expressions** | 5-field format: `minute hour day month weekday` |\n",
    "| **DAG Dependencies** | Fan-out (parallel) and fan-in (wait for all) patterns |\n",
    "| **Repair Runs** | Re-execute only the failed task + downstream — saves compute cost |\n",
    "| **Task Values** | `dbutils.jobs.taskValues` for passing data between tasks |\n",
    "| **System Tables** | `system.lakeflow.job_run_timeline` for monitoring and auditing |\n",
    "| **Cluster Selection** | Job clusters for automation, All-purpose for interactive work |\n",
    "\n",
    "> **Exam Tip:** Focus on **when** to use each trigger type, how repair runs scope re-execution, and the cost implications of cluster selection."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LAB_08_code",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
