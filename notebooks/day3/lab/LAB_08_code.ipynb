{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41699978-84ad-4b47-a921-54997813609c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# LAB 08: Lakeflow Jobs -- Triggers, Dependencies & Orchestration\n",
    "\n",
    "**Duration:** ~30 min | **Day:** 3 | **Difficulty:** Intermediate\n",
    "**After module:** M08: Lakeflow Jobs & Orchestration\n",
    "\n",
    "> *\"The RetailHub pipeline works -- now automate it. Configure triggers, define task dependencies, handle failures with repair runs, and monitor via system tables.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "885a5382-9fb1-4177-a26f-4a04a1ef9d2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76ea348f-e058-4144-a29f-ecbf80b581cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../setup/00_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef6a9321-1d04-4929-8c9d-d0eee934dc79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Quide how to create jobs :\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c126c89-4a1a-465c-9da0-ba7d4c3448b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Workshop Section 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0405ea48-2910-4744-aeff-a59996890974",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "customer_pipeline_job definition\n",
    "\n",
    "Go to Jobs & Pipelines -> Next click on Job button\n",
    "![image_1771359192397.png](./image_1771359192397.png \"image_1771359192397.png\")\n",
    "\n",
    "Create unique name of your job\n",
    "\n",
    "![image_1771359690424.png](./image_1771359690424.png \"image_1771359690424.png\")\n",
    "\n",
    "IN Job details you will find information about create person (you) and unique Job ID - name not to be uniqie becasue each job ID have a uniqeue ID \n",
    "\n",
    "![image_1771359327410.png](./image_1771359327410.png \"image_1771359327410.png\")\n",
    "\n",
    "Add job parameters as catalog and source path \n",
    "\n",
    "![image_1771359805604.png](./image_1771359805604.png \"image_1771359805604.png\")\n",
    "\n",
    "Next clikc to add first task :\n",
    "\n",
    "![image_1771359378110.png](./image_1771359378110.png \"image_1771359378110.png\")\n",
    "\n",
    "\n",
    "\n",
    "![image_1771359619306.png](./image_1771359619306.png \"image_1771359619306.png\")\n",
    "choose notebook type task and write \n",
    "task name bronze_customer\n",
    "type Notebook\n",
    "source workspace\n",
    "Path : choose path where you have notebook with that table definition it should be path \" \" \n",
    "Compute * - pleae use our shared computed to run this job immedietly * it si not good practices becasue typcily we need to creat job compute for job purpuse.\n",
    "At the eend please click create task \n",
    "\n",
    "Next please do the same job for silver_customer \n",
    "\n",
    "that should be result of your job:\n",
    "\n",
    "![image_1771360453155.png](./image_1771360453155.png \"image_1771360453155.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b4e5408-1e93-42b9-9b3e-2ef6c0057ca4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "order_job definition\n",
    "\n",
    "please again create new job,  and do the same step as before and plase use name orders_pipeline.\n",
    "\n",
    "that should be look like your pipeline \n",
    "\n",
    "![image_1771360520728.png](./image_1771360520728.png \"image_1771360520728.png\")\n",
    "\n",
    "Next please go to trigger and scheulde table update trigger, please choos silver_customer\n",
    "\n",
    "![image_1771360648106.png](./image_1771360648106.png \"image_1771360648106.png\")\n",
    "\n",
    "\n",
    "ok now we are ready to run job -please run first - customer job \n",
    "\n",
    "![image_1771360722266.png](./image_1771360722266.png \"image_1771360722266.png\")\n",
    "\n",
    "As the result you should see that both job are run and silver_customer update trigger another job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba19a4a6-5def-487e-9997-1a29731c3b68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "yaml for demo_order_pipeline\n",
    "\n",
    "resources:\n",
    "  jobs:\n",
    "    demo_orders_pipeline:\n",
    "      name: demo_orders_pipeline\n",
    "      trigger:\n",
    "        pause_status: UNPAUSED\n",
    "        table_update:\n",
    "          table_names:\n",
    "            - retailhub_trainer.silver.silver_customers\n",
    "      tasks:\n",
    "        - task_key: bronze_orders\n",
    "          notebook_task:\n",
    "            notebook_path: /Workspace/Users/krzysztof.burejza@outlook.com/databricks-de-associate-training/materials/medallion/bronze_orders\n",
    "            source: WORKSPACE\n",
    "          existing_cluster_id: 0215-230117-3c5px09z\n",
    "        - task_key: silver_customer\n",
    "          depends_on:\n",
    "            - task_key: bronze_orders\n",
    "          notebook_task:\n",
    "            notebook_path: /Workspace/Users/krzysztof.burejza@outlook.com/databricks-de-associate-training/materials/medallion/silver_customers\n",
    "            source: WORKSPACE\n",
    "          existing_cluster_id: 0215-230117-3c5px09z\n",
    "        - task_key: gold_daily_orders\n",
    "          depends_on:\n",
    "            - task_key: silver_customer\n",
    "          notebook_task:\n",
    "            notebook_path: /Workspace/Users/krzysztof.burejza@outlook.com/databricks-de-associate-training/materials/medallion/gold_daily_orders\n",
    "            source: WORKSPACE\n",
    "          existing_cluster_id: 0215-230117-3c5px09z\n",
    "        - task_key: gold_customer_orders_summary\n",
    "          depends_on:\n",
    "            - task_key: gold_daily_orders\n",
    "          notebook_task:\n",
    "            notebook_path: /Workspace/Users/krzysztof.burejza@outlook.com/databricks-de-associate-training/materials/medallion/gold_customer_orders_summary\n",
    "            source: WORKSPACE\n",
    "          existing_cluster_id: 0215-230117-3c5px09z\n",
    "      queue:\n",
    "        enabled: true\n",
    "      parameters:\n",
    "        - name: catalog\n",
    "          default: retailhub_trainer\n",
    "        - name: source_path\n",
    "          default: /Volumes/retailhub_trainer/default/datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a00a4b4e-3399-43c0-a89d-3879054f8021",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "yaml for demo_customer_pipeline\n",
    "\n",
    "resources:\n",
    "  jobs:\n",
    "    demo_customer_pipeline:\n",
    "      name: demo_customer_pipeline\n",
    "      tasks:\n",
    "        - task_key: bronze_customer\n",
    "          notebook_task:\n",
    "            notebook_path: /Workspace/Users/krzysztof.burejza@outlook.com/databricks-de-associate-training/materials/medallion/bronze_customers\n",
    "            source: WORKSPACE\n",
    "          existing_cluster_id: 0215-230117-3c5px09z\n",
    "        - task_key: silver_customer\n",
    "          depends_on:\n",
    "            - task_key: bronze_customer\n",
    "          notebook_task:\n",
    "            notebook_path: /Workspace/Users/krzysztof.burejza@outlook.com/databricks-de-associate-training/materials/medallion/silver_customers\n",
    "            source: WORKSPACE\n",
    "          existing_cluster_id: 0215-230117-3c5px09z\n",
    "      queue:\n",
    "        enabled: true\n",
    "      parameters:\n",
    "        - name: catalog\n",
    "          default: retailhub_trainer\n",
    "        - name: source_path\n",
    "          default: /Volumes/retailhub_trainer/default/datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abde1d6d-237d-4cdd-8655-9969c8f62421",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Workshop Section 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6a2c3ad-5d41-47d2-b086-114d364e1b4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Task 1: Understanding Job Configuration (JSON)\n",
    "\n",
    "Databricks Jobs can be configured via UI or programmatically via REST API / JSON.\n",
    "Examine the job configuration structure below and answer the questions.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"name\": \"RetailHub_Daily_Refresh\",\n",
    "  \"tasks\": [\n",
    "    {\n",
    "      \"task_key\": \"refresh_pipeline\",\n",
    "      \"pipeline_task\": { \"pipeline_id\": \"<PIPELINE_ID>\" },\n",
    "      \"timeout_seconds\": 1800\n",
    "    },\n",
    "    {\n",
    "      \"task_key\": \"validate_results\",\n",
    "      \"depends_on\": [{ \"task_key\": \"refresh_pipeline\" }],\n",
    "      \"notebook_task\": { \"notebook_path\": \"/Workspace/.../task_01_validate\" },\n",
    "      \"max_retries\": 2,\n",
    "      \"retry_on_timeout\": false\n",
    "    },\n",
    "    {\n",
    "      \"task_key\": \"generate_report\",\n",
    "      \"depends_on\": [{ \"task_key\": \"validate_results\" }],\n",
    "      \"notebook_task\": { \"notebook_path\": \"/Workspace/.../task_03_report\" }\n",
    "    }\n",
    "  ],\n",
    "  \"trigger\": {\n",
    "    \"periodic\": { \"interval\": 1, \"unit\": \"DAYS\" }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1deb91d5-bc08-44bf-82e2-6f442f90ae79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Answer the questions about the job configuration above\n",
    "\n",
    "# Q1: How many tasks does this job have?\n",
    "num_tasks = ______  # int\n",
    "\n",
    "# Q2: Which task runs first (has no dependencies)?\n",
    "first_task = \"______\"  # str\n",
    "\n",
    "# Q3: What is the maximum time (in minutes) the pipeline task can run before timeout?\n",
    "timeout_minutes = ______  # int\n",
    "\n",
    "# Q4: How many times will validate_results retry on failure?\n",
    "max_retries = ______  # int\n",
    "\n",
    "# Q5: If refresh_pipeline fails, will validate_results run?\n",
    "validate_runs_on_failure = ______  # bool (True/False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3436de64-98e0-419c-aba5-af363a2e67f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification\n",
    "assert num_tasks == 3, f\"Expected 3 tasks, got {num_tasks}\"\n",
    "assert first_task == \"refresh_pipeline\", f\"First task should be refresh_pipeline, got {first_task}\"\n",
    "assert timeout_minutes == 30, f\"1800 seconds = 30 minutes, got {timeout_minutes}\"\n",
    "assert max_retries == 2, f\"Expected 2 retries, got {max_retries}\"\n",
    "assert validate_runs_on_failure == False, \"Dependent tasks do NOT run if their dependency fails\"\n",
    "print(\"Task 1 PASSED: Job configuration understood correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7c114ec-5acb-4170-8fd2-312d794f38c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Task 2: Trigger Types\n",
    "\n",
    "Match each scenario to the correct trigger type.\n",
    "\n",
    "| Scenario | Trigger Type |\n",
    "|----------|-------------|\n",
    "| Run ETL pipeline every day at 6 AM | ? |\n",
    "| Process files as soon as they land in a Volume | ? |\n",
    "| Continuously process streaming data with minimal latency | ? |\n",
    "| Run only when manually triggered by a data engineer | ? |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "958dc714-e55f-4e5e-a8ce-da34b43b11d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Fill in the correct trigger type for each scenario\n",
    "# Options: \"scheduled\", \"file_arrival\", \"continuous\", \"manual\"\n",
    "\n",
    "trigger_daily_6am = \"______\"\n",
    "trigger_new_files = \"______\"\n",
    "trigger_streaming = \"______\"\n",
    "trigger_adhoc = \"______\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2287c95b-18fd-420c-842c-374a37f967a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification\n",
    "assert trigger_daily_6am == \"scheduled\", f\"Daily at 6 AM = scheduled trigger, got {trigger_daily_6am}\"\n",
    "assert trigger_new_files == \"file_arrival\", f\"Process on file landing = file_arrival, got {trigger_new_files}\"\n",
    "assert trigger_streaming == \"continuous\", f\"Minimal latency streaming = continuous, got {trigger_streaming}\"\n",
    "assert trigger_adhoc == \"manual\", f\"Ad-hoc = manual, got {trigger_adhoc}\"\n",
    "print(\"Task 2 PASSED: Trigger types matched correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e49f6492-3185-4691-92fb-337d63975421",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Task 3: CRON Expressions\n",
    "\n",
    "Write the CRON expression for each schedule.\n",
    "\n",
    "CRON format: `minute hour day_of_month month day_of_week`\n",
    "\n",
    "| Field | Values |\n",
    "|-------|--------|\n",
    "| Minute | 0-59 |\n",
    "| Hour | 0-23 |\n",
    "| Day of month | 1-31 |\n",
    "| Month | 1-12 |\n",
    "| Day of week | 0-6 (0=Sunday) or 1-5 for Mon-Fri |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05dd2cdf-4586-4ba2-8d31-ef0cec777bda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Write CRON expressions\n",
    "\n",
    "# Every day at 6:00 AM\n",
    "cron_daily_6am = \"______\"\n",
    "\n",
    "# Every hour (at minute 0)\n",
    "cron_hourly = \"______\"\n",
    "\n",
    "# Monday to Friday at 8:00 AM\n",
    "cron_weekdays_8am = \"______\"\n",
    "\n",
    "# Every 15 minutes\n",
    "cron_every_15min = \"______\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a209f67-2b37-4325-ae6c-6e428d473ea6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification\n",
    "assert cron_daily_6am == \"0 6 * * *\", f\"Expected '0 6 * * *', got '{cron_daily_6am}'\"\n",
    "assert cron_hourly == \"0 * * * *\", f\"Expected '0 * * * *', got '{cron_hourly}'\"\n",
    "assert cron_weekdays_8am == \"0 8 * * 1-5\", f\"Expected '0 8 * * 1-5', got '{cron_weekdays_8am}'\"\n",
    "assert cron_every_15min == \"*/15 * * * *\", f\"Expected '*/15 * * * *', got '{cron_every_15min}'\"\n",
    "print(\"Task 3 PASSED: CRON expressions correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81184dd8-5990-45aa-8e66-9b73f3da3f38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Task 4: Task Dependencies -- DAG Design\n",
    "\n",
    "The RetailHub team needs a more complex job with the following requirements:\n",
    "\n",
    "1. **Ingest** task runs first (no dependencies)\n",
    "2. **Build DIM tables** and **Build FACT tables** run in parallel AFTER Ingest\n",
    "3. **Generate Report** runs AFTER both DIM and FACT are complete\n",
    "4. **Send Notification** runs AFTER Generate Report\n",
    "\n",
    "Define the dependencies for each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77bccd3d-6dad-4ad6-99ab-f095df688c5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Define task dependencies\n",
    "# Use a list of task names that each task depends on.\n",
    "# An empty list [] means no dependencies (runs first).\n",
    "\n",
    "task_dependencies = {\n",
    "    \"ingest\":            ______,  # list of dependencies\n",
    "    \"build_dim_tables\":  ______,  # list of dependencies\n",
    "    \"build_fact_tables\": ______,  # list of dependencies\n",
    "    \"generate_report\":   ______,  # list of dependencies\n",
    "    \"send_notification\":  ______,  # list of dependencies\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb1eee3a-46db-40cd-ae7b-d7bba8ebedce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification\n",
    "assert task_dependencies[\"ingest\"] == [], \"Ingest has no dependencies\"\n",
    "assert task_dependencies[\"build_dim_tables\"] == [\"ingest\"], \"DIM depends on ingest\"\n",
    "assert task_dependencies[\"build_fact_tables\"] == [\"ingest\"], \"FACT depends on ingest\"\n",
    "assert sorted(task_dependencies[\"generate_report\"]) == [\"build_dim_tables\", \"build_fact_tables\"], \\\n",
    "    \"Report depends on BOTH dim and fact (fan-in)\"\n",
    "assert task_dependencies[\"send_notification\"] == [\"generate_report\"], \"Notification depends on report\"\n",
    "print(\"Task 4 PASSED: DAG dependencies correct\")\n",
    "print()\n",
    "print(\"DAG structure:\")\n",
    "print(\"  ingest\")\n",
    "print(\"    +-- build_dim_tables\")\n",
    "print(\"    +-- build_fact_tables\")\n",
    "print(\"          +-- generate_report (waits for both)\")\n",
    "print(\"                +-- send_notification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95f0187d-0fb8-421b-82c2-b815eb065562",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Task 5: Repair Run Scenarios\n",
    "\n",
    "The job from Task 4 ran, but `build_fact_tables` failed.\n",
    "Answer the following questions about Repair Run behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "853b72b3-e3cf-4a9d-9733-2656ebfb1db6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Answer Repair Run questions\n",
    "\n",
    "# Q1: Which tasks will be RE-EXECUTED during a Repair Run?\n",
    "# Options: list the task names that will run again\n",
    "repair_rerun_tasks = [______]  # list of str\n",
    "\n",
    "# Q2: Will 'ingest' run again during repair?\n",
    "ingest_reruns = ______  # bool\n",
    "\n",
    "# Q3: Will 'build_dim_tables' run again during repair?\n",
    "dim_reruns = ______  # bool\n",
    "\n",
    "# Q4: Is Repair Run cheaper than a full re-run? (less compute used)\n",
    "repair_is_cheaper = ______  # bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36e061dd-66a1-452d-b527-42be1d7306ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification\n",
    "expected_repair = sorted([\"build_fact_tables\", \"generate_report\", \"send_notification\"])\n",
    "assert sorted(repair_rerun_tasks) == expected_repair, \\\n",
    "    f\"Repair re-runs the failed task + all downstream. Expected {expected_repair}, got {sorted(repair_rerun_tasks)}\"\n",
    "assert ingest_reruns == False, \"Ingest succeeded -- NOT re-executed in repair\"\n",
    "assert dim_reruns == False, \"DIM succeeded -- NOT re-executed in repair\"\n",
    "assert repair_is_cheaper == True, \"Repair skips successful tasks, using less compute\"\n",
    "print(\"Task 5 PASSED: Repair Run behavior understood correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76d9e128-7cf2-4595-b86f-01d792a4dc3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Task 6: Passing Parameters Between Tasks (dbutils.jobs.taskValues)\n",
    "\n",
    "In Databricks, tasks within a Job can share data using `dbutils.jobs.taskValues`.\n",
    "\n",
    "**Setter (in Task A):**\n",
    "```python\n",
    "dbutils.jobs.taskValues.set(key=\"row_count\", value=42)\n",
    "```\n",
    "\n",
    "**Getter (in Task B, which depends on Task A):**\n",
    "```python\n",
    "count = dbutils.jobs.taskValues.get(taskKey=\"task_a\", key=\"row_count\")\n",
    "```\n",
    "\n",
    "Complete the code below to simulate parameter passing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5536e587-ae7c-492e-b265-4cf9b9bb2cd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Complete the task value operations\n",
    "# Since we are not inside a Job, we will simulate with a dictionary\n",
    "\n",
    "# Simulated task values store\n",
    "task_values = {}\n",
    "\n",
    "# Task A: \"refresh_pipeline\" -- sets the number of rows processed\n",
    "rows_processed = 15420\n",
    "task_values[\"refresh_pipeline\"] = {\"rows_processed\": ______}  # TODO: set the value\n",
    "\n",
    "# Task A also sets the processing timestamp\n",
    "from datetime import datetime\n",
    "processing_time = datetime.now().isoformat()\n",
    "task_values[\"refresh_pipeline\"][\"processing_time\"] = ______  # TODO: set the value\n",
    "\n",
    "# Task B: \"validate_results\" -- reads values from Task A\n",
    "retrieved_rows = task_values[______][______]  # TODO: get rows_processed from refresh_pipeline\n",
    "retrieved_time = task_values[\"refresh_pipeline\"][\"processing_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8b00d8f-4fe3-413f-b80f-0c43dc7f595f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification\n",
    "assert task_values[\"refresh_pipeline\"][\"rows_processed\"] == 15420, \"rows_processed should be 15420\"\n",
    "assert task_values[\"refresh_pipeline\"][\"processing_time\"] == processing_time, \"processing_time should match\"\n",
    "assert retrieved_rows == 15420, \"Should retrieve 15420 from refresh_pipeline\"\n",
    "print(\"Task 6 PASSED: Task value passing works correctly\")\n",
    "print(f\"  rows_processed: {retrieved_rows}\")\n",
    "print(f\"  processing_time: {retrieved_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b77dceab-16fd-4523-901a-c63bf25f3919",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Task 7: Job Monitoring via System Tables\n",
    "\n",
    "Databricks system tables provide metadata about job executions.\n",
    "Write SQL queries to answer monitoring questions.\n",
    "\n",
    "**Key system tables:**\n",
    "- `system.lakeflow.job_run_timeline` -- Job-level run history\n",
    "- `system.lakeflow.job_task_run_timeline` -- Task-level run details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c61b9ac-8b79-4e0f-869b-dc5769847391",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Write a SQL query to find all FAILED job runs in the last 7 days\n",
    "# Columns available: job_id, run_id, result_state, start_time, end_time\n",
    "\n",
    "query_failed_runs = \"\"\"\n",
    "SELECT job_id, run_id, result_state, start_time, end_time\n",
    "FROM system.lakeflow.job_run_timeline\n",
    "WHERE result_state = '______'\n",
    "  AND start_time >= current_date() - INTERVAL ______ DAYS\n",
    "ORDER BY start_time DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d35c872f-22fa-4258-a91e-af40a14426e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification\n",
    "assert \"FAILED\" in query_failed_runs.upper(), \"Should filter for FAILED result_state\"\n",
    "assert \"7\" in query_failed_runs, \"Should look back 7 days\"\n",
    "assert \"ORDER BY\" in query_failed_runs.upper(), \"Should order results\"\n",
    "print(\"Task 7 PASSED: System table query is correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42349e1f-aa72-473d-8825-5a13a2ed1e57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Task 8: Job Cluster vs All-Purpose Cluster\n",
    "\n",
    "Choose the correct cluster type for each scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "157cfeb4-860d-45d9-9d51-5a696a3e1105",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Assign the correct cluster type\n",
    "# Options: \"job_cluster\" or \"all_purpose\"\n",
    "\n",
    "# Scenario 1: Scheduled nightly ETL job that runs for 2 hours\n",
    "nightly_etl = \"______\"\n",
    "\n",
    "# Scenario 2: Interactive notebook development and exploration\n",
    "interactive_dev = \"______\"\n",
    "\n",
    "# Scenario 3: Ad-hoc data investigation by an analyst\n",
    "adhoc_analysis = \"______\"\n",
    "\n",
    "# Scenario 4: Production ML model training triggered weekly\n",
    "ml_training = \"______\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a252f758-15db-4b25-9351-a9e8443fa4e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification\n",
    "assert nightly_etl == \"job_cluster\", \"Scheduled ETL -> job cluster (cheaper, auto-terminates)\"\n",
    "assert interactive_dev == \"all_purpose\", \"Interactive work -> all-purpose (stays running)\"\n",
    "assert adhoc_analysis == \"all_purpose\", \"Ad-hoc analysis -> all-purpose (interactive)\"\n",
    "assert ml_training == \"job_cluster\", \"Scheduled ML training -> job cluster (cost-effective)\"\n",
    "print(\"Task 8 PASSED: Cluster type selection correct\")\n",
    "print()\n",
    "print(\"Key insight:\")\n",
    "print(\"  job_cluster -> automated/scheduled workloads (cost-optimized)\")\n",
    "print(\"  all_purpose -> interactive/development work (long-running)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c04b7a26-4cdb-4eea-96c2-109a6f65994e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this lab you practiced:\n",
    "- Understanding multi-task Job configuration (JSON structure)\n",
    "- Matching trigger types to business scenarios\n",
    "- Writing CRON expressions for scheduling\n",
    "- Designing task dependency DAGs (fan-out/fan-in)\n",
    "- Repair Run behavior (re-execute failed + downstream only)\n",
    "- Passing parameters between tasks with taskValues\n",
    "- Querying system tables for job monitoring\n",
    "- Choosing between Job cluster and All-purpose cluster\n",
    "\n",
    "> **Exam Tip:** The exam tests understanding of job orchestration concepts: trigger types, dependency DAGs, repair runs, and cluster selection. Focus on WHEN to use each approach rather than memorizing API syntax."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LAB_08_code",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
