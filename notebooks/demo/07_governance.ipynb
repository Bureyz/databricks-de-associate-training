{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "c87b8be8-fafa-4b50-b590-e0fafba772ac",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "# 7. Unity Catalog & Governance \n",
        "\n",
        "**Training Objective:** Master Unity Catalog as a governance platform for Databricks Lakehouse, managing access, data masking, lineage, and audit logging\n",
        "\n",
        "**Topics Covered:**\n",
        "- Unity Catalog Architecture: Metastore, Catalog, Schema, Tables/Views/Volumes\n",
        "- Access Management: GRANT/REVOKE privileges\n",
        "- Data Masking and Row-Level Security\n",
        "- Data Lineage and Audit Logging\n",
        "- Delta Sharing - secure data sharing\n",
        "- Best Practices for Data Governance\n",
        "\n",
        "---\n",
        "\n",
        "## 7.1. Context and Requirements\n",
        "\n",
        "- **Training Day**: Day 3 - Transformation, Governance & Integrations\n",
        "- **Notebook Type**: Demo\n",
        "- **Technical Requirements**:\n",
        " - Databricks Runtime 13.0+ (recommended: 14.3 LTS)\n",
        " - Unity Catalog enabled (required!)\n",
        " - Permissions: CREATE CATALOG, CREATE SCHEMA, GRANT/REVOKE\n",
        " - Cluster: Standard with minimum 2 workers\n",
        "- **Duration**: 45 minutes\n",
        "- **Prerequisites**: 03_databricks_jobs_orchestration.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.2. Theoretical Introduction\n",
        "\n",
        "**Section Objective:** Understanding Unity Catalog as a unified governance platform for data lakehouse\n",
        "\n",
        "**Basic Concepts:**\n",
        "- **Unity Catalog**: Unified governance solution for all data assets\n",
        "- **Metastore**: Region-level container for catalogs (top-level)\n",
        "- **Three-level namespace**: catalog.schema.table\n",
        "- **Securable objects**: Tables, Views, Functions, Volumes, Models\n",
        "- **Fine-grained access control**: Table, column, row-level security\n",
        "- **Automatic lineage**: End-to-end data flow tracking without instrumentation\n",
        "\n",
        "**Unity Catalog Object Hierarchy:**\n",
        "```\n",
        "Metastore (region-level)\n",
        " â†“\n",
        "Catalog (domain/environment)\n",
        " â†“\n",
        "Schema (namespace/layer)\n",
        " â†“\n",
        "Securable Objects:\n",
        " - Tables / Views (data)\n",
        " - Functions (UDF, stored procedures)\n",
        " - Volumes (file storage)\n",
        " - Models (ML models)\n",
        "```\n",
        "\n",
        "**Key Features:**\n",
        "- **Unified governance**: Single platform for data, ML, BI\n",
        "- **ACID transactions**: Transactional guarantees at catalog level\n",
        "- **Audit logging**: Who accessed what and when\n",
        "- **Data discovery**: Metadata search and tagging\n",
        "- **Delta Sharing**: Secure cross-organization sharing\n",
        "\n",
        "**Why is this important?**\n",
        "Unity Catalog solves fundamental governance problems in data lake:\n",
        "- Lack of central access control\n",
        "- Difficulty tracking lineage\n",
        "- No data access audit\n",
        "- Compliance issues (GDPR, HIPAA)\n",
        "- Data silos between teams\n",
        "\n",
        "Unity Catalog provides enterprise-grade governance while maintaining data lakehouse flexibility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.3. Per-User Isolation\n",
        "\n",
        "Run the initialization script for per-user catalog and schema isolation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%run ../00_setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.4. Configuration\n",
        "\n",
        "Library imports and user context display:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths to data directories (subdirectories in DATASET_BASE_PATH from 00_setup)\n",
        "CUSTOMERS_PATH = f\"{DATASET_BASE_PATH}/customers\"\n",
        "ORDERS_PATH = f\"{DATASET_BASE_PATH}/orders\"\n",
        "PRODUCTS_PATH = f\"{DATASET_BASE_PATH}/products\"\n",
        "\n",
        "# Paths to specific files\n",
        "CUSTOMERS_CSV = f\"{CUSTOMERS_PATH}/customers.csv\"\n",
        "ORDERS_JSON = f\"{ORDERS_PATH}/orders_batch.json\"\n",
        "PRODUCTS_PARQUET = f\"{PRODUCTS_PATH}/products.parquet\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.5. Unity Catalog Architecture\n",
        "\n",
        "**Unity Catalog** is a unified governance solution for Databricks Lakehouse.\n",
        "\n",
        "### Object Hierarchy:\n",
        "\n",
        "```\n",
        "Metastore (region-level)\n",
        " â†“\n",
        "Catalog (database/domain)\n",
        " â†“\n",
        "Schema (namespace)\n",
        " â†“\n",
        "Securable Objects:\n",
        " - Tables / Views\n",
        " - Functions (UDF, stored procedures)\n",
        " - Volumes (files storage)\n",
        " - Models (ML models)\n",
        "```\n",
        "\n",
        "### Three-level namespace:\n",
        "```sql\n",
        "catalog.schema.table\n",
        "```\n",
        "\n",
        "Example:\n",
        "```sql\n",
        "main.sales.orders\n",
        "dev.analytics.customer_metrics\n",
        "prod.gold.daily_revenue\n",
        "```\n",
        "\n",
        "### Key Features:\n",
        "- **Unified governance**: single platform for data, ML, BI\n",
        "- **Fine-grained access control**: table, column, row level\n",
        "- **Automatic lineage**: end-to-end data flow tracking\n",
        "- **Audit logging**: who accessed what and when\n",
        "- **Data discovery**: metadata search and tagging\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.5.1. Setup and Basic Operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.5.2. Creating User Groups\n",
        "We create user groups for permission demonstration:\n",
        "- `data_engineers`: Full access to Bronze/Silver schemas\n",
        "- `data_analysts`: Read-only access to Gold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Active context verification:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verification of created schemas\n",
        "schemas = spark.sql(f\"SHOW SCHEMAS IN {CATALOG}\").select(\"databaseName\").collect()\n",
        "schema_names = [row.databaseName for row in schemas]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Active catalog and schema set**\n",
        "\n",
        "We set the default working context - all subsequent operations will be executed in this catalog and schema unless a full path is specified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verification of created schemas\n",
        "spark.sql(f\"SHOW SCHEMAS IN {CATALOG}\").display()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.6. Data Preparation\n",
        "\n",
        "Before we proceed to access management, we will load real data from the dataset/ directory that we will use in the Unity Catalog examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "orders_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").json(ORDERS_JSON)\n",
        "orders_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.orders\")\n",
        "\n",
        "display(spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.orders\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "customers_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(CUSTOMERS_CSV)\n",
        "customers_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.customers\")\n",
        "\n",
        "display(spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "products_df = spark.read.parquet(PRODUCTS_PARQUET)\n",
        "products_df.write.mode(\"overwrite\").saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.products\")\n",
        "display(spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.products\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verification of orders record count\n",
        "spark.sql(f\"SELECT COUNT(*) as count FROM {CATALOG}.{BRONZE_SCHEMA}.orders\").display()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.6.1. Add comments to table and columns\n",
        "\n",
        "You can add descriptive comments to Unity Catalog tables and columns using SQL commands. This improves data discoverability and governance.\n",
        "\n",
        "The cell below demonstrates how to add comments to a table and a specific column using Spark SQL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add comments to table and columns\n",
        "spark.sql(f\"\"\"\n",
        "    COMMENT ON TABLE {CATALOG}.{BRONZE_SCHEMA}.orders IS\n",
        "    'Cleaned orders table with data quality validations applied'\n",
        "\"\"\")\n",
        "\n",
        "spark.sql(f\"\"\"\n",
        "    COMMENT ON COLUMN {CATALOG}.{BRONZE_SCHEMA}.orders.customer_id IS\n",
        "    'Customer identifier - PII data, access restricted'\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.6.2. Add tags to orders table\n",
        "\n",
        "You can classify and manage tables in Unity Catalog using **tags** (key-value pairs). Tags help with data discovery, compliance, and governance (e.g., marking tables as PII, GDPR, or Sensitive).\n",
        "\n",
        "Example:  \n",
        "sql\n",
        "ALTER TABLE ecommerce_platform_trainer.bronze.orders\n",
        "  SET TAGS ('pii' = 'false', 'data_classification' = 'transactional', 'retention' = '7_years');\n",
        "\n",
        "- `pii`: Indicates if table contains personally identifiable information.\n",
        "- `data_classification`: Describes the type of data (e.g., transactional, reference).\n",
        "- `retention`: Specifies data retention policy.\n",
        "\n",
        "You need `APPLY TAG` privilege to add tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add tags to orders table\n",
        "spark.sql(f\"\"\"\n",
        "    ALTER TABLE {CATALOG}.{BRONZE_SCHEMA}.orders \n",
        "    SET TAGS ('sensitivity' = 'high', 'domain' = 'sales')\n",
        "\"\"\")\n",
        "\n",
        "# Add tags to customer_id column\n",
        "spark.sql(f\"\"\"\n",
        "    ALTER TABLE {CATALOG}.{BRONZE_SCHEMA}.orders \n",
        "    ALTER COLUMN customer_id SET TAGS ('pii' = 'true')\n",
        "\"\"\")\n",
        "\n",
        "display(spark.createDataFrame([(\"Status\", \" Tags added to table and column\")], [\"Info\", \"Value\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.7. Creating Tables in Unity Catalog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find all columns marked as PII\n",
        "pii_columns = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        catalog_name, \n",
        "        schema_name, \n",
        "        table_name, \n",
        "        column_name, \n",
        "        tag_value \n",
        "    FROM system.information_schema.column_tags\n",
        "    WHERE tag_name = 'pii' AND tag_value = 'true'\n",
        "      AND catalog_name = '{CATALOG}'\n",
        "\"\"\")\n",
        "\n",
        "display(pii_columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.8. Unity Catalog Volumes\n",
        "\n",
        "**Volumes** are managed spaces for storing files (non-tabular data) in Unity Catalog:\n",
        "- **Managed Volumes**: Databricks manages the file lifecycle\n",
        "- **External Volumes**: connection to external storage locations\n",
        "\n",
        "**Use cases**:\n",
        "- Storing ML models, checkpoints\n",
        "- Staging area for data before ingestion\n",
        "- Archive of documents, logs, reports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating Managed Volume\n",
        "spark.sql(f\"\"\"\n",
        "  CREATE VOLUME IF NOT EXISTS {CATALOG}.{BRONZE_SCHEMA}.{volume_name}\n",
        "  COMMENT 'Managed volume for staging files'\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Volume 'files' created**\n",
        "\n",
        "Volume was created in the Bronze schema as a managed volume for storing staging files, documents, and other non-table resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export customers to CSV in Volume\n",
        "volume_path = f\"/Volumes/{CATALOG}/{BRONZE_SCHEMA}/{volume_name}\"\n",
        "\n",
        "customers_df.coalesce(1).write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .csv(f\"{volume_path}/customers_export\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Customers data exported to Volume**\n",
        "\n",
        "Data has been saved at path: `/Volumes/{CATALOG}/{BRONZE_SCHEMA}/files/customers_export/`\n",
        "\n",
        "Volume enables storing unstructured files alongside tables in Unity Catalog with access control."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify files in Volume\n",
        "dbutils.fs.ls(f\"{volume_path}/customers_export\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.9. Unity Catalog Functions (UDF)\n",
        "\n",
        "**Functions** in Unity Catalog allow:\n",
        "- Creating reusable SQL/Python functions\n",
        "- Centralized management of business logic\n",
        "- Access control through GRANT/REVOKE\n",
        "- Lineage tracking for functions\n",
        "\n",
        "**Function types**:\n",
        "- **Scalar Functions**: return a single value\n",
        "- **Table Functions**: return a table\n",
        "- **SQL Functions**: written in SQL\n",
        "- **Python Functions**: written in Python (UDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.9.1. Data Classification (Tagging)\n",
        "\n",
        "**Tagging** allows data classification (e.g., PII, Sensitive, GDPR) at the table or column level.\n",
        "This facilitates data discovery and governance (e.g., reporting all tables containing personal data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SQL Function - masking customer_id\n",
        "spark.sql(f\"\"\"\n",
        "  CREATE OR REPLACE FUNCTION {CATALOG}.{SILVER_SCHEMA}.mask_customer_id(customer_id STRING)\n",
        "  RETURNS STRING\n",
        "  LANGUAGE SQL\n",
        "  COMMENT 'Masks customer_id, showing only last 3 digits'\n",
        "  RETURN CONCAT('****', SUBSTRING(CAST(customer_id AS STRING), -3))\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test mask_customer_id function\n",
        "result_df = spark.sql(f\"\"\"\n",
        "  SELECT \n",
        "    customer_id,\n",
        "    {CATALOG}.{SILVER_SCHEMA}.mask_customer_id(customer_id) as masked_id,\n",
        "    first_name,\n",
        "    last_name\n",
        "  FROM {CATALOG}.{BRONZE_SCHEMA}.orders\n",
        "  LIMIT 5\n",
        "\"\"\")\n",
        "\n",
        "display(result_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Function categorize_price created**\n",
        "\n",
        "Python UDF function categorizes product prices:\n",
        "- **Low**: < 50\n",
        "- **Medium**: 50-200 \n",
        "- **High**: > 200\n",
        "\n",
        "Python UDF can contain any Python logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Python UDF - price categorization\n",
        "spark.sql(f\"\"\"\n",
        "  CREATE OR REPLACE FUNCTION {CATALOG}.{SILVER_SCHEMA}.categorize_price(price DOUBLE)\n",
        "  RETURNS STRING\n",
        "  LANGUAGE PYTHON\n",
        "  COMMENT 'Categorizes prices: Low, Medium, High'\n",
        "  AS $$\n",
        "    if price < 50:\n",
        "        return \"Low\"\n",
        "    elif price < 200:\n",
        "        return \"Medium\"\n",
        "    else:\n",
        "        return \"High\"\n",
        "  $$\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test categorize_price function\n",
        "result_df = spark.sql(f\"\"\"\n",
        "  SELECT \n",
        "    product_name,\n",
        "    unit_cost,\n",
        "    {CATALOG}.{SILVER_SCHEMA}.categorize_price(unit_cost) as price_category\n",
        "  FROM {CATALOG}.{BRONZE_SCHEMA}.products\n",
        "  ORDER BY unit_cost\n",
        "  LIMIT 10\n",
        "\"\"\")\n",
        "\n",
        "display(result_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.9.2. External Locations & External Tables\n",
        "\n",
        "**External Location** is an object in Unity Catalog that connects a **Storage Credential** (cloud credentials) with a specific path in cloud storage (S3/ADLS/GCS).\n",
        "\n",
        "This allows:\n",
        "1.  Secure access to cloud data without providing keys in code.\n",
        "2.  Creating **External Tables** (data resides outside Databricks managed storage).\n",
        "3.  Creating **External Volumes**.\n",
        "\n",
        "**Requirements:**\n",
        "- `CREATE EXTERNAL LOCATION` permission on Metastore.\n",
        "- Configured Storage Credential.\n",
        "\n",
        "> **Note**: In the training environment, you may not have permissions to create Storage Credentials. The code below is a reference example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Permissions for data-analysts set**\n",
        "\n",
        "The `data-analysts` group received:\n",
        "- **USE CATALOG**: Access to catalog\n",
        "- **USE SCHEMA**: Access to Silver schema \n",
        "- **SELECT**: Read data from Silver schema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "** Setup: Create groups for demonstration purposes\n",
        "*** Note: This requires account admin privileges. If you don't have them, ensure these groups exist.\n",
        "\n",
        "* TO DO IN GUI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grant catalog access to data analysts\n",
        "spark.sql(f\"\"\"\n",
        "    GRANT USE CATALOG ON CATALOG {CATALOG} TO `data-analysts`\n",
        "\"\"\")\n",
        "\n",
        "spark.sql(f\"\"\"\n",
        "    GRANT USE SCHEMA ON SCHEMA {CATALOG}.{SILVER_SCHEMA} TO `data-analysts`\n",
        "\"\"\")\n",
        "\n",
        "spark.sql(f\"\"\"\n",
        "    GRANT SELECT ON SCHEMA {CATALOG}.{SILVER_SCHEMA} TO `data-analysts`\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grant full access to data engineers\n",
        "spark.sql(f\"\"\"\n",
        "    GRANT USE CATALOG, CREATE SCHEMA ON CATALOG {CATALOG} TO `data-engineers`\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Permissions for Data Analysts (Gold Layer):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GRANT dla data-analysts na Gold schema\n",
        "spark.sql(f\"\"\"\n",
        "  GRANT USE SCHEMA ON SCHEMA {CATALOG}.{GOLD_SCHEMA} TO `data-analysts`\n",
        "\"\"\")\n",
        "\n",
        "spark.sql(f\"\"\"\n",
        "  GRANT SELECT ON SCHEMA {CATALOG}.{GOLD_SCHEMA} TO `data-analysts`\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Table-specific access control**\n",
        "\n",
        "Fine-grained permissions:\n",
        "- **finance-team**: Access to customer_order_summary (revenue analysis)\n",
        "- **marketing-team**: Access to customers_masked (customer insights with PII masking)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GRANT EXECUTE na Functions\n",
        "spark.sql(f\"\"\"\n",
        "  GRANT EXECUTE ON FUNCTION {CATALOG}.{SILVER_SCHEMA}.mask_customer_id TO `data-analysts`\n",
        "\"\"\")\n",
        "\n",
        "spark.sql(f\"\"\"\n",
        "  GRANT EXECUTE ON FUNCTION {CATALOG}.{SILVER_SCHEMA}.categorize_price TO `data-analysts`\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify permissions on table\n",
        "spark.sql(f\"\"\"\n",
        "    SHOW GRANTS ON TABLE {CATALOG}.{BRONZE_SCHEMA}.customers\n",
        "\"\"\").display()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7.10. Data Masking and Row-Level Security\n",
        "\n",
        "### 7.10.1. Column-level masking (Dynamic Views):\n",
        "\n",
        "Use `current_user()` and `is_account_group_member()` functions for conditional masking:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create masked view for PII data\n",
        "spark.sql(f\"\"\"\n",
        "  CREATE OR REPLACE VIEW {CATALOG}.{GOLD_SCHEMA}.customers_masked AS\n",
        "  SELECT \n",
        "    customer_id,\n",
        "    CASE \n",
        "      WHEN is_account_group_member('pii-access-group') THEN first_name\n",
        "      ELSE CONCAT(LEFT(first_name, 1), '***')\n",
        "    END as first_name,\n",
        "    CASE \n",
        "      WHEN is_account_group_member('pii-access-group') THEN last_name\n",
        "      ELSE CONCAT(LEFT(last_name, 1), '***')\n",
        "    END as last_name,\n",
        "    city,\n",
        "    country,\n",
        "    registration_date\n",
        "  FROM {CATALOG}.{BRONZE_SCHEMA}.customers\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**View customers_masked created**\n",
        "\n",
        "View with dynamic PII data masking:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test View z maskowaniem\n",
        "result_df = spark.sql(f\"\"\"\n",
        "  SELECT * FROM {CATALOG}.{GOLD_SCHEMA}.customers_masked LIMIT 10\n",
        "\"\"\")\n",
        "\n",
        "display(result_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%sql\n",
        "CREATE FUNCTION customer_mask(custoemr_id STRING)\n",
        "  RETURN CASE WHEN is_account_group_member('data-engineers') THEN custoemr_id ELSE 'CUST-**-****' END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the customers_masked table schema without CTAS\n",
        "spark.sql(f\"\"\"\n",
        "  CREATE OR REPLACE TABLE {CATALOG}.{SILVER_SCHEMA}.customers_masked (\n",
        "    customer_id STRING,\n",
        "    customer_id_masket STRING MASK customer_mask,\n",
        "    first_name STRING,\n",
        "    last_name STRING,\n",
        "    email STRING,\n",
        "    country STRING\n",
        "  )\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(spark.sql(f\"SELECT * FROM {CATALOG}.{SILVER_SCHEMA}.customers_masked\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.sql(\n",
        "    f\"\"\"\n",
        "    CREATE OR REPLACE VIEW {CATALOG}.{GOLD_SCHEMA}.orders_hashed AS\n",
        "    SELECT \n",
        "        order_id,\n",
        "        SHA2(CAST(customer_id AS STRING), 256) as customer_id_hash,\n",
        "        product_id,\n",
        "        quantity,\n",
        "        total_amount,\n",
        "        order_datetime\n",
        "    FROM {CATALOG}.{BRONZE_SCHEMA}.orders\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "display(\n",
        "    spark.createDataFrame(\n",
        "        [\n",
        "            (\"View\", f\"{CATALOG}.{GOLD_SCHEMA}.orders_hashed\"),\n",
        "            (\"Masking\", \"customer_id â†’ SHA2-256 hash\"),\n",
        "            (\"Purpose\", \"Analysts can aggregate without revealing customer_id\")\n",
        "        ],\n",
        "        [\"Parameter\", \"Value\"]\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**View orders_hashed created**\n",
        "\n",
        "Customer_id is hashed using SHA2-256. This enables:\n",
        "- **Analysts**: Data aggregation without revealing customer_id\n",
        "- **Privacy**: Maintaining anonymity while preserving grouping capability\n",
        "- **Compliance**: Meeting GDPR/privacy regulations requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(spark.sql(f\"SELECT * FROM {CATALOG}.{GOLD_SCHEMA}.orders_hashed\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.10.2. Row-Level Security (RLS):\n",
        "\n",
        "Restrict which rows users can see based on their identity or group membership:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**RLS View customers_rls created**\n",
        "\n",
        "Row-Level Security filters data based on group membership:\n",
        "- **global-access**: Sees all customers\n",
        "- **east-coast-team**: Only customers from NY, NJ, NC, GA \n",
        "- **west-coast-team**: Only customers from CA\n",
        "- **midwest-team**: Only customers from FL, IL, TX, MI\n",
        "- **Other groups**: No access (FALSE)\n",
        "\n",
        "Automatic row filtering without data duplication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating RLS view - access per region (state)\n",
        "spark.sql(f\"\"\"\n",
        "    CREATE OR REPLACE VIEW {CATALOG}.{GOLD_SCHEMA}.customers_rls AS\n",
        "    SELECT *\n",
        "    FROM {CATALOG}.{BRONZE_SCHEMA}.customers\n",
        "    WHERE \n",
        "        CASE \n",
        "            WHEN is_account_group_member('global-access') THEN TRUE\n",
        "            WHEN is_account_group_member('east-coast-team') THEN UPPER(state) IN ('NY', 'NJ', 'NC', 'GA')\n",
        "            WHEN is_account_group_member('west-coast-team') THEN UPPER(state) = 'CA'\n",
        "            WHEN is_account_group_member('midwest-team') THEN UPPER(state) IN ('FL', 'IL', 'TX', 'MI')\n",
        "            ELSE FALSE\n",
        "        END\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(spark.createDataFrame([\n",
        "    (\"RLS View\", f\"{CATALOG}.{GOLD_SCHEMA}.customers_rls\"),\n",
        "    (\"Mechanism\", \"Filtering per state based on group membership\"),\n",
        "    (\"global-access\", \"All customers\"),\n",
        "    (\"east-coast-team\", \"NY, NJ, NC, GA\"),\n",
        "    (\"west-coast-team\", \"CA\"),\n",
        "    (\"midwest-team\", \"FL, IL, TX, MI\")\n",
        "], [\"Group\", \"Visibility\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Granting permissions to RLS Views:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.11. Access Management: GRANT / REVOKE\n",
        "\n",
        "### Privileges Hierarchy in Unity Catalog:\n",
        "\n",
        "**Privilege levels**:\n",
        "1. **Metastore-level**: CREATE CATALOG, USE CATALOG\n",
        "2. **Catalog-level**: USE CATALOG, CREATE SCHEMA\n",
        "3. **Schema-level**: USE SCHEMA, CREATE TABLE, CREATE FUNCTION, CREATE VOLUME\n",
        "4. **Object-level**: SELECT, MODIFY (INSERT/UPDATE/DELETE/MERGE), EXECUTE\n",
        "\n",
        "**Securable Objects - Inheritance**:\n",
        "- Privileges inherit down the hierarchy\n",
        "- GRANT on Catalog â†’ inherits to all Schemas and Tables\n",
        "- GRANT on Schema â†’ inherits to all Tables in that Schema\n",
        "- You can grant privileges at specific level for fine-grained control\n",
        "\n",
        "### GRANT/REVOKE Examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GRANT access to customers_rls\n",
        "spark.sql(\n",
        "    f\"\"\"\n",
        "    GRANT SELECT ON VIEW {CATALOG}.{GOLD_SCHEMA}.customers_rls TO `account users`\n",
        "    \"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Granting permissions to orders_rls**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GRANT access to orders_rls\n",
        "spark.sql(f\"\"\"\n",
        "  GRANT SELECT ON VIEW {CATALOG}.{GOLD_SCHEMA}.orders_hashed TO `account users`\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Revoking access to base tables (Enforcement):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Revoke direct access to base table\n",
        "spark.sql(f\"\"\"\n",
        "    REVOKE SELECT ON TABLE {CATALOG}.{BRONZE_SCHEMA}.orders FROM `account users`\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**RLS Views - Access control setup**\n",
        "\n",
        "Security pattern:\n",
        "1. **GRANT SELECT** on RLS Views for `all-users`\n",
        "2. **REVOKE SELECT** on base tables (force Views usage)\n",
        "3. **Automatic filtering** based on group membership\n",
        "\n",
        "Users can SELECT from Views, but not from base tables - enforcing RLS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7.12. Data Lineage i Audit Logging\n",
        "\n",
        "### 7.12.1. Querying Data Lineage:\n",
        "\n",
        "Unity Catalog automatically tracks lineage for:\n",
        "- Table â†’ Table (ETL transformations)\n",
        "- Notebook â†’ Table (data writes)\n",
        "- Dashboard â†’ Table (BI queries)\n",
        "- ML Model â†’ Table (training data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**General Table Lineage**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query table lineage from system tables\n",
        "lineage_df = spark.sql(f\"\"\"\n",
        "  SELECT \n",
        "    source_table_full_name,\n",
        "    source_type,\n",
        "    target_table_full_name,\n",
        "    target_type,\n",
        "    event_date,\n",
        "    created_by\n",
        "  FROM system.access.table_lineage\n",
        "  WHERE target_table_full_name LIKE '{CATALOG}.%'\n",
        "  ORDER BY event_date DESC\n",
        "  LIMIT 50\n",
        "\"\"\")\n",
        "\n",
        "display(lineage_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Lineage for tables in catalog displayed**\n",
        "\n",
        "The system automatically tracks lineage for:\n",
        "- **Table â†’ Table**: ETL transformations\n",
        "- **Notebook â†’ Table**: Data writes \n",
        "- **Dashboard â†’ Table**: BI queries\n",
        "- **ML Model â†’ Table**: Training data\n",
        "\n",
        "Lineage is available through `system.access.table_lineage` without additional instrumentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**1. Upstream Lineage (Sources)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find upstream dependencies (sources) for a table\n",
        "upstream_df = spark.sql(f\"\"\"\n",
        "    SELECT DISTINCT\n",
        "        source_table_full_name,\n",
        "        source_type\n",
        "    FROM system.access.table_lineage\n",
        "    WHERE target_table_full_name = '{CATALOG}.{SILVER_SCHEMA}.customer_order_summary'\n",
        "\"\"\")\n",
        "\n",
        "display(upstream_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**â¬† Upstream: Source tables for customer_order_summary**\n",
        "\n",
        "Shows all tables used as data sources in the `customer_order_summary` View. Helpful for impact analysis when making changes to upstream tables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**2. Downstream Lineage (Consumers)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find downstream dependencies (consumers) of a table\n",
        "downstream_df = spark.sql(f\"\"\"\n",
        "    SELECT DISTINCT\n",
        "        target_table_full_name,\n",
        "        target_type\n",
        "    FROM system.access.table_lineage\n",
        "    WHERE source_table_full_name = '{CATALOG}.{BRONZE_SCHEMA}.customers'\n",
        "\"\"\")\n",
        "\n",
        "display(downstream_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Downstream: Views/Tables consuming customers**\n",
        "\n",
        "Shows all Views and tables that consume data from the `customers` table. Critical for understanding impact of changes and data governance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**3. Column-Level Lineage**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column-level lineage (if available)\n",
        "column_lineage = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        source_table_full_name,\n",
        "        source_column_name,\n",
        "        target_table_full_name,\n",
        "        target_column_name,\n",
        "        event_date\n",
        "    FROM system.access.column_lineage\n",
        "    WHERE target_table_full_name = '{CATALOG}.{SILVER_SCHEMA}.customer_order_summary'\n",
        "    ORDER BY target_column_name\n",
        "\"\"\")\n",
        "\n",
        "display(column_lineage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Column-level lineage for customer_order_summary**\n",
        "\n",
        "Unity Catalog tracks lineage at column level - which columns in source tables affect which columns in the target table. Detailed information for data governance and impact analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.12.2. Audit Logging:\n",
        "\n",
        "Unity Catalog logs all access and operations:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**1. General Audit Logs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query audit logs\n",
        "audit_df = spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        event_time,\n",
        "        user_identity.email as user_email,\n",
        "        service_name,\n",
        "        action_name,\n",
        "        request_params.full_name_arg as table_name,\n",
        "        response.status_code,\n",
        "        request_id\n",
        "    FROM system.access.audit\n",
        "    WHERE action_name IN ('getTable', 'createTable', 'deleteTable', 'updateTable')\n",
        "        AND event_date >= current_date() - INTERVAL 7 DAYS\n",
        "    ORDER BY event_time DESC\n",
        "    LIMIT 100\n",
        "\"\"\")\n",
        "audit_df.display()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**2. Sensitive Data Access**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Track who accessed sensitive tables\n",
        "sensitive_access = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        event_time,\n",
        "        user_identity.email as user,\n",
        "        action_name,\n",
        "        request_params.full_name_arg as table_accessed,\n",
        "        source_ip_address\n",
        "    FROM system.access.audit\n",
        "    WHERE request_params.full_name_arg LIKE '{CATALOG}.%.customers%'\n",
        "        AND action_name = 'getTable'\n",
        "        AND event_date >= current_date() - INTERVAL 7 DAYS\n",
        "    ORDER BY event_time DESC\n",
        "    LIMIT 100\n",
        "\"\"\")\n",
        "\n",
        "display(sensitive_access)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ðŸ”’ Audit logs: Access to customers table (last 7 days)**\n",
        "\n",
        "Monitoring access to sensitive tables with PII data:\n",
        "- **Who**: User email\n",
        "- **When**: Event time \n",
        "- **What**: Table name\n",
        "- **From where**: Source IP address\n",
        "\n",
        "Critical for compliance (GDPR, HIPAA) and security monitoring."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**3. Privilege Changes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grant/Revoke audit trail\n",
        "grant_audit = spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        event_time,\n",
        "        user_identity.email as admin_user,\n",
        "        action_name,\n",
        "        request_params.privilege as privilege_granted,\n",
        "        request_params.securable_full_name as object_name,\n",
        "        request_params.principal as grantee\n",
        "    FROM system.access.audit\n",
        "    WHERE action_name IN ('grantPrivilege', 'revokePrivilege')\n",
        "        AND event_date >= current_date() - INTERVAL 30 DAYS\n",
        "    ORDER BY event_time DESC\n",
        "\"\"\")\n",
        "\n",
        "display(grant_audit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Audit trail of privilege changes**\n",
        "\n",
        "Complete audit trail of permission changes:\n",
        "- **Admin user**: Who executed GRANT/REVOKE\n",
        "- **Action**: grantPrivilege or revokePrivilege\n",
        "- **Privilege**: Which permission (SELECT, MODIFY, etc.)\n",
        "- **Object**: On which object (table, schema, catalog)\n",
        "- **Grantee**: To whom permissions were granted/revoked\n",
        "\n",
        "Essential for governance and compliance audits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7.13. Delta Sharing\n",
        "\n",
        "**Delta Sharing** = Secure data sharing protocol (cross-org, cross-cloud)\n",
        "\n",
        "### 7.13.1. Components:\n",
        "- **Share**: collection of tables to share\n",
        "- **Recipient**: organization/user receiving data\n",
        "- **Provider**: data owner (you)\n",
        "\n",
        "### 7.13.2. Create Share:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating Share for external partners\n",
        "share_name = f\"{CATALOG}_partner_share\"\n",
        "\n",
        "spark.sql(f\"\"\"\n",
        "  CREATE SHARE IF NOT EXISTS {share_name}\n",
        "  COMMENT 'Data sharing for business partners'\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Share '{share_name}' created**\n",
        "\n",
        "Delta Sharing Share is a collection of tables for secure sharing with external partners:\n",
        "- **Cross-org**: Between different Databricks organizations\n",
        "- **Cross-cloud**: AWS â†” Azure â†” GCP \n",
        "- **Open protocol**: Open-source standard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add table to Share (Gold layer only - aggregated data)\n",
        "spark.sql(f\"\"\"\n",
        "  ALTER SHARE {share_name}\n",
        "  ADD TABLE {CATALOG}.{GOLD_SCHEMA}.fact_sales\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.sql(f\"\"\"\n",
        "  ALTER SHARE {share_name}\n",
        "  ADD SCHEMA {CATALOG}.{SILVER_SCHEMA}\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Table customer_order_summary added to Share**\n",
        "\n",
        "Best practice: Share only Gold layer (aggregated data):\n",
        "- **Security**: No access to raw data\n",
        "- **Privacy**: Aggregations hide individual records\n",
        "- **Stability**: Gold layer has stable schema and structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Tables in Share verified**\n",
        "\n",
        "Share currently contains the added tables and can be shared with recipients. Recipients will receive an activation link to consume shared data via Delta Sharing protocol."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify Share contents\n",
        "spark.sql(f\"SHOW ALL IN SHARE {share_name}\").display()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.13.3. Create Recipient:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.13.4. Consuming shared data (as recipient):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.13.5. Best practices for Delta Sharing:\n",
        "\n",
        "1. **Share only aggregated/gold data**: don't share raw/bronze layers\n",
        "2. **Use views for masking**: create view with masked PII before sharing\n",
        "3. **Monitor access**: track who accesses shared data\n",
        "4. **Version control**: use table versions for stable APIs\n",
        "5. **Documentation**: clear documentation for recipients\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7.14. Summary\n",
        "\n",
        "### 7.14.1. You learned:\n",
        "\n",
        " **Unity Catalog Architecture**: Metastore â†’ Catalog â†’ Schema â†’ Tables \n",
        " **Access Control**: GRANT/REVOKE privileges at multiple levels \n",
        " **Data Masking**: Column-level masking with dynamic views \n",
        " **Row-Level Security**: Filter data based on user identity \n",
        " **Data Lineage**: Track data flow through system tables \n",
        " **Audit Logging**: Monitor who accessed what and when \n",
        " **Delta Sharing**: Secure cross-organization data sharing \n",
        "\n",
        "### 7.14.2. Key Takeaways:\n",
        "\n",
        "1. **Unified Governance**: Single platform for all data assets\n",
        "2. **Fine-grained Control**: Table, column, row-level security\n",
        "3. **Automatic Lineage**: No extra instrumentation needed\n",
        "4. **Compliance-ready**: Audit logs for regulatory requirements\n",
        "5. **Secure Sharing**: Delta Sharing for external collaboration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.15. Troubleshooting\n",
        "\n",
        "### Problem 1: \"Table or view not found\"\n",
        "**Cause**: Missing USE CATALOG or USE SCHEMA permissions \n",
        "**Solution**:\n",
        "```sql\n",
        "GRANT USE CATALOG ON CATALOG <catalog_name> TO <principal>;\n",
        "GRANT USE SCHEMA ON SCHEMA <catalog>.<schema> TO <principal>;\n",
        "```\n",
        "\n",
        "### Problem 2: \"Permission denied\" on SELECT\n",
        "**Cause**: Missing SELECT permissions on table \n",
        "**Solution**:\n",
        "```sql\n",
        "GRANT SELECT ON TABLE <catalog>.<schema>.<table> TO <principal>;\n",
        "-- or on entire schema:\n",
        "GRANT SELECT ON SCHEMA <catalog>.<schema> TO <principal>;\n",
        "```\n",
        "\n",
        "### Problem 3: \"Cannot execute function\"\n",
        "**Cause**: Missing EXECUTE permission on function \n",
        "**Solution**:\n",
        "```sql\n",
        "GRANT EXECUTE ON FUNCTION <catalog>.<schema>.<function_name> TO <principal>;\n",
        "```\n",
        "\n",
        "### Problem 4: \"Volume not accessible\"\n",
        "**Cause**: Missing READ VOLUME / WRITE VOLUME permissions \n",
        "**Solution**:\n",
        "```sql\n",
        "GRANT READ VOLUME ON VOLUME <catalog>.<schema>.<volume> TO <principal>;\n",
        "GRANT WRITE VOLUME ON VOLUME <catalog>.<schema>.<volume> TO <principal>;\n",
        "```\n",
        "\n",
        "### Problem 5: RLS View not filtering data\n",
        "**Cause**: User doesn't belong to any group defined in CASE WHEN \n",
        "**Solution**: Add user to appropriate group or add default fallback in View\n",
        "\n",
        "### Problem 6: Lineage not showing dependencies\n",
        "**Cause**: Lineage is automatic but may be delayed by a few minutes \n",
        "**Solution**: Wait 5-10 minutes and query system.access.table_lineage again\n",
        "\n",
        "### Problem 7: Share not visible to recipient\n",
        "**Cause**: Recipient hasn't activated the activation link \n",
        "**Solution**: Send activation link from DESCRIBE RECIPIENT\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.16. Best Practices Summary\n",
        "\n",
        "### 1. **Catalog Organization**\n",
        "- Use environment-based catalogs: `dev`, `test`, `prod`\n",
        "- Organize schemas by layers: `bronze`, `silver`, `gold`\n",
        "- Apply naming conventions: `<catalog>.<schema>.<object>`\n",
        "\n",
        "### 2. **Access Control**\n",
        "- **Principle of Least Privilege**: Grant minimum required permissions\n",
        "- Use groups, not individual users\n",
        "- Inheritance: GRANT on Catalog â†’ inherits to Schema â†’ inherits to Tables\n",
        "- Regularly audit permissions (SHOW GRANTS)\n",
        "\n",
        "### 3. **Data Masking & RLS**\n",
        "- Mask PII in Views for users without pii-access-group\n",
        "- Use RLS for multi-tenant scenarios\n",
        "- Always test masking with different group memberships\n",
        "\n",
        "### 4. **Lineage & Audit**\n",
        "- Leverage automatic lineage to track data flow\n",
        "- Regularly check audit logs for sensitive tables\n",
        "- Monitor lineage after pipeline changes\n",
        "\n",
        "### 5. **Delta Sharing**\n",
        "- Share only Gold layer (aggregated data)\n",
        "- Use masked Views in Share\n",
        "- Document Share contracts for recipients\n",
        "\n",
        "### 6. **Documentation & Governance**\n",
        "- Add COMMENT to all tables, views, functions\n",
        "- Use Table Properties for metadata (owner, PII, retention)\n",
        "- Regularly check governance health checks\n",
        "\n",
        "### 7. **Volumes & Functions**\n",
        "- Use Managed Volumes for ML artifacts and staging\n",
        "- Centralize business logic in UC Functions\n",
        "- Control access through GRANT EXECUTE\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": null,
      "dashboards": [],
      "environmentMetadata": null,
      "inputWidgetPreferences": null,
      "language": "python",
      "notebookMetadata": {
        "mostRecentlyExecutedCommandWithImplicitDF": {
          "commandId": 4729063667486166,
          "dataframes": [
            "_sqldf"
          ]
        },
        "pythonIndentUnit": 4
      },
      "notebookName": "07_governance",
      "widgets": {}
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
