{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb6d571c-ebfe-44a0-908e-8079cd0e7cf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Medallion Architecture\n",
    "\n",
    "## The Story: Building the Data Factory\n",
    "\n",
    "Your e-commerce platform now has Delta Lake running. But there's chaos:\n",
    "- 15 different notebooks, each loading data differently\n",
    "- No naming conventions - `customers_v2_final_FINAL.csv` anyone?\n",
    "- Data Scientists use Bronze data directly (with all the garbage)\n",
    "- Finance dashboard shows different numbers than Marketing\n",
    "\n",
    "**The CTO asks:** \"Can we standardize this before we hire more engineers?\"\n",
    "\n",
    "**The Answer:** Medallion Architecture - a proven pattern used by Netflix, Uber, and thousands of companies.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Medallion? (The Engineering Case)\n",
    "\n",
    "### The Pattern\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   BRONZE    â”‚ â”€â”€â–¶  â”‚   SILVER    â”‚ â”€â”€â–¶  â”‚    GOLD     â”‚\n",
    "â”‚   (Raw)     â”‚      â”‚  (Cleaned)  â”‚      â”‚ (Business)  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ - As-is     â”‚      â”‚ - Validated â”‚      â”‚ - Star schemaâ”‚\n",
    "â”‚ - Append    â”‚      â”‚ - Dedupe    â”‚      â”‚ - Aggregates â”‚\n",
    "â”‚ - All types â”‚      â”‚ - Typed     â”‚      â”‚ - SCD appliedâ”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "     Ingest              Transform           Serve\n",
    "```\n",
    "\n",
    "### Why Three Layers? (Not Two, Not Five)\n",
    "\n",
    "| Layers | Problem |\n",
    "|--------|---------|\n",
    "| **1 layer** | No separation of concerns, risky transformations on raw data |\n",
    "| **2 layers** | \"Where do we put validation?\" - unclear ownership |\n",
    "| **3 layers** | Clear responsibilities, testable boundaries |\n",
    "| **5+ layers** | Over-engineering, increased latency, maintenance nightmare |\n",
    "\n",
    "*Three layers hit the sweet spot for most organizations.*\n",
    "\n",
    "### Layer Responsibilities\n",
    "\n",
    "| Layer | Owner | SLA | Access |\n",
    "|-------|-------|-----|--------|\n",
    "| **Bronze** | Data Engineering | 15 min from source | Engineers only |\n",
    "| **Silver** | Data Engineering | 1 hour from Bronze | Engineers + DS |\n",
    "| **Gold** | Analytics Engineering | 4 hours from Silver | Everyone |\n",
    "\n",
    "### Cost Implications\n",
    "\n",
    "| Decision | Cost Impact |\n",
    "|----------|-------------|\n",
    "| Keep Bronze forever | Storage grows linearly (plan retention!) |\n",
    "| Skip Silver layer | Technical debt accumulates, bugs in Gold |\n",
    "| Over-aggregate Gold | Every new question = new pipeline |\n",
    "\n",
    "**Recommendation:** \n",
    "- Bronze: 90 days retention (or regulatory requirement)\n",
    "- Silver: Indefinite (your master data)\n",
    "- Gold: Depends on use case (often regenerated)\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "| Layer | Tables | Transformations |\n",
    "|-------|--------|-----------------|\n",
    "| **Bronze** | customers, orders, products | Raw ingestion, metadata |\n",
    "| **Silver** | customers, orders, products | Validation, dedup, typing |\n",
    "| **Gold** | fact_sales, dim_customer, dim_product, dim_date | Star schema, SCD |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Medallion Architecture - Theory\n",
    "\n",
    "## ðŸŽ¯ Cel\n",
    "\n",
    "Zrozumienie wzorca **Medallion Architecture** - standaryzowanego podejÅ›cia do organizacji danych w Data Lakehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Story: Building the Data Factory\n",
    "\n",
    "Twoja platforma e-commerce ma Delta Lake. Ale panuje chaos:\n",
    "- 15 rÃ³Å¼nych notebookÃ³w, kaÅ¼dy Å‚aduje dane inaczej\n",
    "- Brak konwencji nazewnictwa - `customers_v2_final_FINAL.csv`\n",
    "- Data Scientists uÅ¼ywajÄ… danych Bronze bezpoÅ›rednio (ze wszystkimi bÅ‚Ä™dami)\n",
    "- Dashboard finansowy pokazuje inne liczby niÅ¼ marketing\n",
    "\n",
    "**CTO pyta:** \"Czy moÅ¼emy to ustandaryzowaÄ‡?\"\n",
    "\n",
    "**OdpowiedÅº:** Medallion Architecture - sprawdzony wzorzec uÅ¼ywany przez Netflix, Uber i tysiÄ…ce firm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¥‰ðŸ¥ˆðŸ¥‡ The Medallion Pattern\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚     BRONZE      â”‚ â”€â”€â–¶  â”‚     SILVER      â”‚ â”€â”€â–¶  â”‚      GOLD       â”‚\n",
    "â”‚     (Raw)       â”‚      â”‚   (Cleaned)     â”‚      â”‚   (Business)    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ â€¢ Dane surowe   â”‚      â”‚ â€¢ Walidacja     â”‚      â”‚ â€¢ Star schema   â”‚\n",
    "â”‚ â€¢ Append-only   â”‚      â”‚ â€¢ Deduplikacja  â”‚      â”‚ â€¢ Agregacje     â”‚\n",
    "â”‚ â€¢ Wszystkie typyâ”‚      â”‚ â€¢ Typowanie     â”‚      â”‚ â€¢ SCD applied   â”‚\n",
    "â”‚ â€¢ +Metadata     â”‚      â”‚ â€¢ Normalizacja  â”‚      â”‚ â€¢ Business readyâ”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "      INGEST                TRANSFORM               SERVE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Three Layers? (Not Two, Not Five)\n",
    "\n",
    "| Layers | Problem |\n",
    "|--------|---------|\n",
    "| **1 layer** | Brak separacji, ryzykowne transformacje na surowych danych |\n",
    "| **2 layers** | \"Gdzie umieÅ›ciÄ‡ walidacjÄ™?\" - niejasna odpowiedzialnoÅ›Ä‡ |\n",
    "| **3 layers** | Jasne odpowiedzialnoÅ›ci, testowalne granice |\n",
    "| **5+ layers** | Over-engineering, zwiÄ™kszona latencja, koszmar maintenance |\n",
    "\n",
    "**Trzy warstwy to sweet spot dla wiÄ™kszoÅ›ci organizacji.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Responsibilities\n",
    "\n",
    "| Layer | Owner | SLA | Access | Retencja |\n",
    "|-------|-------|-----|--------|----------|\n",
    "| **Bronze** | Data Engineering | 15 min od ÅºrÃ³dÅ‚a | Engineers only | 90 dni (lub regulatory) |\n",
    "| **Silver** | Data Engineering | 1h od Bronze | Engineers + DS | NieokreÅ›lona (master data) |\n",
    "| **Gold** | Analytics Engineering | 4h od Silver | Wszyscy | ZaleÅ¼y od use case |\n",
    "\n",
    "### Bronze Layer Details\n",
    "- **Cel:** Zachowanie surowych danych bez modyfikacji\n",
    "- **Transformacje:** Tylko dodanie metadanych (`_metadata.file_path`, `ingestion_ts`, `load_ts`)\n",
    "- **Format:** Delta Table (Streaming Table w Lakeflow)\n",
    "- **Schemat:** CzÄ™sto inferowany, moÅ¼e ewoluowaÄ‡\n",
    "\n",
    "### Silver Layer Details\n",
    "- **Cel:** Dane oczyszczone, zwalidowane, znormalizowane\n",
    "- **Transformacje:** Walidacja, deduplikacja, typowanie, obliczenia pochodne\n",
    "- **Format:** Delta Table z constraints\n",
    "- **Schemat:** ÅšciÅ›le zdefiniowany, wersjonowany\n",
    "\n",
    "### Gold Layer Details\n",
    "- **Cel:** Dane gotowe do konsumpcji biznesowej\n",
    "- **Transformacje:** Star schema, agregacje, SCD\n",
    "- **Format:** Materialized Views lub Delta Tables\n",
    "- **Schemat:** Zoptymalizowany pod zapytania analityczne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What We'll Build\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   BRONZE     â”‚     â”‚    SILVER    â”‚     â”‚      GOLD        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ customers    â”‚ â”€â”€â–¶ â”‚ customers    â”‚ â”€â”€â–¶ â”‚ dim_customer     â”‚\n",
    "â”‚ orders       â”‚ â”€â”€â–¶ â”‚ orders       â”‚ â”€â”€â–¶ â”‚ fact_sales       â”‚\n",
    "â”‚ products     â”‚ â”€â”€â–¶ â”‚ products     â”‚ â”€â”€â–¶ â”‚ dim_product      â”‚\n",
    "â”‚              â”‚     â”‚              â”‚     â”‚ dim_date         â”‚\n",
    "â”‚              â”‚     â”‚              â”‚     â”‚ dim_payment      â”‚\n",
    "â”‚              â”‚     â”‚              â”‚     â”‚ dim_store        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: SCD Type 1 & Type 2\n",
    "\n",
    "## ðŸŽ¯ Cel\n",
    "\n",
    "Zrozumienie **Slowly Changing Dimensions** - jak Å›ledziÄ‡ zmiany w danych wymiarowych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co to jest SCD?\n",
    "\n",
    "**Slowly Changing Dimension (SCD)** to koncepcja z data warehousing opisujÄ…ca jak obsÅ‚ugiwaÄ‡ zmiany w danych wymiarowych.\n",
    "\n",
    "### PrzykÅ‚ad: Zmiana adresu klienta\n",
    "\n",
    "Klient Jan Kowalski przeprowadziÅ‚ siÄ™ z Warszawy do Krakowa. Co robimy?\n",
    "\n",
    "| Typ | Strategia | Rezultat |\n",
    "|-----|-----------|----------|\n",
    "| **SCD Type 0** | Retain original | Zawsze Warszawa |\n",
    "| **SCD Type 1** | Overwrite | Tylko KrakÃ³w |\n",
    "| **SCD Type 2** | Track history | Oba rekordy z datami |\n",
    "| **SCD Type 3** | Add column | `current_city=KrakÃ³w`, `previous_city=Warszawa` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCD Type 1 - Overwrite\n",
    "\n",
    "### Charakterystyka:\n",
    "- **Najprostsze** podejÅ›cie\n",
    "- **Brak historii** - stare wartoÅ›ci sÄ… nadpisywane\n",
    "- **Use cases:** Korekty bÅ‚Ä™dÃ³w, dane ktÃ³re nie wymagajÄ… historii\n",
    "\n",
    "### Przed zmianÄ…:\n",
    "```\n",
    "| customer_id | name         | city      |\n",
    "|-------------|--------------|----------|\n",
    "| C001        | Jan Kowalski | Warszawa |\n",
    "```\n",
    "\n",
    "### Po zmianie:\n",
    "```\n",
    "| customer_id | name         | city    |\n",
    "|-------------|--------------|--------|\n",
    "| C001        | Jan Kowalski | KrakÃ³w |\n",
    "```\n",
    "\n",
    "### Implementacja w SQL (MERGE):\n",
    "```sql\n",
    "MERGE INTO dim_customer t\n",
    "USING source_customers s\n",
    "ON t.customer_id = s.customer_id\n",
    "WHEN MATCHED THEN UPDATE SET *\n",
    "WHEN NOT MATCHED THEN INSERT *\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCD Type 2 - Track History\n",
    "\n",
    "### Charakterystyka:\n",
    "- **PeÅ‚na historia** zmian\n",
    "- **Dodatkowe kolumny:** `__START_AT`, `__END_AT` (lub `valid_from`, `valid_to`)\n",
    "- **Use cases:** Audyt, analiza historyczna, compliance\n",
    "\n",
    "### Przed zmianÄ…:\n",
    "```\n",
    "| customer_id | name         | city      | __START_AT          | __END_AT |\n",
    "|-------------|--------------|-----------|---------------------|----------|\n",
    "| C001        | Jan Kowalski | Warszawa  | 2024-01-01 00:00:00 | NULL     |\n",
    "```\n",
    "\n",
    "### Po zmianie:\n",
    "```\n",
    "| customer_id | name         | city      | __START_AT          | __END_AT            |\n",
    "|-------------|--------------|-----------|---------------------|---------------------|\n",
    "| C001        | Jan Kowalski | Warszawa  | 2024-01-01 00:00:00 | 2024-06-15 10:30:00 |\n",
    "| C001        | Jan Kowalski | KrakÃ³w    | 2024-06-15 10:30:00 | NULL                |\n",
    "```\n",
    "\n",
    "### Zapytania:\n",
    "```sql\n",
    "-- Aktualny stan (snapshot)\n",
    "SELECT * FROM silver_customers WHERE __END_AT IS NULL;\n",
    "\n",
    "-- Stan na okreÅ›lony moment (time travel biznesowy)\n",
    "SELECT * FROM silver_customers \n",
    "WHERE '2024-03-01' BETWEEN __START_AT AND COALESCE(__END_AT, '9999-12-31');\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCD Type 2 w Lakeflow - AUTO CDC\n",
    "\n",
    "Databricks Lakeflow oferuje wbudowanÄ… obsÅ‚ugÄ™ SCD Type 2 przez `AUTO CDC INTO`:\n",
    "\n",
    "```sql\n",
    "-- Tworzenie tabeli docelowej SCD2\n",
    "CREATE OR REFRESH STREAMING TABLE silver_customers (\n",
    "  customer_id        STRING,\n",
    "  first_name         STRING,\n",
    "  last_name          STRING,\n",
    "  city               STRING,\n",
    "  -- Kolumny SCD2 dodawane automatycznie:\n",
    "  __START_AT         TIMESTAMP,\n",
    "  __END_AT           TIMESTAMP\n",
    ");\n",
    "\n",
    "-- Flow z AUTO CDC dla SCD2\n",
    "CREATE FLOW silver_customers_scd2_flow\n",
    "AS AUTO CDC INTO silver_customers\n",
    "FROM STREAM bronze_customers\n",
    "KEYS (customer_id)         -- Klucz biznesowy\n",
    "SEQUENCE BY ingestion_ts   -- Kolumna okreÅ›lajÄ…ca kolejnoÅ›Ä‡\n",
    "STORED AS SCD TYPE 2;      -- Typ SCD\n",
    "```\n",
    "\n",
    "### Kluczowe elementy:\n",
    "- **KEYS** - kolumny identyfikujÄ…ce rekord biznesowo\n",
    "- **SEQUENCE BY** - kolumna okreÅ›lajÄ…ca kolejnoÅ›Ä‡ zmian\n",
    "- **STORED AS SCD TYPE 1|2** - typ SCD\n",
    "\n",
    "### SCD Type 1 w Lakeflow:\n",
    "```sql\n",
    "CREATE FLOW silver_products_scd1_flow\n",
    "AS AUTO CDC INTO silver_products\n",
    "FROM bronze_products\n",
    "KEYS (product_id)\n",
    "SEQUENCE BY ingestion_ts\n",
    "STORED AS SCD TYPE 1;  -- Nadpisywanie bez historii\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Lakeflow Pipelines - Deklaracje\n",
    "\n",
    "## ðŸŽ¯ Cel\n",
    "\n",
    "Poznanie skÅ‚adni **Spark Declarative Pipelines** w SQL i PySpark - deklaratywnego sposobu definiowania pipeline'Ã³w danych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co to jest Lakeflow?\n",
    "\n",
    "**Lakeflow** (dawniej Delta Live Tables - DLT) to framework Databricks do budowania deklaratywnych pipeline'Ã³w danych.\n",
    "\n",
    "### Deklaratywne vs Imperatywne\n",
    "\n",
    "| PodejÅ›cie | PrzykÅ‚ad | Charakterystyka |\n",
    "|-----------|----------|----------------|\n",
    "| **Imperatywne** | `df.write.mode(\"overwrite\")...` | Opisujesz JAK |\n",
    "| **Deklaratywne** | `CREATE TABLE AS SELECT...` | Opisujesz CO |\n",
    "\n",
    "### KorzyÅ›ci Lakeflow:\n",
    "- âœ… **Automatyczne zarzÄ…dzanie zaleÅ¼noÅ›ciami** miÄ™dzy tabelami\n",
    "- âœ… **Wbudowana jakoÅ›Ä‡ danych** - constraints z akcjami\n",
    "- âœ… **Unified batch/streaming** - ten sam kod\n",
    "- âœ… **Automatyczne recovery** po awariach\n",
    "- âœ… **Lineage i monitoring** out-of-the-box\n",
    "- âœ… **Incremental processing** - tylko nowe dane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typy Tabel w Lakeflow\n",
    "\n",
    "| Typ | UÅ¼ycie | Charakterystyka |\n",
    "|-----|--------|----------------|\n",
    "| **STREAMING TABLE** | Dane ÅºrÃ³dÅ‚owe, append-only | Przetwarza tylko nowe dane |\n",
    "| **MATERIALIZED VIEW** | Agregacje, transformacje | Zawsze peÅ‚ne przeliczenie |\n",
    "| **VIEW** | Logika poÅ›rednia | Nie materializowana |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL: STREAMING TABLE (Bronze)\n",
    "\n",
    "```sql\n",
    "CREATE OR REFRESH STREAMING TABLE bronze_customers\n",
    "AS\n",
    "SELECT\n",
    "  customer_id,\n",
    "  first_name,\n",
    "  last_name,\n",
    "  email,\n",
    "  city,\n",
    "  _metadata.file_path                AS source_file_path,\n",
    "  _metadata.file_modification_time   AS ingestion_ts,\n",
    "  current_timestamp()                AS load_ts\n",
    "FROM STREAM read_files(\n",
    "  '${customer_path}',\n",
    "  format           => 'csv',\n",
    "  header           => true,\n",
    "  inferColumnTypes => true\n",
    ");\n",
    "```\n",
    "\n",
    "### Kluczowe elementy:\n",
    "- **`STREAM read_files(...)`** - czyta pliki w trybie strumieniowym\n",
    "- **`_metadata`** - metadane pliku ÅºrÃ³dÅ‚owego\n",
    "- **`${variable}`** - parametry pipeline'u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL: STREAMING TABLE z CONSTRAINTS (Silver)\n",
    "\n",
    "```sql\n",
    "CREATE OR REFRESH STREAMING TABLE silver_orders\n",
    "(\n",
    "  CONSTRAINT valid_order_id EXPECT (order_id IS NOT NULL)\n",
    "    ON VIOLATION DROP ROW,\n",
    "  CONSTRAINT valid_customer EXPECT (customer_id IS NOT NULL)\n",
    "    ON VIOLATION DROP ROW,\n",
    "  CONSTRAINT valid_quantity EXPECT (quantity > 0)\n",
    "    ON VIOLATION DROP ROW,\n",
    "  CONSTRAINT valid_price EXPECT (unit_price >= 0)\n",
    "    ON VIOLATION FAIL UPDATE\n",
    ")\n",
    "AS\n",
    "SELECT\n",
    "  order_id,\n",
    "  customer_id,\n",
    "  product_id,\n",
    "  CAST(order_datetime AS TIMESTAMP) AS order_ts,\n",
    "  quantity,\n",
    "  unit_price,\n",
    "  (quantity * unit_price) AS gross_amount\n",
    "FROM STREAM(bronze_orders);\n",
    "```\n",
    "\n",
    "### Akcje dla CONSTRAINTS:\n",
    "- **`DROP ROW`** - usuÅ„ nieprawidÅ‚owy rekord\n",
    "- **`FAIL UPDATE`** - zatrzymaj pipeline\n",
    "- Brak akcji - tylko logowanie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL: MATERIALIZED VIEW (Gold)\n",
    "\n",
    "```sql\n",
    "-- Wymiar - aktualny snapshot z SCD2\n",
    "CREATE OR REFRESH MATERIALIZED VIEW dim_customer\n",
    "AS\n",
    "SELECT\n",
    "  customer_id,\n",
    "  first_name,\n",
    "  last_name,\n",
    "  email,\n",
    "  city,\n",
    "  customer_segment\n",
    "FROM silver_customers\n",
    "WHERE __END_AT IS NULL;\n",
    "\n",
    "-- Wymiar daty\n",
    "CREATE OR REFRESH MATERIALIZED VIEW dim_date\n",
    "AS\n",
    "SELECT DISTINCT\n",
    "  CAST(date_format(order_date, 'yyyyMMdd') AS INT) AS date_key,\n",
    "  order_date AS date,\n",
    "  year(order_date) AS year,\n",
    "  quarter(order_date) AS quarter,\n",
    "  month(order_date) AS month\n",
    "FROM silver_orders;\n",
    "\n",
    "-- Fakt - streaming z Silver\n",
    "CREATE OR REFRESH STREAMING TABLE fact_sales\n",
    "AS\n",
    "SELECT\n",
    "  order_id,\n",
    "  customer_id,\n",
    "  product_id,\n",
    "  order_date_key,\n",
    "  quantity,\n",
    "  gross_amount,\n",
    "  net_amount\n",
    "FROM STREAM(silver_orders);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co to jest FLOW?\n",
    "\n",
    "**FLOW** to mechanizm Lakeflow pozwalajÄ…cy na:\n",
    "1. **Rozdzielenie definicji tabeli od ÅºrÃ³dÅ‚a danych**\n",
    "2. **Wiele ÅºrÃ³deÅ‚ do jednej tabeli** (np. backfill + streaming)\n",
    "3. **CDC (Change Data Capture)** z automatycznym SCD\n",
    "\n",
    "### Anatomia FLOW:\n",
    "\n",
    "```sql\n",
    "-- 1. Definiujemy pustÄ… tabelÄ™ docelowÄ…\n",
    "CREATE OR REFRESH STREAMING TABLE bronze_orders;\n",
    "\n",
    "-- 2. Definiujemy FLOW(s) ktÃ³re jÄ… zasilajÄ…\n",
    "CREATE FLOW flow_name\n",
    "AS INSERT INTO target_table BY NAME\n",
    "SELECT ... FROM source;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLOW: Backfill + Streaming Pattern\n",
    "\n",
    "```sql\n",
    "-- Tabela docelowa\n",
    "CREATE OR REFRESH STREAMING TABLE bronze_orders;\n",
    "\n",
    "-- FLOW 1: Jednorazowy backfill\n",
    "CREATE FLOW bronze_orders_backfill\n",
    "AS \n",
    "INSERT INTO ONCE bronze_orders BY NAME\n",
    "SELECT\n",
    "  order_id,\n",
    "  customer_id,\n",
    "  product_id,\n",
    "  order_datetime,\n",
    "  'batch' AS source_system,\n",
    "  _metadata.file_path AS source_file_path,\n",
    "  current_timestamp() AS load_ts\n",
    "FROM read_files(\n",
    "  '${order_path}/orders_batch.json',\n",
    "  format => 'json'\n",
    ");\n",
    "\n",
    "-- FLOW 2: CiÄ…gÅ‚y streaming\n",
    "CREATE FLOW bronze_orders_stream\n",
    "AS \n",
    "INSERT INTO bronze_orders BY NAME\n",
    "SELECT\n",
    "  order_id,\n",
    "  customer_id,\n",
    "  'stream' AS source_system,\n",
    "  _metadata.file_path AS source_file_path,\n",
    "  current_timestamp() AS load_ts\n",
    "FROM STREAM read_files(\n",
    "  '${order_path}/stream/orders_stream_*.json',\n",
    "  format => 'json'\n",
    ");\n",
    "```\n",
    "\n",
    "### Kluczowe elementy:\n",
    "- **`INSERT INTO ONCE`** - wykonaj raz (backfill)\n",
    "- **`INSERT INTO`** - ciÄ…gÅ‚e wstawianie (streaming)\n",
    "- **`BY NAME`** - mapowanie kolumn po nazwie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLOW: AUTO CDC dla SCD Type 2\n",
    "\n",
    "```sql\n",
    "-- Tabela SCD2 ze schematem\n",
    "CREATE OR REFRESH STREAMING TABLE silver_customers (\n",
    "  customer_id        STRING,\n",
    "  first_name         STRING,\n",
    "  last_name          STRING,\n",
    "  email              STRING,\n",
    "  city               STRING,\n",
    "  __START_AT         TIMESTAMP,\n",
    "  __END_AT           TIMESTAMP\n",
    ");\n",
    "\n",
    "-- AUTO CDC Flow\n",
    "CREATE FLOW silver_customers_scd2_flow\n",
    "AS AUTO CDC INTO silver_customers\n",
    "FROM STREAM bronze_customers\n",
    "KEYS (customer_id)\n",
    "SEQUENCE BY ingestion_ts\n",
    "STORED AS SCD TYPE 2;\n",
    "```\n",
    "\n",
    "### Co robi AUTO CDC?\n",
    "1. PorÃ³wnuje nowe rekordy z istniejÄ…cymi po `KEYS`\n",
    "2. Wykrywa zmiany w kolumnach\n",
    "3. Dla SCD2: zamyka stary rekord (`__END_AT`), wstawia nowy\n",
    "4. Dla SCD1: nadpisuje istniejÄ…cy rekord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Declarations\n",
    "\n",
    "```python\n",
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# STREAMING TABLE\n",
    "@dlt.table(\n",
    "    name=\"bronze_customers\",\n",
    "    comment=\"Raw customers from CSV\"\n",
    ")\n",
    "def bronze_customers():\n",
    "    return (\n",
    "        spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"csv\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .load(spark.conf.get(\"customer_path\"))\n",
    "            .select(\n",
    "                \"*\",\n",
    "                \"_metadata.file_path\".alias(\"source_file_path\"),\n",
    "                current_timestamp().alias(\"load_ts\")\n",
    "            )\n",
    "    )\n",
    "\n",
    "# MATERIALIZED VIEW\n",
    "@dlt.table(name=\"dim_customer\")\n",
    "def dim_customer():\n",
    "    return (\n",
    "        dlt.read(\"silver_customers\")\n",
    "            .filter(col(\"__END_AT\").isNull())\n",
    "            .select(\"customer_id\", \"first_name\", \"last_name\")\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark: Expectations (Constraints)\n",
    "\n",
    "```python\n",
    "@dlt.table(name=\"silver_orders\")\n",
    "@dlt.expect_or_drop(\"valid_order_id\", \"order_id IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_customer\", \"customer_id IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_quantity\", \"quantity > 0\")\n",
    "@dlt.expect_or_fail(\"valid_price\", \"unit_price >= 0\")\n",
    "def silver_orders():\n",
    "    return (\n",
    "        dlt.readStream(\"bronze_orders\")\n",
    "            .select(\n",
    "                \"order_id\",\n",
    "                \"customer_id\",\n",
    "                col(\"order_datetime\").cast(\"timestamp\").alias(\"order_ts\"),\n",
    "                (col(\"quantity\") * col(\"unit_price\")).alias(\"gross_amount\")\n",
    "            )\n",
    "    )\n",
    "```\n",
    "\n",
    "### Dekoratory expectations:\n",
    "- **`@dlt.expect`** - tylko logowanie\n",
    "- **`@dlt.expect_or_drop`** - usuÅ„ rekord\n",
    "- **`@dlt.expect_or_fail`** - zatrzymaj pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark: apply_changes (CDC)\n",
    "\n",
    "```python\n",
    "import dlt\n",
    "\n",
    "# Definiujemy tabelÄ™ docelowÄ…\n",
    "dlt.create_streaming_table(\n",
    "    name=\"silver_customers\",\n",
    "    schema=\"\"\"\n",
    "        customer_id STRING,\n",
    "        first_name STRING,\n",
    "        city STRING,\n",
    "        __START_AT TIMESTAMP,\n",
    "        __END_AT TIMESTAMP\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Definiujemy CDC flow\n",
    "dlt.apply_changes(\n",
    "    target=\"silver_customers\",\n",
    "    source=\"bronze_customers\",\n",
    "    keys=[\"customer_id\"],\n",
    "    sequence_by=\"ingestion_ts\",\n",
    "    stored_as_scd_type=2  # lub 1\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: DEMO - Budowa Pipeline'u\n",
    "\n",
    "## ðŸŽ¯ Cel\n",
    "\n",
    "Praktyczna budowa pipeline'u Lakeflow krok po kroku w Databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PrzeglÄ…d PlikÃ³w SQL\n",
    "\n",
    "Nasz pipeline skÅ‚ada siÄ™ z plikÃ³w SQL w folderze `lakeflow/`:\n",
    "\n",
    "```\n",
    "lakeflow/\n",
    "â”œâ”€â”€ 01_bronze/\n",
    "â”‚   â”œâ”€â”€ bronze_customers.sql\n",
    "â”‚   â”œâ”€â”€ bronze_orders.sql\n",
    "â”‚   â””â”€â”€ bronze_products.sql\n",
    "â”œâ”€â”€ 02_silver/\n",
    "â”‚   â”œâ”€â”€ silver_customers.sql\n",
    "â”‚   â”œâ”€â”€ silver_orders.sql\n",
    "â”‚   â””â”€â”€ silver_products.sql\n",
    "â””â”€â”€ 03_gold/\n",
    "    â”œâ”€â”€ dim_customer.sql\n",
    "    â”œâ”€â”€ dim_date.sql\n",
    "    â”œâ”€â”€ dim_payment_method.sql\n",
    "    â”œâ”€â”€ dim_product.sql\n",
    "    â”œâ”€â”€ dim_store.sql\n",
    "    â””â”€â”€ fact_sales.sql\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Upload SQL Files\n",
    "\n",
    "### Opcja A: Przez UI\n",
    "1. Databricks â†’ Workspace â†’ Users â†’ (twÃ³j folder)\n",
    "2. UtwÃ³rz folder `lakeflow_pipeline`\n",
    "3. Upload plikÃ³w SQL z folderu `lakeflow/`\n",
    "\n",
    "### Opcja B: Przez Repos\n",
    "1. Databricks â†’ Repos â†’ Add Repo\n",
    "2. Sklonuj repozytorium szkoleniowe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Pipeline\n",
    "\n",
    "1. **Workflows â†’ Delta Live Tables â†’ Create Pipeline**\n",
    "\n",
    "2. **General:**\n",
    "   - **Pipeline name:** `training_medallion_pipeline`\n",
    "   - **Product edition:** `Advanced` (dla SCD Type 2)\n",
    "   - **Pipeline mode:** `Triggered`\n",
    "\n",
    "3. **Source code:**\n",
    "   - `/Workspace/Users/{user}/lakeflow_pipeline/01_bronze/`\n",
    "   - `/Workspace/Users/{user}/lakeflow_pipeline/02_silver/`\n",
    "   - `/Workspace/Users/{user}/lakeflow_pipeline/03_gold/`\n",
    "\n",
    "4. **Destination:**\n",
    "   - **Catalog:** `training_catalog`\n",
    "   - **Target schema:** `{user}_lakeflow`\n",
    "\n",
    "5. **Compute:**\n",
    "   - **Cluster mode:** `Enhanced autoscaling`\n",
    "   - **Min workers:** 1, **Max workers:** 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure Variables\n",
    "\n",
    "W **Configuration** â†’ **Add configuration**:\n",
    "\n",
    "| Key | Value |\n",
    "|-----|-------|\n",
    "| `customer_path` | `/Volumes/training_catalog/default/kion_datasets/customers` |\n",
    "| `order_path` | `/Volumes/training_catalog/default/kion_datasets/orders` |\n",
    "| `product_path` | `/Volumes/training_catalog/default/kion_datasets/products` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Validate & Run\n",
    "\n",
    "1. Kliknij **Validate** - sprawdÅº skÅ‚adniÄ™ i DAG\n",
    "2. Kliknij **Start** - uruchom pipeline\n",
    "3. Obserwuj wykonanie:\n",
    "   - **Initializing** â†’ **Setting up tables** â†’ **Running** â†’ **Completed**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Verify Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uruchom w Databricks po wykonaniu pipeline'u\n",
    "\n",
    "catalog_name = spark.conf.get(\"catalog_name\")\n",
    "user_schema = spark.conf.get(\"user_schema\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Created tables:\")\n",
    "print(\"=\" * 50)\n",
    "display(spark.sql(f\"SHOW TABLES IN {catalog_name}.{user_schema}_lakeflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SprawdÅº Bronze - surowe dane z metadatami\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT * \n",
    "    FROM {catalog_name}.{user_schema}_lakeflow.bronze_customers \n",
    "    LIMIT 5\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SprawdÅº Silver customers - SCD Type 2\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        customer_id, first_name, last_name, city,\n",
    "        __START_AT, __END_AT\n",
    "    FROM {catalog_name}.{user_schema}_lakeflow.silver_customers \n",
    "    ORDER BY customer_id, __START_AT\n",
    "    LIMIT 10\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SprawdÅº Gold - dim_customer (aktualny snapshot)\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT * \n",
    "    FROM {catalog_name}.{user_schema}_lakeflow.dim_customer \n",
    "    LIMIT 5\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SprawdÅº fact_sales z joinami do wymiarÃ³w\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        f.order_id,\n",
    "        c.first_name || ' ' || c.last_name AS customer_name,\n",
    "        p.product_name,\n",
    "        d.date,\n",
    "        f.quantity,\n",
    "        f.net_amount\n",
    "    FROM {catalog_name}.{user_schema}_lakeflow.fact_sales f\n",
    "    LEFT JOIN {catalog_name}.{user_schema}_lakeflow.dim_customer c ON f.customer_id = c.customer_id\n",
    "    LEFT JOIN {catalog_name}.{user_schema}_lakeflow.dim_product p ON f.product_id = p.product_id\n",
    "    LEFT JOIN {catalog_name}.{user_schema}_lakeflow.dim_date d ON f.order_date_key = d.date_key\n",
    "    LIMIT 10\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Incremental Processing\n",
    "\n",
    "1. Dodaj nowy plik do folderu orders/stream/\n",
    "2. Uruchom pipeline ponownie\n",
    "3. SprawdÅº Event Log - powinien przetworzyÄ‡ tylko nowe pliki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test SCD Type 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZnajdÅº klientÃ³w z historiÄ… zmian\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        customer_id, first_name, city,\n",
    "        __START_AT, __END_AT,\n",
    "        CASE WHEN __END_AT IS NULL THEN 'Current' ELSE 'Historical' END AS status\n",
    "    FROM {catalog_name}.{user_schema}_lakeflow.silver_customers\n",
    "    WHERE customer_id IN (\n",
    "        SELECT customer_id \n",
    "        FROM {catalog_name}.{user_schema}_lakeflow.silver_customers \n",
    "        GROUP BY customer_id HAVING COUNT(*) > 1\n",
    "    )\n",
    "    ORDER BY customer_id, __START_AT\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring i Troubleshooting\n",
    "\n",
    "### Event Log\n",
    "- Kliknij na tabelÄ™ w DAG â†’ szczegÃ³Å‚y przetwarzania\n",
    "- Metrics: records processed, dropped rows, duration\n",
    "\n",
    "### Typowe problemy:\n",
    "\n",
    "| Problem | Przyczyna | RozwiÄ…zanie |\n",
    "|---------|-----------|-------------|\n",
    "| Pipeline hangs | Zbyt maÅ‚y klaster | ZwiÄ™ksz min workers |\n",
    "| Missing data | Constraint DROP ROW | SprawdÅº Data Quality tab |\n",
    "| Schema mismatch | Zmiana schematu | Full refresh |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ“‹ Summary\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### Medallion Architecture\n",
    "- **Bronze:** Surowe dane + metadata\n",
    "- **Silver:** Oczyszczone, zwalidowane\n",
    "- **Gold:** Business-ready, star schema\n",
    "\n",
    "### SCD Types\n",
    "- **SCD Type 1:** Overwrite - brak historii\n",
    "- **SCD Type 2:** Track history - `__START_AT`, `__END_AT`\n",
    "\n",
    "### Lakeflow Declarations\n",
    "- **STREAMING TABLE:** Dane append-only\n",
    "- **MATERIALIZED VIEW:** Agregacje\n",
    "- **FLOW:** CDC, backfill+streaming\n",
    "\n",
    "### Best Practices\n",
    "1. Constraints z `DROP ROW` dla jakoÅ›ci\n",
    "2. Backfill (ONCE) + streaming FLOWs\n",
    "3. SCD Type 2 dla wymiarÃ³w z historiÄ…\n",
    "4. STREAMING TABLE dla faktÃ³w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Resources\n",
    "\n",
    "- [Databricks Lakeflow Docs](https://docs.databricks.com/en/delta-live-tables/index.html)\n",
    "- [Medallion Architecture](https://www.databricks.com/glossary/medallion-architecture)\n",
    "- [SCD with Lakeflow](https://docs.databricks.com/en/delta-live-tables/cdc.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## â–¶ï¸ Next Steps\n",
    "\n",
    "1. **Notebook 06:** Orchestration - Jobs & Workflows\n",
    "2. **Notebook 07:** Unity Catalog & Governance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Medallion Architecture - Theory\n",
    "\n",
    "## ðŸŽ¯ Cel\n",
    "\n",
    "Zrozumienie wzorca **Medallion Architecture** - standaryzowanego podejÅ›cia do organizacji danych w Data Lakehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Story: Building the Data Factory\n",
    "\n",
    "Twoja platforma e-commerce ma Delta Lake. Ale panuje chaos:\n",
    "- 15 rÃ³Å¼nych notebookÃ³w, kaÅ¼dy Å‚aduje dane inaczej\n",
    "- Brak konwencji nazewnictwa - `customers_v2_final_FINAL.csv`\n",
    "- Data Scientists uÅ¼ywajÄ… danych Bronze bezpoÅ›rednio (ze wszystkimi bÅ‚Ä™dami)\n",
    "- Dashboard finansowy pokazuje inne liczby niÅ¼ marketing\n",
    "\n",
    "**CTO pyta:** \"Czy moÅ¼emy to ustandaryzowaÄ‡?\"\n",
    "\n",
    "**OdpowiedÅº:** Medallion Architecture - sprawdzony wzorzec uÅ¼ywany przez Netflix, Uber i tysiÄ…ce firm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¥‰ðŸ¥ˆðŸ¥‡ The Medallion Pattern\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚     BRONZE      â”‚ â”€â”€â–¶  â”‚     SILVER      â”‚ â”€â”€â–¶  â”‚      GOLD       â”‚\n",
    "â”‚     (Raw)       â”‚      â”‚   (Cleaned)     â”‚      â”‚   (Business)    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ â€¢ Dane surowe   â”‚      â”‚ â€¢ Walidacja     â”‚      â”‚ â€¢ Star schema   â”‚\n",
    "â”‚ â€¢ Append-only   â”‚      â”‚ â€¢ Deduplikacja  â”‚      â”‚ â€¢ Agregacje     â”‚\n",
    "â”‚ â€¢ Wszystkie typyâ”‚      â”‚ â€¢ Typowanie     â”‚      â”‚ â€¢ SCD applied   â”‚\n",
    "â”‚ â€¢ +Metadata     â”‚      â”‚ â€¢ Normalizacja  â”‚      â”‚ â€¢ Business readyâ”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "      INGEST                TRANSFORM               SERVE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Three Layers? (Not Two, Not Five)\n",
    "\n",
    "| Layers | Problem |\n",
    "|--------|---------|\n",
    "| **1 layer** | Brak separacji, ryzykowne transformacje na surowych danych |\n",
    "| **2 layers** | \"Gdzie umieÅ›ciÄ‡ walidacjÄ™?\" - niejasna odpowiedzialnoÅ›Ä‡ |\n",
    "| **3 layers** | Jasne odpowiedzialnoÅ›ci, testowalne granice |\n",
    "| **5+ layers** | Over-engineering, zwiÄ™kszona latencja, koszmar maintenance |\n",
    "\n",
    "**Trzy warstwy to sweet spot dla wiÄ™kszoÅ›ci organizacji.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Responsibilities\n",
    "\n",
    "| Layer | Owner | SLA | Access | Retencja |\n",
    "|-------|-------|-----|--------|----------|\n",
    "| **Bronze** | Data Engineering | 15 min od ÅºrÃ³dÅ‚a | Engineers only | 90 dni (lub regulatory) |\n",
    "| **Silver** | Data Engineering | 1h od Bronze | Engineers + DS | NieokreÅ›lona (master data) |\n",
    "| **Gold** | Analytics Engineering | 4h od Silver | Wszyscy | ZaleÅ¼y od use case |\n",
    "\n",
    "### Bronze Layer Details\n",
    "- **Cel:** Zachowanie surowych danych bez modyfikacji\n",
    "- **Transformacje:** Tylko dodanie metadanych (`_metadata.file_path`, `ingestion_ts`, `load_ts`)\n",
    "- **Format:** Delta Table (Streaming Table w Lakeflow)\n",
    "- **Schemat:** CzÄ™sto inferowany, moÅ¼e ewoluowaÄ‡\n",
    "\n",
    "### Silver Layer Details\n",
    "- **Cel:** Dane oczyszczone, zwalidowane, znormalizowane\n",
    "- **Transformacje:** Walidacja, deduplikacja, typowanie, obliczenia pochodne\n",
    "- **Format:** Delta Table z constraints\n",
    "- **Schemat:** ÅšciÅ›le zdefiniowany, wersjonowany\n",
    "\n",
    "### Gold Layer Details\n",
    "- **Cel:** Dane gotowe do konsumpcji biznesowej\n",
    "- **Transformacje:** Star schema, agregacje, SCD\n",
    "- **Format:** Materialized Views lub Delta Tables\n",
    "- **Schemat:** Zoptymalizowany pod zapytania analityczne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What We'll Build\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   BRONZE     â”‚     â”‚    SILVER    â”‚     â”‚      GOLD        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ customers    â”‚ â”€â”€â–¶ â”‚ customers    â”‚ â”€â”€â–¶ â”‚ dim_customer     â”‚\n",
    "â”‚ orders       â”‚ â”€â”€â–¶ â”‚ orders       â”‚ â”€â”€â–¶ â”‚ fact_sales       â”‚\n",
    "â”‚ products     â”‚ â”€â”€â–¶ â”‚ products     â”‚ â”€â”€â–¶ â”‚ dim_product      â”‚\n",
    "â”‚              â”‚     â”‚              â”‚     â”‚ dim_date         â”‚\n",
    "â”‚              â”‚     â”‚              â”‚     â”‚ dim_payment      â”‚\n",
    "â”‚              â”‚     â”‚              â”‚     â”‚ dim_store        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: SCD Type 1 & Type 2\n",
    "\n",
    "## ðŸŽ¯ Cel\n",
    "\n",
    "Zrozumienie **Slowly Changing Dimensions** - jak Å›ledziÄ‡ zmiany w danych wymiarowych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co to jest SCD?\n",
    "\n",
    "**Slowly Changing Dimension (SCD)** to koncepcja z data warehousing opisujÄ…ca jak obsÅ‚ugiwaÄ‡ zmiany w danych wymiarowych.\n",
    "\n",
    "### PrzykÅ‚ad: Zmiana adresu klienta\n",
    "\n",
    "Klient Jan Kowalski przeprowadziÅ‚ siÄ™ z Warszawy do Krakowa. Co robimy?\n",
    "\n",
    "| Typ | Strategia | Rezultat |\n",
    "|-----|-----------|----------|\n",
    "| **SCD Type 0** | Retain original | Zawsze Warszawa |\n",
    "| **SCD Type 1** | Overwrite | Tylko KrakÃ³w |\n",
    "| **SCD Type 2** | Track history | Oba rekordy z datami |\n",
    "| **SCD Type 3** | Add column | `current_city=KrakÃ³w`, `previous_city=Warszawa` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCD Type 1 - Overwrite\n",
    "\n",
    "### Charakterystyka:\n",
    "- **Najprostsze** podejÅ›cie\n",
    "- **Brak historii** - stare wartoÅ›ci sÄ… nadpisywane\n",
    "- **Use cases:** Korekty bÅ‚Ä™dÃ³w, dane ktÃ³re nie wymagajÄ… historii\n",
    "\n",
    "### Przed zmianÄ…:\n",
    "```\n",
    "| customer_id | name         | city      |\n",
    "|-------------|--------------|----------|\n",
    "| C001        | Jan Kowalski | Warszawa |\n",
    "```\n",
    "\n",
    "### Po zmianie:\n",
    "```\n",
    "| customer_id | name         | city    |\n",
    "|-------------|--------------|--------|\n",
    "| C001        | Jan Kowalski | KrakÃ³w |\n",
    "```\n",
    "\n",
    "### Implementacja w SQL (MERGE):\n",
    "```sql\n",
    "MERGE INTO dim_customer t\n",
    "USING source_customers s\n",
    "ON t.customer_id = s.customer_id\n",
    "WHEN MATCHED THEN UPDATE SET *\n",
    "WHEN NOT MATCHED THEN INSERT *\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCD Type 2 - Track History\n",
    "\n",
    "### Charakterystyka:\n",
    "- **PeÅ‚na historia** zmian\n",
    "- **Dodatkowe kolumny:** `__START_AT`, `__END_AT` (lub `valid_from`, `valid_to`)\n",
    "- **Use cases:** Audyt, analiza historyczna, compliance\n",
    "\n",
    "### Przed zmianÄ…:\n",
    "```\n",
    "| customer_id | name         | city      | __START_AT          | __END_AT |\n",
    "|-------------|--------------|-----------|---------------------|----------|\n",
    "| C001        | Jan Kowalski | Warszawa  | 2024-01-01 00:00:00 | NULL     |\n",
    "```\n",
    "\n",
    "### Po zmianie:\n",
    "```\n",
    "| customer_id | name         | city      | __START_AT          | __END_AT            |\n",
    "|-------------|--------------|-----------|---------------------|---------------------|\n",
    "| C001        | Jan Kowalski | Warszawa  | 2024-01-01 00:00:00 | 2024-06-15 10:30:00 |\n",
    "| C001        | Jan Kowalski | KrakÃ³w    | 2024-06-15 10:30:00 | NULL                |\n",
    "```\n",
    "\n",
    "### Zapytania:\n",
    "```sql\n",
    "-- Aktualny stan (snapshot)\n",
    "SELECT * FROM silver_customers WHERE __END_AT IS NULL;\n",
    "\n",
    "-- Stan na okreÅ›lony moment (time travel biznesowy)\n",
    "SELECT * FROM silver_customers \n",
    "WHERE '2024-03-01' BETWEEN __START_AT AND COALESCE(__END_AT, '9999-12-31');\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCD Type 2 w Lakeflow - AUTO CDC\n",
    "\n",
    "Databricks Lakeflow oferuje wbudowanÄ… obsÅ‚ugÄ™ SCD Type 2 przez `AUTO CDC INTO`:\n",
    "\n",
    "```sql\n",
    "-- Tworzenie tabeli docelowej SCD2\n",
    "CREATE OR REFRESH STREAMING TABLE silver_customers (\n",
    "  customer_id        STRING,\n",
    "  first_name         STRING,\n",
    "  last_name          STRING,\n",
    "  city               STRING,\n",
    "  -- Kolumny SCD2 dodawane automatycznie:\n",
    "  __START_AT         TIMESTAMP,\n",
    "  __END_AT           TIMESTAMP\n",
    ");\n",
    "\n",
    "-- Flow z AUTO CDC dla SCD2\n",
    "CREATE FLOW silver_customers_scd2_flow\n",
    "AS AUTO CDC INTO silver_customers\n",
    "FROM STREAM bronze_customers\n",
    "KEYS (customer_id)         -- Klucz biznesowy\n",
    "SEQUENCE BY ingestion_ts   -- Kolumna okreÅ›lajÄ…ca kolejnoÅ›Ä‡\n",
    "STORED AS SCD TYPE 2;      -- Typ SCD\n",
    "```\n",
    "\n",
    "### Kluczowe elementy:\n",
    "- **KEYS** - kolumny identyfikujÄ…ce rekord biznesowo\n",
    "- **SEQUENCE BY** - kolumna okreÅ›lajÄ…ca kolejnoÅ›Ä‡ zmian\n",
    "- **STORED AS SCD TYPE 1|2** - typ SCD\n",
    "\n",
    "### SCD Type 1 w Lakeflow:\n",
    "```sql\n",
    "CREATE FLOW silver_products_scd1_flow\n",
    "AS AUTO CDC INTO silver_products\n",
    "FROM bronze_products\n",
    "KEYS (product_id)\n",
    "SEQUENCE BY ingestion_ts\n",
    "STORED AS SCD TYPE 1;  -- Nadpisywanie bez historii\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Lakeflow Pipelines - Deklaracje\n",
    "\n",
    "## ðŸŽ¯ Cel\n",
    "\n",
    "Poznanie skÅ‚adni **Spark Declarative Pipelines** w SQL i PySpark - deklaratywnego sposobu definiowania pipeline'Ã³w danych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co to jest Lakeflow?\n",
    "\n",
    "**Lakeflow** (dawniej Delta Live Tables - DLT) to framework Databricks do budowania deklaratywnych pipeline'Ã³w danych.\n",
    "\n",
    "### Deklaratywne vs Imperatywne\n",
    "\n",
    "| PodejÅ›cie | PrzykÅ‚ad | Charakterystyka |\n",
    "|-----------|----------|----------------|\n",
    "| **Imperatywne** | `df.write.mode(\"overwrite\")...` | Opisujesz JAK |\n",
    "| **Deklaratywne** | `CREATE TABLE AS SELECT...` | Opisujesz CO |\n",
    "\n",
    "### KorzyÅ›ci Lakeflow:\n",
    "- âœ… **Automatyczne zarzÄ…dzanie zaleÅ¼noÅ›ciami** miÄ™dzy tabelami\n",
    "- âœ… **Wbudowana jakoÅ›Ä‡ danych** - constraints z akcjami\n",
    "- âœ… **Unified batch/streaming** - ten sam kod\n",
    "- âœ… **Automatyczne recovery** po awariach\n",
    "- âœ… **Lineage i monitoring** out-of-the-box\n",
    "- âœ… **Incremental processing** - tylko nowe dane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typy Tabel w Lakeflow\n",
    "\n",
    "| Typ | UÅ¼ycie | Charakterystyka |\n",
    "|-----|--------|----------------|\n",
    "| **STREAMING TABLE** | Dane ÅºrÃ³dÅ‚owe, append-only | Przetwarza tylko nowe dane |\n",
    "| **MATERIALIZED VIEW** | Agregacje, transformacje | Zawsze peÅ‚ne przeliczenie |\n",
    "| **VIEW** | Logika poÅ›rednia | Nie materializowana |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Declarations\n",
    "\n",
    "### 1. STREAMING TABLE - Dane strumieniowe (Bronze)\n",
    "\n",
    "```sql\n",
    "-- Najprostsza forma - inline query\n",
    "CREATE OR REFRESH STREAMING TABLE bronze_customers\n",
    "AS\n",
    "SELECT\n",
    "  customer_id,\n",
    "  first_name,\n",
    "  last_name,\n",
    "  email,\n",
    "  city,\n",
    "  -- Metadata\n",
    "  _metadata.file_path                AS source_file_path,\n",
    "  _metadata.file_modification_time   AS ingestion_ts,\n",
    "  current_timestamp()                AS load_ts\n",
    "FROM STREAM read_files(\n",
    "  '${customer_path}',\n",
    "  format           => 'csv',\n",
    "  header           => true,\n",
    "  inferColumnTypes => true\n",
    ");\n",
    "```\n",
    "\n",
    "### Kluczowe elementy:\n",
    "- **`STREAM read_files(...)`** - czyta pliki w trybie strumieniowym (tylko nowe)\n",
    "- **`_metadata`** - metadane pliku ÅºrÃ³dÅ‚owego\n",
    "- **`${variable}`** - parametry pipeline'u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. STREAMING TABLE z CONSTRAINTS (Silver)\n",
    "\n",
    "```sql\n",
    "CREATE OR REFRESH STREAMING TABLE silver_orders\n",
    "(\n",
    "  -- Constraints z akcjami\n",
    "  CONSTRAINT valid_order_id EXPECT (order_id IS NOT NULL)\n",
    "    ON VIOLATION DROP ROW,\n",
    "  CONSTRAINT valid_customer EXPECT (customer_id IS NOT NULL)\n",
    "    ON VIOLATION DROP ROW,\n",
    "  CONSTRAINT valid_quantity EXPECT (quantity > 0)\n",
    "    ON VIOLATION DROP ROW,\n",
    "  CONSTRAINT valid_price EXPECT (unit_price >= 0)\n",
    "    ON VIOLATION FAIL UPDATE   -- Zatrzymaj pipeline!\n",
    ")\n",
    "AS\n",
    "SELECT\n",
    "  order_id,\n",
    "  customer_id,\n",
    "  product_id,\n",
    "  CAST(order_datetime AS TIMESTAMP) AS order_ts,\n",
    "  quantity,\n",
    "  unit_price,\n",
    "  (quantity * unit_price) AS gross_amount\n",
    "FROM STREAM(bronze_orders);\n",
    "```\n",
    "\n",
    "### Akcje dla CONSTRAINTS:\n",
    "- **`DROP ROW`** - usuÅ„ nieprawidÅ‚owy rekord (logowane w metrykach)\n",
    "- **`FAIL UPDATE`** - zatrzymaj pipeline przy naruszeniu\n",
    "- Brak akcji - tylko logowanie (warning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. MATERIALIZED VIEW (Gold)\n",
    "\n",
    "```sql\n",
    "-- Wymiar - aktualny snapshot z SCD2\n",
    "CREATE OR REFRESH MATERIALIZED VIEW dim_customer\n",
    "AS\n",
    "SELECT\n",
    "  customer_id,\n",
    "  first_name,\n",
    "  last_name,\n",
    "  email,\n",
    "  city,\n",
    "  country,\n",
    "  customer_segment\n",
    "FROM silver_customers\n",
    "WHERE __END_AT IS NULL;  -- Tylko aktywne rekordy\n",
    "\n",
    "-- Wymiar daty - generowany z danych\n",
    "CREATE OR REFRESH MATERIALIZED VIEW dim_date\n",
    "AS\n",
    "SELECT DISTINCT\n",
    "  CAST(date_format(order_date, 'yyyyMMdd') AS INT) AS date_key,\n",
    "  order_date AS date,\n",
    "  year(order_date) AS year,\n",
    "  quarter(order_date) AS quarter,\n",
    "  month(order_date) AS month,\n",
    "  day(order_date) AS day\n",
    "FROM silver_orders;\n",
    "\n",
    "-- Fakt - streaming z Silver\n",
    "CREATE OR REFRESH STREAMING TABLE fact_sales\n",
    "AS\n",
    "SELECT\n",
    "  order_id,\n",
    "  customer_id,\n",
    "  product_id,\n",
    "  order_date_key,\n",
    "  quantity,\n",
    "  gross_amount,\n",
    "  net_amount\n",
    "FROM STREAM(silver_orders);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co to jest FLOW?\n",
    "\n",
    "**FLOW** to mechanizm Lakeflow pozwalajÄ…cy na:\n",
    "1. **Rozdzielenie definicji tabeli od ÅºrÃ³dÅ‚a danych**\n",
    "2. **Wiele ÅºrÃ³deÅ‚ do jednej tabeli** (np. backfill + streaming)\n",
    "3. **CDC (Change Data Capture)** z automatycznym SCD\n",
    "\n",
    "### Anatomia FLOW:\n",
    "\n",
    "```sql\n",
    "-- 1. Najpierw definiujemy pustÄ… tabelÄ™ docelowÄ…\n",
    "CREATE OR REFRESH STREAMING TABLE bronze_orders;\n",
    "\n",
    "-- 2. Potem definiujemy FLOW(s) ktÃ³re jÄ… zasilajÄ…\n",
    "CREATE FLOW flow_name\n",
    "AS INSERT INTO target_table BY NAME\n",
    "SELECT ... FROM source;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PrzykÅ‚ad FLOW: Backfill + Streaming Pattern\n",
    "\n",
    "Typowy scenariusz: mamy dane historyczne (batch) i nowe dane (streaming).\n",
    "\n",
    "```sql\n",
    "-- Tabela docelowa\n",
    "CREATE OR REFRESH STREAMING TABLE bronze_orders;\n",
    "\n",
    "-- FLOW 1: Jednorazowy backfill z pliku historycznego\n",
    "CREATE FLOW bronze_orders_backfill\n",
    "AS \n",
    "INSERT INTO ONCE bronze_orders BY NAME  -- ONCE = wykonaj raz\n",
    "SELECT\n",
    "  order_id,\n",
    "  customer_id,\n",
    "  product_id,\n",
    "  order_datetime,\n",
    "  'batch' AS source_system,\n",
    "  _metadata.file_path AS source_file_path,\n",
    "  current_timestamp() AS load_ts\n",
    "FROM read_files(\n",
    "  '${order_path}/orders_batch.json',\n",
    "  format => 'json'\n",
    ");\n",
    "\n",
    "-- FLOW 2: CiÄ…gÅ‚y streaming z nowych plikÃ³w\n",
    "CREATE FLOW bronze_orders_stream\n",
    "AS \n",
    "INSERT INTO bronze_orders BY NAME  -- Bez ONCE = continuous\n",
    "SELECT\n",
    "  order_id,\n",
    "  customer_id,\n",
    "  product_id,\n",
    "  order_datetime,\n",
    "  'stream' AS source_system,\n",
    "  _metadata.file_path AS source_file_path,\n",
    "  current_timestamp() AS load_ts\n",
    "FROM STREAM read_files(\n",
    "  '${order_path}/stream/orders_stream_*.json',\n",
    "  format => 'json'\n",
    ");\n",
    "```\n",
    "\n",
    "### Kluczowe elementy:\n",
    "- **`INSERT INTO ONCE`** - wykonaj raz (backfill)\n",
    "- **`INSERT INTO`** - ciÄ…gÅ‚e wstawianie (streaming)\n",
    "- **`BY NAME`** - mapowanie kolumn po nazwie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PrzykÅ‚ad FLOW: AUTO CDC dla SCD Type 2\n",
    "\n",
    "```sql\n",
    "-- Tabela SCD2 ze zdefiniowanym schematem\n",
    "CREATE OR REFRESH STREAMING TABLE silver_customers (\n",
    "  customer_id        STRING,\n",
    "  first_name         STRING,\n",
    "  last_name          STRING,\n",
    "  email              STRING,\n",
    "  city               STRING,\n",
    "  country            STRING,\n",
    "  customer_segment   STRING,\n",
    "  source_file_path   STRING,\n",
    "  ingestion_ts       TIMESTAMP,\n",
    "  -- Kolumny SCD2\n",
    "  __START_AT         TIMESTAMP,\n",
    "  __END_AT           TIMESTAMP\n",
    ");\n",
    "\n",
    "-- AUTO CDC Flow\n",
    "CREATE FLOW silver_customers_scd2_flow\n",
    "AS AUTO CDC INTO silver_customers\n",
    "FROM STREAM bronze_customers\n",
    "KEYS (customer_id)         -- Klucz biznesowy\n",
    "SEQUENCE BY ingestion_ts   -- KolejnoÅ›Ä‡ zmian\n",
    "STORED AS SCD TYPE 2;      -- Tryb SCD\n",
    "```\n",
    "\n",
    "### Co robi AUTO CDC?\n",
    "1. PorÃ³wnuje nowe rekordy z istniejÄ…cymi po `KEYS`\n",
    "2. Wykrywa zmiany w kolumnach\n",
    "3. Dla SCD2: zamyka stary rekord (`__END_AT`), wstawia nowy\n",
    "4. Dla SCD1: nadpisuje istniejÄ…cy rekord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Declarations\n",
    "\n",
    "Te same koncepcje w PySpark z uÅ¼yciem dekoratorÃ³w `dlt`:\n",
    "\n",
    "```python\n",
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# STREAMING TABLE\n",
    "@dlt.table(\n",
    "    name=\"bronze_customers\",\n",
    "    comment=\"Raw customers from CSV\"\n",
    ")\n",
    "def bronze_customers():\n",
    "    return (\n",
    "        spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"csv\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .load(spark.conf.get(\"customer_path\"))\n",
    "            .select(\n",
    "                \"*\",\n",
    "                \"_metadata.file_path\".alias(\"source_file_path\"),\n",
    "                current_timestamp().alias(\"load_ts\")\n",
    "            )\n",
    "    )\n",
    "\n",
    "# MATERIALIZED VIEW\n",
    "@dlt.table(\n",
    "    name=\"dim_customer\",\n",
    "    comment=\"Customer dimension - current snapshot\"\n",
    ")\n",
    "def dim_customer():\n",
    "    return (\n",
    "        dlt.read(\"silver_customers\")\n",
    "            .filter(col(\"__END_AT\").isNull())\n",
    "            .select(\n",
    "                \"customer_id\",\n",
    "                \"first_name\", \n",
    "                \"last_name\",\n",
    "                \"email\",\n",
    "                \"city\"\n",
    "            )\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark z Expectations (Constraints)\n",
    "\n",
    "```python\n",
    "@dlt.table(\n",
    "    name=\"silver_orders\",\n",
    "    comment=\"Cleaned and validated orders\"\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_order_id\", \"order_id IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_customer\", \"customer_id IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_quantity\", \"quantity > 0\")\n",
    "@dlt.expect_or_fail(\"valid_price\", \"unit_price >= 0\")  # Zatrzymaj pipeline!\n",
    "def silver_orders():\n",
    "    return (\n",
    "        dlt.readStream(\"bronze_orders\")\n",
    "            .select(\n",
    "                \"order_id\",\n",
    "                \"customer_id\",\n",
    "                \"product_id\",\n",
    "                col(\"order_datetime\").cast(\"timestamp\").alias(\"order_ts\"),\n",
    "                \"quantity\",\n",
    "                \"unit_price\",\n",
    "                (col(\"quantity\") * col(\"unit_price\")).alias(\"gross_amount\")\n",
    "            )\n",
    "    )\n",
    "```\n",
    "\n",
    "### Dekoratory expectations:\n",
    "- **`@dlt.expect`** - tylko logowanie\n",
    "- **`@dlt.expect_or_drop`** - usuÅ„ rekord\n",
    "- **`@dlt.expect_or_fail`** - zatrzymaj pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark apply_changes (CDC)\n",
    "\n",
    "```python\n",
    "import dlt\n",
    "\n",
    "# Definiujemy tabelÄ™ docelowÄ…\n",
    "dlt.create_streaming_table(\n",
    "    name=\"silver_customers\",\n",
    "    schema=\"\"\"\n",
    "        customer_id STRING,\n",
    "        first_name STRING,\n",
    "        last_name STRING,\n",
    "        email STRING,\n",
    "        city STRING,\n",
    "        __START_AT TIMESTAMP,\n",
    "        __END_AT TIMESTAMP\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Definiujemy CDC flow\n",
    "dlt.apply_changes(\n",
    "    target=\"silver_customers\",\n",
    "    source=\"bronze_customers\",\n",
    "    keys=[\"customer_id\"],\n",
    "    sequence_by=\"ingestion_ts\",\n",
    "    stored_as_scd_type=2  # lub 1\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: DEMO - Budowa Pipeline'u\n",
    "\n",
    "## ðŸŽ¯ Cel\n",
    "\n",
    "Praktyczna budowa pipeline'u Lakeflow krok po kroku w Databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PrzeglÄ…d PlikÃ³w SQL\n",
    "\n",
    "Nasz pipeline skÅ‚ada siÄ™ z plikÃ³w SQL w folderze `lakeflow/`:\n",
    "\n",
    "```\n",
    "lakeflow/\n",
    "â”œâ”€â”€ 01_bronze/\n",
    "â”‚   â”œâ”€â”€ bronze_customers.sql    # STREAMING TABLE\n",
    "â”‚   â”œâ”€â”€ bronze_orders.sql       # STREAMING TABLE + 2 FLOWs\n",
    "â”‚   â””â”€â”€ bronze_products.sql     # MATERIALIZED VIEW\n",
    "â”œâ”€â”€ 02_silver/\n",
    "â”‚   â”œâ”€â”€ silver_customers.sql    # SCD Type 2 + AUTO CDC FLOW\n",
    "â”‚   â”œâ”€â”€ silver_orders.sql       # STREAMING TABLE + constraints\n",
    "â”‚   â””â”€â”€ silver_products.sql     # MATERIALIZED VIEW\n",
    "â””â”€â”€ 03_gold/\n",
    "    â”œâ”€â”€ dim_customer.sql        # MATERIALIZED VIEW (snapshot)\n",
    "    â”œâ”€â”€ dim_date.sql            # MATERIALIZED VIEW\n",
    "    â”œâ”€â”€ dim_payment_method.sql  # MATERIALIZED VIEW\n",
    "    â”œâ”€â”€ dim_product.sql         # MATERIALIZED VIEW\n",
    "    â”œâ”€â”€ dim_store.sql           # MATERIALIZED VIEW\n",
    "    â””â”€â”€ fact_sales.sql          # STREAMING TABLE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Upload SQL Files to Workspace\n",
    "\n",
    "### Opcja A: Przez UI\n",
    "1. W Databricks â†’ Workspace â†’ Users â†’ (twÃ³j folder)\n",
    "2. UtwÃ³rz folder `lakeflow_pipeline`\n",
    "3. Upload wszystkich plikÃ³w SQL z folderu `lakeflow/`\n",
    "\n",
    "### Opcja B: Przez Repos\n",
    "1. Databricks â†’ Repos â†’ Add Repo\n",
    "2. Sklonuj repozytorium z kodem szkoleniowym\n",
    "3. Pliki SQL bÄ™dÄ… w `lakeflow/`\n",
    "\n",
    "### Opcja C: Notebook z %run\n",
    "MoÅ¼esz teÅ¼ skopiowaÄ‡ zawartoÅ›Ä‡ SQL do jednego notebooka z odpowiednimi sekcjami."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Pipeline\n",
    "\n",
    "1. **Workflows â†’ Delta Live Tables â†’ Create Pipeline**\n",
    "\n",
    "2. **General:**\n",
    "   - **Pipeline name:** `training_medallion_pipeline`\n",
    "   - **Product edition:** `Advanced` (dla SCD Type 2)\n",
    "   - **Pipeline mode:** `Triggered` (lub `Continuous` dla produkcji)\n",
    "\n",
    "3. **Source code:**\n",
    "   - Kliknij \"Add source code\"\n",
    "   - Dodaj Å›cieÅ¼ki do wszystkich plikÃ³w SQL:\n",
    "     - `/Workspace/Users/{user}/lakeflow_pipeline/01_bronze/`\n",
    "     - `/Workspace/Users/{user}/lakeflow_pipeline/02_silver/`\n",
    "     - `/Workspace/Users/{user}/lakeflow_pipeline/03_gold/`\n",
    "\n",
    "4. **Destination:**\n",
    "   - **Catalog:** `training_catalog`\n",
    "   - **Target schema:** `{user}_lakeflow`\n",
    "\n",
    "5. **Compute:**\n",
    "   - **Cluster mode:** `Enhanced autoscaling`\n",
    "   - **Min workers:** 1\n",
    "   - **Max workers:** 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure Pipeline Variables\n",
    "\n",
    "W sekcji **Configuration** â†’ **Add configuration**:\n",
    "\n",
    "| Key | Value |\n",
    "|-----|-------|\n",
    "| `customer_path` | `/Volumes/training_catalog/default/kion_datasets/customers` |\n",
    "| `order_path` | `/Volumes/training_catalog/default/kion_datasets/orders` |\n",
    "| `product_path` | `/Volumes/training_catalog/default/kion_datasets/products` |\n",
    "\n",
    "Te zmienne sÄ… uÅ¼ywane w plikach SQL jako `${customer_path}` itp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Validate Pipeline\n",
    "\n",
    "1. Kliknij **Validate** (nie uruchamia pipeline, tylko sprawdza skÅ‚adniÄ™)\n",
    "\n",
    "2. SprawdÅº **DAG** (Directed Acyclic Graph):\n",
    "   - Powinien pokazywaÄ‡ wszystkie tabele i ich zaleÅ¼noÅ›ci\n",
    "   - Bronze â†’ Silver â†’ Gold\n",
    "\n",
    "3. Typowe bÅ‚Ä™dy walidacji:\n",
    "   - Missing variable: `${variable}` nie zdefiniowana\n",
    "   - Syntax error: bÅ‚Ä…d SQL\n",
    "   - Missing table reference: tabela ÅºrÃ³dÅ‚owa nie istnieje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run Pipeline\n",
    "\n",
    "1. Kliknij **Start** \n",
    "\n",
    "2. Obserwuj wykonanie:\n",
    "   - **Initializing** - uruchamianie klastra\n",
    "   - **Setting up tables** - tworzenie tabel\n",
    "   - **Running** - przetwarzanie danych\n",
    "   - **Completed** - sukces!\n",
    "\n",
    "3. **Event Log** pokazuje szczegÃ³Å‚y:\n",
    "   - LiczbÄ™ przetworzonych rekordÃ³w\n",
    "   - Naruszenia constraints (dropped rows)\n",
    "   - Czas wykonania kaÅ¼dego etapu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Verify Results\n",
    "\n",
    "Po zakoÅ„czeniu pipeline'u sprawdÅº wyniki:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uruchom w Databricks po wykonaniu pipeline'u\n",
    "\n",
    "catalog_name = spark.conf.get(\"catalog_name\")\n",
    "user_schema = spark.conf.get(\"user_schema\")\n",
    "\n",
    "# Lista tabel w schemacie\n",
    "print(\"=\" * 50)\n",
    "print(\"Created tables:\")\n",
    "print(\"=\" * 50)\n",
    "display(spark.sql(f\"SHOW TABLES IN {catalog_name}.{user_schema}_lakeflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SprawdÅº Bronze - surowe dane z metadatami\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT * \n",
    "    FROM {catalog_name}.{user_schema}_lakeflow.bronze_customers \n",
    "    LIMIT 5\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SprawdÅº Silver customers - SCD Type 2\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        first_name,\n",
    "        last_name,\n",
    "        city,\n",
    "        __START_AT,\n",
    "        __END_AT\n",
    "    FROM {catalog_name}.{user_schema}_lakeflow.silver_customers \n",
    "    ORDER BY customer_id, __START_AT\n",
    "    LIMIT 10\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SprawdÅº Gold - dim_customer (aktualny snapshot)\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT * \n",
    "    FROM {catalog_name}.{user_schema}_lakeflow.dim_customer \n",
    "    LIMIT 5\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SprawdÅº fact_sales z joinami do wymiarÃ³w\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        f.order_id,\n",
    "        c.first_name || ' ' || c.last_name AS customer_name,\n",
    "        p.product_name,\n",
    "        d.date,\n",
    "        f.quantity,\n",
    "        f.net_amount\n",
    "    FROM {catalog_name}.{user_schema}_lakeflow.fact_sales f\n",
    "    LEFT JOIN {catalog_name}.{user_schema}_lakeflow.dim_customer c ON f.customer_id = c.customer_id\n",
    "    LEFT JOIN {catalog_name}.{user_schema}_lakeflow.dim_product p ON f.product_id = p.product_id\n",
    "    LEFT JOIN {catalog_name}.{user_schema}_lakeflow.dim_date d ON f.order_date_key = d.date_key\n",
    "    LIMIT 10\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test Incremental Processing\n",
    "\n",
    "Pipeline jest incremental - przetworzy tylko nowe dane przy kolejnym uruchomieniu.\n",
    "\n",
    "### Test:\n",
    "1. Dodaj nowy plik `orders_stream_016.json` do folderu orders/stream/\n",
    "2. Uruchom pipeline ponownie (Start)\n",
    "3. SprawdÅº Event Log - powinien przetworzyÄ‡ tylko 1 plik\n",
    "\n",
    "### Obserwacje:\n",
    "- **STREAMING TABLEs** - przetworzÄ… tylko nowe pliki\n",
    "- **MATERIALIZED VIEWs** - przeliczÄ… siÄ™ caÅ‚kowicie (ale szybko z cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test SCD Type 2\n",
    "\n",
    "Aby zobaczyÄ‡ SCD Type 2 w akcji:\n",
    "\n",
    "1. Dodaj plik z aktualizacjÄ… klienta (np. zmiana miasta):\n",
    "   - Plik `customers_update.csv` z rekordem dla istniejÄ…cego customer_id\n",
    "   \n",
    "2. Uruchom pipeline ponownie\n",
    "\n",
    "3. SprawdÅº silver_customers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZnajdÅº klientÃ³w z historiÄ… zmian (wiele rekordÃ³w)\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        first_name,\n",
    "        city,\n",
    "        __START_AT,\n",
    "        __END_AT,\n",
    "        CASE WHEN __END_AT IS NULL THEN 'Current' ELSE 'Historical' END AS status\n",
    "    FROM {catalog_name}.{user_schema}_lakeflow.silver_customers\n",
    "    WHERE customer_id IN (\n",
    "        SELECT customer_id \n",
    "        FROM {catalog_name}.{user_schema}_lakeflow.silver_customers \n",
    "        GROUP BY customer_id \n",
    "        HAVING COUNT(*) > 1\n",
    "    )\n",
    "    ORDER BY customer_id, __START_AT\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring i Troubleshooting\n",
    "\n",
    "### Event Log\n",
    "- **Kliknij na tabelÄ™ w DAG** â†’ pokazuje szczegÃ³Å‚y przetwarzania\n",
    "- **Metrics:** records processed, dropped rows, duration\n",
    "\n",
    "### Data Quality Tab\n",
    "- Pokazuje naruszenia constraints\n",
    "- Historia jakoÅ›ci danych\n",
    "\n",
    "### Typowe problemy:\n",
    "\n",
    "| Problem | Przyczyna | RozwiÄ…zanie |\n",
    "|---------|-----------|-------------|\n",
    "| Pipeline hangs | Zbyt maÅ‚y klaster | ZwiÄ™ksz min workers |\n",
    "| Missing data | Constraint DROP ROW | SprawdÅº Data Quality tab |\n",
    "| Schema mismatch | Zmiana schematu ÅºrÃ³dÅ‚a | Full refresh lub SCHEMA EVOLUTION |\n",
    "| Duplicate keys | Brak deduplikacji | Dodaj constraint lub ROW_NUMBER |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ“‹ Summary\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### Medallion Architecture\n",
    "- **Bronze:** Surowe dane + metadata, append-only\n",
    "- **Silver:** Oczyszczone, zwalidowane, znormalizowane\n",
    "- **Gold:** Business-ready, star schema, agregacje\n",
    "\n",
    "### SCD Types\n",
    "- **SCD Type 1:** Overwrite - brak historii\n",
    "- **SCD Type 2:** Track history - `__START_AT`, `__END_AT`\n",
    "- Lakeflow obsÅ‚uguje oba przez `AUTO CDC INTO`\n",
    "\n",
    "### Lakeflow Declarations\n",
    "- **STREAMING TABLE:** Dla danych strumieniowych/append\n",
    "- **MATERIALIZED VIEW:** Dla agregacji/transformacji\n",
    "- **FLOW:** Rozdziela definicjÄ™ tabeli od ÅºrÃ³dÅ‚a, obsÅ‚uguje CDC\n",
    "\n",
    "### Best Practices\n",
    "1. UÅ¼ywaj constraints z `DROP ROW` dla jakoÅ›ci danych\n",
    "2. Oddzielaj backfill (ONCE) od streaming (continuous) FLOWs\n",
    "3. Dla wymiarÃ³w z historiÄ… - SCD Type 2\n",
    "4. Dla faktÃ³w - STREAMING TABLE\n",
    "5. Dla wymiarÃ³w statycznych - MATERIALIZED VIEW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Resources\n",
    "\n",
    "- [Databricks Lakeflow Documentation](https://docs.databricks.com/en/delta-live-tables/index.html)\n",
    "- [Medallion Architecture Best Practices](https://www.databricks.com/glossary/medallion-architecture)\n",
    "- [SCD Type 2 with Lakeflow](https://docs.databricks.com/en/delta-live-tables/cdc.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## â–¶ï¸ Next Steps\n",
    "\n",
    "1. **Notebook 06:** Orchestration - Jobs & Workflows\n",
    "2. **Notebook 07:** Unity Catalog & Governance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Medallion Architecture - Theory\n",
    "\n",
    "## ðŸŽ¯ Cel\n",
    "\n",
    "Zrozumienie wzorca **Medallion Architecture** - standaryzowanego podejÅ›cia do organizacji danych w Data Lakehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Story: Building the Data Factory\n",
    "\n",
    "Twoja platforma e-commerce ma Delta Lake. Ale panuje chaos:\n",
    "- 15 rÃ³Å¼nych notebookÃ³w, kaÅ¼dy Å‚aduje dane inaczej\n",
    "- Brak konwencji nazewnictwa - `customers_v2_final_FINAL.csv`\n",
    "- Data Scientists uÅ¼ywajÄ… danych Bronze bezpoÅ›rednio (ze wszystkimi bÅ‚Ä™dami)\n",
    "- Dashboard finansowy pokazuje inne liczby niÅ¼ marketing\n",
    "\n",
    "**CTO pyta:** \"Czy moÅ¼emy to ustandaryzowaÄ‡?\"\n",
    "\n",
    "**OdpowiedÅº:** Medallion Architecture - sprawdzony wzorzec uÅ¼ywany przez Netflix, Uber i tysiÄ…ce firm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¥‰ðŸ¥ˆðŸ¥‡ The Medallion Pattern\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚     BRONZE      â”‚ â”€â”€â–¶  â”‚     SILVER      â”‚ â”€â”€â–¶  â”‚      GOLD       â”‚\n",
    "â”‚     (Raw)       â”‚      â”‚   (Cleaned)     â”‚      â”‚   (Business)    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ â€¢ Dane surowe   â”‚      â”‚ â€¢ Walidacja     â”‚      â”‚ â€¢ Star schema   â”‚\n",
    "â”‚ â€¢ Append-only   â”‚      â”‚ â€¢ Deduplikacja  â”‚      â”‚ â€¢ Agregacje     â”‚\n",
    "â”‚ â€¢ Wszystkie typyâ”‚      â”‚ â€¢ Typowanie     â”‚      â”‚ â€¢ SCD applied   â”‚\n",
    "â”‚ â€¢ +Metadata     â”‚      â”‚ â€¢ Normalizacja  â”‚      â”‚ â€¢ Business readyâ”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "      INGEST                TRANSFORM               SERVE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Three Layers? (Not Two, Not Five)\n",
    "\n",
    "| Layers | Problem |\n",
    "|--------|---------|\n",
    "| **1 layer** | Brak separacji, ryzykowne transformacje na surowych danych |\n",
    "| **2 layers** | \"Gdzie umieÅ›ciÄ‡ walidacjÄ™?\" - niejasna odpowiedzialnoÅ›Ä‡ |\n",
    "| **3 layers** | Jasne odpowiedzialnoÅ›ci, testowalne granice |\n",
    "| **5+ layers** | Over-engineering, zwiÄ™kszona latencja, koszmar maintenance |\n",
    "\n",
    "**Trzy warstwy to sweet spot dla wiÄ™kszoÅ›ci organizacji.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Responsibilities\n",
    "\n",
    "| Layer | Owner | SLA | Access | Retencja |\n",
    "|-------|-------|-----|--------|----------|\n",
    "| **Bronze** | Data Engineering | 15 min od ÅºrÃ³dÅ‚a | Engineers only | 90 dni (lub regulatory) |\n",
    "| **Silver** | Data Engineering | 1h od Bronze | Engineers + DS | NieokreÅ›lona (master data) |\n",
    "| **Gold** | Analytics Engineering | 4h od Silver | Wszyscy | ZaleÅ¼y od use case |\n",
    "\n",
    "### Bronze Layer Details\n",
    "- **Cel:** Zachowanie surowych danych bez modyfikacji\n",
    "- **Transformacje:** Tylko dodanie metadanych (`_metadata.file_path`, `ingestion_ts`, `load_ts`)\n",
    "- **Format:** Delta Table (Streaming Table w Lakeflow)\n",
    "- **Schemat:** CzÄ™sto inferowany, moÅ¼e ewoluowaÄ‡\n",
    "\n",
    "### Silver Layer Details\n",
    "- **Cel:** Dane oczyszczone, zwalidowane, znormalizowane\n",
    "- **Transformacje:** Walidacja, deduplikacja, typowanie, obliczenia pochodne\n",
    "- **Format:** Delta Table z constraints\n",
    "- **Schemat:** ÅšciÅ›le zdefiniowany, wersjonowany\n",
    "\n",
    "### Gold Layer Details\n",
    "- **Cel:** Dane gotowe do konsumpcji biznesowej\n",
    "- **Transformacje:** Star schema, agregacje, SCD\n",
    "- **Format:** Materialized Views lub Delta Tables\n",
    "- **Schemat:** Zoptymalizowany pod zapytania analityczne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What We'll Build\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   BRONZE     â”‚     â”‚    SILVER    â”‚     â”‚      GOLD        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ customers    â”‚ â”€â”€â–¶ â”‚ customers    â”‚ â”€â”€â–¶ â”‚ dim_customer     â”‚\n",
    "â”‚ orders       â”‚ â”€â”€â–¶ â”‚ orders       â”‚ â”€â”€â–¶ â”‚ fact_sales       â”‚\n",
    "â”‚ products     â”‚ â”€â”€â–¶ â”‚ products     â”‚ â”€â”€â–¶ â”‚ dim_product      â”‚\n",
    "â”‚              â”‚     â”‚              â”‚     â”‚ dim_date         â”‚\n",
    "â”‚              â”‚     â”‚              â”‚     â”‚ dim_payment      â”‚\n",
    "â”‚              â”‚     â”‚              â”‚     â”‚ dim_store        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: SCD Type 1 & Type 2\n",
    "\n",
    "## ðŸŽ¯ Cel\n",
    "\n",
    "Zrozumienie **Slowly Changing Dimensions** - jak Å›ledziÄ‡ zmiany w danych wymiarowych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co to jest SCD?\n",
    "\n",
    "**Slowly Changing Dimension (SCD)** to koncepcja z data warehousing opisujÄ…ca jak obsÅ‚ugiwaÄ‡ zmiany w danych wymiarowych.\n",
    "\n",
    "### PrzykÅ‚ad: Zmiana adresu klienta\n",
    "\n",
    "Klient Jan Kowalski przeprowadziÅ‚ siÄ™ z Warszawy do Krakowa. Co robimy?\n",
    "\n",
    "| Typ | Strategia | Rezultat |\n",
    "|-----|-----------|----------|\n",
    "| **SCD Type 0** | Retain original | Zawsze Warszawa |\n",
    "| **SCD Type 1** | Overwrite | Tylko KrakÃ³w |\n",
    "| **SCD Type 2** | Track history | Oba rekordy z datami |\n",
    "| **SCD Type 3** | Add column | `current_city=KrakÃ³w`, `previous_city=Warszawa` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCD Type 1 - Overwrite\n",
    "\n",
    "### Charakterystyka:\n",
    "- **Najprostsze** podejÅ›cie\n",
    "- **Brak historii** - stare wartoÅ›ci sÄ… nadpisywane\n",
    "- **Use cases:** Korekty bÅ‚Ä™dÃ³w, dane ktÃ³re nie wymagajÄ… historii\n",
    "\n",
    "### Przed zmianÄ…:\n",
    "```\n",
    "| customer_id | name         | city      |\n",
    "|-------------|--------------|----------|\n",
    "| C001        | Jan Kowalski | Warszawa |\n",
    "```\n",
    "\n",
    "### Po zmianie:\n",
    "```\n",
    "| customer_id | name         | city    |\n",
    "|-------------|--------------|--------|\n",
    "| C001        | Jan Kowalski | KrakÃ³w |\n",
    "```\n",
    "\n",
    "### Implementacja w SQL (MERGE):\n",
    "```sql\n",
    "MERGE INTO dim_customer t\n",
    "USING source_customers s\n",
    "ON t.customer_id = s.customer_id\n",
    "WHEN MATCHED THEN UPDATE SET *\n",
    "WHEN NOT MATCHED THEN INSERT *\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCD Type 2 - Track History\n",
    "\n",
    "### Charakterystyka:\n",
    "- **PeÅ‚na historia** zmian\n",
    "- **Dodatkowe kolumny:** `__START_AT`, `__END_AT` (lub `valid_from`, `valid_to`)\n",
    "- **Use cases:** Audyt, analiza historyczna, compliance\n",
    "\n",
    "### Przed zmianÄ…:\n",
    "```\n",
    "| customer_id | name         | city      | __START_AT          | __END_AT |\n",
    "|-------------|--------------|-----------|---------------------|----------|\n",
    "| C001        | Jan Kowalski | Warszawa  | 2024-01-01 00:00:00 | NULL     |\n",
    "```\n",
    "\n",
    "### Po zmianie:\n",
    "```\n",
    "| customer_id | name         | city      | __START_AT          | __END_AT            |\n",
    "|-------------|--------------|-----------|---------------------|---------------------|\n",
    "| C001        | Jan Kowalski | Warszawa  | 2024-01-01 00:00:00 | 2024-06-15 10:30:00 |\n",
    "| C001        | Jan Kowalski | KrakÃ³w    | 2024-06-15 10:30:00 | NULL                |\n",
    "```\n",
    "\n",
    "### Zapytania:\n",
    "```sql\n",
    "-- Aktualny stan (snapshot)\n",
    "SELECT * FROM silver_customers WHERE __END_AT IS NULL;\n",
    "\n",
    "-- Stan na okreÅ›lony moment (time travel biznesowy)\n",
    "SELECT * FROM silver_customers \n",
    "WHERE '2024-03-01' BETWEEN __START_AT AND COALESCE(__END_AT, '9999-12-31');\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCD Type 2 w Lakeflow - APPLY CHANGES INTO\n",
    "\n",
    "Databricks Lakeflow oferuje wbudowanÄ… obsÅ‚ugÄ™ SCD Type 2 przez `APPLY CHANGES INTO`:\n",
    "\n",
    "```sql\n",
    "-- Tworzenie tabeli docelowej SCD2\n",
    "CREATE OR REFRESH STREAMING TABLE silver_customers (\n",
    "  customer_id        STRING,\n",
    "  first_name         STRING,\n",
    "  last_name          STRING,\n",
    "  city               STRING,\n",
    "  -- Kolumny SCD2 dodawane automatycznie:\n",
    "  __START_AT         TIMESTAMP,\n",
    "  __END_AT           TIMESTAMP\n",
    ");\n",
    "\n",
    "-- Flow z AUTO CDC dla SCD2\n",
    "CREATE FLOW silver_customers_scd2_flow\n",
    "AS AUTO CDC INTO silver_customers\n",
    "FROM STREAM bronze_customers\n",
    "KEYS (customer_id)         -- Klucz biznesowy\n",
    "SEQUENCE BY ingestion_ts   -- Kolumna okreÅ›lajÄ…ca kolejnoÅ›Ä‡\n",
    "STORED AS SCD TYPE 2;      -- Typ SCD\n",
    "```\n",
    "\n",
    "### Kluczowe elementy:\n",
    "- **KEYS** - kolumny identyfikujÄ…ce rekord biznesowo\n",
    "- **SEQUENCE BY** - kolumna okreÅ›lajÄ…ca kolejnoÅ›Ä‡ zmian\n",
    "- **STORED AS SCD TYPE 1|2** - typ SCD\n",
    "\n",
    "### SCD Type 1 w Lakeflow:\n",
    "```sql\n",
    "CREATE FLOW silver_products_scd1_flow\n",
    "AS AUTO CDC INTO silver_products\n",
    "FROM bronze_products\n",
    "KEYS (product_id)\n",
    "SEQUENCE BY ingestion_ts\n",
    "STORED AS SCD TYPE 1;  -- Nadpisywanie bez historii\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Lakeflow Pipelines - Deklaracje\n",
    "\n",
    "## ðŸŽ¯ Cel\n",
    "\n",
    "Poznanie skÅ‚adni **Spark Declarative Pipelines** w SQL i PySpark - deklaratywnego sposobu budowania pipeline'Ã³w."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co to jest Lakeflow?\n",
    "\n",
    "**Lakeflow** (dawniej Delta Live Tables - DLT) to framework Databricks do budowania deklaratywnych pipeline'Ã³w danych.\n",
    "\n",
    "### Deklaratywne vs Imperatywne\n",
    "\n",
    "| PodejÅ›cie | PrzykÅ‚ad | Charakterystyka |\n",
    "|-----------|----------|----------------|\n",
    "| **Imperatywne** | `df.write.mode(\"overwrite\")...` | Opisujesz JAK |\n",
    "| **Deklaratywne** | `CREATE TABLE AS SELECT...` | Opisujesz CO |\n",
    "\n",
    "### KorzyÅ›ci Lakeflow:\n",
    "- âœ… **Automatyczne zarzÄ…dzanie zaleÅ¼noÅ›ciami** miÄ™dzy tabelami\n",
    "- âœ… **Wbudowana jakoÅ›Ä‡ danych** - constraints z akcjami\n",
    "- âœ… **Unified batch/streaming** - ten sam kod\n",
    "- âœ… **Automatyczne recovery** po awariach\n",
    "- âœ… **Lineage i monitoring** out-of-the-box\n",
    "- âœ… **Incremental processing** - tylko nowe dane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typy Tabel w Lakeflow\n",
    "\n",
    "| Typ | UÅ¼ycie | Charakterystyka |\n",
    "|-----|--------|----------------|\n",
    "| **STREAMING TABLE** | Dane ÅºrÃ³dÅ‚owe, append-only | Przetwarza tylko nowe dane |\n",
    "| **MATERIALIZED VIEW** | Agregacje, transformacje | Zawsze peÅ‚ne przeliczenie |\n",
    "| **VIEW** | Logika poÅ›rednia | Nie materializowana |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Declarations\n",
    "\n",
    "### 1. STREAMING TABLE - Dane strumieniowe (Bronze)\n",
    "\n",
    "```sql\n",
    "-- Najprostsza forma - inline query\n",
    "CREATE OR REFRESH STREAMING TABLE bronze_customers\n",
    "AS\n",
    "SELECT\n",
    "  customer_id,\n",
    "  first_name,\n",
    "  last_name,\n",
    "  email,\n",
    "  city,\n",
    "  -- Metadata\n",
    "  _metadata.file_path                AS source_file_path,\n",
    "  _metadata.file_modification_time   AS ingestion_ts,\n",
    "  current_timestamp()                AS load_ts\n",
    "FROM STREAM read_files(\n",
    "  '${customer_path}',\n",
    "  format           => 'csv',\n",
    "  header           => true,\n",
    "  inferColumnTypes => true\n",
    ");\n",
    "```\n",
    "\n",
    "### Kluczowe elementy:\n",
    "- **`STREAM read_files(...)`** - czyta pliki w trybie strumieniowym (tylko nowe)\n",
    "- **`_metadata`** - metadane pliku ÅºrÃ³dÅ‚owego\n",
    "- **`${variable}`** - parametry pipeline'u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. STREAMING TABLE z CONSTRAINTS (Silver)\n",
    "\n",
    "```sql\n",
    "CREATE OR REFRESH STREAMING TABLE silver_orders\n",
    "(\n",
    "  -- Constraints z akcjami\n",
    "  CONSTRAINT valid_order_id EXPECT (order_id IS NOT NULL)\n",
    "    ON VIOLATION DROP ROW,\n",
    "  CONSTRAINT valid_customer EXPECT (customer_id IS NOT NULL)\n",
    "    ON VIOLATION DROP ROW,\n",
    "  CONSTRAINT valid_quantity EXPECT (quantity > 0)\n",
    "    ON VIOLATION DROP ROW,\n",
    "  CONSTRAINT valid_price EXPECT (unit_price >= 0)\n",
    "    ON VIOLATION FAIL UPDATE   -- Zatrzymaj pipeline!\n",
    ")\n",
    "AS\n",
    "SELECT\n",
    "  order_id,\n",
    "  customer_id,\n",
    "  product_id,\n",
    "  CAST(order_datetime AS TIMESTAMP) AS order_ts,\n",
    "  quantity,\n",
    "  unit_price,\n",
    "  (quantity * unit_price) AS gross_amount\n",
    "FROM STREAM(bronze_orders);\n",
    "```\n",
    "\n",
    "### Akcje dla CONSTRAINTS:\n",
    "- **`DROP ROW`** - usuÅ„ nieprawidÅ‚owy rekord (logowane w metrykach)\n",
    "- **`FAIL UPDATE`** - zatrzymaj pipeline przy naruszeniu\n",
    "- Brak akcji - tylko logowanie (warning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. MATERIALIZED VIEW (Gold)\n",
    "\n",
    "```sql\n",
    "-- Wymiar - aktualny snapshot z SCD2\n",
    "CREATE OR REFRESH MATERIALIZED VIEW dim_customer\n",
    "AS\n",
    "SELECT\n",
    "  customer_id,\n",
    "  first_name,\n",
    "  last_name,\n",
    "  email,\n",
    "  city,\n",
    "  country,\n",
    "  customer_segment\n",
    "FROM silver_customers\n",
    "WHERE __END_AT IS NULL;  -- Tylko aktywne rekordy\n",
    "\n",
    "-- Wymiar daty - generowany z danych\n",
    "CREATE OR REFRESH MATERIALIZED VIEW dim_date\n",
    "AS\n",
    "SELECT DISTINCT\n",
    "  CAST(date_format(order_date, 'yyyyMMdd') AS INT) AS date_key,\n",
    "  order_date AS date,\n",
    "  year(order_date) AS year,\n",
    "  quarter(order_date) AS quarter,\n",
    "  month(order_date) AS month,\n",
    "  day(order_date) AS day\n",
    "FROM silver_orders;\n",
    "\n",
    "-- Fakt - streaming z Silver\n",
    "CREATE OR REFRESH STREAMING TABLE fact_sales\n",
    "AS\n",
    "SELECT\n",
    "  order_id,\n",
    "  customer_id,\n",
    "  product_id,\n",
    "  order_date_key,\n",
    "  quantity,\n",
    "  gross_amount,\n",
    "  net_amount\n",
    "FROM STREAM(silver_orders);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co to jest FLOW?\n",
    "\n",
    "**FLOW** to mechanizm Lakeflow pozwalajÄ…cy na:\n",
    "1. **Rozdzielenie definicji tabeli od ÅºrÃ³dÅ‚a danych**\n",
    "2. **Wiele ÅºrÃ³deÅ‚ do jednej tabeli** (np. backfill + streaming)\n",
    "3. **CDC (Change Data Capture)** z automatycznym SCD\n",
    "\n",
    "### Anatomia FLOW:\n",
    "\n",
    "```sql\n",
    "-- 1. Najpierw definiujemy pustÄ… tabelÄ™ docelowÄ…\n",
    "CREATE OR REFRESH STREAMING TABLE bronze_orders;\n",
    "\n",
    "-- 2. Potem definiujemy FLOW(s) ktÃ³re jÄ… zasilajÄ…\n",
    "CREATE FLOW flow_name\n",
    "AS INSERT INTO target_table BY NAME\n",
    "SELECT ... FROM source;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PrzykÅ‚ady FLOW\n",
    "\n",
    "### 1. Backfill + Streaming Pattern\n",
    "\n",
    "Typowy scenariusz: mamy dane historyczne (batch) i nowe dane (streaming).\n",
    "\n",
    "```sql\n",
    "-- Tabela docelowa\n",
    "CREATE OR REFRESH STREAMING TABLE bronze_orders;\n",
    "\n",
    "-- FLOW 1: Jednorazowy backfill z pliku historycznego\n",
    "CREATE FLOW bronze_orders_backfill\n",
    "AS \n",
    "INSERT INTO ONCE bronze_orders BY NAME  -- ONCE = wykonaj raz\n",
    "SELECT\n",
    "  order_id,\n",
    "  customer_id,\n",
    "  product_id,\n",
    "  order_datetime,\n",
    "  'batch' AS source_system,\n",
    "  _metadata.file_path AS source_file_path,\n",
    "  current_timestamp() AS load_ts\n",
    "FROM read_files(\n",
    "  '${order_path}/orders_batch.json',\n",
    "  format => 'json'\n",
    ");\n",
    "\n",
    "-- FLOW 2: CiÄ…gÅ‚y streaming z nowych plikÃ³w\n",
    "CREATE FLOW bronze_orders_stream\n",
    "AS \n",
    "INSERT INTO bronze_orders BY NAME  -- Bez ONCE = continuous\n",
    "SELECT\n",
    "  order_id,\n",
    "  customer_id,\n",
    "  product_id,\n",
    "  order_datetime,\n",
    "  'stream' AS source_system,\n",
    "  _metadata.file_path AS source_file_path,\n",
    "  current_timestamp() AS load_ts\n",
    "FROM STREAM read_files(\n",
    "  '${order_path}/stream/orders_stream_*.json',\n",
    "  format => 'json'\n",
    ");\n",
    "```\n",
    "\n",
    "### Kluczowe elementy:\n",
    "- **`INSERT INTO ONCE`** - wykonaj raz (backfill)\n",
    "- **`INSERT INTO`** - ciÄ…gÅ‚e wstawianie (streaming)\n",
    "- **`BY NAME`** - mapowanie kolumn po nazwie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. AUTO CDC FLOW dla SCD Type 2\n",
    "\n",
    "```sql\n",
    "-- Tabela SCD2 ze zdefiniowanym schematem\n",
    "CREATE OR REFRESH STREAMING TABLE silver_customers (\n",
    "  customer_id        STRING,\n",
    "  first_name         STRING,\n",
    "  last_name          STRING,\n",
    "  email              STRING,\n",
    "  city               STRING,\n",
    "  country            STRING,\n",
    "  customer_segment   STRING,\n",
    "  source_file_path   STRING,\n",
    "  ingestion_ts       TIMESTAMP,\n",
    "  -- Kolumny SCD2\n",
    "  __START_AT         TIMESTAMP,\n",
    "  __END_AT           TIMESTAMP\n",
    ");\n",
    "\n",
    "-- AUTO CDC Flow\n",
    "CREATE FLOW silver_customers_scd2_flow\n",
    "AS AUTO CDC INTO silver_customers\n",
    "FROM STREAM bronze_customers\n",
    "KEYS (customer_id)         -- Klucz biznesowy\n",
    "SEQUENCE BY ingestion_ts   -- KolejnoÅ›Ä‡ zmian\n",
    "STORED AS SCD TYPE 2;      -- Tryb SCD\n",
    "```\n",
    "\n",
    "### Co robi AUTO CDC?\n",
    "1. PorÃ³wnuje nowe rekordy z istniejÄ…cymi po `KEYS`\n",
    "2. Wykrywa zmiany w kolumnach\n",
    "3. Dla SCD2: zamyka stary rekord (`__END_AT`), wstawia nowy\n",
    "4. Dla SCD1: nadpisuje istniejÄ…cy rekord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Declarations\n",
    "\n",
    "Te same koncepcje w PySpark z uÅ¼yciem dekoratorÃ³w `dlt`:\n",
    "\n",
    "```python\n",
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# STREAMING TABLE\n",
    "@dlt.table(\n",
    "    name=\"bronze_customers\",\n",
    "    comment=\"Raw customers from CSV\"\n",
    ")\n",
    "def bronze_customers():\n",
    "    return (\n",
    "        spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"csv\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .load(spark.conf.get(\"customer_path\"))\n",
    "            .select(\n",
    "                \"*\",\n",
    "                \"_metadata.file_path\".alias(\"source_file_path\"),\n",
    "                current_timestamp().alias(\"load_ts\")\n",
    "            )\n",
    "    )\n",
    "\n",
    "# MATERIALIZED VIEW\n",
    "@dlt.table(\n",
    "    name=\"dim_customer\",\n",
    "    comment=\"Customer dimension - current snapshot\"\n",
    ")\n",
    "def dim_customer():\n",
    "    return (\n",
    "        dlt.read(\"silver_customers\")\n",
    "            .filter(col(\"__END_AT\").isNull())\n",
    "            .select(\n",
    "                \"customer_id\",\n",
    "                \"first_name\", \n",
    "                \"last_name\",\n",
    "                \"email\",\n",
    "                \"city\"\n",
    "            )\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark z Expectations (Constraints)\n",
    "\n",
    "```python\n",
    "@dlt.table(\n",
    "    name=\"silver_orders\",\n",
    "    comment=\"Cleaned and validated orders\"\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_order_id\", \"order_id IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_customer\", \"customer_id IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_quantity\", \"quantity > 0\")\n",
    "@dlt.expect_or_fail(\"valid_price\", \"unit_price >= 0\")  # Zatrzymaj pipeline!\n",
    "def silver_orders():\n",
    "    return (\n",
    "        dlt.readStream(\"bronze_orders\")\n",
    "            .select(\n",
    "                \"order_id\",\n",
    "                \"customer_id\",\n",
    "                \"product_id\",\n",
    "                col(\"order_datetime\").cast(\"timestamp\").alias(\"order_ts\"),\n",
    "                \"quantity\",\n",
    "                \"unit_price\",\n",
    "                (col(\"quantity\") * col(\"unit_price\")).alias(\"gross_amount\")\n",
    "            )\n",
    "    )\n",
    "```\n",
    "\n",
    "### Dekoratory expectations:\n",
    "- **`@dlt.expect`** - tylko logowanie\n",
    "- **`@dlt.expect_or_drop`** - usuÅ„ rekord\n",
    "- **`@dlt.expect_or_fail`** - zatrzymaj pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark APPLY CHANGES (CDC)\n",
    "\n",
    "```python\n",
    "import dlt\n",
    "\n",
    "# Definiujemy tabelÄ™ docelowÄ…\n",
    "dlt.create_streaming_table(\n",
    "    name=\"silver_customers\",\n",
    "    schema=\"\"\"\n",
    "        customer_id STRING,\n",
    "        first_name STRING,\n",
    "        last_name STRING,\n",
    "        email STRING,\n",
    "        city STRING,\n",
    "        __START_AT TIMESTAMP,\n",
    "        __END_AT TIMESTAMP\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Definiujemy CDC flow\n",
    "dlt.apply_changes(\n",
    "    target=\"silver_customers\",\n",
    "    source=\"bronze_customers\",\n",
    "    keys=[\"customer_id\"],\n",
    "    sequence_by=\"ingestion_ts\",\n",
    "    stored_as_scd_type=2  # lub 1\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: DEMO - Budowa Pipeline'u\n",
    "\n",
    "## ðŸŽ¯ Cel\n",
    "\n",
    "Praktyczna budowa pipeline'u Lakeflow krok po kroku w Databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PrzeglÄ…d PlikÃ³w SQL\n",
    "\n",
    "Nasz pipeline skÅ‚ada siÄ™ z plikÃ³w SQL w folderze `lakeflow/`:\n",
    "\n",
    "```\n",
    "lakeflow/\n",
    "â”œâ”€â”€ 01_bronze/\n",
    "â”‚   â”œâ”€â”€ bronze_customers.sql    # STREAMING TABLE\n",
    "â”‚   â”œâ”€â”€ bronze_orders.sql       # STREAMING TABLE + 2 FLOWs\n",
    "â”‚   â””â”€â”€ bronze_products.sql     # MATERIALIZED VIEW\n",
    "â”œâ”€â”€ 02_silver/\n",
    "â”‚   â”œâ”€â”€ silver_customers.sql    # SCD Type 2 + AUTO CDC FLOW\n",
    "â”‚   â”œâ”€â”€ silver_orders.sql       # STREAMING TABLE + constraints\n",
    "â”‚   â””â”€â”€ silver_products.sql     # MATERIALIZED VIEW\n",
    "â””â”€â”€ 03_gold/\n",
    "    â”œâ”€â”€ dim_customer.sql        # MATERIALIZED VIEW (snapshot)\n",
    "    â”œâ”€â”€ dim_date.sql            # MATERIALIZED VIEW\n",
    "    â”œâ”€â”€ dim_payment_method.sql  # MATERIALIZED VIEW\n",
    "    â”œâ”€â”€ dim_product.sql         # MATERIALIZED VIEW\n",
    "    â”œâ”€â”€ dim_store.sql           # MATERIALIZED VIEW\n",
    "    â””â”€â”€ fact_sales.sql          # STREAMING TABLE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Upload SQL Files to Workspace\n",
    "\n",
    "### Opcja A: Przez UI\n",
    "1. W Databricks â†’ Workspace â†’ Users â†’ (twÃ³j folder)\n",
    "2. UtwÃ³rz folder `lakeflow_pipeline`\n",
    "3. Upload wszystkich plikÃ³w SQL z folderu `lakeflow/`\n",
    "\n",
    "### Opcja B: Przez Repos\n",
    "1. Databricks â†’ Repos â†’ Add Repo\n",
    "2. Sklonuj repozytorium z kodem szkoleniowym\n",
    "3. Pliki SQL bÄ™dÄ… w `lakeflow/`\n",
    "\n",
    "### Opcja C: Notebook z %run\n",
    "MoÅ¼esz teÅ¼ skopiowaÄ‡ zawartoÅ›Ä‡ SQL do jednego notebooka z odpowiednimi sekcjami."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Pipeline\n",
    "\n",
    "1. **Workflows â†’ Delta Live Tables â†’ Create Pipeline**\n",
    "\n",
    "2. **General:**\n",
    "   - **Pipeline name:** `training_medallion_pipeline`\n",
    "   - **Product edition:** `Advanced` (dla SCD Type 2)\n",
    "   - **Pipeline mode:** `Triggered` (lub `Continuous` dla produkcji)\n",
    "\n",
    "3. **Source code:**\n",
    "   - Kliknij \"Add source code\"\n",
    "   - Dodaj Å›cieÅ¼ki do wszystkich plikÃ³w SQL:\n",
    "     - `/Workspace/Users/{user}/lakeflow_pipeline/01_bronze/`\n",
    "     - `/Workspace/Users/{user}/lakeflow_pipeline/02_silver/`\n",
    "     - `/Workspace/Users/{user}/lakeflow_pipeline/03_gold/`\n",
    "\n",
    "4. **Destination:**\n",
    "   - **Catalog:** `training_catalog`\n",
    "   - **Target schema:** `{user}_lakeflow`\n",
    "\n",
    "5. **Compute:**\n",
    "   - **Cluster mode:** `Enhanced autoscaling`\n",
    "   - **Min workers:** 1\n",
    "   - **Max workers:** 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure Pipeline Variables\n",
    "\n",
    "W sekcji **Configuration** â†’ **Add configuration**:\n",
    "\n",
    "| Key | Value |\n",
    "|-----|-------|\n",
    "| `customer_path` | `/Volumes/training_catalog/default/kion_datasets/customers` |\n",
    "| `order_path` | `/Volumes/training_catalog/default/kion_datasets/orders` |\n",
    "| `product_path` | `/Volumes/training_catalog/default/kion_datasets/products` |\n",
    "\n",
    "Te zmienne sÄ… uÅ¼ywane w plikach SQL jako `${customer_path}` itp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Validate Pipeline\n",
    "\n",
    "1. Kliknij **Validate** (nie uruchamia pipeline, tylko sprawdza skÅ‚adniÄ™)\n",
    "\n",
    "2. SprawdÅº **DAG** (Directed Acyclic Graph):\n",
    "   - Powinien pokazywaÄ‡ wszystkie tabele i ich zaleÅ¼noÅ›ci\n",
    "   - Bronze â†’ Silver â†’ Gold\n",
    "\n",
    "3. Typowe bÅ‚Ä™dy walidacji:\n",
    "   - Missing variable: `${variable}` nie zdefiniowana\n",
    "   - Syntax error: bÅ‚Ä…d SQL\n",
    "   - Missing table reference: tabela ÅºrÃ³dÅ‚owa nie istnieje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run Pipeline\n",
    "\n",
    "1. Kliknij **Start** \n",
    "\n",
    "2. Obserwuj wykonanie:\n",
    "   - **Initializing** - uruchamianie klastra\n",
    "   - **Setting up tables** - tworzenie tabel\n",
    "   - **Running** - przetwarzanie danych\n",
    "   - **Completed** - sukces!\n",
    "\n",
    "3. **Event Log** pokazuje szczegÃ³Å‚y:\n",
    "   - LiczbÄ™ przetworzonych rekordÃ³w\n",
    "   - Naruszenia constraints (dropped rows)\n",
    "   - Czas wykonania kaÅ¼dego etapu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Verify Results\n",
    "\n",
    "Po zakoÅ„czeniu pipeline'u sprawdÅº wyniki:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uruchom w Databricks po wykonaniu pipeline'u\n",
    "\n",
    "catalog_name = spark.conf.get(\"catalog_name\")\n",
    "user_schema = spark.conf.get(\"user_schema\")\n",
    "\n",
    "# Lista tabel w schemacie\n",
    "print(\"=\" * 50)\n",
    "print(\"Created tables:\")\n",
    "print(\"=\" * 50)\n",
    "display(spark.sql(f\"SHOW TABLES IN {catalog_name}.{user_schema}_lakeflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SprawdÅº Bronze - surowe dane z metadatami\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT * \n",
    "    FROM {catalog_name}.{user_schema}_lakeflow.bronze_customers \n",
    "    LIMIT 5\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SprawdÅº Silver customers - SCD Type 2\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        first_name,\n",
    "        last_name,\n",
    "        city,\n",
    "        __START_AT,\n",
    "        __END_AT\n",
    "    FROM {catalog_name}.{user_schema}_lakeflow.silver_customers \n",
    "    ORDER BY customer_id, __START_AT\n",
    "    LIMIT 10\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SprawdÅº Gold - dim_customer (aktualny snapshot)\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT * \n",
    "    FROM {catalog_name}.{user_schema}_lakeflow.dim_customer \n",
    "    LIMIT 5\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SprawdÅº fact_sales z joinami do wymiarÃ³w\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        f.order_id,\n",
    "        c.first_name || ' ' || c.last_name AS customer_name,\n",
    "        p.product_name,\n",
    "        d.date,\n",
    "        f.quantity,\n",
    "        f.net_amount\n",
    "    FROM {catalog_name}.{user_schema}_lakeflow.fact_sales f\n",
    "    LEFT JOIN {catalog_name}.{user_schema}_lakeflow.dim_customer c ON f.customer_id = c.customer_id\n",
    "    LEFT JOIN {catalog_name}.{user_schema}_lakeflow.dim_product p ON f.product_id = p.product_id\n",
    "    LEFT JOIN {catalog_name}.{user_schema}_lakeflow.dim_date d ON f.order_date_key = d.date_key\n",
    "    LIMIT 10\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test Incremental Processing\n",
    "\n",
    "Pipeline jest incremental - przetworzy tylko nowe dane przy kolejnym uruchomieniu.\n",
    "\n",
    "### Test:\n",
    "1. Dodaj nowy plik `orders_stream_016.json` do folderu orders/stream/\n",
    "2. Uruchom pipeline ponownie (Start)\n",
    "3. SprawdÅº Event Log - powinien przetworzyÄ‡ tylko 1 plik\n",
    "\n",
    "### Obserwacje:\n",
    "- **STREAMING TABLEs** - przetworzÄ… tylko nowe pliki\n",
    "- **MATERIALIZED VIEWs** - przeliczÄ… siÄ™ caÅ‚kowicie (ale szybko z cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test SCD Type 2\n",
    "\n",
    "Aby zobaczyÄ‡ SCD Type 2 w akcji:\n",
    "\n",
    "1. Dodaj plik z aktualizacjÄ… klienta (np. zmiana miasta):\n",
    "   - Plik `customers_update.csv` z rekordem dla istniejÄ…cego customer_id\n",
    "   \n",
    "2. Uruchom pipeline ponownie\n",
    "\n",
    "3. SprawdÅº silver_customers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZnajdÅº klientÃ³w z historiÄ… zmian (wiele rekordÃ³w)\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        first_name,\n",
    "        city,\n",
    "        __START_AT,\n",
    "        __END_AT,\n",
    "        CASE WHEN __END_AT IS NULL THEN 'Current' ELSE 'Historical' END AS status\n",
    "    FROM {catalog_name}.{user_schema}_lakeflow.silver_customers\n",
    "    WHERE customer_id IN (\n",
    "        SELECT customer_id \n",
    "        FROM {catalog_name}.{user_schema}_lakeflow.silver_customers \n",
    "        GROUP BY customer_id \n",
    "        HAVING COUNT(*) > 1\n",
    "    )\n",
    "    ORDER BY customer_id, __START_AT\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring i Troubleshooting\n",
    "\n",
    "### Event Log\n",
    "- **Kliknij na tabelÄ™ w DAG** â†’ pokazuje szczegÃ³Å‚y przetwarzania\n",
    "- **Metrics:** records processed, dropped rows, duration\n",
    "\n",
    "### Data Quality Tab\n",
    "- Pokazuje naruszenia constraints\n",
    "- Historia jakoÅ›ci danych\n",
    "\n",
    "### Typowe problemy:\n",
    "\n",
    "| Problem | Przyczyna | RozwiÄ…zanie |\n",
    "|---------|-----------|-------------|\n",
    "| Pipeline hangs | Zbyt maÅ‚y klaster | ZwiÄ™ksz min workers |\n",
    "| Missing data | Constraint DROP ROW | SprawdÅº Data Quality tab |\n",
    "| Schema mismatch | Zmiana schematu ÅºrÃ³dÅ‚a | Full refresh lub SCHEMA EVOLUTION |\n",
    "| Duplicate keys | Brak deduplikacji | Dodaj constraint lub ROW_NUMBER |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ“‹ Summary\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### Medallion Architecture\n",
    "- **Bronze:** Surowe dane + metadata, append-only\n",
    "- **Silver:** Oczyszczone, zwalidowane, znormalizowane\n",
    "- **Gold:** Business-ready, star schema, agregacje\n",
    "\n",
    "### SCD Types\n",
    "- **SCD Type 1:** Overwrite - brak historii\n",
    "- **SCD Type 2:** Track history - `__START_AT`, `__END_AT`\n",
    "- Lakeflow obsÅ‚uguje oba przez `APPLY CHANGES INTO` / `AUTO CDC`\n",
    "\n",
    "### Lakeflow Declarations\n",
    "- **STREAMING TABLE:** Dla danych strumieniowych/append\n",
    "- **MATERIALIZED VIEW:** Dla agregacji/transformacji\n",
    "- **FLOW:** Rozdziela definicjÄ™ tabeli od ÅºrÃ³dÅ‚a, obsÅ‚uguje CDC\n",
    "\n",
    "### Best Practices\n",
    "1. UÅ¼ywaj constraints z `DROP ROW` dla jakoÅ›ci danych\n",
    "2. Oddzielaj backfill (ONCE) od streaming (continuous) FLOWs\n",
    "3. Dla wymiarÃ³w z historiÄ… - SCD Type 2\n",
    "4. Dla faktÃ³w - STREAMING TABLE\n",
    "5. Dla wymiarÃ³w statycznych - MATERIALIZED VIEW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Resources\n",
    "\n",
    "- [Databricks Lakeflow Documentation](https://docs.databricks.com/en/delta-live-tables/index.html)\n",
    "- [Medallion Architecture Best Practices](https://www.databricks.com/glossary/medallion-architecture)\n",
    "- [SCD Type 2 with Lakeflow](https://docs.databricks.com/en/delta-live-tables/cdc.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## â–¶ï¸ Next Steps\n",
    "\n",
    "1. **Notebook 06:** Orchestration - Jobs & Workflows\n",
    "2. **Notebook 07:** Unity Catalog & Governance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d54ecb3-6758-4504-b2d9-e38d7067126e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Context and Requirements\n",
    "\n",
    "**Requirements:**\n",
    "- Databricks Runtime 16.4 LTS or newer (recommended: 17.3 LTS)\n",
    "- Unity Catalog enabled\n",
    "- Access to `/Volumes/` or DBFS for datasets\n",
    "- `00_setup.ipynb` notebook executed (for user isolation)\n",
    "\n",
    "**Prerequisite:**\n",
    "- Completed training (or familiarity with) notebooks:\n",
    "  - `01_databricks_lakehouse_intro.ipynb`\n",
    "  - `02_data_import_exploration.ipynb`\n",
    "  - `03_delta_lake_operations.ipynb`\n",
    "  - `04_optimization_best_practices.ipynb`\n",
    "  - `05_lakeflow_connection.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4698a3de-470f-41c4-a11a-5dbec2b244a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Theoretical Introduction - Medallion Architecture\n",
    "\n",
    "### What is Medallion Architecture?\n",
    "\n",
    "**Medallion Architecture** is a design pattern for organizing data in a Data Lakehouse.\n",
    "\n",
    "It divides data into three layers based on **quality level and purpose**:\n",
    "\n",
    "```\n",
    "ðŸ“¥ RAW DATA          ðŸ”„ CLEAN DATA           ðŸ“Š BUSINESS DATA\n",
    "   (Sources)            (Validated)              (Analytics)\n",
    "      â†“                     â†“                        â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   BRONZE    â”‚ â”€â”€â–¶  â”‚   SILVER    â”‚ â”€â”€â–¶     â”‚    GOLD     â”‚\n",
    "â”‚  Raw Data   â”‚      â”‚   Clean     â”‚         â”‚  Business   â”‚\n",
    "â”‚  \"As-Is\"    â”‚      â”‚  Validated  â”‚         â”‚   Ready     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Bronze Layer - Raw Data\n",
    "\n",
    "**Purpose:** Landing zone for raw data without transformations.\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **Raw data \"as-is\"** - no business logic changes\n",
    "- **Append-only** - data is only added, never modified\n",
    "- **Multi-format support** - JSON, CSV, Parquet, XML\n",
    "- **Schema-on-read** - flexible schema\n",
    "- **Audit metadata** - source, timestamp, file path\n",
    "\n",
    "**Metadata Columns (Lakeflow standard):**\n",
    "```python\n",
    "source_file_path     # Source file path\n",
    "ingestion_ts         # File processing timestamp  \n",
    "load_ts              # Load to Bronze timestamp\n",
    "source_system        # Source system (batch/stream)\n",
    "```\n",
    "\n",
    "**Use Cases:**\n",
    "- âœ… Backup & recovery of raw data\n",
    "- âœ… Reprocessing pipelines from scratch\n",
    "- âœ… Compliance & audit (full history)\n",
    "- âœ… Data science exploration\n",
    "\n",
    "---\n",
    "\n",
    "### Silver Layer - Clean & Validated\n",
    "\n",
    "**Purpose:** Cleansing and validation for analytical use.\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **Deduplication** - removal of duplicate records\n",
    "- **Type casting** - correct data types\n",
    "- **Null handling** - replacement with defaults or Unknown\n",
    "- **Business rules** - validation and calculated fields\n",
    "- **SCD (Slowly Changing Dimensions)** - history tracking\n",
    "\n",
    "**Transformations:**\n",
    "```python\n",
    "# Calculated measures (Lakeflow model)\n",
    "gross_amount = quantity * unit_price\n",
    "discount_amount = gross_amount * discount_percent / 100\n",
    "net_amount = gross_amount - discount_amount\n",
    "\n",
    "# Quality flags\n",
    "is_return = 1 if quantity < 0 else 0\n",
    "is_future_dated = 1 if order_date > current_date else 0\n",
    "is_unknown_customer = 1 if customer_id IS NULL else 0\n",
    "```\n",
    "\n",
    "**Constraints (Lakeflow):**\n",
    "```sql\n",
    "CONSTRAINT valid_order_id EXPECT (order_id IS NOT NULL) ON VIOLATION DROP ROW\n",
    "CONSTRAINT valid_quantity EXPECT (quantity IS NOT NULL AND quantity <> 0) ON VIOLATION DROP ROW\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Gold Layer - Business Ready\n",
    "\n",
    "**Purpose:** Aggregated data ready for Business Intelligence.\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **Star Schema** - fact tables + dimensions\n",
    "- **Pre-calculated aggregates** - daily, monthly, yearly\n",
    "- **Domain-specific views** - Marketing, Finance, Operations\n",
    "- **Performance optimized** - indexes, partitioning\n",
    "\n",
    "**Star Schema Example:**\n",
    "```\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚  dim_date   â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚dim_customer â”‚â”€â”€â”€â”€â”‚  fact_sales   â”‚â”€â”€â”€â”€â”‚ dim_product â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â”‚\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚                         â”‚\n",
    "       â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "       â”‚  dim_store  â”‚         â”‚dim_payment_methodâ”‚\n",
    "       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ETL vs ELT - Paradigm Shift\n",
    "\n",
    "**Traditional ETL (Extract-Transform-Load):**\n",
    "```\n",
    "Source â†’ Transform (ETL Server) â†’ Data Warehouse\n",
    "         âš ï¸  Expensive compute\n",
    "         âš ï¸  Limited scalability\n",
    "```\n",
    "\n",
    "**Modern ELT (Extract-Load-Transform):**\n",
    "```\n",
    "Source â†’ Load (Data Lake) â†’ Transform (Spark/DBR)\n",
    "         âœ… Cheap storage\n",
    "         âœ… Unlimited scalability\n",
    "         âœ… Raw data preserved\n",
    "```\n",
    "\n",
    "**Medallion = ELT Pattern:**\n",
    "```\n",
    "Source â†’ Bronze (Load) â†’ Silver (Transform) â†’ Gold (Aggregate)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Production Benefits\n",
    "\n",
    "**1. Data Recovery:**\n",
    "```python\n",
    "# Full pipeline can be reprocessed from Bronze\n",
    "bronze_data = spark.table(\"bronze.orders_raw\")\n",
    "# Re-run: Bronze â†’ Silver â†’ Gold\n",
    "```\n",
    "\n",
    "**2. Schema Evolution:**\n",
    "```python\n",
    "# New columns in source don't break pipeline\n",
    "# They go to _rescued_data column\n",
    "```\n",
    "\n",
    "**3. Compliance & Audit:**\n",
    "```python\n",
    "# Full history: who, what, when loaded\n",
    "# Retention: 3-7 years (legal regulations)\n",
    "```\n",
    "\n",
    "**4. Performance Isolation:**\n",
    "```python\n",
    "# Bronze: Write-optimized (streaming ingestion)\n",
    "# Silver: Balanced (MERGE operations)\n",
    "# Gold: Read-optimized (BI queries)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Lakeflow vs Notebook - Comparison\n",
    "\n",
    "| Aspect | Lakeflow (SQL) | Notebook (PySpark) |\n",
    "|--------|----------------|-------------------|\n",
    "| **Processing** | Streaming (real-time) | Batch (scheduled) |\n",
    "| **Syntax** | SQL DDL | Python/PySpark |\n",
    "| **SCD Type 2** | AUTO CDC | Manual MERGE |\n",
    "| **Constraints** | EXPECT + ON VIOLATION | .filter() |\n",
    "| **Orchestration** | Pipelines (auto) | Jobs (manual) |\n",
    "| **Use Case** | Production | Development/POC |\n",
    "\n",
    "**Important:** Both approaches create an **identical data model**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70768a08-3efd-458d-b186-e11c7a6372f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Per-User Isolation\n",
    "\n",
    "Run the initialization script for per-user catalog and schema isolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2beb7c6c-feee-4484-8a79-ce93f91916b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9434034c-106e-4069-b478-88abd48f1594",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuration\n",
    "\n",
    "Library imports and environment variable setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "523e2bef-ff77-470c-a1ee-0038087a9919",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# Display user context\n",
    "display({\n",
    "    \"Catalog\": CATALOG,\n",
    "    \"Schema Bronze\": BRONZE_SCHEMA,\n",
    "    \"Schema Silver\": SILVER_SCHEMA,\n",
    "    \"Schema Gold\": GOLD_SCHEMA,\n",
    "    \"User\": raw_user,\n",
    "    \"Dataset base\": DATASET_BASE_PATH\n",
    "})\n",
    "\n",
    "# Set catalog and schema as defaults\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08e0638d-70a5-4dff-a648-0f4135573287",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### User Context\n",
    "\n",
    "Displaying current environment configuration and data paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecef0f98-c5ff-4cb0-b805-a98e5ff1ef5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"USE CATALOG {CATALOG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebe1d96a-1763-46d2-b5d9-21012675e073",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Unity Catalog Configuration:**\n",
    "\n",
    "Setting the default catalog for all operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0552d1fb-1052-45c6-bb26-944b565508d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 1: Bronze Layer - Raw Data Landing\n",
    "\n",
    "**Section Objective:** Understand the role of Bronze layer as a landing zone for raw data.\n",
    "\n",
    "### Bronze Layer - Key Characteristics\n",
    "\n",
    "**1. Raw Data \"As-Is\"**\n",
    "- Data saved without value transformations\n",
    "- Original format preserved\n",
    "- Multi-format support (JSON, CSV, Parquet)\n",
    "\n",
    "**2. Append-Only Pattern**\n",
    "- Never delete/modify data\n",
    "- Immutable history\n",
    "- Time-travel capability\n",
    "\n",
    "**3. Audit Metadata**\n",
    "```python\n",
    "# Audit metadata in Bronze (Lakeflow compliant)\n",
    "source_file_path     # Source file path\n",
    "ingestion_ts         # File processing timestamp\n",
    "load_ts              # Load to Bronze timestamp\n",
    "```\n",
    "\n",
    "**4. Schema-on-Read**\n",
    "- Flexible schema (can change)\n",
    "- Rescued data column for unknown columns\n",
    "- Reprocessing capability\n",
    "\n",
    "### Bronze Tables - Data Model (Lakeflow compliant)\n",
    "\n",
    "**`bronze_customers`** (CSV):\n",
    "- `customer_id`, `first_name`, `last_name`, `email`, `phone`\n",
    "- `city`, `state`, `country`, `registration_date`, `customer_segment`\n",
    "- Metadata: `source_file_path`, `ingestion_ts`, `load_ts`\n",
    "\n",
    "**`bronze_orders`** (JSON):\n",
    "- `order_id`, `customer_id`, `product_id`, `store_id`\n",
    "- `order_datetime`, `quantity`, `unit_price`, `discount_percent`, `total_amount`\n",
    "- `payment_method`, `source_system`\n",
    "- Metadata: `source_file_path`, `ingestion_ts`, `load_ts`\n",
    "\n",
    "**`bronze_products`** (Parquet):\n",
    "- `product_id`, `product_name`, `subcategory_code`, `brand`\n",
    "- `unit_cost`, `list_price`, `weight_kg`, `status`\n",
    "- Metadata: `source_file_path`, `ingestion_ts`, `load_ts`\n",
    "\n",
    "### Why is Bronze Important?\n",
    "\n",
    "**1. Data Recovery**\n",
    "```python\n",
    "# We can reprocess pipeline from Bronze\n",
    "bronze_data = spark.table(\"bronze.orders_raw\")\n",
    "# Re-run transformations â†’ Silver â†’ Gold\n",
    "```\n",
    "\n",
    "**2. Schema Evolution**\n",
    "```python\n",
    "# New columns in source don't break pipeline\n",
    "# They go to _rescued_data\n",
    "```\n",
    "\n",
    "**3. Compliance & Audit**\n",
    "```python\n",
    "# Full history: who, what, when loaded\n",
    "# Retention: 3-7 years (legal regulations)\n",
    "```\n",
    "\n",
    "**4. Data Science Exploration**\n",
    "```python\n",
    "# Analysts can explore raw data\n",
    "# Create new features from raw data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e23da394-d79a-4736-850e-ab58bc53ec44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 1.1: Bronze Layer Inspection\n",
    "\n",
    "**Objective:** Check data in Bronze layer and understand its structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4b1181c-3562-4a7d-b195-47e50d4db1c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 1.1: Ingestion to Bronze Layer (Raw Data)\n",
    "\n",
    "We load data directly from source files (CSV, JSON, Parquet) into Bronze tables.\n",
    "This makes the notebook independent from previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20f099bb-cb2f-4f54-9a2b-0952d6669819",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Load Customers (CSV) - Lakeflow model compliant\n",
    "customers_path = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "\n",
    "customers_df = (spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(customers_path)\n",
    "    # Add metadata columns (as in Lakeflow)\n",
    "    .withColumn(\"source_file_path\", F.input_file_name())\n",
    "    .withColumn(\"ingestion_ts\", F.current_timestamp())\n",
    "    .withColumn(\"load_ts\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "display(customers_df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2decbd30-ab2f-430d-95a1-16f6daafa3e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Load Products (Parquet) - Lakeflow model compliant\n",
    "products_path = f\"{DATASET_BASE_PATH}/products/products.parquet\"\n",
    "\n",
    "products_df = (spark.read\n",
    "    .format(\"parquet\")\n",
    "    .load(products_path)\n",
    "    # Add metadata columns (as in Lakeflow)\n",
    "    .withColumn(\"source_file_path\", F.input_file_name())\n",
    "    .withColumn(\"ingestion_ts\", F.current_timestamp())\n",
    "    .withColumn(\"load_ts\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "# Write to Bronze\n",
    "(products_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(f\"{BRONZE_SCHEMA}.products_raw\")\n",
    ")\n",
    "\n",
    "display({\"status\": f\"âœ… Table {BRONZE_SCHEMA}.products_raw created/overwritten\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d988c36-1e83-42e1-9272-6ff06c699b46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Load Orders (JSON) - Lakeflow model compliant\n",
    "orders_path = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "\n",
    "orders_df = (spark.read\n",
    "    .format(\"json\")\n",
    "    .load(orders_path)\n",
    "    # Add metadata columns (as in Lakeflow)\n",
    "    .withColumn(\"source_system\", F.lit(\"batch\"))\n",
    "    .withColumn(\"source_file_path\", F.input_file_name())\n",
    "    .withColumn(\"ingestion_ts\", F.current_timestamp())\n",
    "    .withColumn(\"load_ts\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "# Write to Bronze\n",
    "(orders_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(f\"{BRONZE_SCHEMA}.orders_raw\")\n",
    ")\n",
    "\n",
    "display({\"status\": f\"âœ… Table {BRONZE_SCHEMA}.orders_raw created/overwritten\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82dff817-d83f-423a-b45b-e8f686721b41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write Customers to Bronze\n",
    "(customers_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(f\"{BRONZE_SCHEMA}.customers_raw\")\n",
    ")\n",
    "\n",
    "display({\"status\": f\"âœ… Table {BRONZE_SCHEMA}.customers_raw created/overwritten\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "590ad9a4-728b-4688-ade7-54b19a80dd38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Created Bronze tables:**\n",
    "\n",
    "Conversion of batch data to raw tables completed successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf979d26-91e7-44e4-b426-b573656a2b60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inspekcja Bronze Layer\n",
    "bronze_tables = [\"customers_raw\", \"orders_raw\", \"products_raw\"]\n",
    "results = []\n",
    "\n",
    "for table in bronze_tables:\n",
    "    full_table = f\"{CATALOG}.{BRONZE_SCHEMA}.{table}\"\n",
    "    \n",
    "    if spark.catalog.tableExists(full_table):\n",
    "        df = spark.table(full_table)\n",
    "        results.append({\n",
    "            \"table\": table,\n",
    "            \"status\": \"âœ…\",\n",
    "            \"records\": df.count(),\n",
    "            \"columns\": len(df.columns)\n",
    "        })\n",
    "\n",
    "display(spark.createDataFrame(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ca8aba4-c589-4bb8-b2fc-ea9034ded662",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Bronze Layer Inspection Summary:**\n",
    "\n",
    "Bronze Layer contains RAW data without transformations, maintains full history (append-only), and serves as the foundation for further transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1b7d116-87c4-44f0-9790-54bc9ef7417f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 2: Silver Layer - Cleansing & Validation\n",
    "\n",
    "**Section Objective:** Transform data from Bronze to Silver applying data quality rules.\n",
    "\n",
    "### Silver Layer - Data Model (Lakeflow compliant)\n",
    "\n",
    "**`silver_orders`** - Cleaned and calculated measures:\n",
    "- Keys: `order_id`, `customer_id`, `product_id`, `store_id`\n",
    "- Dates: `order_ts`, `order_date`, `order_date_key` (INT: yyyyMMdd)\n",
    "- Original measures: `quantity`, `unit_price`, `discount_percent`\n",
    "- **Calculated measures:**\n",
    "  - `gross_amount = quantity * unit_price`\n",
    "  - `discount_amount = quantity * unit_price * discount_percent / 100`\n",
    "  - `net_amount = gross_amount - discount_amount`\n",
    "- `payment_method_code` (standardized)\n",
    "- **Quality flags:**\n",
    "  - `is_return` (quantity < 0 OR total_amount < 0)\n",
    "  - `is_future_dated` (order_date > current_date)\n",
    "  - `is_unknown_customer`, `is_unknown_product`\n",
    "\n",
    "**`silver_customers`** (SCD Type 2):\n",
    "- Fields: `customer_id`, `first_name`, `last_name`, `email`, `phone`, `city`, `state`, `country`\n",
    "- SCD2 columns: `__START_AT`, `__END_AT` (or `effective_from`, `effective_to`)\n",
    "\n",
    "**`silver_products`**:\n",
    "- `product_id`, `product_name`, `subcategory_code`, `brand`\n",
    "- `unit_cost`, `list_price`, `weight_kg`, `status`\n",
    "- `is_active`, `is_unknown`\n",
    "\n",
    "### Constraints (as in Lakeflow)\n",
    "\n",
    "```sql\n",
    "CONSTRAINT valid_order_id EXPECT (order_id IS NOT NULL) ON VIOLATION DROP ROW\n",
    "CONSTRAINT valid_customer EXPECT (customer_id IS NOT NULL) ON VIOLATION DROP ROW\n",
    "CONSTRAINT valid_product EXPECT (product_id IS NOT NULL) ON VIOLATION DROP ROW\n",
    "CONSTRAINT valid_quantity EXPECT (quantity IS NOT NULL AND quantity <> 0) ON VIOLATION DROP ROW\n",
    "CONSTRAINT valid_unit_price EXPECT (unit_price IS NOT NULL AND unit_price >= 0) ON VIOLATION DROP ROW\n",
    "```\n",
    "\n",
    "### MERGE Operation - Deduplication Pattern\n",
    "\n",
    "**Problem:** Bronze contains duplicates (append-only)\n",
    "\n",
    "**Solution:** MERGE in Silver (upsert)\n",
    "\n",
    "```sql\n",
    "MERGE INTO silver.orders_clean AS target\n",
    "USING (\n",
    "    SELECT DISTINCT *\n",
    "    FROM bronze.orders_raw\n",
    "    WHERE ingestion_ts > (\n",
    "        SELECT MAX(load_ts) FROM silver.orders_clean\n",
    "    )\n",
    ") AS source\n",
    "ON target.order_id = source.order_id\n",
    "WHEN MATCHED THEN UPDATE SET *\n",
    "WHEN NOT MATCHED THEN INSERT *\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9464d305-e7d7-4cc8-8553-d31e08fbc43f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 2.1: Bronze â†’ Silver Transformation (Orders)\n",
    "\n",
    "**Objective:** Transform orders from Bronze to Silver with cleansing and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3788a6a3-beac-48bc-8bae-5c536d87c206",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Bronze â†’ Silver: Orders (Lakeflow model compliant)\n",
    "bronze_orders = spark.table(f\"{BRONZE_SCHEMA}.orders_raw\")\n",
    "\n",
    "# Transform & Validate - calculations as in Lakeflow silver_orders\n",
    "silver_orders = (bronze_orders\n",
    "    # Keys\n",
    "    .withColumn(\"order_id\", F.col(\"order_id\"))\n",
    "    .withColumn(\"customer_id\", F.col(\"customer_id\"))\n",
    "    .withColumn(\"product_id\", F.col(\"product_id\"))\n",
    "    .withColumn(\"store_id\", F.col(\"store_id\"))\n",
    "    \n",
    "    # Dates\n",
    "    .withColumn(\"order_ts\", F.to_timestamp(F.col(\"order_datetime\")))\n",
    "    .withColumn(\"order_date\", F.to_date(F.col(\"order_datetime\")))\n",
    "    .withColumn(\"order_date_key\", F.date_format(F.col(\"order_datetime\"), \"yyyyMMdd\").cast(\"int\"))\n",
    "    \n",
    "    # Original measures\n",
    "    .withColumn(\"quantity\", F.col(\"quantity\").cast(\"int\"))\n",
    "    .withColumn(\"unit_price\", F.col(\"unit_price\").cast(\"double\"))\n",
    "    .withColumn(\"discount_percent\", F.col(\"discount_percent\").cast(\"int\"))\n",
    "    \n",
    "    # Calculated measures (Lakeflow compliant)\n",
    "    .withColumn(\"gross_amount\", F.col(\"quantity\") * F.col(\"unit_price\"))\n",
    "    .withColumn(\"discount_amount\", F.col(\"quantity\") * F.col(\"unit_price\") * F.col(\"discount_percent\") / 100.0)\n",
    "    .withColumn(\"net_amount\", \n",
    "        F.col(\"gross_amount\") - F.col(\"discount_amount\")\n",
    "    )\n",
    "    \n",
    "    # Payment method (standardized)\n",
    "    .withColumn(\"payment_method_code\", F.coalesce(F.col(\"payment_method\"), F.lit(\"Unknown\")))\n",
    "    .withColumn(\"source_system\", F.col(\"source_system\"))\n",
    "    \n",
    "    # Quality flags (Lakeflow compliant)\n",
    "    .withColumn(\"is_return\", \n",
    "        F.when((F.col(\"quantity\") < 0) | (F.col(\"total_amount\") < 0), 1).otherwise(0)\n",
    "    )\n",
    "    .withColumn(\"is_future_dated\",\n",
    "        F.when(F.col(\"order_date\") > F.current_date(), 1).otherwise(0)\n",
    "    )\n",
    "    .withColumn(\"is_unknown_customer\",\n",
    "        F.when(F.col(\"customer_id\").isNull() | (F.col(\"customer_id\") == \"CUST999999\"), 1).otherwise(0)\n",
    "    )\n",
    "    .withColumn(\"is_unknown_product\",\n",
    "        F.when(F.col(\"product_id\").isNull() | (F.col(\"product_id\") == \"PROD999999\"), 1).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    # Validation - DROP invalid rows (as CONSTRAINT in Lakeflow)\n",
    "    .filter(F.col(\"order_id\").isNotNull())\n",
    "    .filter(F.col(\"customer_id\").isNotNull())\n",
    "    .filter(F.col(\"product_id\").isNotNull())\n",
    "    .filter((F.col(\"quantity\").isNotNull()) & (F.col(\"quantity\") != 0))\n",
    "    .filter((F.col(\"unit_price\").isNotNull()) & (F.col(\"unit_price\") >= 0))\n",
    "    \n",
    "    # Select final columns\n",
    "    .select(\n",
    "        \"order_id\", \"customer_id\", \"product_id\", \"store_id\",\n",
    "        \"order_ts\", \"order_date\", \"order_date_key\",\n",
    "        \"quantity\", \"unit_price\", \"discount_percent\",\n",
    "        \"gross_amount\", \"discount_amount\", \"net_amount\",\n",
    "        \"payment_method_code\", \"source_system\",\n",
    "        \"is_return\", \"is_future_dated\", \"is_unknown_customer\", \"is_unknown_product\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Write to Silver\n",
    "silver_orders.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{SILVER_SCHEMA}.orders_clean\")\n",
    "\n",
    "# Stats\n",
    "bronze_count = bronze_orders.count()\n",
    "silver_count = silver_orders.count()\n",
    "dropped_count = bronze_count - silver_count\n",
    "\n",
    "display({\n",
    "    \"bronze_records\": bronze_count,\n",
    "    \"silver_records\": silver_count,\n",
    "    \"dropped_by_constraints\": dropped_count,\n",
    "    \"status\": f\"âœ… Created {SILVER_SCHEMA}.orders_clean\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88aea90d-66a3-40c0-89aa-38282ba4fd4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preview Silver Orders with new calculations\n",
    "display(\n",
    "    spark.table(f\"{SILVER_SCHEMA}.orders_clean\")\n",
    "    .select(\"order_id\", \"order_date\", \"quantity\", \"unit_price\", \"discount_percent\", \n",
    "            \"gross_amount\", \"discount_amount\", \"net_amount\", \"is_return\", \"is_future_dated\")\n",
    "    .limit(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "286c2aeb-daba-4beb-b522-17f82a05667b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Note about production:**\n",
    "\n",
    "In a production environment, we would use MERGE operation for deduplication instead of simple overwrite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "374501e7-a2e0-471b-944c-76dfe5085199",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 3: SCD (Slowly Changing Dimensions)\n",
    "\n",
    "**Section Objective:** Implementation of SCD Type 1 and Type 2 for tracking data changes.\n",
    "\n",
    "### SCD Model in Lakeflow\n",
    "\n",
    "In our Lakeflow pipeline, `silver_customers` uses **SCD Type 2** with columns:\n",
    "- `__START_AT` - timestamp of record validity start\n",
    "- `__END_AT` - timestamp of validity end (NULL = current record)\n",
    "\n",
    "```sql\n",
    "-- Lakeflow AUTO CDC for SCD2\n",
    "CREATE FLOW silver_customers_scd2_flow\n",
    "AS AUTO CDC INTO silver_customers\n",
    "FROM STREAM bronze_customers\n",
    "KEYS (customer_id)\n",
    "SEQUENCE BY ingestion_ts\n",
    "STORED AS SCD TYPE 2;\n",
    "```\n",
    "\n",
    "### SCD Types - Overview\n",
    "\n",
    "| Type | Strategy | History | Use Case |\n",
    "|------|----------|---------|----------|\n",
    "| **Type 0** | No changes allowed | N/A | Reference data (countries) |\n",
    "| **Type 1** | Overwrite | âŒ No | Current state only |\n",
    "| **Type 2** | Add new row | âœ… Yes | Full history tracking |\n",
    "| **Type 3** | Add new column | âš ï¸ Limited | Previous value only |\n",
    "\n",
    "---\n",
    "\n",
    "### SCD Type 1 - Overwrite\n",
    "\n",
    "**Strategy:** Overwrite old value with new (no history)\n",
    "\n",
    "**Implementation:** Simple UPDATE/MERGE\n",
    "\n",
    "**Pros:**\n",
    "- âœ… Simple implementation\n",
    "- âœ… No history bloat\n",
    "- âœ… Always current values\n",
    "\n",
    "**Cons:**\n",
    "- âŒ No historical tracking\n",
    "- âŒ Can't analyze \"as of date\"\n",
    "- âŒ Lose audit trail\n",
    "\n",
    "**Use Cases:**\n",
    "- Correcting data entry errors\n",
    "- Non-critical attributes (e.g., marketing preferences)\n",
    "\n",
    "---\n",
    "\n",
    "### SCD Type 2 - Historical Tracking (used in Lakeflow)\n",
    "\n",
    "**Strategy:** Add new record for each change (full history)\n",
    "\n",
    "**SCD Type 2 Columns (Lakeflow format):**\n",
    "- `__START_AT`: Validity start timestamp\n",
    "- `__END_AT`: Validity end timestamp (NULL = current)\n",
    "\n",
    "**Query current records:**\n",
    "```sql\n",
    "SELECT * FROM silver_customers WHERE __END_AT IS NULL\n",
    "```\n",
    "\n",
    "**Query historical (as of date):**\n",
    "```sql\n",
    "SELECT * FROM silver_customers \n",
    "WHERE '2024-01-15' >= __START_AT \n",
    "  AND ('2024-01-15' < __END_AT OR __END_AT IS NULL)\n",
    "```\n",
    "\n",
    "**Use Cases:**\n",
    "- Customer dimensions (address, preferences)\n",
    "- Product dimensions (price history)\n",
    "- Compliance & audit requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9749366-4468-48ca-a120-2b9c5d49e2ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 3.1: SCD Type 1 - Customers (Overwrite)\n",
    "\n",
    "**Objective:** SCD Type 1 implementation - simple overwrite without history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1275366-9948-4eb1-b51f-c05361925ba8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SCD Type 1: Customers (Current State Only)\n",
    "bronze_customers = spark.table(f\"{BRONZE_SCHEMA}.customers_raw\")\n",
    "\n",
    "# Transform to Silver (SCD Type 1 - current state only)\n",
    "customers_type1 = (bronze_customers\n",
    "    .withColumn(\"customer_id\", F.col(\"customer_id\"))\n",
    "    .withColumn(\"first_name\", F.trim(F.col(\"first_name\")))\n",
    "    .withColumn(\"last_name\", F.trim(F.col(\"last_name\")))\n",
    "    .withColumn(\"email\", F.lower(F.trim(F.col(\"email\"))))\n",
    "    .withColumn(\"phone\", F.col(\"phone\"))\n",
    "    .withColumn(\"city\", F.initcap(F.trim(F.col(\"city\"))))\n",
    "    .withColumn(\"state\", F.upper(F.trim(F.col(\"state\"))))\n",
    "    .withColumn(\"country\", F.upper(F.trim(F.col(\"country\"))))\n",
    "    .withColumn(\"registration_date\", F.to_date(F.col(\"registration_date\")))\n",
    "    .withColumn(\"customer_segment\", F.col(\"customer_segment\"))\n",
    "    .withColumn(\"updated_at\", F.current_timestamp())\n",
    "    .select(\n",
    "        \"customer_id\", \"first_name\", \"last_name\", \"email\", \"phone\",\n",
    "        \"city\", \"state\", \"country\", \"registration_date\", \"customer_segment\",\n",
    "        \"updated_at\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create/Replace table (Type 1 = overwrite)\n",
    "customers_type1.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{SILVER_SCHEMA}.customers_type1\")\n",
    "\n",
    "display({\n",
    "    \"status\": f\"âœ… Created {SILVER_SCHEMA}.customers_type1\",\n",
    "    \"records\": customers_type1.count(),\n",
    "    \"note\": \"SCD Type 1: Always current state, no history\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f43331c-54d8-48eb-855c-25446cdfdcf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Created SCD Type 1 table:**\n",
    "\n",
    "Table `customers_type1` always contains current state without history. Displaying sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab4bdfe6-b1bd-445f-96a9-3552dde12a3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(f\"{SILVER_SCHEMA}.customers_type1\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dce3f1b-f4cd-4402-9425-28094e0aea14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### UPDATE Simulation - city change for customer_id=1\n",
    "\n",
    "**BEFORE CHANGE:** Displaying current state of customer_id=1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e219614-09c7-43ac-ae61-8538fa366665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT customer_id, first_name, last_name, city, state, country, customer_segment\n",
    "        FROM {SILVER_SCHEMA}.customers_type1 \n",
    "        WHERE customer_id = 'CUST000001'\n",
    "    \"\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe7b3c22-a848-4bf4-bcb8-cd54d7c0cc95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UPDATE Simulation - city change for customer_id\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "updates_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"phone\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"registration_date\", StringType(), True),\n",
    "    StructField(\"customer_segment\", StringType(), True)\n",
    "])\n",
    "\n",
    "updates_data = [(\"CUST000001\", \"Jesse\", \"Hoffman\", \"jesse.hoffman@interia.pl\", \"+48 694 026 542\", \n",
    "                 \"KrakÃ³w\", \"MA\", \"POL\", \"2025-10-16\", \"Premium\")]  # Changed city & country!\n",
    "updates_df = spark.createDataFrame(updates_data, updates_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7f52e56-b3e9-46fc-aff5-efc8a61d81da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SCD Type 1: MERGE (Overwrite)\n",
    "updates_df.createOrReplaceTempView(\"customer_updates\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    MERGE INTO {SILVER_SCHEMA}.customers_type1 AS target\n",
    "    USING customer_updates AS source\n",
    "    ON target.customer_id = source.customer_id\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "        target.city = source.city,\n",
    "        target.state = source.state,\n",
    "        target.country = source.country,\n",
    "        target.customer_segment = source.customer_segment,\n",
    "        target.updated_at = current_timestamp()\n",
    "    WHEN NOT MATCHED THEN INSERT (\n",
    "        customer_id, first_name, last_name, email, phone,\n",
    "        city, state, country, registration_date, customer_segment, updated_at\n",
    "    ) VALUES (\n",
    "        source.customer_id, source.first_name, source.last_name, source.email, source.phone,\n",
    "        source.city, source.state, source.country, CAST(source.registration_date AS DATE), \n",
    "        source.customer_segment, current_timestamp()\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e413e8a-cc48-4aea-8a75-714883b378c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**AFTER CHANGE (SCD Type 1 - overwrite):** Displaying updated state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bb04bf8-098d-4e8b-81ce-4937c7bbc285",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT customer_id, first_name, last_name, city, state, country, customer_segment\n",
    "        FROM {SILVER_SCHEMA}.customers_type1 \n",
    "        WHERE customer_id = 'CUST000001'\n",
    "    \"\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eca129b6-b66d-4697-ad9b-a5ee2ca9bdf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**âš ï¸  NOTE:** Change history has been LOST\n",
    "\n",
    "**ðŸ’¡ Old value (Warsaw) was overwritten (KrakÃ³w)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85dd1603-be26-43f4-a11c-a5b63916563b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 3.2: SCD Type 2 - Customers (Historical Tracking)\n",
    "\n",
    "**Objective:** SCD Type 2 implementation - full change history tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c946d8d2-f9a3-4c96-a02b-2a5e62db1c3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SCD Type 2: Customers (Historical Tracking - Lakeflow format)\n",
    "bronze_customers = spark.table(f\"{BRONZE_SCHEMA}.customers_raw\")\n",
    "\n",
    "# Transform with SCD Type 2 columns (Lakeflow format: __START_AT, __END_AT)\n",
    "customers_type2_initial = (bronze_customers\n",
    "    .withColumn(\"customer_id\", F.col(\"customer_id\"))\n",
    "    .withColumn(\"first_name\", F.trim(F.col(\"first_name\")))\n",
    "    .withColumn(\"last_name\", F.trim(F.col(\"last_name\")))\n",
    "    .withColumn(\"email\", F.lower(F.trim(F.col(\"email\"))))\n",
    "    .withColumn(\"phone\", F.col(\"phone\"))\n",
    "    .withColumn(\"city\", F.initcap(F.trim(F.col(\"city\"))))\n",
    "    .withColumn(\"state\", F.upper(F.trim(F.col(\"state\"))))\n",
    "    .withColumn(\"country\", F.upper(F.trim(F.col(\"country\"))))\n",
    "    .withColumn(\"registration_date\", F.to_date(F.col(\"registration_date\")))\n",
    "    .withColumn(\"customer_segment\", F.col(\"customer_segment\"))\n",
    "    # SCD Type 2 columns (Lakeflow format)\n",
    "    .withColumn(\"__START_AT\", F.current_timestamp())\n",
    "    .withColumn(\"__END_AT\", F.lit(None).cast(\"timestamp\"))  # NULL = current record\n",
    "    .select(\n",
    "        \"customer_id\", \"first_name\", \"last_name\", \"email\", \"phone\",\n",
    "        \"city\", \"state\", \"country\", \"registration_date\", \"customer_segment\",\n",
    "        \"__START_AT\", \"__END_AT\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create initial table\n",
    "customers_type2_initial.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{SILVER_SCHEMA}.customers_type2\")\n",
    "\n",
    "display({\n",
    "    \"status\": f\"âœ… Created {SILVER_SCHEMA}.customers_type2\",\n",
    "    \"records\": customers_type2_initial.count(),\n",
    "    \"columns\": \"__START_AT, __END_AT (Lakeflow format)\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "763e196a-932c-48fd-8939-1c0b6083d5ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Created SCD Type 2 table:**\n",
    "\n",
    "Table `customers_type2` contains columns: `__START_AT`, `__END_AT` (Lakeflow format). Displaying sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71425113-e3a6-48f3-8cbd-2920c28e480d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(f\"{SILVER_SCHEMA}.customers_type2\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2411a97-d595-462a-ade1-c9310856ae85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### CHANGE Simulation - city change for customer_id=1\n",
    "\n",
    "**BEFORE CHANGE:** Displaying current state of customer_id=1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bea30188-f58b-4dfd-95d3-c1b723c211cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT customer_id, first_name, last_name, city, country, __START_AT, __END_AT\n",
    "        FROM {SILVER_SCHEMA}.customers_type2 \n",
    "        WHERE customer_id = 'CUST000001'\n",
    "        ORDER BY __START_AT\n",
    "    \"\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "175771fa-be97-4a3c-92f3-03b49d931890",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UPDATE Simulation for SCD Type 2 - same update as for Type 1\n",
    "# We use the same customer_updates view (already created earlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7386067-a0be-41b2-830e-fafe2324f7ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 1: Close old records** (set __END_AT):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8433a6fc-75a5-4374-bee0-477ad3b18b87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SCD Type 2: Step 1 - Close old records (set __END_AT)\n",
    "spark.sql(f\"\"\"\n",
    "    MERGE INTO {SILVER_SCHEMA}.customers_type2 AS target\n",
    "    USING (\n",
    "        SELECT DISTINCT u.customer_id\n",
    "        FROM customer_updates u\n",
    "        INNER JOIN {SILVER_SCHEMA}.customers_type2 t\n",
    "            ON u.customer_id = t.customer_id\n",
    "        WHERE t.__END_AT IS NULL\n",
    "          AND (u.city != t.city OR u.country != t.country OR u.customer_segment != t.customer_segment)\n",
    "    ) AS changed\n",
    "    ON target.customer_id = changed.customer_id \n",
    "       AND target.__END_AT IS NULL\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "        target.__END_AT = current_timestamp()\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45a7fc4a-134f-4e68-8f79-02130d00d991",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 2: Insert new versions** - adding new records with updated values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d3190e0-2dfc-4d49-85e8-b4db59340413",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SCD Type 2: Step 2 - Insert new versions\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO {SILVER_SCHEMA}.customers_type2\n",
    "    SELECT \n",
    "        u.customer_id,\n",
    "        u.first_name,\n",
    "        u.last_name,\n",
    "        u.email,\n",
    "        u.phone,\n",
    "        u.city,\n",
    "        u.state,\n",
    "        u.country,\n",
    "        CAST(u.registration_date AS DATE),\n",
    "        u.customer_segment,\n",
    "        current_timestamp() AS __START_AT,\n",
    "        CAST(NULL AS TIMESTAMP) AS __END_AT\n",
    "    FROM customer_updates u\n",
    "    WHERE NOT EXISTS (\n",
    "        SELECT 1 FROM {SILVER_SCHEMA}.customers_type2 existing\n",
    "        WHERE existing.customer_id = u.customer_id\n",
    "          AND existing.__END_AT IS NULL\n",
    "          AND existing.city = u.city\n",
    "          AND existing.country = u.country\n",
    "          AND existing.customer_segment = u.customer_segment\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33209949-bc92-4a7e-bea1-b5fca1c388a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**AFTER CHANGE (SCD Type 2 - historical tracking):**\n",
    "\n",
    "History has been preserved! Displaying all versions of customer_id=1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41062031-ab11-4094-9f4c-bcca0d829853",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display change history for CUST000001\n",
    "display(\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            first_name,\n",
    "            city,\n",
    "            country,\n",
    "            customer_segment,\n",
    "            __START_AT,\n",
    "            __END_AT,\n",
    "            CASE WHEN __END_AT IS NULL THEN 'CURRENT' ELSE 'HISTORICAL' END AS status\n",
    "        FROM {SILVER_SCHEMA}.customers_type2 \n",
    "        WHERE customer_id = 'CUST000001'\n",
    "        ORDER BY __START_AT\n",
    "    \"\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "775408fa-c690-4bd5-9dec-1f732dab647d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**âœ… History preserved! (Lakeflow format)**\n",
    "\n",
    "We now have 2 records:\n",
    "- **Record 1**: New York, USA (__END_AT = closing timestamp)\n",
    "- **Record 2**: KrakÃ³w, POL (__END_AT = NULL = current record)\n",
    "\n",
    "In Lakeflow, the same effect is achieved automatically via `AUTO CDC ... STORED AS SCD TYPE 2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92e2db7b-c943-451c-92ef-f00ac04b0f2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example: Query 'as of date'\n",
    "\n",
    "**Where did the customer live 1 month ago?** SCD Type 2 enables temporal queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65a9b936-9a36-4c09-94a0-eb0d9a344761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "one_month_ago = (datetime.now() - timedelta(days=30)).date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fc0ede0-faa5-4e39-969d-960aa68d67bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Query \"as of date\" - Lakeflow format\n",
    "display(\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            first_name,\n",
    "            city,\n",
    "            country,\n",
    "            __START_AT,\n",
    "            __END_AT\n",
    "        FROM {SILVER_SCHEMA}.customers_type2\n",
    "        WHERE customer_id = 'CUST000001'\n",
    "          AND '{one_month_ago}' >= __START_AT \n",
    "          AND ('{one_month_ago}' < __END_AT OR __END_AT IS NULL)\n",
    "    \"\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cb23f29-f3a5-4c11-a47e-7a65b3ad44d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 4: Gold Layer - Star Schema (Lakeflow compliant)\n",
    "\n",
    "**Section Objective:** Implementation of Star Schema according to Lakeflow model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca9f9c7f-f53f-4d3b-813b-e6a3524b2974",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Gold Layer - Data Model (Lakeflow)\n",
    "\n",
    "**Fact Table:**\n",
    "- `fact_sales` - main fact table with transactional measures\n",
    "\n",
    "**Dimension Tables:**\n",
    "- `dim_customer` - customer dimension (snapshot from SCD2)\n",
    "- `dim_product` - product dimension\n",
    "- `dim_date` - time dimension\n",
    "- `dim_store` - store dimension\n",
    "- `dim_payment_method` - payment method dimension\n",
    "\n",
    "### Star Schema Diagram\n",
    "\n",
    "```\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚   dim_date      â”‚\n",
    "                    â”‚ date_key (PK)   â”‚\n",
    "                    â”‚ year, quarter   â”‚\n",
    "                    â”‚ month, day      â”‚\n",
    "                    â”‚ is_weekend      â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ dim_customer â”‚    â”‚   fact_sales    â”‚    â”‚ dim_product  â”‚\n",
    "â”‚ customer_id  â”‚â”€â”€â”€â”€â”‚ order_id (PK)   â”‚â”€â”€â”€â”€â”‚ product_id   â”‚\n",
    "â”‚ first_name   â”‚    â”‚ customer_id (FK)â”‚    â”‚ product_name â”‚\n",
    "â”‚ last_name    â”‚    â”‚ product_id (FK) â”‚    â”‚ unit_cost    â”‚\n",
    "â”‚ city, state  â”‚    â”‚ store_id (FK)   â”‚    â”‚ list_price   â”‚\n",
    "â”‚ segment      â”‚    â”‚ date_key (FK)   â”‚    â”‚ is_active    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ payment_method  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    â”‚ quantity        â”‚\n",
    "                    â”‚ unit_price      â”‚\n",
    "                    â”‚ gross_amount    â”‚\n",
    "                    â”‚ discount_amount â”‚\n",
    "                    â”‚ net_amount      â”‚\n",
    "                    â”‚ is_return       â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚                             â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚    dim_store      â”‚     â”‚ dim_payment_method    â”‚\n",
    "    â”‚ store_id          â”‚     â”‚ payment_method_code   â”‚\n",
    "    â”‚ store_code        â”‚     â”‚ payment_method_group  â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Example 4.1: dim_customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ca302cc-5126-4368-a0a9-fd63e8fc27c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Gold: dim_customer (snapshot from SCD2 - only current records)\n",
    "dim_customer = spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        customer_id,\n",
    "        first_name,\n",
    "        last_name,\n",
    "        email,\n",
    "        phone,\n",
    "        city,\n",
    "        state,\n",
    "        country,\n",
    "        registration_date,\n",
    "        customer_segment\n",
    "    FROM {SILVER_SCHEMA}.customers_type2\n",
    "    WHERE __END_AT IS NULL\n",
    "\"\"\")\n",
    "\n",
    "dim_customer.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{GOLD_SCHEMA}.dim_customer\")\n",
    "\n",
    "display({\n",
    "    \"table\": f\"{GOLD_SCHEMA}.dim_customer\",\n",
    "    \"records\": dim_customer.count(),\n",
    "    \"status\": \"âœ… Created\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38d9ee62-82a9-4b71-b387-9424d8ba700d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Created dim_customer table:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5384c3bf-8d4c-4328-af1a-75d91194df3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(f\"{GOLD_SCHEMA}.dim_customer\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "478180ff-d898-45ec-97cc-2c02cbc6be39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 4.2: dim_product (Lakeflow compliant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d15c3c20-39f3-408d-bc21-11a9898e0fe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Created dim_product table:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3ec6158-c3a1-4e06-a0b6-6624bcafc3f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Gold: dim_product (Lakeflow silver_products compliant)\n",
    "from pyspark.sql.types import (\n",
    "    StringType, DoubleType, IntegerType, StructType, StructField\n",
    ")\n",
    "\n",
    "bronze_products = spark.table(f\"{BRONZE_SCHEMA}.products_raw\")\n",
    "\n",
    "dim_product = (bronze_products\n",
    "    .withColumn(\"unit_cost\", F.col(\"unit_cost\").cast(\"double\"))\n",
    "    .withColumn(\"list_price\", F.col(\"list_price\").cast(\"double\"))\n",
    "    .withColumn(\"weight_kg\", F.col(\"weight_kg\").cast(\"double\"))\n",
    "    .withColumn(\"is_active\", \n",
    "        F.when(F.upper(F.col(\"status\")).isin(\"ACTIVE\", \"AVAILABLE\"), 1).otherwise(0)\n",
    "    )\n",
    "    .withColumn(\"is_unknown\", F.lit(0))\n",
    "    .select(\n",
    "        \"product_id\", \"product_name\", \"subcategory_code\", \"brand\",\n",
    "        \"unit_cost\", \"list_price\", \"weight_kg\", \"status\",\n",
    "        \"is_active\", \"is_unknown\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "unknown_schema = StructType([\n",
    "    StructField(\"product_id\", StringType(), False),\n",
    "    StructField(\"product_name\", StringType(), False),\n",
    "    StructField(\"subcategory_code\", StringType(), False),\n",
    "    StructField(\"brand\", StringType(), False),\n",
    "    StructField(\"unit_cost\", DoubleType(), True),\n",
    "    StructField(\"list_price\", DoubleType(), True),\n",
    "    StructField(\"weight_kg\", DoubleType(), True),\n",
    "    StructField(\"status\", StringType(), False),\n",
    "    StructField(\"is_active\", IntegerType(), False),\n",
    "    StructField(\"is_unknown\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "unknown_product = spark.createDataFrame([\n",
    "    (\"UNKNOWN\", \"Unknown product\", \"UNKNOWN\", \"Unknown\", None, None, None, \"Unknown\", 0, 1)\n",
    "], schema=unknown_schema)\n",
    "\n",
    "dim_product_final = dim_product.unionByName(unknown_product)\n",
    "\n",
    "dim_product_final.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n",
    "    f\"{GOLD_SCHEMA}.dim_product\"\n",
    ")\n",
    "\n",
    "display({\n",
    "    \"table\": f\"{GOLD_SCHEMA}.dim_product\",\n",
    "    \"records\": dim_product_final.count(),\n",
    "    \"status\": \"âœ… Created (includes UNKNOWN product)\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aa72ab7-4a44-4fc8-9440-6cad937d403b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(f\"{GOLD_SCHEMA}.dim_product\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b567a9e-ec08-4042-9973-28c9117a90ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 4.3: dim_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dd5eebb-f1b6-47bc-b6a5-387f3c50171f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Created dim_date table:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "305a8aa4-8c58-4290-9595-724ac93b1622",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Gold: dim_date (generated from silver_orders as in Lakeflow)\n",
    "silver_orders = spark.table(f\"{SILVER_SCHEMA}.orders_clean\")\n",
    "\n",
    "dim_date = (silver_orders\n",
    "    .select(\"order_date\")\n",
    "    .filter(F.col(\"order_date\").isNotNull())  # Filter NULL dates\n",
    "    .distinct()\n",
    "    .withColumn(\"date_key\", F.date_format(F.col(\"order_date\"), \"yyyyMMdd\").cast(\"int\"))\n",
    "    .withColumn(\"date\", F.col(\"order_date\"))\n",
    "    .withColumn(\"year\", F.year(F.col(\"order_date\")))\n",
    "    .withColumn(\"quarter\", F.quarter(F.col(\"order_date\")))\n",
    "    .withColumn(\"month\", F.month(F.col(\"order_date\")))\n",
    "    .withColumn(\"day\", F.dayofmonth(F.col(\"order_date\")))\n",
    "    .withColumn(\"day_of_week\", F.date_format(F.col(\"order_date\"), \"E\"))\n",
    "    .withColumn(\"is_weekend\", \n",
    "        F.when(F.date_format(F.col(\"order_date\"), \"E\").isin(\"Sat\", \"Sun\"), 1).otherwise(0)\n",
    "    )\n",
    "    .select(\"date_key\", \"date\", \"year\", \"quarter\", \"month\", \"day\", \"day_of_week\", \"is_weekend\")\n",
    "    .orderBy(\"date_key\")\n",
    ")\n",
    "\n",
    "dim_date.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{GOLD_SCHEMA}.dim_date\")\n",
    "\n",
    "display({\n",
    "    \"table\": f\"{GOLD_SCHEMA}.dim_date\",\n",
    "    \"records\": dim_date.count(),\n",
    "    \"status\": \"âœ… Created\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "179154ab-a6c4-4d1a-92a6-08068f7007ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(f\"{GOLD_SCHEMA}.dim_date\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff36e45c-da96-47e4-a204-b52b08f0d6fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 4.5: dim_store and dim_payment_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cf44a8e-f0e1-4d32-be48-0e478c400076",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Gold: dim_store (generated from silver_orders as in Lakeflow)\n",
    "dim_store = (silver_orders\n",
    "    .select(\"store_id\")\n",
    "    .distinct()\n",
    "    .withColumn(\"store_code\", F.col(\"store_id\"))\n",
    ")\n",
    "\n",
    "dim_store.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{GOLD_SCHEMA}.dim_store\")\n",
    "\n",
    "# Gold: dim_payment_method (generated from silver_orders as in Lakeflow)\n",
    "dim_payment_method = (silver_orders\n",
    "    .select(\"payment_method_code\")\n",
    "    .distinct()\n",
    "    .withColumn(\"payment_method_group\",\n",
    "        F.when(F.col(\"payment_method_code\").isin(\"Credit Card\", \"Debit Card\"), \"Card\")\n",
    "        .when(F.col(\"payment_method_code\") == \"Cash\", \"Cash\")\n",
    "        .when(F.col(\"payment_method_code\") == \"PayPal\", \"Digital wallet\")\n",
    "        .otherwise(\"Other\")\n",
    "    )\n",
    ")\n",
    "\n",
    "dim_payment_method.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{GOLD_SCHEMA}.dim_payment_method\")\n",
    "\n",
    "display({\n",
    "    \"dim_store\": {\"records\": dim_store.count(), \"status\": \"âœ…\"},\n",
    "    \"dim_payment_method\": {\"records\": dim_payment_method.count(), \"status\": \"âœ…\"}\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3332698-6e0c-4850-aa6e-40fc12de8194",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 4.6: fact_sales (fact table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "729404a5-e723-45ef-86bd-496bc6c61842",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Gold: fact_sales (main fact table - Lakeflow compliant)\n",
    "silver_orders = spark.table(f\"{SILVER_SCHEMA}.orders_clean\")\n",
    "\n",
    "fact_sales = (silver_orders\n",
    "    .select(\n",
    "        # Dimension keys\n",
    "        \"order_id\",\n",
    "        \"store_id\",\n",
    "        F.coalesce(F.col(\"customer_id\"), F.lit(\"UNKNOWN\")).alias(\"customer_id\"),\n",
    "        F.coalesce(F.col(\"product_id\"), F.lit(\"UNKNOWN\")).alias(\"product_id\"),\n",
    "        \"payment_method_code\",\n",
    "        \"order_date_key\",\n",
    "        \"order_ts\",\n",
    "        \n",
    "        # Measures\n",
    "        \"quantity\",\n",
    "        \"unit_price\",\n",
    "        \"discount_percent\",\n",
    "        \"gross_amount\",\n",
    "        \"discount_amount\",\n",
    "        \"net_amount\",\n",
    "        \n",
    "        # Flags\n",
    "        \"is_return\",\n",
    "        \"is_future_dated\",\n",
    "        \"is_unknown_customer\",\n",
    "        \"is_unknown_product\",\n",
    "        \n",
    "        # Lineage\n",
    "        \"source_system\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fact_sales.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{GOLD_SCHEMA}.fact_sales\")\n",
    "\n",
    "display({\n",
    "    \"table\": f\"{GOLD_SCHEMA}.fact_sales\",\n",
    "    \"records\": fact_sales.count(),\n",
    "    \"status\": \"âœ… Created\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cebe494-ec36-4d91-aedc-89f3627f0e6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Preview fact_sales with dimension joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "523621ea-175a-40a3-90ee-006a4f736d41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Query Star Schema - sample sales report\n",
    "display(\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            d.date,\n",
    "            d.day_of_week,\n",
    "            c.first_name || ' ' || c.last_name AS customer_name,\n",
    "            c.customer_segment,\n",
    "            p.product_name,\n",
    "            p.brand,\n",
    "            pm.payment_method_group,\n",
    "            f.quantity,\n",
    "            f.gross_amount,\n",
    "            f.discount_amount,\n",
    "            f.net_amount\n",
    "        FROM {GOLD_SCHEMA}.fact_sales f\n",
    "        JOIN {GOLD_SCHEMA}.dim_date d ON f.order_date_key = d.date_key\n",
    "        JOIN {GOLD_SCHEMA}.dim_customer c ON f.customer_id = c.customer_id\n",
    "        JOIN {GOLD_SCHEMA}.dim_product p ON f.product_id = p.product_id\n",
    "        JOIN {GOLD_SCHEMA}.dim_payment_method pm ON f.payment_method_code = pm.payment_method_code\n",
    "        WHERE f.is_return = 0 AND f.is_future_dated = 0\n",
    "        ORDER BY f.net_amount DESC\n",
    "        LIMIT 10\n",
    "    \"\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "039dde8f-ea44-43e0-97bd-f628e4969004",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 5: Summary & Best Practices\n",
    "\n",
    "### What was achieved?\n",
    "\n",
    "âœ… **1. Medallion Architecture Implementation (Lakeflow compliant)**\n",
    "\n",
    "**ðŸ¥‰ Bronze Layer:**\n",
    "- `bronze.customers_raw` - CSV with metadata (source_file_path, ingestion_ts, load_ts)\n",
    "- `bronze.orders_raw` - JSON with source_system (batch/stream)\n",
    "- `bronze.products_raw` - Parquet\n",
    "\n",
    "**ðŸ¥ˆ Silver Layer:**\n",
    "- `silver.orders_clean` - with calculations (gross_amount, discount_amount, net_amount)\n",
    "- `silver.orders_clean` - with flags (is_return, is_future_dated, is_unknown_*)\n",
    "- `silver.customers_type1` - SCD Type 1 (current state)\n",
    "- `silver.customers_type2` - SCD Type 2 (__START_AT, __END_AT)\n",
    "\n",
    "**ðŸ¥‡ Gold Layer (Star Schema):**\n",
    "- `fact_sales` - fact table with measures and flags\n",
    "- `dim_customer` - snapshot from SCD2\n",
    "- `dim_product` - with is_active, is_unknown\n",
    "- `dim_date` - date_key, year, quarter, month, is_weekend\n",
    "- `dim_store` - store_id, store_code\n",
    "- `dim_payment_method` - payment_method_group (Card/Cash/Digital wallet)\n",
    "\n",
    "âœ… **2. SCD (Slowly Changing Dimensions)**\n",
    "- **Type 1**: Overwrite (customers_type1)\n",
    "- **Type 2**: Historical tracking with __START_AT/__END_AT (customers_type2, Lakeflow format)\n",
    "\n",
    "âœ… **3. Data Quality (Lakeflow CONSTRAINTS compliant)**\n",
    "- valid_order_id: DROP ROW if NULL\n",
    "- valid_customer: DROP ROW if NULL\n",
    "- valid_product: DROP ROW if NULL\n",
    "- valid_quantity: DROP ROW if NULL or 0\n",
    "- valid_unit_price: DROP ROW if NULL or < 0\n",
    "\n",
    "### Data Model - Comparison with Lakeflow\n",
    "\n",
    "| Lakeflow (Streaming) | Notebook (Batch) | Status |\n",
    "|---------------------|------------------|--------|\n",
    "| `bronze_customers` (STREAMING TABLE) | `bronze.customers_raw` | âœ… |\n",
    "| `bronze_orders` (STREAMING TABLE + FLOW) | `bronze.orders_raw` | âœ… |\n",
    "| `bronze_products` (MATERIALIZED VIEW) | `bronze.products_raw` | âœ… |\n",
    "| `silver_customers` (SCD2 AUTO CDC) | `silver.customers_type2` | âœ… |\n",
    "| `silver_orders` (CONSTRAINTS) | `silver.orders_clean` | âœ… |\n",
    "| `fact_sales` (STREAMING TABLE) | `gold.fact_sales` | âœ… |\n",
    "| `dim_customer` (MV) | `gold.dim_customer` | âœ… |\n",
    "| `dim_product` (MV) | `gold.dim_product` | âœ… |\n",
    "| `dim_date` (MV) | `gold.dim_date` | âœ… |\n",
    "| `dim_store` (MV) | `gold.dim_store` | âœ… |\n",
    "| `dim_payment_method` (MV) | `gold.dim_payment_method` | âœ… |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "ðŸ’¡ **1. Lakeflow Streaming vs Batch**\n",
    "```\n",
    "Lakeflow: CREATE OR REFRESH STREAMING TABLE + FLOW\n",
    "Notebook: spark.read â†’ transform â†’ write (batch)\n",
    "\n",
    "Both methods create an identical data model!\n",
    "```\n",
    "\n",
    "ðŸ’¡ **2. Calculations in Silver (not Bronze)**\n",
    "```python\n",
    "# Silver orders - calculated measures\n",
    "gross_amount = quantity * unit_price\n",
    "discount_amount = gross_amount * discount_percent / 100\n",
    "net_amount = gross_amount - discount_amount\n",
    "```\n",
    "\n",
    "ðŸ’¡ **3. SCD Type 2 - Lakeflow format**\n",
    "```python\n",
    "# Lakeflow: AUTO CDC ... STORED AS SCD TYPE 2\n",
    "# Notebook: Manual MERGE with __START_AT, __END_AT\n",
    "\n",
    "# Query current: WHERE __END_AT IS NULL\n",
    "# Query historical: WHERE date BETWEEN __START_AT AND __END_AT\n",
    "```\n",
    "\n",
    "ðŸ’¡ **4. Star Schema Query Pattern**\n",
    "```sql\n",
    "SELECT f.*, d.year, c.customer_segment, p.product_name\n",
    "FROM fact_sales f\n",
    "JOIN dim_date d ON f.order_date_key = d.date_key\n",
    "JOIN dim_customer c ON f.customer_id = c.customer_id\n",
    "JOIN dim_product p ON f.product_id = p.product_id\n",
    "```\n",
    "\n",
    "**ðŸ› ï¸ Lakeflow Pipeline:**\n",
    "- Review SQL files in `/Lakeflow/` for production-ready streaming implementation\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** ðŸŽ‰ \n",
    "You have completed the Medallion Architecture implementation following the Lakeflow model!\n",
    "You are now ready to build production-grade data lakehouse pipelines!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e58748e-1fd8-4de1-86ff-8e3e13e32eb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 6: Resource Cleanup\n",
    "\n",
    "**Note:** This section is optional. Run only if you want to delete all data created in this notebook.\n",
    "\n",
    "### Option 1: Check created resources (recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83e99a85-b868-44ca-a6fc-205ea2f3d6fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Data is preserved for further use**\n",
    "\n",
    "To delete all tables, run the next cell in the optional section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dea6788-3610-410f-a222-6962f3f01fc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Option 2: Delete all resources (only if you really want to)\n",
    "\n",
    "**WARNING:** This will delete all Silver and Gold tables created in this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6409dd7e-5b44-4ca7-9aba-e089f8b849d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Option 2: Delete all resources (ONLY IF YOU ARE SURE!)\n",
    "\n",
    "# âš ï¸  WARNING: Uncomment the code below only if you want to delete everything!\n",
    "\n",
    "\"\"\"\n",
    "print(\"=== ðŸ—‘ï¸  DELETING MEDALLION RESOURCES ===\\n\")\n",
    "\n",
    "# List of tables to delete (Star Schema)\n",
    "tables_to_drop = [\n",
    "    # Silver\n",
    "    f\"{SILVER_SCHEMA}.orders_clean\",\n",
    "    f\"{SILVER_SCHEMA}.customers_type1\",\n",
    "    f\"{SILVER_SCHEMA}.customers_type2\",\n",
    "    # Gold - Star Schema\n",
    "    f\"{GOLD_SCHEMA}.fact_sales\",\n",
    "    f\"{GOLD_SCHEMA}.dim_customer\",\n",
    "    f\"{GOLD_SCHEMA}.dim_product\",\n",
    "    f\"{GOLD_SCHEMA}.dim_date\",\n",
    "    f\"{GOLD_SCHEMA}.dim_store\",\n",
    "    f\"{GOLD_SCHEMA}.dim_payment_method\"\n",
    "]\n",
    "\n",
    "print(\"Deleting tables...\\n\")\n",
    "for table in tables_to_drop:\n",
    "    full_table = f\"{CATALOG}.{table}\"\n",
    "    try:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {full_table}\")\n",
    "        print(f\"  âœ“ Deleted: {table}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âš ï¸  Error with {table}: {e}\")\n",
    "\n",
    "print(\"\\nâœ… Cleanup completed!\")\n",
    "\"\"\"\n",
    "\n",
    "display({\n",
    "    \"status\": \"âš ï¸ CLEANUP CODE IS COMMENTED OUT\",\n",
    "    \"info\": \"Uncomment the code above only if you want to delete all resources\",\n",
    "    \"recommendation\": \"Keep the data for subsequent notebooks and workshops!\",\n",
    "    \"next\": \"04_optimization_best_practices.ipynb\"\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "06_medallion_architecture",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
