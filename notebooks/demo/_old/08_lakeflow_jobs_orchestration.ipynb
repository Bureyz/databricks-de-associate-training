{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6438c89-2989-4fec-be2e-47ec390656d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Lakeflow Jobs - Workflow Orchestration\n",
    "\n",
    "---\n",
    "\n",
    "## Business Context\n",
    "\n",
    "**KION** needs to automate their daily ETL process:\n",
    "- Order data ingestion at 2:00 AM\n",
    "- Transformation to Silver layer\n",
    "- Aggregations to Gold\n",
    "- Notifications on errors\n",
    "\n",
    "In this module, you'll learn how to create and configure Lakeflow Jobs through the UI.\n",
    "\n",
    "---\n",
    "\n",
    "## Agenda\n",
    "\n",
    "1. Introduction to Lakeflow Jobs\n",
    "2. Preparing notebooks for Job (tasks)\n",
    "3. [UI DEMO] Creating Multi-task Job\n",
    "4. [UI DEMO] Triggers and Schedule\n",
    "5. [UI DEMO] Options, Retry and Alerting\n",
    "6. Practice: Widgets and Parameters\n",
    "7. Practice: Passing Data Between Tasks\n",
    "8. Monitoring via System Tables\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71db7212-7f11-4069-ab66-c9871e71ad34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Introduction to Lakeflow Jobs\n",
    "\n",
    "**Lakeflow Jobs** (formerly Databricks Jobs) is a managed orchestration service.\n",
    "\n",
    "### When to Use Jobs?\n",
    "\n",
    "| Scenario | Solution |\n",
    "|----------|----------|\n",
    "| ETL pipeline with multiple steps | Multi-task Job |\n",
    "| Daily report at fixed time | Scheduled Job |\n",
    "| Reaction to new files | File Arrival Trigger |\n",
    "| ML training pipeline | Job with notebook tasks |\n",
    "| Run DLT pipeline | Job with DLT task |\n",
    "\n",
    "### Jobs vs DLT (Lakeflow Pipelines)\n",
    "\n",
    "| Feature | Jobs | DLT |\n",
    "|---------|------|-----|\n",
    "| Orchestration | General | ETL only |\n",
    "| Dependencies | Manual configuration | Automatic (DAG) |\n",
    "| Data Quality | Custom code | Built-in expectations |\n",
    "| Flexibility | High | Opinionated |\n",
    "\n",
    "**Best Practice**: Use DLT for ETL pipelines, Jobs for orchestrating DLT + other tasks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5ccfda7-f73d-48db-9f7a-a1949f3b1fd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Preparing Notebooks for Job\n",
    "\n",
    "Below are 3 simple notebooks that we'll use in the demo.\n",
    "\n",
    "**Instructions**: \n",
    "1. Create folder `/Workspace/Users/<your-email>/jobs_demo/`\n",
    "2. Copy each of the following code snippets to a separate notebook\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c19928fd-f9d4-492a-8ebd-795f31842597",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 1: Validate Source (`task_01_validate`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8637c35-566c-4699-99ec-ebc5dcbaac45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TASK 1: Validate Source Data\n",
    "# Copy this code to notebook: task_01_validate\n",
    "# =============================================================================\n",
    "\n",
    "# Parameters from Job\n",
    "dbutils.widgets.text(\"source_table\", \"samples.nyctaxi.trips\")\n",
    "dbutils.widgets.text(\"min_rows\", \"100\")\n",
    "\n",
    "source_table = dbutils.widgets.get(\"source_table\")\n",
    "min_rows = int(dbutils.widgets.get(\"min_rows\"))\n",
    "\n",
    "print(f\"Validating: {source_table}\")\n",
    "print(f\"Minimum rows required: {min_rows}\")\n",
    "\n",
    "# Validation\n",
    "df = spark.table(source_table)\n",
    "row_count = df.count()\n",
    "\n",
    "if row_count < min_rows:\n",
    "    raise Exception(f\"Validation FAILED: {row_count} rows < {min_rows} minimum\")\n",
    "\n",
    "print(f\"Validation PASSED: {row_count} rows\")\n",
    "\n",
    "# Return result to next task\n",
    "import json\n",
    "dbutils.notebook.exit(json.dumps({\n",
    "    \"status\": \"SUCCESS\",\n",
    "    \"source_table\": source_table,\n",
    "    \"row_count\": row_count\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dc5bc67-4e8c-49ed-b93a-044b1ac2c002",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "task_02_transform%md\n",
    "### Task 2: Transform Data (`task_02_transform`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3101b9b2-13b5-459d-9cd6-e80212a47adc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TASK 2: Transform Data\n",
    "# Copy this code to notebook: task_02_transform\n",
    "# =============================================================================\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "import json\n",
    "\n",
    "# Parameters\n",
    "dbutils.widgets.text(\"source_table\", \"samples.nyctaxi.trips\")\n",
    "dbutils.widgets.text(\"run_date\", \"\")\n",
    "\n",
    "source_table = dbutils.widgets.get(\"source_table\")\n",
    "run_date = dbutils.widgets.get(\"run_date\") or str(current_date())\n",
    "\n",
    "# Get result from previous task (optional)\n",
    "try:\n",
    "    prev_result = dbutils.jobs.taskValues.get(\n",
    "        taskKey=\"validate\",\n",
    "        key=\"returnValue\",\n",
    "        default=\"{}\"\n",
    "    )\n",
    "    prev_data = json.loads(prev_result)\n",
    "    print(f\"Previous task result: {prev_data}\")\n",
    "except:\n",
    "    print(\"Running standalone (no previous task)\")\n",
    "\n",
    "# Transformation\n",
    "print(f\"Transforming: {source_table}\")\n",
    "\n",
    "df = spark.table(source_table)\n",
    "\n",
    "df_transformed = (\n",
    "    df\n",
    "    .withColumn(\"trip_duration_minutes\", \n",
    "                round((col(\"tpep_dropoff_datetime\").cast(\"long\") - \n",
    "                       col(\"tpep_pickup_datetime\").cast(\"long\")) / 60, 2))\n",
    "    .withColumn(\"cost_per_mile\", \n",
    "                when(col(\"trip_distance\") > 0, \n",
    "                     round(col(\"fare_amount\") / col(\"trip_distance\"), 2))\n",
    "                .otherwise(0))\n",
    "    .withColumn(\"processing_date\", lit(run_date))\n",
    ")\n",
    "\n",
    "row_count = df_transformed.count()\n",
    "print(f\"Transformed {row_count} rows\")\n",
    "\n",
    "# Display sample\n",
    "df_transformed.select(\n",
    "    \"trip_distance\", \"fare_amount\", \"trip_duration_minutes\", \"cost_per_mile\"\n",
    ").show(5)\n",
    "\n",
    "# Return result\n",
    "dbutils.notebook.exit(json.dumps({\n",
    "    \"status\": \"SUCCESS\",\n",
    "    \"rows_transformed\": row_count\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dae53bf-dbb5-418a-99cc-8f3c2f4548f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 3: Generate Report (`task_03_report`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c0efa18-b0a7-4924-be15-bd9f2f8ac53e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TASK 3: Generate Report\n",
    "# Copy this code to notebook: task_03_report\n",
    "# =============================================================================\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Parameters\n",
    "dbutils.widgets.text(\"source_table\", \"samples.nyctaxi.trips\")\n",
    "\n",
    "source_table = dbutils.widgets.get(\"source_table\")\n",
    "\n",
    "print(f\"Generating report from: {source_table}\")\n",
    "\n",
    "# Aggregations\n",
    "df = spark.table(source_table)\n",
    "\n",
    "report = df.agg(\n",
    "    count(\"*\").alias(\"total_trips\"),\n",
    "    round(sum(\"fare_amount\"), 2).alias(\"total_revenue\"),\n",
    "    round(avg(\"fare_amount\"), 2).alias(\"avg_fare\"),\n",
    "    round(avg(\"trip_distance\"), 2).alias(\"avg_distance\"),\n",
    "    round(max(\"fare_amount\"), 2).alias(\"max_fare\")\n",
    ").collect()[0]\n",
    "\n",
    "# Display report\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DAILY REPORT\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total Trips:    {report.total_trips:,}\")\n",
    "print(f\"Total Revenue:  ${report.total_revenue:,.2f}\")\n",
    "print(f\"Avg Fare:       ${report.avg_fare:.2f}\")\n",
    "print(f\"Avg Distance:   {report.avg_distance:.2f} miles\")\n",
    "print(f\"Max Fare:       ${report.max_fare:.2f}\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Generated at:   {datetime.now()}\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# Return result\n",
    "dbutils.notebook.exit(json.dumps({\n",
    "    \"status\": \"SUCCESS\",\n",
    "    \"total_trips\": report.total_trips,\n",
    "    \"total_revenue\": float(report.total_revenue)\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "383e0836-5ba6-4b45-b4d7-d101e7195631",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 3. [UI DEMO] Creating Multi-task Job\n",
    "\n",
    "### Checklist for instructor:\n",
    "\n",
    "**Step 1: Create new Job**\n",
    "- [ ] Workflows → Create Job\n",
    "- [ ] Name: `KION_Demo_ETL_Pipeline`\n",
    "\n",
    "**Step 2: Add Task 1 (Validate)**\n",
    "- [ ] Task name: `validate`\n",
    "- [ ] Type: Notebook\n",
    "- [ ] Path: `/Workspace/.../task_01_validate`\n",
    "- [ ] Cluster: Serverless or new Job Cluster\n",
    "- [ ] Parameters: `source_table` = `samples.nyctaxi.trips`\n",
    "\n",
    "**Step 3: Add Task 2 (Transform)**\n",
    "- [ ] Task name: `transform`\n",
    "- [ ] Depends on: `validate`\n",
    "- [ ] Path: `/Workspace/.../task_02_transform`\n",
    "- [ ] Parameters: `source_table` = `samples.nyctaxi.trips`\n",
    "\n",
    "**Step 4: Add Task 3 (Report)**\n",
    "- [ ] Task name: `report`\n",
    "- [ ] Depends on: `transform`\n",
    "- [ ] Path: `/Workspace/.../task_03_report`\n",
    "\n",
    "**Step 5: Run Job**\n",
    "- [ ] Run now\n",
    "- [ ] Show: DAG visualization\n",
    "- [ ] Show: Task logs and output\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b68b0a63-260e-44b7-8cc6-49e32fe17f8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. [UI DEMO] Triggers and Schedule\n",
    "\n",
    "### Checklist for instructor:\n",
    "\n",
    "**Trigger Options** (Triggers tab):\n",
    "\n",
    "| Trigger Type | Usage | Example |\n",
    "|--------------|-------|---------|\n",
    "| **Scheduled** | Fixed schedule | Daily at 2:00 |\n",
    "| **File arrival** | Reaction to new files | New file in `/landing/` |\n",
    "| **Continuous** | Continuous processing | Streaming-like |\n",
    "| **Manual** | On-demand | Testing |\n",
    "\n",
    "**Demo: Scheduled Trigger**\n",
    "- [ ] Add trigger → Scheduled\n",
    "- [ ] Cron expression: `0 0 2 * * ?` (daily at 2:00)\n",
    "- [ ] Timezone: `Europe/Warsaw`\n",
    "- [ ] Show: Preview next runs\n",
    "\n",
    "**Demo: File Arrival Trigger** (optional)\n",
    "- [ ] Add trigger → File arrival\n",
    "- [ ] URL: Unity Catalog Volume path\n",
    "- [ ] Min files: 1\n",
    "- [ ] Wait time: 5 minutes\n",
    "\n",
    "### Useful CRON expressions:\n",
    "\n",
    "```\n",
    "0 0 2 * * ?        # Daily at 2:00\n",
    "0 0 * * * ?        # Every hour\n",
    "0 0 9 ? * MON-FRI  # Mon-Fri at 9:00\n",
    "0 0 0 1 * ?        # First day of month\n",
    "0 */15 * * * ?     # Every 15 minutes\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cb23815-b85e-4a39-a390-aa5472c7728c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. [UI DEMO] Options, Retry and Alerting\n",
    "\n",
    "### Checklist for instructor:\n",
    "\n",
    "**Task-level options** (per task):\n",
    "- [ ] Timeout: 30 minutes\n",
    "- [ ] Retries: 2\n",
    "- [ ] Retry delay: 60 seconds\n",
    "\n",
    "**Job-level options** (top panel):\n",
    "- [ ] Max concurrent runs: 1 (prevents overlap)\n",
    "- [ ] Job timeout: 2 hours\n",
    "\n",
    "**Email Notifications**:\n",
    "- [ ] On failure: `team@company.com`\n",
    "- [ ] On success: (optional)\n",
    "- [ ] On start: (optional)\n",
    "\n",
    "**Webhook Integration** (Slack/Teams):\n",
    "- [ ] Admin Settings → Destinations\n",
    "- [ ] Add webhook URL\n",
    "- [ ] Assign to Job\n",
    "\n",
    "### When to use Retry?\n",
    "\n",
    "| Scenario | Retry? | Why |\n",
    "|----------|--------|-----|\n",
    "| Network timeout | Yes | Transient error |\n",
    "| API rate limit | Yes | Transient error |\n",
    "| Data quality issue | No | Retry won't fix data |\n",
    "| Code bug | No | Retry won't fix code |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4705da0b-5173-452b-9faa-e0b72ea180e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Practice: Widgets and Parameters\n",
    "\n",
    "Databricks Widgets allow you to parameterize notebooks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56cd432f-018e-41f9-9a49-6f508bf37262",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Widget types\n",
    "\n",
    "# Text - any text\n",
    "dbutils.widgets.text(\"environment\", \"dev\", \"Environment\")\n",
    "\n",
    "# Dropdown - select from list\n",
    "dbutils.widgets.dropdown(\"region\", \"EU\", [\"EU\", \"US\", \"APAC\"], \"Region\")\n",
    "\n",
    "# Combobox - dropdown with typing option\n",
    "dbutils.widgets.combobox(\"table\", \"orders\", [\"orders\", \"customers\", \"products\"], \"Table\")\n",
    "\n",
    "# Multiselect - multiple selection\n",
    "dbutils.widgets.multiselect(\"columns\", \"id\", [\"id\", \"name\", \"date\", \"amount\"], \"Columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cee2d90-5fea-4b8f-baf9-af5c6ee98e1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Getting values\n",
    "environment = dbutils.widgets.get(\"environment\")\n",
    "region = dbutils.widgets.get(\"region\")\n",
    "table = dbutils.widgets.get(\"table\")\n",
    "columns = dbutils.widgets.get(\"columns\")  # returns comma-separated string\n",
    "\n",
    "print(f\"Environment: {environment}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Table: {table}\")\n",
    "print(f\"Columns: {columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b5a3402-9560-46ae-8012-65a21292d4fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dynamic parameters in Job\n",
    "# These values are available when notebook is run as a task in Job\n",
    "\n",
    "dynamic_params = {\n",
    "    \"{{job.start_time.iso_date}}\": \"Run date (YYYY-MM-DD)\",\n",
    "    \"{{job.start_time}}\": \"Full timestamp\",\n",
    "    \"{{job.id}}\": \"Job ID\",\n",
    "    \"{{run.id}}\": \"Current run ID\",\n",
    "    \"{{task.name}}\": \"Current task name\"\n",
    "}\n",
    "\n",
    "for param, description in dynamic_params.items():\n",
    "    print(f\"{param:35} -> {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70a532f0-4d64-41a4-8190-d6e2b46c6aa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Widget cleanup\n",
    "dbutils.widgets.removeAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e65c4d1c-e3de-4fe1-b9b0-3171e310a696",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 7. Practice: Passing Data Between Tasks\n",
    "\n",
    "Two ways to pass data:\n",
    "\n",
    "1. **dbutils.notebook.exit()** - returns value from notebook\n",
    "2. **dbutils.jobs.taskValues** - reads value from previous task\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd84ec1c-04e8-478a-a836-e6da5fc78563",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task A - sends data\n",
    "import json\n",
    "\n",
    "result = {\n",
    "    \"rows_processed\": 1500,\n",
    "    \"max_date\": \"2024-01-15\",\n",
    "    \"status\": \"SUCCESS\"\n",
    "}\n",
    "\n",
    "# Return as JSON string\n",
    "# dbutils.notebook.exit(json.dumps(result))\n",
    "print(f\"Task A would exit with: {json.dumps(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92c8501a-780e-4f2c-802b-eaf5fc8e0330",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task B - receives data from Task A\n",
    "import json\n",
    "\n",
    "# In real Job:\n",
    "# task_a_output = dbutils.jobs.taskValues.get(\n",
    "#     taskKey=\"task_a\",           # name of previous task\n",
    "#     key=\"returnValue\",          # key (default \"returnValue\")\n",
    "#     default=\"{}\",               # default value\n",
    "#     debugValue=\"{}\"             # value for local testing\n",
    "# )\n",
    "\n",
    "# Simulation\n",
    "task_a_output = '{\"rows_processed\": 1500, \"max_date\": \"2024-01-15\", \"status\": \"SUCCESS\"}'\n",
    "\n",
    "data = json.loads(task_a_output)\n",
    "print(f\"Received from Task A:\")\n",
    "print(f\"  Rows: {data['rows_processed']}\")\n",
    "print(f\"  Max date: {data['max_date']}\")\n",
    "print(f\"  Status: {data['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e2b2d45-caed-4fb4-b734-32f7d68d3da0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 8. Monitoring via System Tables\n",
    "\n",
    "Databricks provides system tables with Job run history.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c00e9872-1d5c-474d-bc40-4ef736240b43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        *\n",
    "    FROM system.lakeflow.job_run_timeline\n",
    "    ORDER BY 1 DESC\n",
    "    LIMIT 20\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dda9551-edd8-42b2-b8bf-5ceeda5a55b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        DATE(period_start_time) as run_date,\n",
    "        run_name as job_name,\n",
    "        ROUND(\n",
    "            AVG(\n",
    "                (UNIX_TIMESTAMP(period_end_time) - UNIX_TIMESTAMP(period_start_time)) / 60\n",
    "            ), 1\n",
    "        ) as avg_duration_min,\n",
    "        COUNT(*) as runs\n",
    "    FROM system.lakeflow.job_run_timeline\n",
    "    WHERE period_start_time >= current_date() - INTERVAL 14 DAYS\n",
    "        AND result_state = 'SUCCESS'\n",
    "    GROUP BY run_date, job_name\n",
    "    ORDER BY run_date DESC\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70c0aaf9-a51b-4fe2-9d45-6f7659e4487b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### In this module you learned:\n",
    "\n",
    "| Topic | Key Elements |\n",
    "|-------|--------------|\n",
    "| **Multi-task Jobs** | DAG workflow, task dependencies |\n",
    "| **Triggers** | Scheduled (CRON), File arrival, Continuous |\n",
    "| **Options** | Timeout, Retry, Max concurrent runs |\n",
    "| **Alerting** | Email, Webhooks (Slack/Teams) |\n",
    "| **Parameters** | Widgets, dynamic values, taskValues |\n",
    "| **Monitoring** | System tables, success rate, duration trends |\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Serverless** - use for most Jobs that need to start immediately (fast start, auto-scaling)\n",
    "2. **Idempotency** - Job should be safe to re-run\n",
    "3. **Retry** - only for transient errors (network, API)\n",
    "4. **Alerting** - always configure error notifications\n",
    "5. **Monitoring** - regularly check success rate and trends\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Databricks Jobs Documentation](https://docs.databricks.com/workflows/jobs/jobs.html)\n",
    "- [Serverless Jobs](https://docs.databricks.com/en/jobs/serverless.html)\n",
    "- [Jobs Best Practices](https://docs.databricks.com/workflows/jobs/jobs-best-practices.html)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_lakeflow_jobs_orchestration",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
