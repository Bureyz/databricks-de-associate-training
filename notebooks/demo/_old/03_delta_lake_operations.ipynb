{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2399a946-d3b0-4e05-9eee-0e6803d13246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Delta Lake Operations\n",
    "\n",
    "## The Story: From \"Data Swamp\" to Reliable Data\n",
    "\n",
    "Your e-commerce company's previous data platform was a **Data Lake disaster**:\n",
    "\n",
    "- Marketing updated customer segments, breaking downstream reports\n",
    "- A failed job left half-written files - data corrupted for 2 days\n",
    "- Finance asked for \"data from last month\" - no way to get it\n",
    "- Data Scientists couldn't trust the data - \"is this the latest version?\"\n",
    "\n",
    "**Delta Lake solves these problems.** This is the most important technology in modern Lakehouse.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Delta Lake Matters (The Business Case)\n",
    "\n",
    "### Problems Delta Lake Solves\n",
    "\n",
    "| Problem | Before Delta | With Delta |\n",
    "|---------|--------------|------------|\n",
    "| Concurrent writes | Data corruption | ACID transactions |\n",
    "| Schema changes | Silent data quality issues | Schema enforcement |\n",
    "| Data recovery | \"We don't have backups\" | Time Travel (30 days default) |\n",
    "| Incremental updates | Full reloads only | MERGE, UPDATE, DELETE |\n",
    "| Query performance | Full scans always | Z-ORDER, data skipping |\n",
    "\n",
    "### When NOT to Use Delta\n",
    "\n",
    "| Scenario | Better Alternative | Why |\n",
    "|----------|-------------------|-----|\n",
    "| Streaming with <1s latency | Kafka, Redis | Delta optimized for seconds, not milliseconds |\n",
    "| Simple file storage | Parquet, CSV | No need for transactions overhead |\n",
    "| Cross-platform sharing | Iceberg | Better Snowflake/BigQuery interop |\n",
    "| Append-only logs | Plain Parquet | Simpler, cheaper |\n",
    "\n",
    "### Delta vs Iceberg vs Hudi\n",
    "\n",
    "| Feature | Delta | Iceberg | Hudi |\n",
    "|---------|-------|---------|------|\n",
    "| **Primary backer** | Databricks | Netflix/Apple/Snowflake | Uber |\n",
    "| **Best for** | Databricks ecosystem | Multi-engine (Spark, Trino, Flink) | Streaming upserts |\n",
    "| **Governance** | Unity Catalog | Open catalog | Limited |\n",
    "| **Adoption** | Highest in Databricks shops | Growing fast | Niche |\n",
    "\n",
    "*Recommendation: If you're on Databricks, use Delta. If multi-cloud/multi-engine, consider Iceberg.*\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "| Topic | Why It Matters |\n",
    "|-------|---------------|\n",
    "| Delta Log internals | Debug issues, understand versioning |\n",
    "| MERGE operations | Implement SCD Type 1/2 |\n",
    "| Time Travel | Disaster recovery, audit compliance |\n",
    "| Optimization | 10x query performance improvement |\n",
    "| Change Data Feed | Incremental downstream processing |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47092836-2e35-43c3-90dc-a4fc8aac0974",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Context and Requirements\n",
    "\n",
    "- **Training Day**: Day 2 - Lakehouse & Delta Lake\n",
    "- **Notebook Type**: Demo\n",
    "- **Technical Requirements**:\n",
    "  - Databricks Runtime 16.4 LTS or 17.3 LTS (Spark 4.0)\n",
    "  - Unity Catalog enabled\n",
    "  - Permissions: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Cluster: Standard with 2-4 workers (or Serverless Compute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cc52d93-9548-447f-8dab-f27a5c8c2e53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Theoretical Introduction\n",
    "\n",
    "**Section Objective:** Introduction to Delta Lake as a transactional storage layer over Data Lake\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Delta Lake**: Open-source storage layer providing ACID transactions for Apache Spark\n",
    "- **Delta Log**: Transactional log storing metadata about all table changes\n",
    "- **Schema Enforcement**: Automatic schema validation on write\n",
    "- **Time Travel**: Ability to access previous versions of data\n",
    "\n",
    "**Why is this important?**\n",
    "Delta Lake solves fundamental Data Lake problems: lack of transactions, schema drift, update difficulties, and quality assurance. It provides Data Warehouse reliability with Data Lake flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a0b4b70-aae4-4747-8785-bafd0dc63143",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Per-user Isolation\n",
    "\n",
    "Run the initialization script for per-user catalog and schema isolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b62fe846-46a6-48d7-a324-f40d8a22295f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4710c49-0275-4ffb-9e16-23204c742948",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuration\n",
    "\n",
    "Import libraries and set environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10cd956a-79c5-45b9-8164-c2f42e0d0266",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Display user context\n",
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (CATALOG, BRONZE_SCHEMA, SILVER_SCHEMA, GOLD_SCHEMA)\n",
    "    ], ['catalog', 'bronze_schema', 'silver_schema', 'gold_schema'])\n",
    ")\n",
    "\n",
    "# Set catalog and schema as default\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b0e248d-2ca8-460c-8ff0-7c7867b82062",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 1: Delta Lake Core Features\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Delta Lake is a transactional layer over Parquet that provides ACID properties (Atomicity, Consistency, Isolation, Durability). Every operation on a Delta table is recorded in the Delta Log - a JSON file containing metadata about changes.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **ACID Transactions**: All operations are atomic and consistent\n",
    "- **Delta Log**: `_delta_log/` folder with JSON files describing each transaction\n",
    "- **Schema Enforcement**: Automatic schema validation\n",
    "- **Unified Batch and Streaming**: One table supports both batch and streaming\n",
    "\n",
    "**Practical Application:**\n",
    "- Transactional updates in Data Lake\n",
    "- Ensuring data quality through schema validation\n",
    "- Unified data access for batch and streaming workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72717cea-7a0e-4e4a-86b5-e8fb77f1c92f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 1.1: Creating the First Delta Table\n",
    "\n",
    "**Objective:** Demonstration of creating a Delta table and basic properties\n",
    "\n",
    "**Approach:**\n",
    "1. Load data from Unity Catalog Volume\n",
    "2. Create a managed table in Delta format\n",
    "3. Explore Delta Log and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31cee2c9-543f-4d8d-a0fe-ca8d0e1961ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load customer data from Unity Catalog Volume\n",
    "customers_df = (spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(f\"{DATASET_BASE_PATH}/customers/customers.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adb4f7fe-8c01-47f9-aa6c-08a5ad5a41db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Create managed Delta table:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31ca80d2-937b-4641-a991-b5304dbe089c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create managed Delta table\n",
    "customers_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b70f208-811a-4771-842b-0efe24393911",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Display result:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "571c8136-e46f-470c-b07a-defa9853fd36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe3cc9b1-49cd-4a29-b4a6-167e2066ccfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "A managed Delta table was created in Unity Catalog. The Delta format automatically:\n",
    "- Created `_delta_log/` folder with transaction metadata\n",
    "- Registered table schema in Unity Catalog\n",
    "- Applied Parquet compression with additional Delta features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f68e66d-3c39-45a9-867d-04faf98c7993",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 1.2: Schema Enforcement in Action\n",
    "\n",
    "**Objective:** Demonstration of automatic schema validation during data insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f15f1cb0-1881-43a9-b3b5-66fe572eab24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check current table schema\n",
    "spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "979038ab-96fc-46fe-a329-04f1421a627d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Attempt to insert data with invalid schema (missing columns)\n",
    "invalid_data = spark.createDataFrame([\n",
    "    (\"CUST999999\", \"Test\", \"Customer\", \"invalid_email\", \"+48 123 456 789\")  # Missing city, state, country, registration_date, customer_segment\n",
    "], [\"customer_id\", \"first_name\", \"last_name\", \"email\", \"phone\"])\n",
    "\n",
    "try:\n",
    "    invalid_data.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "except Exception as e:\n",
    "    display(\n",
    "        spark.createDataFrame([\n",
    "            (\"Schema enforcement in action\", str(e)[:200] + \"...\")\n",
    "        ], [\"message\", \"error\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96866648-1cba-4918-b69f-ed26f44d9cba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "Schema enforcement automatically rejected data with invalid type. Delta Lake compares the new data schema with the table schema and blocks incompatible insertions, ensuring consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20a4edd0-0bcf-4633-9e01-9219dd52beff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create table with Identity Column and Generated Column\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {CATALOG}.{BRONZE_SCHEMA}.orders_modern (\n",
    "    order_sk BIGINT GENERATED ALWAYS AS IDENTITY,  -- Surrogate Key\n",
    "    order_id STRING,\n",
    "    total_amount DOUBLE,\n",
    "    order_timestamp TIMESTAMP,\n",
    "    order_date DATE GENERATED ALWAYS AS (CAST(order_timestamp AS DATE)) -- Auto-calculated\n",
    ") USING DELTA\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1b489be-2c56-4d77-a0fa-6e5ddf0a6710",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.orders_modern\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "060e0435-54a0-4e9d-b736-a5a080c25518",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's check the result. Note the automatically populated columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd1ebce2-0e87-4cd6-900d-84300b219ee2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Insert data without specifying generated columns\n",
    "spark.sql(f\"\"\"\n",
    "INSERT INTO {CATALOG}.{BRONZE_SCHEMA}.orders_modern (order_id, total_amount, order_timestamp)\n",
    "VALUES \n",
    "    ('ORD-001', 150.50, current_timestamp()),\n",
    "    ('ORD-002', 200.00, current_timestamp())\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efda8bd2-4df9-4e97-bc7a-6c32e712db98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now we will insert data. Note that in the `INSERT` query we omit `order_sk` and `order_date` columns.\n",
    "- `order_sk`: will be generated automatically (unique number).\n",
    "- `order_date`: will be calculated based on `order_timestamp`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81ce2558-42e7-4604-a1b6-f93099b3dec5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 2: Schema Evolution\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Schema Evolution allows for controlled addition of new columns to existing Delta tables without interrupting application operations. Delta Lake supports additive schema changes automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1646c21e-2c91-4c5b-a292-1769fcb28454",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 2.1: Automatic Column Addition\n",
    "\n",
    "**Objective:** Demonstration of automatic schema evolution when adding new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcd016ab-6033-450c-993c-736a543c1091",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data with additional column (customer_tier)\n",
    "extended_customers = spark.createDataFrame([\n",
    "    (\"CUST010001\", \"New\", \"Customer\", \"new@example.com\", \"+48 111 222 333\", \"Warsaw\", \"MZ\", \"Poland\", \"2023-12-01\", \"Basic\", \"Premium\"),\n",
    "    (\"CUST010002\", \"Another\", \"Customer\", \"another@example.com\", \"+48 444 555 666\", \"Krakow\", \"MP\", \"Poland\", \"2023-12-02\", \"Premium\", \"Standard\")\n",
    "], [\"customer_id\", \"first_name\", \"last_name\", \"email\", \"phone\", \"city\", \"state\", \"country\", \"registration_date\", \"customer_segment\", \"customer_tier\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2507581-ded5-4a20-b5b9-e97e43a34547",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Enable automatic schema evolution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf7e76ad-b678-4153-89d7-90554c61a3a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "extended_customers = extended_customers.withColumn(\"registration_date\", col(\"registration_date\").cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5fdf80c-f30d-47fe-b1a2-9f68a3729fce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enable automatic schema evolution\n",
    "extended_customers.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1518e23a-8363-4877-b893-4e8e07ca6cc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check new schema\n",
    "spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f94c7709-7cef-44df-8c40-50f38317a32f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify data - new column has NULL for old records\n",
    "display(\n",
    "    spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "    .select(\"customer_id\", \"first_name\", \"last_name\", \"customer_tier\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73521e5b-62b4-488f-b058-82830bff3b7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add CHECK constraint: customer_id must start with CUST\n",
    "try:\n",
    "    spark.sql(f\"\"\"\n",
    "        ALTER TABLE {CATALOG}.{BRONZE_SCHEMA}.customers_delta\n",
    "        ADD CONSTRAINT valid_customer_id CHECK (customer_id LIKE 'CUST%')\n",
    "    \"\"\")\n",
    "    print(\"Constraint 'valid_customer_id' added successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Info: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dcb6ad5-e056-4f4f-b806-bd2ccfa4e244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Attempt to insert invalid data (customer_id does not start with CUST)\n",
    "try:\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO {CATALOG}.{BRONZE_SCHEMA}.customers_delta (customer_id, first_name, last_name, email, phone, city, state, country, registration_date, customer_segment)\n",
    "        VALUES ('INVALID123', 'Bad', 'Customer', 'bad@example.com', '+48 000 000 000', 'Test', 'TS', 'Poland', '2023-01-01', 'Basic')\n",
    "    \"\"\")\n",
    "except Exception as e:\n",
    "    print(f\"Expected Data Quality error:\\n{str(e)[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e00d2222-3618-4527-ac91-d7ecfa5e6f77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Constraint has been added. Now let's try to insert data that violates it (customer_id does not start with 'CUST').\n",
    "We expect Delta Lake to block this operation and return a `CheckConstraintViolation` error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bf1163c-58cf-4f79-8d58-ef63042079ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 2.5: Data Quality & Constraints\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Delta Lake allows defining **Constraints** that guarantee data quality at the table level. This works similarly to traditional SQL databases.\n",
    "\n",
    "**Constraint Types:**\n",
    "- `NOT NULL`: Enforces the presence of a value.\n",
    "- `CHECK`: Enforces any logical condition (e.g., `age > 0`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a734b47f-5831-4600-a773-6aa00773cc74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 3: Time Travel and Disaster Recovery\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Time Travel is a key Delta Lake feature enabling access to previous versions of data. It is based on the Copy-on-Write mechanism - every change creates a new version of files, while old versions remain available.\n",
    "\n",
    "**Disaster Recovery:**\n",
    "Thanks to Time Travel, we can not only read old data but also **restore** the table to a previous state using the `RESTORE` command. This is crucial in case of accidental data deletion or incorrect updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c2cb29b-8e7f-4658-a721-708509eac8a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 3.1: Table History Exploration\n",
    "\n",
    "**Objective:** Use DESCRIBE HISTORY to analyze all operations on the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c04fc3c8-9ee7-46ba-b006-7720c6e9425f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show history of all operations on the table\n",
    "display(\n",
    "    spark.sql(f\"DESCRIBE HISTORY {CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4be441c5-adca-457c-946e-aa82c24652e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 3.2: Time Travel queries\n",
    "\n",
    "**Objective:** Access previous versions of data using VERSION AS OF and TIMESTAMP AS OF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77275765-d45f-4f0e-8dae-5960bf89778b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Access data from version 0 (before schema evolution)\n",
    "version_0_data = spark.sql(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM {CATALOG}.{BRONZE_SCHEMA}.customers_delta VERSION AS OF 1\n",
    "    ORDER BY customer_id\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "728bbf49-cde3-48aa-a194-e8adda94b464",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(version_0_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8590607e-9be2-4538-b801-7af362dee42d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compare record count between versions\n",
    "current_count = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\").count()\n",
    "version_0_count = spark.sql(f\"SELECT * FROM {CATALOG}.{BRONZE_SCHEMA}.customers_delta VERSION AS OF 0\").count()\n",
    "\n",
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"Current version\", current_count),\n",
    "        (\"Version 0\", version_0_count)\n",
    "    ], [\"version\", \"record_count\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfc5ee0d-ad03-49dc-b057-53ee4fac01cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Error simulation: Accidental deletion of all data\n",
    "spark.sql(f\"DELETE FROM {CATALOG}.{BRONZE_SCHEMA}.customers_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56cdf368-2fdf-4fcb-8b72-dd9d85fb0408",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Record count after RESTORE:\", spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8f1771d-6b35-456c-a185-63d700989479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The table has been restored. Let's verify the record count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "162295f9-8a08-4b56-8d8c-1ebad5bad7f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Fix: RESTORE to version before deletion\n",
    "# Get the last good version (before DELETE)\n",
    "last_good_version = spark.sql(f\"DESCRIBE HISTORY {CATALOG}.{BRONZE_SCHEMA}.customers_delta\").select(\"version\").limit(2).collect()[1][0]\n",
    "\n",
    "print(f\"Restoring to version: {last_good_version}\")\n",
    "\n",
    "spark.sql(f\"RESTORE TABLE {CATALOG}.{BRONZE_SCHEMA}.customers_delta TO VERSION AS OF {last_good_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a8a28c9-7013-47ee-94ce-28a811168d9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now we will use **Time Travel** to find the last correct version (before deletion) and restore the table using the `RESTORE` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2a11cbe-8f7b-4f58-9455-2e1d3592a0d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Record count after failure:\", spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "477375f5-c3fd-463d-8628-2942eee51726",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Oops! We deleted all data. Let's check if the table is actually empty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e060050-02c3-438e-a225-624ad45d416a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 3.3: Disaster Recovery - RESTORE TABLE\n",
    "\n",
    "**Objective:** Restore the table to the state before the erroneous operation (failure simulation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "309db8fd-2525-427b-b331-f0d7d67b34ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 4: CRUD Operations\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Delta Lake supports the full range of CRUD operations (Create, Read, Update, Delete), making it ideal for transactional workloads in Data Lake. All operations are atomic and ACID-compliant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad370348-1447-49a3-bb42-4781ec23d3c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 4.1: INSERT operation\n",
    "\n",
    "**Objective:** Adding new records to an existing table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d3c3c31-4fc1-47d9-9ed7-a60b34af2a8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# INSERT new customers\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO {CATALOG}.{BRONZE_SCHEMA}.customers_delta\n",
    "    (customer_id, first_name, last_name, email, phone, city, state, country, registration_date, customer_segment, customer_tier)\n",
    "    VALUES \n",
    "        ('CUST020001', 'Insert', 'Customer1', 'insert1@example.com', '+48 111 111 111', 'Warsaw', 'MZ', 'Poland', '2023-12-10', 'Premium', 'Gold'),\n",
    "        ('CUST020002', 'Insert', 'Customer2', 'insert2@example.com', '+48 222 222 222', 'Gdansk', 'PM', 'Poland', '2023-12-11', 'Basic', 'Silver')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6677e39a-5cf7-4598-9c79-e2315710ed6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Verify insertion:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deb5c2e9-3ca5-4703-9d99-3b6c984f0969",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify insertion\n",
    "display(\n",
    "    spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "    .filter(F.col(\"customer_id\").like(\"CUST02%\"))\n",
    "    .orderBy(\"customer_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85c568b0-f0df-4071-a780-ff62f1d8592c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 4.2: UPDATE operation\n",
    "\n",
    "**Objective:** Updating existing records in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc7aa31b-8acb-4a8e-aa37-816718b42bd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UPDATE customer tier for specific customers\n",
    "spark.sql(f\"\"\"\n",
    "    UPDATE {CATALOG}.{BRONZE_SCHEMA}.customers_delta\n",
    "    SET customer_tier = 'Platinum'\n",
    "    WHERE customer_id IN ('CUST010001', 'CUST020001')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89f68659-9863-4720-bd49-11b39379fd50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Verify update:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15980065-806c-4a45-a358-28f8a3e8db30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify update\n",
    "display(\n",
    "    spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "    .filter(F.col(\"customer_tier\") == \"Platinum\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1563ba09-e4f7-48ef-b495-ab3cead61fe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 4.3: DELETE operation\n",
    "\n",
    "**Objective:** Deleting records from a Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07f9d3db-5d08-4180-b68a-bf843d78ec03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DELETE specific customer\n",
    "spark.sql(f\"\"\"\n",
    "    DELETE FROM {CATALOG}.{BRONZE_SCHEMA}.customers_delta\n",
    "    WHERE customer_id = 'CUST020002'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33503d56-53c3-49fa-bf44-1793ae2530bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Verify deletion:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "775563ff-3a28-4a9c-9254-22fc4b8b521f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify deletion\n",
    "deleted_check = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\") \\\n",
    "    .filter(F.col(\"customer_id\") == \"CUST020002\") \\\n",
    "    .count()\n",
    "\n",
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"Records with customer_id CUST020002\", deleted_check)\n",
    "    ], [\"description\", \"count\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a119d3c-01af-4e99-97cf-02893bdd1e08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 5: MERGE INTO Operations\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "MERGE INTO is a powerful operation enabling upsert (update + insert) in a single transaction. Especially useful when processing changes from transactional systems (CDC - Change Data Capture)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c107079-d995-447e-94d4-8447ec753f32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 5.1: Basic MERGE INTO\n",
    "\n",
    "**Objective:** Demonstration of upsert operation - update existing and insert new records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc93805f-ae93-4add-8239-9bd51e690821",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare data for merge (mix of updates and new records)\n",
    "merge_data = spark.createDataFrame([\n",
    "    (\"CUST010001\", \"Updated\", \"Name\", \"updated@example.com\", \"+48 999 999 999\", \"Poznan\", \"WP\", \"Poland\", \"2023-12-01\", \"VIP\", \"Diamond\"),  # Update\n",
    "    (\"CUST030001\", \"Brand\", \"New\", \"brand.new@example.com\", \"+48 777 777 777\", \"Wroclaw\", \"DS\", \"Poland\", \"2023-12-15\", \"Basic\", \"Bronze\"),   # Insert\n",
    "    (\"CUST030002\", \"Another\", \"New\", \"another.new@example.com\", \"+48 888 888 888\", \"Lodz\", \"LD\", \"Poland\", \"2023-12-16\", \"Premium\", \"Silver\") # Insert\n",
    "], [\"customer_id\", \"first_name\", \"last_name\", \"email\", \"phone\", \"city\", \"state\", \"country\", \"registration_date\", \"customer_segment\", \"customer_tier\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ff59cc6-4daf-4de8-be63-b9751f34f721",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Create temporary view for merge operation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5347719a-eebd-419f-a7ea-8edd83438a56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create temporary view for merge operation\n",
    "merge_data.createOrReplaceTempView(\"customer_updates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42e0a7ba-52dc-4100-b8e0-881c1b07a4a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Execute MERGE operation (Upsert):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ae1e75a-cef0-4932-b7c5-fddcb41d03df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MERGE INTO operation\n",
    "spark.sql(f\"\"\"\n",
    "    MERGE INTO {CATALOG}.{BRONZE_SCHEMA}.customers_delta AS target\n",
    "    USING customer_updates AS source\n",
    "    ON target.customer_id = source.customer_id\n",
    "    \n",
    "    WHEN MATCHED THEN\n",
    "        UPDATE SET\n",
    "            first_name = source.first_name,\n",
    "            last_name = source.last_name,\n",
    "            email = source.email,\n",
    "            phone = source.phone,\n",
    "            city = source.city,\n",
    "            state = source.state,\n",
    "            country = source.country,\n",
    "            customer_segment = source.customer_segment,\n",
    "            customer_tier = source.customer_tier\n",
    "    \n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT (customer_id, first_name, last_name, email, phone, city, state, country, registration_date, customer_segment, customer_tier)\n",
    "        VALUES (source.customer_id, source.first_name, source.last_name, source.email, source.phone, source.city, source.state, source.country, source.registration_date, source.customer_segment, source.customer_tier)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edb3ffeb-9131-4d83-b20c-21bc538c3965",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Verify MERGE results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cb851ea-582e-46cf-918e-44bd88a3f61b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify MERGE results\n",
    "display(\n",
    "    spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "    .filter(F.col(\"customer_id\").isin([\"CUST010001\", \"CUST030001\", \"CUST030002\"]))\n",
    "    .orderBy(\"customer_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d83043cd-1b3d-4b1f-9c44-7c56c49579ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 6: Metadata and Analytics\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Delta Lake offers rich metadata about tables and operations. DESCRIBE DETAIL provides information about file structure, partitioning, and table properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b41b54e-e42f-4574-91d6-8f59a9c82491",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 6.1: DESCRIBE DETAIL\n",
    "\n",
    "**Objective:** Analysis of Delta table metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1fea90d-f3d2-461c-8bdd-963eedf8363a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Detailed table information\n",
    "display(\n",
    "    spark.sql(f\"DESCRIBE DETAIL {CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7f1f123-d733-4f02-8889-d5115fd8117f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 6.2: Operation History Analysis\n",
    "\n",
    "**Objective:** Deeper analysis of history and operation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83a5161d-4a4d-4595-958b-97ceab526b18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# History with additional metrics\n",
    "history_df = spark.sql(f\"DESCRIBE HISTORY {CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "\n",
    "display(\n",
    "    history_df.select(\n",
    "        \"version\", \n",
    "        \"timestamp\", \n",
    "        \"operation\", \n",
    "        \"operationMetrics.numTargetRowsInserted\",\n",
    "        \"operationMetrics.numTargetRowsUpdated\",\n",
    "        \"operationMetrics.numTargetRowsDeleted\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeb4ea8e-c231-4d53-ac61-0270295df465",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 6.3: Delta Log Internals (Deep Dive)\n",
    "\n",
    "**Objective:** Understanding how Delta Lake ensures ACID by looking \"under the hood\" at JSON files in `_delta_log`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fe0293e-ae4d-4227-80d8-e1c6feb05563",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Get table path and _delta_log:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cccb6f31-7937-48f9-915f-9aab0aac1ab7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get table path\n",
    "table_path = spark.sql(f\"DESCRIBE DETAIL {CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "delta_log_path = f\"{table_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddf8e731-d94f-4529-bf9d-9424957324a1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"location\":1500},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764068621478}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dbf4f1f-fe60-4368-892c-28b7412a3320",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Display files in _delta_log:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b571a7c8-9a39-45ec-afb0-70e41304cdb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Below is a preview of the content of the last transaction JSON file.\n",
    "It contains metadata about operations, such as:\n",
    "- `add`: adding a new Parquet file with data.\n",
    "- `remove`: logical deletion of a file (e.g., during DELETE or OPTIMIZE operation).\n",
    "- `commitInfo`: metadata about the transaction itself (who, when, what operation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbe25cc7-562d-4e34-bb80-c90076b52e28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# View Delta table history using SQL\n",
    "display(spark.sql(f\"DESCRIBE HISTORY {CATALOG}.{BRONZE_SCHEMA}.customers_delta\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43df1ad8-f00a-4cd7-9523-19ab5fe37b08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Analysis of the last transaction file (JSON):**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdcc63dc-5ddb-43bc-a6c9-d66bd06a3e9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 7: Optimization (Intro)\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Delta Lake offers a range of optimization mechanisms. In this notebook, we will focus on the basic **OPTIMIZE** operation (file compaction) and **VACUUM** (cleaning).\n",
    "\n",
    "> **Deep Dive:** Advanced techniques such as **ZORDER BY**, **Partitioning**, and **Liquid Clustering** are discussed in detail in notebook **05_optimization_best_practices.ipynb**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f9c4f15-246d-45c6-a0bc-1e30045a1480",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 7.1: OPTIMIZE (File Compaction)\n",
    "\n",
    "**Objective:** Compacting small files (small files problem) into larger ones, which improves read performance.\n",
    "We can optionally add the `ZORDER BY` clause (discussed in notebook 05) to additionally sort the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1174aa1f-f82a-41ef-b546-03926982229b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Execute file compaction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc6a4dca-4276-4b31-8204-b32d76b8f008",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# OPTIMIZE table\n",
    "optimize_result = spark.sql(f\"\"\"\n",
    "    OPTIMIZE {CATALOG}.{BRONZE_SCHEMA}.customers_delta\n",
    "\"\"\")\n",
    "\n",
    "display(optimize_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc7cccdb-5e2c-4660-873c-350dcb6a7e5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 7.3: Liquid Clustering (Mention)\n",
    "\n",
    "**Modern Alternative:**\n",
    "Databricks introduced **Liquid Clustering** - a new technique that replaces traditional partitioning and ZORDER.\n",
    "Liquid Clustering automatically manages data layout, adapting to query patterns.\n",
    "\n",
    "> **Deep Dive:** Detailed discussion and examples of Liquid Clustering can be found in notebook **05_optimization_best_practices.ipynb**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a90350c3-592c-4c03-9bb7-92ba92bb9619",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 7.2: VACUUM operation\n",
    "\n",
    "**Objective:** Removing old files (older than retention period) that are no longer needed for Time Travel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bac9e7c-da95-4f09-ab92-e02fb4688889",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Disable retention check (demo only):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11f438fa-d6ee-451c-b027-1f00e59a663e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# VACUUM - remove files older than 0 hours (demo only!)\n",
    "# In production: default 7 days, minimum 0 hours with flag\n",
    "spark.sql(\"SET spark.databricks.delta.retentionDurationCheck.enabled = false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c2c177c-7137-417b-b9bc-699ac095e39c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Run VACUUM (remove old files):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd0c5a27-f19a-4dcf-b4ec-b42bc4fefcdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vacuum_result = spark.sql(f\"\"\"\n",
    "    VACUUM {CATALOG}.{BRONZE_SCHEMA}.customers_delta RETAIN 0 HOURS\n",
    "\"\"\")\n",
    "\n",
    "display(vacuum_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d500c2a-b140-4774-8fd8-f6a8f72a61b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 8: Change Data Feed\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Change Data Feed (CDF) is a Delta Lake feature enabling tracking of all changes in a table. Every INSERT, UPDATE, DELETE operation is recorded with additional metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e75dfe2b-5e95-40ec-a93e-4877d0aa8bde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 8.1: Enabling Change Data Feed\n",
    "\n",
    "**Objective:** Activating CDF for an existing table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "639ae751-8b2f-4b3b-a095-7ab6b2a93aa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Enable Change Data Feed:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c751f7dd-54c7-4b24-9e5a-7fc3f832e94b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enable Change Data Feed\n",
    "spark.sql(f\"\"\"\n",
    "    ALTER TABLE {CATALOG}.{BRONZE_SCHEMA}.customers_delta \n",
    "    SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96151b57-02c5-4570-ad7e-fb8c34e54a13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 8.2: Generating changes for CDF\n",
    "\n",
    "**Objective:** Executing operations that will be tracked by CDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4960a15-1701-4643-9d18-cfc296f18be1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Insert new record (INSERT):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "224323c5-701f-46c8-b900-7037e6362131",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Make more changes after enabling CDF\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO {CATALOG}.{BRONZE_SCHEMA}.customers_delta\n",
    "    (customer_id, first_name, last_name, email, phone, city, state, country, registration_date, customer_segment, customer_tier)\n",
    "    VALUES ('CUST040001', 'CDF', 'TestCustomer', 'cdf@example.com', '+48 555 555 555', 'Szczecin', 'ZP', 'Poland', '2023-12-20', 'Basic', 'Bronze')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bf311b0-bc4a-448a-ab77-7304902cfc10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Update record (UPDATE):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46fa0519-a710-4a32-926b-21abf892d711",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    UPDATE {CATALOG}.{BRONZE_SCHEMA}.customers_delta\n",
    "    SET customer_tier = 'Gold'\n",
    "    WHERE customer_id = 'CUST040001'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43687b9b-6070-4ea4-9e45-29e3dcf4990e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 8.3: Reading Change Data Feed\n",
    "\n",
    "**Objective:** Analysis of all changes recorded by CDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57d10350-d356-4a9e-8528-43dc4f5d6fde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check CDF properties\n",
    "table_properties = spark.sql(f\"SHOW TBLPROPERTIES {CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "cdf_enabled = table_properties.filter(F.col(\"key\") == \"delta.enableChangeDataFeed\").count() > 0\n",
    "\n",
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"Change Data Feed enabled\", cdf_enabled)\n",
    "    ], [\"property\", \"status\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97e4f5d7-e05e-4add-98f3-82b949e78d68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Read changes (CDF):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa382740-a553-4a6c-b279-d7486d9d11ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Batch read change data feed from specific version\n",
    "# First let's check current version\n",
    "current_version = spark.sql(f\"DESCRIBE HISTORY {CATALOG}.{BRONZE_SCHEMA}.customers_delta\").select(\"version\").first()[0]\n",
    "cdf_start_version = max(0, current_version - 3)  # Last 3 versions\n",
    "\n",
    "display(current_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccada4af-f023-443e-b317-f4ed7774cdcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "changes_batch = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"readChangeFeed\", \"true\") \\\n",
    "    .option(\"startingVersion\", cdf_start_version) \\\n",
    "    .table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "242df922-463b-4450-9fef-4af54d0536ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(changes_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0689ff28-d0a9-4932-8774-1bcdb63386c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    changes_batch.select(\n",
    "        \"customer_id\", \"first_name\", \"last_name\", \"customer_tier\", \n",
    "        \"_change_type\", \"_commit_version\", \"_commit_timestamp\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a666f44b-a6a5-4638-b9e1-58ece1a432d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Validation and Verification\n",
    "\n",
    "### Checklist - What you should achieve:\n",
    "- [ ] Delta table created with automatic schema enforcement\n",
    "- [ ] Schema evolution - customer_tier column added\n",
    "- [ ] Time Travel queries work for previous versions\n",
    "- [ ] CRUD operations (INSERT, UPDATE, DELETE) executed correctly\n",
    "- [ ] MERGE INTO implemented with upsert logic\n",
    "- [ ] Optimization OPTIMIZE and ZORDER applied\n",
    "- [ ] Change Data Feed enabled and recording changes\n",
    "\n",
    "### Verification commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9abaf3af-9908-419b-b1b5-c3600a0877b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Best Practices\n",
    "\n",
    "### Performance:\n",
    "- Use ZORDER BY for columns frequently appearing in WHERE clauses\n",
    "- Regularly run OPTIMIZE for small files compaction\n",
    "- Partitioning only for very large tables (TB+) with skewed data\n",
    "\n",
    "### Code Quality:\n",
    "- Always use explicit schema instead of inferSchema in production\n",
    "- Implement schema evolution strategy for backward compatibility\n",
    "- Use MERGE INTO instead of separate DELETE + INSERT operations\n",
    "\n",
    "### Data Quality:\n",
    "- Enable Change Data Feed for audit trails and compliance\n",
    "- Regular backup via Time Travel snapshots\n",
    "- Implement data validation rules in Delta constraints\n",
    "\n",
    "### Governance:\n",
    "- Set appropriate retention periods for compliance requirements\n",
    "- Use Unity Catalog permissions for row/column level security\n",
    "- Document schema changes and business logic in table comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b73ca7f6-db6e-4976-995a-aba8499e7337",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Problem 1: Schema enforcement error\n",
    "**Symptoms:**\n",
    "- AnalysisException during INSERT/MERGE with incompatible schema\n",
    "- \"Cannot write incompatible datatype\" message\n",
    "\n",
    "**Solution:**\n",
    "```python\n",
    "# Use mergeSchema option for schema evolution\n",
    "df.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\")\n",
    "```\n",
    "\n",
    "### Problem 2: Time Travel - version not found\n",
    "**Symptoms:**\n",
    "File not found for specific version after VACUUM\n",
    "\n",
    "**Solution:**\n",
    "Check retention period and available versions via DESCRIBE HISTORY\n",
    "\n",
    "### Problem 3: VACUUM removes too many files\n",
    "**Symptoms:** Time Travel queries fail after VACUUM\n",
    "\n",
    "**Solution:**\n",
    "Set appropriate retention period (default 7 days minimum)\n",
    "\n",
    "### Debugging tips:\n",
    "- Use `DESCRIBE HISTORY` to understand table operations\n",
    "- Check `DESCRIBE DETAIL` for file metadata\n",
    "- Verify table properties via `SHOW TBLPROPERTIES`\n",
    "- Monitor `_delta_log/` folder for troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8c2ffac-8f8c-4971-8de8-7f49727847b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "### What has been achieved:\n",
    "- Demonstration of Delta Lake ACID properties and schema enforcement\n",
    "- Hands-on Schema Evolution with automatic column addition\n",
    "- Time Travel queries for historical data access\n",
    "- Complete CRUD operations (CREATE, READ, UPDATE, DELETE)\n",
    "- Advanced MERGE INTO for upsert scenarios\n",
    "- Performance optimization with OPTIMIZE, ZORDER, VACUUM\n",
    "- Change Data Feed for comprehensive audit trails\n",
    "\n",
    "### Key Takeaways:\n",
    "1. **Delta Lake = Data Lake + ACID**: Combines Data Lake flexibility with transactional reliability\n",
    "2. **Schema Evolution safely**: Additive changes are automatic, breaking changes require planning\n",
    "3. **Time Travel + Copy-on-Write**: Every version is preserved, enabling rollback and audit\n",
    "\n",
    "### Quick Reference - Key Commands:\n",
    "\n",
    "| Operation | PySpark | SQL |\n",
    "|----------|---------|-----|\n",
    "| Create Delta Table | `df.write.format(\"delta\").saveAsTable()` | `CREATE TABLE USING DELTA` |\n",
    "| Time Travel | `spark.read.format(\"delta\").option(\"versionAsOf\", 1)` | `SELECT * FROM table VERSION AS OF 1` |\n",
    "| MERGE | `DeltaTable.forName().merge().execute()` | `MERGE INTO target USING source` |\n",
    "| Optimize | N/A | `OPTIMIZE table ZORDER BY col` |\n",
    "| History | N/A | `DESCRIBE HISTORY table` |\n",
    "\n",
    "### Additional Resources:\n",
    "- [Delta Lake Documentation](https://docs.delta.io/)\n",
    "- [Delta Lake Best Practices](https://docs.databricks.com/delta/best-practices.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "033141e2-8e5d-4719-9736-457b8b626118",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Resource Cleanup\n",
    "\n",
    "Clean up resources created during the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e44c0a94-4fc6-4db6-8401-70759423a3c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optional test resource cleanup\n",
    "# NOTE: Run only if you want to delete all created data\n",
    "\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "# spark.sql(\"DROP VIEW IF EXISTS customer_updates\")\n",
    "# spark.catalog.clearCache()\n",
    "\n",
    "# display(spark.createDataFrame([(\"Resources have been cleaned\", \"\")], [\"status\", \"result\"]))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_delta_lake_operations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
