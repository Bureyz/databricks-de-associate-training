{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "657047e2-fd5c-4e60-9173-07d93b535bad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Introduction to Databricks Lakehouse\n",
    "\n",
    "## The Story: Why Are We Here?\n",
    "\n",
    "You're a **Data Engineer at a growing e-commerce company**. The business has:\n",
    "- **10,000+ customers** across multiple regions\n",
    "- **50,000+ orders** per month, growing 30% YoY\n",
    "- **500+ products** with complex pricing and inventory\n",
    "\n",
    "**The Problem:**\n",
    "- Data lives in CSV files, JSON APIs, and legacy databases\n",
    "- Data Scientists wait 2 days to get clean data\n",
    "- BI dashboards show data from yesterday (T-1 latency)\n",
    "- Nobody knows which version of \"customer data\" is correct\n",
    "- GDPR compliance is a nightmare - no audit trail\n",
    "\n",
    "**Your Mission:**\n",
    "Build a modern data platform that provides:\n",
    "- Real-time data access (minutes, not days)\n",
    "- Single source of truth (no more \"which spreadsheet is correct?\")\n",
    "- Full audit trail and governance\n",
    "- Self-service analytics for business users\n",
    "\n",
    "**The Solution:** Databricks Lakehouse with Unity Catalog\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn (and Why It Matters)\n",
    "\n",
    "| Topic | Why It Matters for You |\n",
    "|-------|------------------------|\n",
    "| Lakehouse Architecture | Understand trade-offs vs. traditional DW + Lake |\n",
    "| Unity Catalog | Implement governance without slowing down |\n",
    "| Compute Options | Choose right cluster type, control costs |\n",
    "| Delta Lake | ACID transactions on files - no more corrupt data |\n",
    "\n",
    "**Target Audience:** Experienced Data Engineers who want to understand *when* and *why* to use these tools, not just *how*.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a99c61e6-53bd-409e-847f-46e32bc931fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Context and Requirements\n",
    "\n",
    "- **Training Day**: Day 1 - Fundamentals & Exploration\n",
    "- **Notebook Type**: Demo\n",
    "- **Technical Requirements**:\n",
    "  - Databricks Runtime 13.0+ (recommended: 14.3 LTS)\n",
    "  - Unity Catalog enabled\n",
    "  - Permissions: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Cluster: Standard with 2-4 workers\n",
    "- **Duration**: 20 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56333524-0ce0-4ecc-9fd7-22ea9d849be1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 1: Lakehouse Architecture - The \"Why\"\n",
    "\n",
    "### The Evolution of Data Architectures\n",
    "\n",
    "**Generation 1: Data Warehouse (1990s-2000s)**\n",
    "- Teradata, Oracle, SQL Server\n",
    "- Structured data only, expensive storage\n",
    "- Great for BI, terrible for ML/unstructured data\n",
    "\n",
    "**Generation 2: Data Lake (2010s)**\n",
    "- Hadoop, S3, ADLS\n",
    "- Cheap storage, any format\n",
    "- Problem: \"Data Swamp\" - no governance, no ACID, unreliable\n",
    "\n",
    "**Generation 3: Lakehouse (2020s)**\n",
    "- Delta Lake, Iceberg, Hudi\n",
    "- Best of both: cheap storage + ACID + governance\n",
    "- Single platform for BI, ML, streaming\n",
    "\n",
    "### Decision Framework: When to Use What?\n",
    "\n",
    "| Scenario | Recommended Architecture | Why |\n",
    "|----------|--------------------------|-----|\n",
    "| Regulatory industry (banking, healthcare) | Lakehouse with strong governance | Audit trails, lineage, access control |\n",
    "| ML-heavy workloads | Lakehouse | Native support for feature stores, MLflow |\n",
    "| Legacy BI migration | Start with DW, migrate to Lakehouse | Lower risk, gradual transition |\n",
    "| Startup with greenfield | Lakehouse from day 1 | Future-proof, lower TCO |\n",
    "| Real-time + batch hybrid | Lakehouse with Delta | Unified streaming and batch |\n",
    "\n",
    "### Cost Comparison (Rough Estimates)\n",
    "\n",
    "| Component | Traditional (DW + Lake) | Lakehouse |\n",
    "|-----------|------------------------|-----------|\n",
    "| Storage | $$$$ (2x for Lake + DW) | $$ (single copy) |\n",
    "| ETL Compute | $$$ (sync jobs) | $$ (no sync needed) |\n",
    "| Governance Tools | $$$ (separate tools) | $ (built-in) |\n",
    "| Latency | Hours (ETL sync) | Minutes (direct access) |\n",
    "| **Total TCO** | **Higher** | **30-50% lower** |\n",
    "\n",
    "*Note: Actual costs depend on workload. Run POC with your data to validate.*\n",
    "\n",
    "### Alternatives to Databricks Lakehouse\n",
    "\n",
    "| Alternative | Pros | Cons | When to Choose |\n",
    "|-------------|------|------|----------------|\n",
    "| **Snowflake** | Mature, great SQL | Separate from ML, vendor lock-in | Pure SQL/BI workloads |\n",
    "| **BigQuery** | Serverless, cheap storage | GCP-only, less flexible | GCP shop, ad-hoc analytics |\n",
    "| **Spark + Iceberg on K8s** | Open source, no vendor | Complex ops, no unified governance | Strong DevOps team, cost-sensitive |\n",
    "| **Databricks** | Unified platform, strong ML/AI | Premium pricing | ML + Analytics + Streaming |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a45d94d-3139-49cf-bef1-760ae552b359",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Per-user Isolation\n",
    "\n",
    "Run the initialization script for per-user catalog and schema isolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f4d94df-6da8-468c-84ce-e4601a325e0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a822578-04ff-426c-91d0-619b23bf3f76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuration\n",
    "\n",
    "Import libraries and set environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f74fb0b1-9b23-4d51-b1fb-b12913824fc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import re\n",
    "\n",
    "# Display user context (variables from 00_setup)\n",
    "print(\"=== User Context ===\")\n",
    "print(f\"Catalog: {CATALOG}\")\n",
    "print(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "print(f\"Schema Silver: {SILVER_SCHEMA}\")\n",
    "print(f\"Schema Gold: {GOLD_SCHEMA}\")\n",
    "print(f\"User: {raw_user}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6be0ad0d-871c-49b0-ae7c-b5423e4ab8a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set catalog as default\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8883389d-dfce-4ac3-9beb-dee233de75b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 1: Lakehouse Architecture Concept\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Lakehouse is a modern data architecture that combines the benefits of Data Lake (low storage cost, support for various formats) with the benefits of Data Warehouse (reliability, SQL query performance, transaction management). A key element is Delta Lake - a metadata layer ensuring ACID transactions on Parquet files.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **ACID Transactions**: Atomicity, Consistency, Isolation, Durability - guarantees ensuring reliability of data operations\n",
    "- **Delta Lake**: Open-source storage layer ensuring transactionality on files in Data Lake\n",
    "- **Unity Catalog**: Unified system for data, metadata, and access control management\n",
    "\n",
    "**Practical Application:**\n",
    "- Elimination of data duplication between analytical and operational systems\n",
    "- Simultaneous support for BI, Data Science, and Machine Learning\n",
    "- Reduction of infrastructure and maintenance costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad256fd2-ae90-4d9c-8681-be489429f78b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Comparison of Traditional Architecture vs Lakehouse\n",
    "\n",
    "**Objective:** Visualize differences between traditional approach (Data Lake + Data Warehouse) and Lakehouse.\n",
    "\n",
    "**Traditional Architecture:**\n",
    "```\n",
    "Raw Data → Data Lake (S3/ADLS) → ETL Process → Data Warehouse (Snowflake/Redshift) → BI Tools\n",
    "                                ↓\n",
    "                         ML/Data Science (separate copy)\n",
    "```\n",
    "\n",
    "**Lakehouse Architecture:**\n",
    "```\n",
    "Raw Data → Delta Lake (single source of truth) → BI Tools + ML + Real-time Analytics\n",
    "```\n",
    "\n",
    "**Lakehouse Benefits:**\n",
    "- Single copy of data (single source of truth)\n",
    "- Lower storage costs\n",
    "- Elimination of synchronization latency\n",
    "- Common governance for all use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "604dbe6a-70c7-4147-8713-f922e3d88de4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 2: Databricks Platform Elements\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "The Databricks platform consists of several key components that together create a complete environment for working with data in the Lakehouse architecture.\n",
    "\n",
    "**Key Components:**\n",
    "- **Workspace**: Working environment containing notebooks, experiments, folders, and resources\n",
    "- **Catalog Explorer**: Interface for managing catalogs, schemas, tables, and views\n",
    "- **Git Folders (formerly Repos)**: Git integration for versioning notebooks and code\n",
    "- **Volumes**: Management of unstructured files (images, models, artifacts)\n",
    "- **DBFS (Databricks File System)**: Virtual file system over cloud storage\n",
    "\n",
    "**Practical Application:**\n",
    "- Workspace organizes projects and team collaboration\n",
    "- Catalog Explorer enables data exploration and governance\n",
    "- Git Folders integrates development workflow with Git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95bf774f-a63e-4432-ba50-c8da3d316631",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 2.1: Workspace Exploration\n",
    "\n",
    "**Objective:** Familiarize with Databricks Workspace interface\n",
    "\n",
    "**Workspace Elements:**\n",
    "1. **Sidebar** (left side):\n",
    "   - Workspace: Folders and notebooks\n",
    "   - Git Folders: Git Integration\n",
    "   - Compute: Cluster management\n",
    "   - Workflows: Databricks Jobs\n",
    "   - Catalog: Unity Catalog explorer\n",
    "\n",
    "2. **Main Panel**: Notebook editor or details view\n",
    "\n",
    "3. **Top Bar**: Quick access to compute, account, help\n",
    "\n",
    "**Navigation Instructions:**\n",
    "- Use the left menu to switch between sections\n",
    "- In the Catalog section, you can browse catalogs, schemas, and tables\n",
    "- In the Compute section, you manage Spark clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef0fa899-84b5-4b93-835c-f0afe15d4d57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 2.2: Catalog Explorer - Unity Catalog Structure\n",
    "\n",
    "**Objective:** Understand object hierarchy in Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7d181c5-b49b-4c44-bad0-d18ae5988b31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display current catalog and schema\n",
    "current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "current_schema = spark.sql(\"SELECT current_schema()\").collect()[0][0]\n",
    "\n",
    "print(f\"Current catalog: {current_catalog}\")\n",
    "print(f\"Current schema: {current_schema}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0069c11c-67d5-4a54-a1a7-fbb67f5d80e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Unity Catalog Hierarchy:**\n",
    "\n",
    "```\n",
    "Metastore\n",
    "  ├── Catalog (e.g., main, dev, prod)\n",
    "  │   ├── Schema/Database (e.g., bronze, silver, gold)\n",
    "  │   │   ├── Tables (Delta Tables)\n",
    "  │   │   ├── Views (SQL Views)\n",
    "  │   │   ├── Functions (UDFs)\n",
    "  │   │   └── Volumes (for files)\n",
    "```\n",
    "\n",
    "Unity Catalog organizes data in three levels: `catalog.schema.table`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab97de26-6e2a-4b67-87fb-7c9f012c65ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "Unity Catalog organizes data in hierarchy: Metastore → Catalog → Schema → Objects (Tables/Views/Functions). This structure enables:\n",
    "- Logical separation of environments (dev/test/prod)\n",
    "- Granular access control at each level\n",
    "- Easy management of namespaces and project isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a81256c7-e5d6-4494-af87-36671b2da3ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 2.3: Browsing Catalogs and Schemas\n",
    "\n",
    "**Objective:** Programmatic listing of objects in Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b4a711e-7765-4839-87bf-be0f1cc17263",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of all catalogs available to the user\n",
    "catalogs_df = spark.sql(\"SHOW CATALOGS\")\n",
    "display(catalogs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1acf7186-a6c5-4fe8-a5e2-3f8722729d73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of schemas in the current catalog\n",
    "schemas_df = spark.sql(f\"SHOW SCHEMAS IN {CATALOG}\")\n",
    "display(schemas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b47526a4-c364-44d8-9df1-c19945d0c6eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "Commands `SHOW CATALOGS` and `SHOW SCHEMAS` allow exploring Unity Catalog structure. Each user sees only objects they have permissions for. Per-user isolation (as in our `00_setup`) ensures each training participant has their own workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b04ffdba-2d7b-4363-8c15-9312d59f107a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.4 Git Folders (Repos) and Git Integration\n",
    "\n",
    "In practice, working with code in Databricks should be based on **Git Folders** (formerly Repos), not single, orphaned notebooks in Workspace.\n",
    "\n",
    "Typical workflow:\n",
    "\n",
    "1. **Create Git Folder** in Databricks: `Workspace → Git Folders → Add Repo`.\n",
    "2. **Connect to Git** (GitHub / Azure DevOps / other).\n",
    "3. Work on **feature branches** (e.g., `feature/cleaning-module`).\n",
    "4. Regularly:\n",
    "   - commit and push changes from Databricks to remote repo,\n",
    "   - create PR and merge to main/dev.\n",
    "\n",
    "Best Practices:\n",
    "\n",
    "- One repo per project/domain (e.g., `databricks-dea-training-kion`).\n",
    "- Do not work in **Workspace root** – always in **Git Folders**.\n",
    "- Training notebooks, test data, and README can be in one repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3b97f4c-f2d7-476b-876c-03e662fc8258",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.5 Volumes vs DBFS – where to store files?\n",
    "\n",
    "In new workspaces based on Unity Catalog, the preferred place for storing files are **Volumes**.\n",
    "\n",
    "- `dbfs:/` is treated as a **legacy** layer or auxiliary area.\n",
    "- `volume://catalog.schema.volume_name` is a fully managed, UC-controlled data area (permissions, audit, lineage).\n",
    "\n",
    "Volume Definition Example (SQL):\n",
    "\n",
    "```sql\n",
    "CREATE VOLUME IF NOT EXISTS ${catalog}.${schema}.training_volume\n",
    "COMMENT 'Workspace for training purposes';\n",
    "```\n",
    "\n",
    "Usage Example in PySpark:\n",
    "\n",
    "```python\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "\n",
    "volume_path = f\"volume://{catalog}.{schema}.training_volume\"\n",
    "display(dbutils.fs.ls(volume_path))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59fe7bbe-41c2-4fc0-8247-61dbd858854f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.6 Serverless SQL / SQL Warehouse – when to use instead of notebook cluster?\n",
    "\n",
    "Besides notebook clusters, Databricks offers **SQL Warehouse (serverless)** – SQL query engine optimized for BI and ad-hoc analytics.\n",
    "\n",
    "When to use:\n",
    "- Reporting in Power BI / other BI tools.\n",
    "- Business analysts / power users working mainly in SQL.\n",
    "- Interactive dashboards and ad-hoc queries to **Gold** layer.\n",
    "\n",
    "Differences from all-purpose cluster:\n",
    "- Billing based on **DBU SQL** (different rates).\n",
    "- Automatic provisioning / scaling.\n",
    "- Isolation of BI workload from engineering clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0269415-6dda-473d-a771-236303dbe4c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 3: Compute - Making Smart Choices\n",
    "\n",
    "### The Real Question: How Much Will This Cost?\n",
    "\n",
    "As a Data Engineer, you'll be asked: *\"Why is our Databricks bill so high?\"*\n",
    "\n",
    "Understanding compute options is essential for cost control.\n",
    "\n",
    "### Compute Options Comparison\n",
    "\n",
    "| Type | Startup Time | Cost Model | Best For |\n",
    "|------|--------------|------------|----------|\n",
    "| **All-Purpose Cluster** | 3-5 min | Per-minute (running) | Interactive development, exploration |\n",
    "| **Job Cluster** | 3-5 min | Per-minute (only during job) | Scheduled production jobs |\n",
    "| **Serverless (Preview)** | <10 sec | Per-query DBUs | Ad-hoc queries, variable workloads |\n",
    "| **SQL Warehouse** | 0 (Serverless) or 3-5 min | Per-query DBUs | BI tools, SQL analysts |\n",
    "\n",
    "### Cost Optimization Strategies\n",
    "\n",
    "**1. Right-size clusters:**\n",
    "- Development: 2-4 workers, smallest instance type\n",
    "- Production: Autoscaling 2-10 workers based on workload\n",
    "\n",
    "**2. Use Spot/Preemptible instances:**\n",
    "- 60-80% cost savings for workers\n",
    "- Driver on on-demand (stability)\n",
    "- Trade-off: Job may be interrupted\n",
    "\n",
    "**3. Photon Engine:**\n",
    "- 2-3x faster for aggregations/joins\n",
    "- ~2x DBU cost, but finishes faster = often cheaper\n",
    "- Enable for: large scans, aggregations, joins\n",
    "- Skip for: simple transformations, ML training\n",
    "\n",
    "**4. Cluster policies:**\n",
    "- Enforce maximum worker count\n",
    "- Require autoscaling\n",
    "- Set auto-termination (e.g., 30 min idle)\n",
    "\n",
    "### Decision Tree: Which Compute to Use?\n",
    "\n",
    "```\n",
    "Is it a scheduled production job?\n",
    "├── YES → Job Cluster (ephemeral, cost-efficient)\n",
    "└── NO → Is it interactive development?\n",
    "         ├── YES → All-Purpose Cluster (shared, always-on during work hours)\n",
    "         └── NO → Is it SQL/BI query?\n",
    "                  ├── YES → SQL Warehouse (optimized for SQL, connects to BI tools)\n",
    "                  └── NO → Serverless Notebook (instant start, pay-per-use)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "677ff10d-6ff8-4a96-b12f-5e84aba7f563",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 3.1: Cluster Information\n",
    "\n",
    "**Objective:** Check current cluster configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d8c329d-eb70-4c58-a701-ad269466d321",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Spark Context Information\n",
    "spark_version = spark.version\n",
    "app_name = spark.sparkContext.appName\n",
    "master = spark.sparkContext.master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92b57d4d-6aa9-4bdf-9bea-0b239e059ad5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "The `spark` object is the entry point to Spark functionality. It contains information about the application version, name, and master (usually `local[*]` in single-node mode or cluster manager address)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e683c880-a33e-42c3-a526-c6da591bc240",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Spark Version: {spark_version}\")\n",
    "print(f\"Application: {app_name}\")\n",
    "print(f\"Master: {master}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31fa54e2-0f80-4c19-b604-13306507efde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Number of executors\n",
    "num_executors = len(spark.sparkContext._jsc.sc().statusTracker().getExecutorInfos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91374905-ca6b-46c1-86da-c94a5549cede",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Runtime version\n",
    "dbr_version = spark.conf.get(\"spark.databricks.clusterUsageTags.sparkVersion\", \"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7de0918-1b85-4f28-ac88-75b62996290e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Photon enabled?\n",
    "photon_enabled = spark.conf.get(\"spark.databricks.photon.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8caaae99-cbbe-475a-86d3-29e73437c53c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "This code shows basic information about the Spark cluster. The number of executors (workers) can change dynamically with autoscaling enabled. Photon Engine, if enabled, automatically accelerates SQL queries and DataFrame operations without code changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc6efe33-547e-46aa-9d14-224669f28b27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 3.2: Cluster Configuration Best Practices\n",
    "\n",
    "**Objective:** Learn recommendations for different use cases\n",
    "\n",
    "**For Development (All-Purpose Cluster):**\n",
    "- Runtime: 14.3 LTS (Long Term Support)\n",
    "- Workers: 2-4 (autoscaling 2-8 for larger projects)\n",
    "- Node type: Standard DS3_v2 (Azure) or m5.xlarge (AWS)\n",
    "- Photon: Enabled\n",
    "- Spot instances: No (for stability)\n",
    "\n",
    "**For Production (Job Cluster):**\n",
    "- Runtime: 14.3 LTS\n",
    "- Workers: autoscaling 2-20 (depending on load)\n",
    "- Node type: Memory-optimized (DS4_v2, m5.2xlarge)\n",
    "- Photon: Enabled\n",
    "- Spot instances: Yes (60-80% workers)\n",
    "- Auto-termination: 10 minutes of inactivity\n",
    "\n",
    "**For ML Workloads:**\n",
    "- Runtime: 14.3 ML (includes ML libraries)\n",
    "- Workers: GPU-enabled (NC6s_v3, p3.2xlarge)\n",
    "- Single-node mode for prototyping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "566b74f2-66f4-4f1a-b5c9-15dd70cafa28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 4: Magic Commands in Notebooks\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Databricks notebooks support magic commands - special commands starting with `%` that control the cell language or execute system operations. Magic commands allow mixing languages in a single notebook and interacting with the file system.\n",
    "\n",
    "**Available magic commands:**\n",
    "- **%python**: Python cell (default)\n",
    "- **%sql**: SQL cell\n",
    "- **%scala**: Scala cell\n",
    "- **%r**: R cell\n",
    "- **%md**: Markdown cell (documentation)\n",
    "- **%fs**: File system operations (DBFS)\n",
    "- **%sh**: Shell commands\n",
    "- **%run**: Run another notebook (like import)\n",
    "- **%pip**: Install Python libraries (notebook-scoped)\n",
    "- **%skip**: Skips cell execution\n",
    "\n",
    "**Practical Application:**\n",
    "- Combining SQL and Python in one workflow\n",
    "- Inline documentation with Markdown\n",
    "- File operations with %fs\n",
    "- Code modularization with %run\n",
    "- Dependency management with %pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14084b35-dd2b-4bc3-abcb-b170d0e48117",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Monitoring and Logging – Where to Look for Problems?\n",
    "\n",
    "When working with clusters and Jobs, it's worth knowing the basic places where we look for diagnostic information:\n",
    "\n",
    "- **Cluster → Event log** – cluster start/stop, autoscaling, infrastructure errors.\n",
    "- **Spark UI** (Jobs, SQL, Storage, Environment tabs) – execution plan, shuffling, task-level errors.\n",
    "- **Driver / Executor logs** – detailed Python/Scala stacktraces.\n",
    "- **Job Run page** – status of individual tasks, retries, execution time.\n",
    "\n",
    "Best Practices:\n",
    "- For longer pipelines, always check Spark UI (SQL/Jobs section).\n",
    "- Log critical application logs to Delta tables / storage, not just cluster logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15a320a9-fd52-454d-9a08-5c2074746fef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 4.1: SQL Magic Command Demonstration\n",
    "\n",
    "**Objective:** Execute SQL query directly in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cdd5b6e-ce7d-4a23-9056-467163b1657a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SQL magic command allows writing pure SQL without Python wrapper\n",
    "\n",
    "SELECT \n",
    "  current_catalog() as catalog,\n",
    "  current_schema() as schema,\n",
    "  current_user() as user,\n",
    "  current_timestamp() as timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72219dd5-27a3-4b77-8ed5-a3d12215b52f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "The `%sql` magic command changes the cell language to SQL. Results are automatically displayed as a table. SQL in Databricks is full Spark SQL with Delta Lake extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83c62a01-def3-4a05-9cee-251b9683b774",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 4.2: File System Operations with fs\n",
    "\n",
    "**Objective:** Explore DBFS file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "132863d8-0852-49fc-bb62-3c4d3a87e1ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List root directories in DBFS\n",
    "dbutils.fs.ls(\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "760a718c-6473-4233-83ec-526d06ec733e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "DBFS (Databricks File System) is an abstraction over cloud storage (S3, ADLS, GCS). The `%fs` command or `dbutils.fs` allows file operations. In Unity Catalog, it is recommended to use **Volumes** instead of DBFS for better governance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cda7429b-9d22-4a72-99f8-e546014c7529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 4.3: Mixing Languages - Python and SQL\n",
    "\n",
    "**Objective:** Demonstrate seamless transition between Python and SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "039f22a9-d548-4c7b-9532-6ced5708a3aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Python: Raw data definition\n",
    "data = [\n",
    "    (1, \"Alice\", \"Engineering\", 95000),\n",
    "    (2, \"Bob\", \"Sales\", 75000),\n",
    "    (3, \"Charlie\", \"Engineering\", 105000),\n",
    "    (4, \"Diana\", \"Marketing\", 68000),\n",
    "    (5, \"Eve\", \"Engineering\", 98000)\n",
    "]\n",
    "\n",
    "# Schema definition\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"department\", StringType(), False),\n",
    "    StructField(\"salary\", IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "289d1ce0-479b-47f5-9e30-50c09b90c688",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e34557e1-9ac9-4f3a-88c9-3f40d3c8426f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register as temp view for SQL access\n",
    "df.createOrReplaceTempView(\"employees_temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4071b2d3-e8f0-4027-a850-c18c2fc7b71a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Temp view created: employees_temp**\n",
    "\n",
    "Temp view allows access to DataFrame from SQL cells in the same notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f59fce2a-0196-4145-8f6c-24d1f5d70b13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SQL: Aggregation on Python data\n",
    "\n",
    "SELECT \n",
    "  department,\n",
    "  COUNT(*) as employee_count,\n",
    "  AVG(salary) as avg_salary,\n",
    "  MAX(salary) as max_salary\n",
    "FROM employees_temp\n",
    "GROUP BY department\n",
    "ORDER BY avg_salary DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9ac5da9-74e5-4025-babf-c3310d15b209",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "This example shows the power of Databricks notebooks: data preparation in Python (convenient API, libraries), then analysis in SQL (declarative queries, clarity). Temp views are visible throughout the notebook regardless of cell language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ef0c1cd-1d7c-4fad-910b-8087ab6fc581",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 4.4: Library Management (%pip)\n",
    "\n",
    "In Databricks, we can install notebook-scoped Python libraries using the `%pip` command. This is the recommended approach instead of global installation on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b998dba-508e-4ee7-a9a5-96f8419efe33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install emoji library\n",
    "%pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "731f59a0-93e7-4245-b94e-504b7b0c5bfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import emoji\n",
    "print(emoji.emojize('Databricks is :fire:'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6662cc7-46a9-4f65-915a-18079ed97f68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 4.5: Databricks Assistant (AI)\n",
    "\n",
    "In 2025, coding work is assisted by AI. Databricks has a built-in assistant (**Databricks Assistant**) that is context-aware of your data (knows table schemas in Unity Catalog!).\n",
    "\n",
    "**How to use?**\n",
    "1. Shortcut **Cmd+I** (Mac) or **Ctrl+I** (Windows) inside a cell.\n",
    "2. \"Assistant\" side panel.\n",
    "\n",
    "**What is it for?**\n",
    "- **Code Generation**: \"Write a SQL query that calculates average sales by region from the sales table\".\n",
    "- **Code Explanation**: Select a complex snippet and ask \"Explain this code\".\n",
    "- **Fixing errors**: When a cell returns an error, click \"Diagnose Error\" – the assistant will explain the cause and propose a fix.\n",
    "- **Transformation**: \"Rewrite this code from PySpark to SQL\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5d45e64-069d-4a60-b9b7-8d3fcb777f7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cffcf7a7-4b58-4f36-a746-71603cb32a1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 5: Unity Catalog vs Hive Metastore\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Databricks supports two metadata systems: legacy Hive Metastore and modern Unity Catalog. Unity Catalog is recommended for all new projects due to advanced governance and security features.\n",
    "\n",
    "**Key Differences:**\n",
    "\n",
    "| Aspect | Hive Metastore | Unity Catalog |\n",
    "|--------|----------------|---------------|\n",
    "| **Governance** | Limited | Full: RBAC, masking, audit |\n",
    "| **Namespace** | 2-level (db.table) | 3-level (catalog.schema.table) |\n",
    "| **Cross-workspace** | No | Yes (shared metastore) |\n",
    "| **Lineage** | None | End-to-end lineage |\n",
    "| **Data Sharing** | Limited | Delta Sharing protocol |\n",
    "| **Isolation** | Workspace-level | Catalog-level |\n",
    "\n",
    "**Why Unity Catalog?**\n",
    "- Central access management for all workspaces\n",
    "- Automatic lineage for audit and compliance\n",
    "- Fine-grained permissions (column-level, row-level)\n",
    "- Integration with external systems (Delta Sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f41c629-f1dd-4331-a607-b16b4c7a8314",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 5.1: Namespace - Hive vs Unity Catalog\n",
    "\n",
    "**Objective:** Compare table access syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ff6cc60-b5c6-4119-849f-76a3c175f19b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Hive Metastore (legacy)**\n",
    "\n",
    "- **Syntax**: `database.table`\n",
    "- **Example**: `default.sales_data`\n",
    "- **Limitations**: No fine-grained permissions, no lineage, workspace isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4da5a2ba-76c0-4230-b5ee-a5a47656beb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Unity Catalog (modern)**\n",
    "\n",
    "- **Syntax**: `catalog.schema.table`\n",
    "- **Example**: `prod.gold.sales_summary`\n",
    "\n",
    "**3-level Namespace Advantages:**\n",
    "- Environment separation (dev/test/prod catalogs)\n",
    "- Better permissions (grant at catalog level)\n",
    "- Metastore sharing between workspaces\n",
    "- End-to-end lineage\n",
    "- Fine-grained access control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11188262-35c1-49e3-8b92-38362635bd2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 5.2: Creating a Table in Unity Catalog\n",
    "\n",
    "**Objective:** Demonstrate full syntax with 3-level namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19c1afdc-9dba-48ea-a8fd-4f31ff2d05bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create sample table in Unity Catalog\n",
    "table_name = f\"{CATALOG}.{BRONZE_SCHEMA}.lakehouse_demo\"\n",
    "\n",
    "# Demo data\n",
    "demo_data = [\n",
    "    (1, \"Unity Catalog\", \"Enabled\", \"2024-01-15\"),\n",
    "    (2, \"Delta Lake\", \"Enabled\", \"2024-01-15\"),\n",
    "    (3, \"Photon Engine\", \"Enabled\", \"2024-01-15\"),\n",
    "    (4, \"Hive Metastore\", \"Legacy\", \"2024-01-15\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd8f2775-8274-4791-ab51-d147a1be2078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Schema definition\n",
    "demo_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"feature\", StringType(), False),\n",
    "    StructField(\"status\", StringType(), False),\n",
    "    StructField(\"date\", StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a60aa549-2b4b-4714-944c-49cd6ab52d59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "demo_df = spark.createDataFrame(demo_data, demo_schema)\n",
    "display(demo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e40fd65e-3d9b-4ed0-a807-3aa070e50685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save as Delta Table in Unity Catalog\n",
    "demo_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1506a541-232b-4b70-b15e-17fe667a9944",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification\n",
    "display(spark.table(table_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17ffb9ec-997d-4305-affd-991831d89bfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Success!** The table has been created in Unity Catalog. We can now verify its existence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "490272be-2a84-46e2-a66d-0d6d1d573ff8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 🎯 Task: Check in UI\n",
    "\n",
    "1. Click **Catalog** in the left sidebar.\n",
    "2. Find your catalog (name in `CATALOG` variable, e.g., `ecommerce_platform_...`).\n",
    "3. Expand the `bronze` schema (or other defined in `BRONZE_SCHEMA`).\n",
    "4. Click on the `lakehouse_demo` table.\n",
    "5. See tabs: **Sample Data** (preview) and **Lineage** (data origin)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ec8af90-b3b3-43db-a965-d0c39e0c44f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "The table was created with a full 3-level namespace. In Unity Catalog, every table automatically:\n",
    "- Is managed by the governance system\n",
    "- Has tracked lineage\n",
    "- Has permissions assigned based on catalog and schema\n",
    "- Is available in Catalog Explorer for exploration\n",
    "\n",
    "**Managed vs External Tables:**\n",
    "The table above is a **Managed Table**. Databricks manages both metadata and data files (in default catalog/schema storage). Dropping the table (`DROP TABLE`) also deletes the data.\n",
    "\n",
    "**External Table** is created when we provide `LOCATION 'path'`. Then `DROP TABLE` removes only metadata, and files remain in storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfeca6d4-c870-4788-aa01-7dfe3acc3780",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Bonus: Delta Time Travel (Teaser)\n",
    "\n",
    "Every operation on a Delta table is recorded in the transaction log. This allows us to track change history and go back in time (Time Travel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f56ed7b3-67af-47e2-821a-abcabe4a484c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"DESCRIBE HISTORY {CATALOG}.{BRONZE_SCHEMA}.lakehouse_demo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e16fc12-df73-414e-af57-a9002b9a34c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY ecommerce_platform_trainer.bronze.lakehouse_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c93db4f3-fee5-4c6f-bfd4-c8f7c5da2637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 5.3: Unity Catalog Metadata Exploration\n",
    "\n",
    "**Objective:** Use Unity Catalog information schema system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d670b941-54a6-4572-9e1d-96623e199e7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Unity Catalog provides system.information_schema for metadata\n",
    "\n",
    "-- List tables in our schema\n",
    "SELECT \n",
    "  table_catalog,\n",
    "  table_schema,\n",
    "  table_name,\n",
    "  table_type\n",
    "FROM system.information_schema.tables\n",
    "WHERE table_catalog = 'ecommerce_platform_trainer'\n",
    "  AND table_schema = 'bronze'\n",
    "ORDER BY table_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c83299bc-2015-4d27-82f2-c527bd4a7fb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "Unity Catalog automatically maintains `system.information_schema` - a set of SQL views with metadata about all objects. This is a standard approach compliant with ANSI SQL, facilitating integration with BI and data governance tools.\n",
    "\n",
    "**System Tables:**\n",
    "It is worth knowing that Unity Catalog also provides system tables (requires enabling by admin):\n",
    "- `system.billing.usage`: detailed cost data (DBU)\n",
    "- `system.access.audit`: audit logs (who, what, when)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dc408bc-fa10-46b1-aadf-e21efee43fd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Comparison PySpark vs SQL\n",
    "\n",
    "**DataFrame API (PySpark):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a1a2858-88a3-4425-b9ae-bbcb8dc3fd7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PySpark Approach - programmatic DataFrame API\n",
    "\n",
    "df_pyspark = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.lakehouse_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf9b31f8-ef80-4b2c-9d88-4a7106d1f7c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_pyspark = df_pyspark \\\n",
    "    .filter(F.col(\"status\") == \"Enabled\") \\\n",
    "    .select(\"feature\", \"status\", \"date\") \\\n",
    "    .orderBy(\"feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd1e2252-4c73-4961-b13c-e88b905ec60f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result_pyspark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26f26fb2-d01e-4ddd-9ba5-a4f125828150",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**SQL Equivalent:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "231818c1-6090-40c1-9837-2f3ae9d0c68f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"{CATALOG}.{BRONZE_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96b7c9b1-a1a1-4c15-b6df-8249cd755b2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select * from \n",
    "ecommerce_platform_trainer.bronze.lakehouse_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "169a685d-24ac-4f56-b608-8a240f2cd18f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Parameterization with Databricks Widgets\n",
    "\n",
    "Below we use the **Widgets** mechanism, which allows creating interactive controls in the notebook. This allows passing parameters (e.g., table names, dates) to SQL and Python code, facilitating the building of universal reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3dff4b4-da92-40a8-9785-eb777db88424",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parameterization with Databricks Widgets\n",
    "# Set default values based on variables from 00_setup (if available)\n",
    "# This ensures SQL cells will use the same catalog as Python cells\n",
    "\n",
    "default_catalog = CATALOG if 'CATALOG' in locals() else \"ecommerce_platform_trainer\"\n",
    "default_schema = BRONZE_SCHEMA if 'BRONZE_SCHEMA' in locals() else \"bronze\"\n",
    "\n",
    "dbutils.widgets.text(\"CATALOG\", default_catalog)\n",
    "dbutils.widgets.text(\"BRONZE_SCHEMA\", default_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06523c1b-3a68-42d9-bd21-099e0ff46d49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT \n",
    "  feature,\n",
    "  status,\n",
    "  date\n",
    "FROM IDENTIFIER(:CATALOG || '.' || :BRONZE_SCHEMA || '.lakehouse_demo')\n",
    "WHERE status = 'Enabled'\n",
    "ORDER BY feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdd223e2-d3f6-42ca-ac53-acbbbea0ebbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Comparison:**\n",
    "- **Performance**: Identical - both approaches compile to the same Catalyst query plan\n",
    "- **When to use PySpark**: \n",
    "  - Complex business logic with UDFs\n",
    "  - Dynamic pipelines (parameterization, loops)\n",
    "  - Integration with Python libraries (pandas, scikit-learn)\n",
    "- **When to use SQL**: \n",
    "  - Simple transformations and aggregations\n",
    "  - Team with strong SQL skills\n",
    "  - Migration from traditional Data Warehouse\n",
    "  - Better support for business analysts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7237cb0f-ce86-4125-9a7e-4889656df750",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Validation and Verification\n",
    "\n",
    "### Checklist - What you should understand after this notebook:\n",
    "- [x] Lakehouse concept and benefits over traditional architecture\n",
    "- [x] Workspace Structure: Sidebar, Compute, Catalog Explorer\n",
    "- [x] Unity Catalog Hierarchy: Metastore → Catalog → Schema → Objects\n",
    "- [x] Cluster Types: All-Purpose vs Job, autoscaling, Photon\n",
    "- [x] Magic commands: %sql, %python, %fs, %run, %md\n",
    "- [x] Differences between Hive Metastore (2-level) and Unity Catalog (3-level)\n",
    "- [x] Creating tables in Unity Catalog with full namespace\n",
    "- [x] Accessing metadata via system.information_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3f4a541-553c-430b-b6e8-3aabbca452ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Problem 1: \"Catalog not found\" or \"Schema not found\"\n",
    "**Symptoms:**\n",
    "- Error: `AnalysisException: [SCHEMA_NOT_FOUND]`\n",
    "- Error: `AnalysisException: [CATALOG_NOT_FOUND]`\n",
    "\n",
    "**Solution:**\n",
    "```python\n",
    "# Check available catalogs\n",
    "spark.sql(\"SHOW CATALOGS\").show()\n",
    "\n",
    "# Check available schemas\n",
    "spark.sql(f\"SHOW SCHEMAS IN {CATALOG}\").show()\n",
    "\n",
    "# Ensure you ran %run ./00_setup\n",
    "```\n",
    "\n",
    "### Problem 2: \"Permission denied\" when creating tables\n",
    "**Symptoms:**\n",
    "- Error: `PERMISSION_DENIED: User does not have CREATE TABLE on Schema`\n",
    "\n",
    "**Solution:**\n",
    "- Contact workspace administrator to grant `CREATE TABLE` permissions\n",
    "- Check permissions: `SHOW GRANTS ON SCHEMA catalog.schema`\n",
    "\n",
    "### Problem 3: Cluster not starting or too slow\n",
    "**Symptoms:**\n",
    "- Cluster in \"Pending\" state for a long time\n",
    "- Timeout at startup\n",
    "\n",
    "**Solution:**\n",
    "- Check instance quota in cloud (Azure/AWS/GCP)\n",
    "- Reduce number of workers or choose smaller node type\n",
    "- Disable autoscaling for tests\n",
    "\n",
    "### Problem 4: Magic command %sql not working\n",
    "**Symptoms:**\n",
    "- Syntax error or no results\n",
    "\n",
    "**Solution:**\n",
    "- Ensure `%sql` is the first element in the cell\n",
    "- Check if you are using variables: `${CATALOG}` instead of `{CATALOG}`\n",
    "- For Python variables use: `spark.sql(f\"SELECT ... FROM {CATALOG}.{SCHEMA}.table\")`\n",
    "\n",
    "### Debugging tips:\n",
    "- Use `explain()` on DataFrame to see execution plan\n",
    "- Check cluster logs in Spark UI (\"Cluster\" tab → \"Spark UI\")\n",
    "- Verify data types: `df.printSchema()`\n",
    "- For performance issues check partition count: `df.rdd.getNumPartitions()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46e715d3-bead-4aad-bc6f-9e02f500a4f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Best Practices\n",
    "\n",
    "### Lakehouse Architecture:\n",
    "- Use Unity Catalog instead of Hive Metastore for new projects\n",
    "- Organize data into logical catalogs (e.g., dev/test/prod)\n",
    "- Use naming convention for schemas: bronze/silver/gold\n",
    "- Use Delta Lake as default table format\n",
    "\n",
    "### Workspace Management:\n",
    "- Organize notebooks in folders by projects or teams\n",
    "- Use Git Folders (Repos) for Git integration and versioning\n",
    "- Document notebooks with Markdown cells\n",
    "- Use `%run` for sharing code between notebooks\n",
    "\n",
    "### Cluster Configuration:\n",
    "- Development: small clusters (2-4 workers), no spot instances\n",
    "- Production: autoscaling, spot instances for savings\n",
    "- Enable Photon Engine for SQL/DataFrame queries\n",
    "- Set auto-termination (e.g., 30 min inactivity) for All-Purpose clusters\n",
    "- For Jobs use Job Clusters (ephemeral, optimal costs)\n",
    "\n",
    "### Governance and Security:\n",
    "- Use per-user or per-team catalog isolation\n",
    "- Use 3-level namespace: catalog.schema.table\n",
    "- Assign permissions at schema level, not table level\n",
    "- Monitor access via system.access.audit\n",
    "- Enable lineage for compliance and debugging\n",
    "\n",
    "### Performance:\n",
    "- Prefer Delta Lake over Parquet/CSV for frequent queries\n",
    "- Partition large tables by time or geographic keys\n",
    "- Use Z-ORDER for columns used in WHERE clauses\n",
    "- Regularly run OPTIMIZE and VACUUM on Delta tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a16ec104-b474-4943-b590-30dd012a7aad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "### What was achieved:\n",
    "- Learned Lakehouse concept as evolution of Data Lake + Data Warehouse\n",
    "- Explored Databricks platform elements: Workspace, Compute, Catalog\n",
    "- Understood Unity Catalog hierarchy: Metastore → Catalog → Schema → Objects\n",
    "- Practiced magic commands: %sql, %python, %fs, %pip\n",
    "- Compared Hive Metastore vs Unity Catalog\n",
    "- Created first Delta table in Unity Catalog with 3-level namespace\n",
    "\n",
    "### Key Takeaways:\n",
    "1. **Lakehouse eliminates data duplication**: Single copy serves BI, ML, and real-time analytics\n",
    "2. **Unity Catalog is governance foundation**: 3-level namespace, fine-grained permissions, automatic lineage\n",
    "3. **Clusters are flexible**: Autoscaling and spot instances reduce costs, Photon accelerates queries\n",
    "4. **Notebooks are powerful**: Mixing SQL/Python, magic commands, Git integration via Git Folders\n",
    "5. **Delta Lake is default format**: ACID transactions, time travel, schema evolution\n",
    "\n",
    "### Quick Reference - Key Commands:\n",
    "\n",
    "| Operation | PySpark | SQL |\n",
    "|-----------|---------|-----|\n",
    "| Set catalog | `spark.sql(f\"USE CATALOG {CATALOG}\")` | `USE CATALOG my_catalog` |\n",
    "| List catalogs | `spark.sql(\"SHOW CATALOGS\")` | `SHOW CATALOGS` |\n",
    "| List schemas | `spark.sql(\"SHOW SCHEMAS\")` | `SHOW SCHEMAS` |\n",
    "| Create table | `df.write.saveAsTable(\"cat.schema.table\")` | `CREATE TABLE cat.schema.table AS SELECT ...` |\n",
    "| Read table | `spark.table(\"cat.schema.table\")` | `SELECT * FROM cat.schema.table` |\n",
    "| Metadata | - | `SELECT * FROM system.information_schema.tables` |\n",
    "| Install lib | `%pip install package` | - |\n",
    "\n",
    "### Additional Resources:\n",
    "- [Databricks Lakehouse Fundamentals](https://www.databricks.com/learn/lakehouse)\n",
    "- [Unity Catalog Documentation](https://docs.databricks.com/data-governance/unity-catalog/index.html)\n",
    "- [Delta Lake Guide](https://docs.delta.io/latest/index.html)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7731102079244218,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_databricks_lakehouse_intro",
   "widgets": {
    "BRONZE_SCHEMA": {
     "currentValue": "bronze",
     "nuid": "3eb823de-3328-4753-a2bf-9d863378b4f1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "bronze",
      "label": null,
      "name": "BRONZE_SCHEMA",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "bronze",
      "label": null,
      "name": "BRONZE_SCHEMA",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "CATALOG": {
     "currentValue": "ecommerce_platform_trainer",
     "nuid": "ab4cee0e-5693-4e7e-8e80-189b95567447",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "ecommerce_platform_trainer",
      "label": null,
      "name": "CATALOG",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "ecommerce_platform_trainer",
      "label": null,
      "name": "CATALOG",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
