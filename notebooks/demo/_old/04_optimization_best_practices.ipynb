{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23cffde0-db08-4560-8121-7f59b784a13f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Optimization and Best Practices\n",
    "\n",
    "**Training Objective:** Mastering query and Delta table performance optimization techniques in Databricks.\n",
    "\n",
    "**Topics covered:**\n",
    "- Query optimization: predicate pushdown, file pruning, column pruning\n",
    "- Physical plan analysis (explain())\n",
    "- Table optimization: partitioning, small files problem\n",
    "- Auto optimize / auto compaction\n",
    "- File sizing and ZORDER strategy\n",
    "- Liquid Clustering - modern alternative to partitioning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dfc4662-9aaa-4a92-8d26-2ebb05cfa80c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Context and Requirements\n",
    "\n",
    "- **Training Day**: Day 2 - Delta Lake & Lakehouse\n",
    "- **Notebook Type**: Demo\n",
    "- **Technical Requirements**:\n",
    "  - Databricks Runtime 16.4 LTS or newer (recommended: 17.3 LTS)\n",
    "  - Unity Catalog enabled\n",
    "  - Permissions: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Cluster: Standard with minimum 2 workers or **Serverless Compute** (recommended)\n",
    "- **Dependencies**: Completed notebook `01_delta_lake_operations.ipynb`\n",
    "- **Duration**: ~45 minutes\n",
    "\n",
    "> **Note (2025):** Serverless Compute is now the default mode for new workloads.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9200a175-242b-4dc7-acd4-288781f4e3df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Theoretical Introduction\n",
    "\n",
    "**Section Objective:** Understanding key optimization mechanisms in Databricks and Delta Lake.\n",
    "\n",
    "**Optimization Types:**\n",
    "\n",
    "**1. Query Optimization:**\n",
    "- **Predicate Pushdown**: Pushing filters as low as possible in the execution plan\n",
    "- **Column Pruning**: Reading only required columns (Parquet columnar format)\n",
    "- **File Pruning**: Skipping files irrelevant to the query\n",
    "- **Join Optimization**: Broadcast joins, bucket joins, sortmerge joins\n",
    "\n",
    "**2. Table Optimization:**\n",
    "- **Partitioning**: Physical data separation by key values\n",
    "- **Z-Ordering**: Data clustering in files by selected columns\n",
    "- **Compaction (OPTIMIZE)**: Merging small files into larger ones\n",
    "- **Auto Compaction**: Automatic merging during write\n",
    "\n",
    "**3. Small Files Problem:**\n",
    "Performance issue caused by too many small files in the table. Spark prefers 128MB-1GB files for optimal performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00ee9b43-acbf-4216-a499-8bea3d48739b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Per-user Isolation\n",
    "\n",
    "Run the initialization script for per-user catalog and schema isolation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a250c520-ae2e-4004-b2d6-f2994e411e6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6163cff-16e5-44b6-9cce-a3bf932fe679",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuration\n",
    "\n",
    "Import libraries and set environment variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a0cc155-45ac-4679-bf40-0efeb89e7b09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, lit, count, avg, sum, max, min\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "\n",
    "# Set catalog and schema as default\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f62a5d56-fc77-4c1e-8241-7a415ab5c300",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**User Context:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59a7d3f9-953e-4173-b17e-d19c44d28cc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"CATALOG\", CATALOG),\n",
    "        (\"BRONZE_SCHEMA\", BRONZE_SCHEMA),\n",
    "        (\"SILVER_SCHEMA\", SILVER_SCHEMA),\n",
    "        (\"GOLD_SCHEMA\", GOLD_SCHEMA),\n",
    "        (\"USER\", raw_user)\n",
    "    ], [\"Variable\", \"Value\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b34e9e2-0bd9-43eb-b563-02e892f6494d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 0: Data Preparation\n",
    "\n",
    "This notebook is **fully independent** - it loads source data and creates tables needed for the optimization demo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a79c716-353d-451d-bdc9-42f36aafb449",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Source data paths\n",
    "CUSTOMERS_CSV = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "ORDERS_JSON = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "PRODUCTS_PARQUET = f\"{DATASET_BASE_PATH}/products/products.parquet\"\n",
    "\n",
    "# Table names (unique for this notebook)\n",
    "ORDERS_OPT = f\"{BRONZE_SCHEMA}.orders_optimization\"\n",
    "CUSTOMERS_OPT = f\"{BRONZE_SCHEMA}.customers_optimization\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3adb149a-7307-4539-ad3c-4a94df3fc737",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Load source data:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92f2390e-57c5-4742-bccd-a646584b145f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load customers\n",
    "customers_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(CUSTOMERS_CSV)\n",
    "\n",
    "# Load orders\n",
    "orders_df = spark.read.json(ORDERS_JSON)\n",
    "\n",
    "# Add order_date column (date without time) for partitioning\n",
    "orders_df = orders_df.withColumn(\n",
    "    \"order_date\", \n",
    "    F.to_date(F.col(\"order_datetime\"))\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Customers: {customers_df.count()} records\")\n",
    "print(f\"âœ“ Orders: {orders_df.count()} records\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cf6b602-eab0-4ddb-ab01-af38e593fb4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Save as Delta tables for optimization:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2614896d-055e-41ad-8e08-0fff2ba70486",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save as Delta tables\n",
    "customers_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(CUSTOMERS_OPT)\n",
    "\n",
    "orders_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(ORDERS_OPT)\n",
    "\n",
    "display(spark.createDataFrame([\n",
    "    (\"CUSTOMERS_OPT\", CUSTOMERS_OPT, str(customers_df.count())),\n",
    "    (\"ORDERS_OPT\", ORDERS_OPT, str(orders_df.count()))\n",
    "], [\"Table\", \"Full Name\", \"Records\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "676205fd-452f-43c1-a452-8072b6513717",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "orders_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", f\"{DATASET_BASE_PATH}/delta/orders_optimization\") \\\n",
    "    .saveAsTable(ORDERS_OPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bd499c1-678e-463f-9081-7ef9b14301c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 1: Physical Plan Analysis (explain())\n",
    "\n",
    "**Section Objective:** Learn to analyze query execution plans to identify performance bottlenecks.\n",
    "\n",
    "**Theory:**\n",
    "A physical plan is a detailed map of how Spark executes a query:\n",
    "- **Stages**: Logical processing steps\n",
    "- **Tasks**: Units of work executed on partitions\n",
    "- **Shuffles**: Data exchange between executors\n",
    "- **Pushdowns**: Optimizations pushed to the data source\n",
    "\n",
    "**Types of explain():**\n",
    "- `explain()` - basic plan\n",
    "- `explain(True)` - full plan with details\n",
    "- `explain('extended')` - extended plan\n",
    "- `explain('cost')` - plan with cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "918895ad-596e-4f57-bdb7-05d587e80952",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 1.1: Simple Query Plan Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6531acdf-98d5-45d5-a312-5f369f2fa2ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example 1.1 - Simple Query Plan Analysis\n",
    "\n",
    "simple_query = spark.sql(f\"\"\"\n",
    "    SELECT customer_id, first_name, last_name, customer_segment, city\n",
    "    FROM {CUSTOMERS_OPT}\n",
    "    WHERE customer_segment = 'Premium'\n",
    "    ORDER BY customer_id DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35f19041-0931-4a08-a8e0-20e32a6124ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Basic query plan:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2181b138-61da-4aee-bf65-68a74549c9af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "simple_query.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e42fa92-dd6b-498e-aeb9-ee451569f053",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Extended query plan (with details):**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03aa87a5-a7b2-42a1-b6c3-f8fb0854d2e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "simple_query.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d9af332-8c17-4825-9496-b9029a9421ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 1.2: Predicate Pushdown in Practice\n",
    "\n",
    "**Theory:** Predicate pushdown is an optimization where filters (WHERE conditions) are \"pushed\" as low as possible in the execution plan, preferably to the file reading level. This way we only read data that meets the conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73394fb4-ff75-4f2d-aae5-1f68db484ec6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example 1.2 - Predicate Pushdown\n",
    "\n",
    "filtered_query = spark.sql(f\"\"\"\n",
    "    SELECT order_id, customer_id, total_amount, order_date\n",
    "    FROM {ORDERS_OPT}\n",
    "    WHERE total_amount > 100 \n",
    "    AND order_date >= '2024-01-01'\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8e41032-3c95-4168-bd99-a3b1a3c47c1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Check plan - look for \"PushedFilters\" in the plan:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ead573aa-851f-4ec2-bdc4-413254fe850b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check plan - look for \"PushedFilters\" in the plan\n",
    "filtered_query.explain(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41fc6c80-bebb-424b-a76c-b85f633da3bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**ðŸ’¡ In the plan look for:**\n",
    "- `PushedFilters` - filters pushed to the reading level\n",
    "- `ReadSchema` - only selected columns (column pruning)  \n",
    "- `PartitionFilters` - filters on partitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a51281f-4f44-4ea6-82f2-996f7edaee07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 2: Partitioning Strategy\n",
    "\n",
    "**Section Objective:** Learn to choose optimal partitioning keys for best performance.\n",
    "\n",
    "**Partitioning Theory:**\n",
    "- **Partitioning**: Physical separation of table into directories by column values\n",
    "- **Partition Pruning**: Spark skips entire partitions that are not needed for the query\n",
    "- **Ideal Partitions**: 1-10GB of data per partition, no more than 10,000 partitions\n",
    "\n",
    "**Best Practices:**\n",
    "- Partition by columns frequently used in filters\n",
    "- Avoid partitioning by high cardinality columns\n",
    "- Prefer columns with natural time hierarchy (year/month/day)\n",
    "- Avoid too many small partitions (small files problem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0665d14-7df4-4304-b215-ad5531b8a72a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 2.1: Creating a Partitioned Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c00e19ec-cf42-486f-be1f-0082e1e602d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example 2.1 - Creating a Partitioned Table\n",
    "\n",
    "# Create table partitioned by year and month\n",
    "ORDERS_PARTITIONED = f\"{BRONZE_SCHEMA}.orders_opt_partitioned\"\n",
    "\n",
    "# Add columns for partitioning\n",
    "orders_with_partitions = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        *,\n",
    "        YEAR(order_date) as year,\n",
    "        MONTH(order_date) as month\n",
    "    FROM {ORDERS_OPT}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ffa99b2-1246-49c4-ac26-0e50398d876a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Save as partitioned table:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1ae9d87-c4a4-4e3d-820a-e21e4f36a722",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders_with_partitions.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .saveAsTable(ORDERS_PARTITIONED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2f20147-390f-464a-b2c8-2a2860bc0666",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Check partition structure:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e521c585-00d8-4132-bd73-9e00f315c55c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.sql(f\"DESCRIBE DETAIL {ORDERS_PARTITIONED}\")\n",
    "    .select(\"name\", \"location\", \"partitionColumns\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26ef8762-1a93-41d5-83af-0545643857ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 2.2: Partition Pruning in Action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e30be36f-2a13-44f8-85a8-05c76566b45a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example 2.2 - Partition Pruning\n",
    "\n",
    "# Query that uses partitions (year/month)\n",
    "efficient_query = spark.sql(f\"\"\"\n",
    "    SELECT order_id, customer_id, total_amount, order_date\n",
    "    FROM {ORDERS_PARTITIONED}\n",
    "    WHERE year = 2024 AND month = 1\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87b8b31e-222a-4a5e-b30a-3d852213dd7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Check plan - partition pruning in action:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c854427-294e-446c-846b-c8bbe77a8514",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check plan - look for \"PartitionFilters\"\n",
    "efficient_query.explain(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b6b8cdb-1ef8-4bcc-a64c-d1ffb7ba2a13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Comparison: query WITHOUT partition pruning:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c8552ae-d3d5-4cf4-8fef-8a97c7df2e21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Query that does not use partitions (does not filter by year/month)\n",
    "inefficient_query = spark.sql(f\"\"\"\n",
    "    SELECT order_id, customer_id, total_amount, order_date\n",
    "    FROM {ORDERS_PARTITIONED}\n",
    "    WHERE customer_id = 1\n",
    "\"\"\")\n",
    "\n",
    "inefficient_query.explain(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64af6cdb-4166-4a66-9534-bd992f07f7f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 2b: JOIN Optimization and Shuffle\n",
    "\n",
    "**Section Objective:** Optimizing JOIN operations and minimizing costly shuffle operations.\n",
    "\n",
    "**Theory - JOIN Types in Spark:**\n",
    "\n",
    "| JOIN Type | When Used | Characteristics |\n",
    "|-----------|-----------|-----------------|\n",
    "| **Broadcast Hash Join** | One table < 10MB (default) | Fastest - small table copied to all executors |\n",
    "| **Sort Merge Join** | Both tables large | Requires shuffle and sorting of both sides |\n",
    "| **Shuffle Hash Join** | Medium tables | Shuffle without sorting |\n",
    "\n",
    "**Shuffle - what is it?**\n",
    "- Data exchange between executors (over network)\n",
    "- Most expensive operation in Spark (I/O, network, serialization)\n",
    "- Causes \"stage boundaries\" in DAG\n",
    "\n",
    "**How to minimize shuffle:**\n",
    "1. **Broadcast JOIN** - for small tables (< 10MB, can be increased to 100MB)\n",
    "2. **Repartition** before JOIN - aligning partitions\n",
    "3. **Colocated JOIN** - data already on the same partitions\n",
    "4. **Bucketing** - table pre-partitioning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37daf578-66cb-401e-93e7-c1b5865af67e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 2b.1: Broadcast JOIN - optimization for small tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6604c43-b3b0-4c10-844b-58b04582815c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example 2b.1 - Broadcast JOIN\n",
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Load tables for JOIN\n",
    "orders_df = spark.table(ORDERS_OPT)\n",
    "customers_df = spark.table(CUSTOMERS_OPT)\n",
    "\n",
    "print(f\"Orders: {orders_df.count()} records\")\n",
    "print(f\"Customers: {customers_df.count()} records (small table - candidate for broadcast)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f2b6aa9-d1aa-4476-b413-fb286577e4e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Standard JOIN (no broadcast) - requires shuffle:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0748f9e-6288-4887-b363-48e36cba8ad8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Standard JOIN - Spark decides strategy\n",
    "standard_join = orders_df.join(\n",
    "    customers_df,\n",
    "    orders_df.customer_id == customers_df.customer_id,\n",
    "    \"inner\"\n",
    ").select(\n",
    "    orders_df.order_id,\n",
    "    orders_df.total_amount,\n",
    "    customers_df.first_name,\n",
    "    customers_df.customer_segment\n",
    ")\n",
    "\n",
    "# Check plan - look for \"SortMergeJoin\" or \"BroadcastHashJoin\"\n",
    "standard_join.explain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40c43c66-f59e-418d-b5be-004c1583f577",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Forced Broadcast JOIN - eliminates shuffle:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28beffb3-0349-49c6-854a-c0f6bc884344",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Forced Broadcast JOIN - small table (customers) copied to all executors\n",
    "broadcast_join = orders_df.join(\n",
    "    broadcast(customers_df),  # Force broadcast of smaller table\n",
    "    orders_df.customer_id == customers_df.customer_id,\n",
    "    \"inner\"\n",
    ").select(\n",
    "    orders_df.order_id,\n",
    "    orders_df.total_amount,\n",
    "    customers_df.first_name,\n",
    "    customers_df.customer_segment\n",
    ")\n",
    "\n",
    "# Check plan - should show \"BroadcastHashJoin\"\n",
    "broadcast_join.explain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c37e46ab-6ce8-46f2-bf5f-7ffd3dafb5ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**ðŸ’¡ In the plan look for:**\n",
    "- `BroadcastHashJoin` - broadcast join (no shuffle!)\n",
    "- `SortMergeJoin` - sort merge join (requires shuffle of both sides)\n",
    "- `BroadcastExchange` - copying small table to executors\n",
    "- `ShuffleExchange` - shuffle (expensive operation!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aec715b-465a-491a-8a41-6cad52c4170c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 2b.3: Repartition vs Coalesce - shuffle control\n",
    "\n",
    "**What does it mean?**\n",
    "\n",
    "- **repartition(n)** â€“ causes full data shuffle, distributing it evenly into n partitions. Use when you want to increase partition count or balance data distribution (e.g. before large JOIN).\n",
    "- **coalesce(n)** â€“ merges existing partitions without shuffle (narrow transformation), reducing their count. Use when you want to reduce partition count (e.g. before writing to file), but don't care about even distribution.\n",
    "\n",
    "**Summary:**  \n",
    "Choose `repartition` when you care about even distribution and can accept shuffle cost. Choose `coalesce` when you just want to reduce partition count without expensive shuffle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17aed8d8-935a-4e52-9243-be7f85ecdf19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Repartition vs Coalesce - differences\n",
    "\n",
    "# REPARTITION - full shuffle, even data distribution\n",
    "# Use when: need more partitions OR even distribution\n",
    "orders_repartitioned = orders_df.repartition(10)\n",
    "print(f\"After repartition(10): {orders_repartitioned.rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "# COALESCE - no shuffle, just merging partitions (narrow transformation)\n",
    "# Use when: reducing partition count (e.g. before write)\n",
    "orders_coalesced = orders_df.coalesce(4)\n",
    "print(f\"After coalesce(4): {orders_coalesced.rdd.getNumPartitions()} partitions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d062bea-f5b3-4dc7-a565-4d3721946ded",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Repartition BY column - optimization for JOIN:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d72cbee1-9d79-41b6-9aa8-2772280361cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Repartition BY join key - data with same key goes to same partition\n",
    "# This minimizes shuffle during JOIN (collocated data)\n",
    "\n",
    "orders_by_customer = orders_df.repartition(10, \"customer_id\")\n",
    "customers_by_id = customers_df.repartition(10, \"customer_id\")\n",
    "\n",
    "# Now JOIN will be faster - data is already on same partitions\n",
    "optimized_join = orders_by_customer.join(\n",
    "    customers_by_id,\n",
    "    \"customer_id\",  # Common partition column\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ Data repartitioned by customer_id - JOIN will be faster\")\n",
    "optimized_join.explain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c1c166a-2a5d-4960-89e6-cb93ae31eee1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 2b.4: JOIN Performance Comparison\n",
    "\n",
    "**Theory - when to use which JOIN type:**\n",
    "\n",
    "| Scenario | Recommendation |\n",
    "|----------|----------------|\n",
    "| Small lookup table (< 10MB) | `broadcast(small_df)` |\n",
    "| Small lookup table (10-100MB) | Increase `autoBroadcastJoinThreshold` |\n",
    "| Both tables large | SortMergeJoin (default) + pre-repartition |\n",
    "| Frequent JOIN on same key | Bucketing (at table creation) |\n",
    "| Skewed data (uneven distribution) | Salting or AQE (Adaptive Query Execution) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6eb6608a-7590-4619-a9c5-b54b0da03e00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Performance Comparison: Broadcast vs SortMerge JOIN\n",
    "\n",
    "import time\n",
    "\n",
    "# Test 1: Broadcast JOIN\n",
    "start_broadcast = time.time()\n",
    "result_broadcast = orders_df.join(\n",
    "    broadcast(customers_df),\n",
    "    orders_df.customer_id == customers_df.customer_id\n",
    ").count()\n",
    "time_broadcast = time.time() - start_broadcast\n",
    "\n",
    "# Test 2: Broadcast disabled (forces SortMergeJoin)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "\n",
    "start_sortmerge = time.time()\n",
    "result_sortmerge = orders_df.join(\n",
    "    customers_df,\n",
    "    orders_df.customer_id == customers_df.customer_id\n",
    ").count()\n",
    "time_sortmerge = time.time() - start_sortmerge\n",
    "\n",
    "# Restore default setting\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10 * 1024 * 1024)\n",
    "\n",
    "# Results\n",
    "display(spark.createDataFrame([\n",
    "    (\"Broadcast JOIN\", f\"{time_broadcast:.2f}s\", \"âœ… Fast (no shuffle)\"),\n",
    "    (\"SortMerge JOIN\", f\"{time_sortmerge:.2f}s\", \"âš ï¸ Slower (requires shuffle)\")\n",
    "], [\"JOIN Type\", \"Time\", \"Notes\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "490f3f51-7773-4c85-83fd-6cb74c67d1e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 2b.5: Shuffle Partitions - configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e32a9fea-04ed-42ea-a0f8-a50858076efe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Shuffle Partitions - controlling partition count after shuffle\n",
    "\n",
    "# Check current value (default 200)\n",
    "current_partitions = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "print(f\"Current shuffle.partitions: {current_partitions}\")\n",
    "\n",
    "# For small data (< 1GB) - decrease partition count\n",
    "# spark.conf.set(\"spark.sql.shuffle.partitions\", 10)\n",
    "\n",
    "# For large data (> 100GB) - increase\n",
    "# spark.conf.set(\"spark.sql.shuffle.partitions\", 500)\n",
    "\n",
    "# Best Practice: Adaptive Query Execution (AQE) automatically optimizes\n",
    "print(f\"AQE enabled: {spark.conf.get('spark.sql.adaptive.enabled')}\")\n",
    "print(f\"AQE coalesce partitions: {spark.conf.get('spark.sql.adaptive.coalescePartitions.enabled')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2324c1a2-c156-45f8-889e-76d465326035",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**ðŸ“Š JOIN and Shuffle Optimization Summary:**\n",
    "\n",
    "| Technique | When to use | Benefit |\n",
    "|-----------|-------------|---------|\n",
    "| `broadcast(df)` | Small table < 100MB | Eliminates shuffle |\n",
    "| `repartition(n, col)` | Before JOIN on large tables | Collocated data |\n",
    "| `coalesce(n)` | Reducing partitions before write | No shuffle |\n",
    "| `shuffle.partitions` | Adjusting to data size | Optimal parallelization |\n",
    "| AQE (Adaptive) | Always enabled (default) | Auto-optimization |\n",
    "\n",
    "> **ðŸ’¡ Best Practice:** In Databricks Runtime 14+ AQE is enabled by default and automatically optimizes shuffle partitions, broadcast thresholds and skewed joins.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8db85bc0-8aa8-4630-9e18-69e5530fea96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 3: Small Files Problem\n",
    "\n",
    "**Section Objective:** Understanding and solving the small files problem in Delta Lake.\n",
    "\n",
    "**What is Small Files Problem?**\n",
    "- When a table has too many small files (< 128MB each)\n",
    "- Spark prefers 128MB-1GB files for optimal performance\n",
    "- Small files cause metadata overhead and reduce throughput\n",
    "\n",
    "**Causes of small files:**\n",
    "- Frequent INSERT writes in small batches\n",
    "- High partitioning with small amount of data per partition\n",
    "- Streaming with short trigger intervals\n",
    "\n",
    "**Solutions:**\n",
    "- **OPTIMIZE** - merges small files into larger ones\n",
    "- **Auto Compaction** - automatic merging during write\n",
    "- **Repartition** before write\n",
    "- **Coalesce** for reducing partition count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "965ad450-5f1a-4e33-ac31-ba922037d071",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 3.1: Small Files Problem Simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22a608c1-9ec7-43a3-9b0b-abfd0658aebd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example 3.1 - Small Files Problem Simulation\n",
    "\n",
    "SMALL_FILES_TABLE = f\"{BRONZE_SCHEMA}.small_files_demo\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25e86915-41be-4bd1-8c9a-96cad790baeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Simulate many small writes (each creates a separate file):**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dccc446-bed7-47b1-92f7-2d46a7acb593",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    small_batch = spark.range(i*100, (i+1)*100).select(\n",
    "        col(\"id\"),\n",
    "        (col(\"id\") * 2).alias(\"value\"),\n",
    "        lit(f\"batch_{i}\").alias(\"batch_name\")\n",
    "    )\n",
    "    \n",
    "    small_batch.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(SMALL_FILES_TABLE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3faccb40-d181-48a2-8966-868cf96939c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Check file count:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7199cd96-46ef-408a-9c47-6058b2aa4bd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "detail = spark.sql(f\"DESCRIBE DETAIL {SMALL_FILES_TABLE}\").collect()[0]\n",
    "\n",
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"File count\", str(detail['numFiles'])),\n",
    "        (\"Table size\", f\"{detail['sizeInBytes']} bytes\"),\n",
    "        (\"Avg file size\", f\"{detail['sizeInBytes'] / detail['numFiles']:.0f} bytes\"),\n",
    "        (\"Status\", \"âš ï¸ Problem: too many small files!\")\n",
    "    ], [\"Metric\", \"Value\"])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d568f02-9857-4fa2-bbdd-ef467fa13477",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 3.2: Solution - OPTIMIZE and Auto Compaction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8d3c587-4607-41fa-b0db-3e1ab0774a60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example 3.2 - Small Files Problem Solution\n",
    "\n",
    "# Run OPTIMIZE on table with small files\n",
    "spark.sql(f\"OPTIMIZE {SMALL_FILES_TABLE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bea67be9-12b9-4e95-b8de-a6c2e5fc006f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Check state after OPTIMIZE:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4391f06-7cc2-4368-bebc-ba1a4f74ac9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "detail_after = spark.sql(f\"DESCRIBE DETAIL {SMALL_FILES_TABLE}\").collect()[0]\n",
    "\n",
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"File count (AFTER OPTIMIZE)\", str(detail_after['numFiles'])),\n",
    "        (\"Table size\", f\"{detail_after['sizeInBytes']} bytes\"),\n",
    "        (\"Avg file size\", f\"{detail_after['sizeInBytes'] / detail_after['numFiles']:.0f} bytes\")\n",
    "    ], [\"Metric\", \"Value\"])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b812c27-429f-4174-8592-58a8b942e27b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Enable Auto Compaction for future writes:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4ff41c0-94fc-4307-8808-2ecf8060f880",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    ALTER TABLE {SMALL_FILES_TABLE}\n",
    "    SET TBLPROPERTIES (\n",
    "        'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "        'delta.autoOptimize.autoCompact' = 'true'\n",
    "    )\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2469e89e-dd32-4b2e-990a-9faf9a079077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Check enabled Auto Compaction properties:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "590ec970-c668-4d60-807c-2b0c70c20d9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "properties = spark.sql(f\"SHOW TBLPROPERTIES {SMALL_FILES_TABLE}\").collect()\n",
    "auto_props = [(p['key'], p['value']) for p in properties if 'autoOptimize' in p['key']]\n",
    "\n",
    "display(spark.createDataFrame(auto_props, [\"Property\", \"Value\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b059245f-e9e4-43e3-b375-db8117460191",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 3.3: VACUUM - Removing old files\n",
    "\n",
    "**Theory:**\n",
    "VACUUM removes old files that are no longer needed (after DELETE, UPDATE, MERGE, OPTIMIZE operations). \n",
    "By default Delta Lake retains files for 7 days (168 hours) for Time Travel.\n",
    "\n",
    "**âš ï¸ Warning:** After VACUUM you cannot use Time Travel to versions older than retention period!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de4e8797-b430-4edc-ae27-990c54df9e50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check how many files can be removed (DRY RUN)\n",
    "spark.sql(f\"VACUUM {SMALL_FILES_TABLE} DRY RUN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9923335b-4cb8-4570-80c9-bc5a73b1c7f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Run VACUUM (remove old files):**\n",
    "\n",
    "> **Note:** In production use default retention (7 days). Code below with `RETAIN 0 HOURS` is for demo only!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75d953d2-90c7-4026-8099-de9c6d8bbc39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run VACUUM - remove old files\n",
    "# In demo environment we disable retention check\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "\n",
    "# VACUUM with short retention (DEMO ONLY!)\n",
    "spark.sql(f\"\"\"\n",
    "    VACUUM {ORDERS_OPT} RETAIN 1 HOURS\n",
    "\"\"\")\n",
    "\n",
    "# Restore default setting\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"true\")\n",
    "\n",
    "print(\"âœ… VACUUM executed - old files removed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38c75d4e-fc27-4cfc-b053-d93b48b5baef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 4: ZORDER BY - Advanced Clustering\n",
    "\n",
    "**Section Objective:** Learning to use ZORDER BY to optimize queries with filters and joins.\n",
    "\n",
    "**What is ZORDER BY?**\n",
    "- Multi-dimensional clustering algorithm in Delta Lake\n",
    "- Organizes data in files by values of selected columns\n",
    "- Improves data skipping - skipping unnecessary files during reading\n",
    "- Especially effective for columns frequently used in WHERE filters and JOINs\n",
    "\n",
    "**When to use ZORDER:**\n",
    "- Columns frequently filtered in queries\n",
    "- Columns used in JOIN operations\n",
    "- High-cardinality columns (many unique values)\n",
    "- Maximum 3-4 columns (more = diminishing returns)\n",
    "\n",
    "**ZORDER vs Partitioning:**\n",
    "- Partitioning: physical separation into directories\n",
    "- ZORDER: logical ordering within files (preserves single folder structure)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da4d32df-ebbe-427d-ba8c-720d1006449c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 4.1: ZORDER BY for frequently filtered columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56c5e7c6-4377-40d4-9e18-46ff53c6a9e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run ZORDER BY on most frequently filtered columns\n",
    "spark.sql(f\"\"\"\n",
    "    OPTIMIZE {ORDERS_OPT}\n",
    "    ZORDER BY (customer_id, order_date)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc6cc295-74eb-4af8-b88c-07402191fa62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 4.2: Measuring ZORDER effectiveness - Data Skipping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc56cf1a-23cd-4ede-aa44-64333b3af744",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example 4.2 - Measuring ZORDER effectiveness\n",
    "\n",
    "import time\n",
    "\n",
    "# Query using ZORDER columns\n",
    "# customer_id is STRING (e.g. CUST000123), order_date is DATE\n",
    "test_query = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as cnt, AVG(total_amount) as avg_amount\n",
    "    FROM {ORDERS_OPT}\n",
    "    WHERE customer_id BETWEEN 'CUST000100' AND 'CUST000500'\n",
    "    AND order_date >= '2024-06-01'\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ebb4ddd-45d4-4404-b5bc-4a061dcc9ea4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Execution time measurement:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7882b76-14f9-4a29-a4de-acfd6774461b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "result = test_query.collect()\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"Result\", str(result[0])),\n",
    "        (\"Execution time\", f\"{elapsed:.2f}s\")\n",
    "    ], [\"Metric\", \"Value\"])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c01944f2-4333-4d46-8cb1-fb1190ccc1e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Query plan - check data skipping:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ff59074-1fd3-453b-a930-80d99ceb6cda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check query plan - data skipping\n",
    "test_query.explain(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7319858f-10d0-4cc1-b7d2-043e5af51a7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**ðŸ’¡ In the plan look for:**\n",
    "- `numFilesTotal` vs `numFilesSelected` - how many files were skipped\n",
    "- `metadata time` - metadata parsing time\n",
    "- `files pruned` - data skipping statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c88267bd-b51d-4c25-8d47-d82a5006a672",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 5: Liquid Clustering - Future of Optimization\n",
    "\n",
    "**Section Objective:** Learning Liquid Clustering - modern technique replacing Hive Partitioning and ZORDER.\n",
    "\n",
    "**What is Liquid Clustering?**\n",
    "It's a flexible data layout mechanism that:\n",
    "- Does not require rigid directory structure (like Partitioning)\n",
    "- Allows changing clustering keys without rewriting entire table\n",
    "- Eliminates \"Small Files\" problem related to excessive partitioning\n",
    "- Works incrementally (no need to optimize entire table at once)\n",
    "\n",
    "**When to use?**\n",
    "- Instead of partitioning for most new tables\n",
    "- When partitioning keys have high cardinality\n",
    "- When query patterns change over time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aebdbf22-0336-44c6-bb4e-50cc2ba0b198",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "LIQUID_TABLE = f\"{BRONZE_SCHEMA}.orders_opt_liquid\"\n",
    "\n",
    "# Create table using CLUSTER BY instead of PARTITIONED BY\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {LIQUID_TABLE}\n",
    "CLUSTER BY (customer_id, order_date)\n",
    "AS SELECT * FROM {ORDERS_OPT}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09d932a6-3b8c-47d9-9a43-310dc09902f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Check table properties:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2044deb7-701d-400f-b3f8-0e08c4077ec7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check table properties\n",
    "display(spark.sql(f\"DESCRIBE DETAIL {LIQUID_TABLE}\").select(\"name\", \"clusteringColumns\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca37d586-6ebb-4589-b1a1-6aed6c3e0867",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 5.2: Incremental Optimization\n",
    "\n",
    "**Theory:**\n",
    "Unlike ZORDER, which must recalculate entire partition/table, Liquid Clustering works incrementally. `OPTIMIZE` will organize only data that needs it (e.g. newly added), saving time and resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d43cae37-4bab-4d8b-8660-d31a354d8ccd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run OPTIMIZE - Liquid Clustering knows how to layout data based on table definition\n",
    "spark.sql(f\"OPTIMIZE {LIQUID_TABLE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d6fa602-e15f-4bc6-81ec-cb2cae552e3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Check history to see CLUSTERING operation:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c41681d-7395-4cce-983e-025c6e04de7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.sql(f\"DESCRIBE HISTORY {LIQUID_TABLE}\")\n",
    "    .select(\"version\", \"operation\", \"operationParameters\")\n",
    "    .limit(5)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1848ae0b-3811-4dfb-943e-f5227331f90f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Comparison: Liquid Clustering vs Partitioning + ZORDER\n",
    "\n",
    "| Feature | Partitioning + ZORDER | Liquid Clustering |\n",
    "|---------|-----------------------|-------------------|\n",
    "| **Configuration** | Requires careful selection of partition columns | Flexible `CLUSTER BY` |\n",
    "| **Small Files** | Risk with excessive partitioning | Automatically managed |\n",
    "| **Key Change** | Difficult (requires table rewrite) | Easy (`ALTER TABLE CLUSTER BY`) |\n",
    "| **Optimization** | `OPTIMIZE ZORDER BY` (expensive) | `OPTIMIZE` (incremental) |\n",
    "| **Skew Data** | Susceptible to data skew | Resistant to data skew |\n",
    "\n",
    "**Recommendation:** Use Liquid Clustering for all new tables in Databricks Runtime 13.3+, unless you have specific reason to use partitioning (e.g. compatibility with older readers).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ca92322-c50a-41aa-8a24-007833c526ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "### What we achieved:\n",
    "\n",
    "- **Performance Analysis**: Reading and interpreting physical plans with `explain()`\n",
    "- **Predicate Pushdown**: Identifying bottlenecks and pushed filters\n",
    "- **Partitioning**: Partitioning strategy by frequently filtered columns\n",
    "- **ZORDER BY**: Multi-dimensional clustering for 2-4 columns\n",
    "- **Small Files Problem**: Solving via OPTIMIZE and Auto Compaction\n",
    "- **Liquid Clustering**: Modern alternative to partitioning\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "| # | Rule |\n",
    "|---|------|\n",
    "| 1 | **Analyze before optimizing** - always `explain()` first |\n",
    "| 2 | **Partitioning â‰  ZORDER** - different techniques for different cases |\n",
    "| 3 | **Small files = performance killer** - regular OPTIMIZE |\n",
    "| 4 | **ZORDER BY** - max 3-4 columns, choose most frequently filtered |\n",
    "| 5 | **Liquid Clustering** - prefer for new tables in DBR 13.3+ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59f7365e-c56b-49d8-8b5c-f76a13678b8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Troubleshooting - Performance Diagnosis\n",
    "\n",
    "### ðŸ” Common problems and solutions:\n",
    "\n",
    "**Problem 1: Query runs very slow**\n",
    "```python\n",
    "# Diagnosis:\n",
    "df.explain(True)  # Check execution plan\n",
    "```\n",
    "**Possible causes:**\n",
    "- Missing filters - reading entire table\n",
    "- Shuffle operations - high network traffic\n",
    "- Skewed data - uneven data distribution\n",
    "- Small files - too many small files\n",
    "\n",
    "**Problem 2: \"OutOfMemoryError\" during JOIN**\n",
    "**Solution:**\n",
    "```python\n",
    "# Increase partitions before JOIN\n",
    "df1 = df1.repartition(200, \"join_key\")\n",
    "df2 = df2.repartition(200, \"join_key\")\n",
    "\n",
    "# Or use broadcast join for small tables\n",
    "from pyspark.sql.functions import broadcast\n",
    "result = large_df.join(broadcast(small_df), \"key\")\n",
    "```\n",
    "\n",
    "**Problem 3: Long write times to Delta**\n",
    "**Solution:**\n",
    "- Enable Auto Compaction\n",
    "- Use `coalesce()` before write\n",
    "- Avoid too high partitioning\n",
    "\n",
    "**Problem 4: OPTIMIZE does not improve performance**\n",
    "**Cause:** ZORDER BY is needed for specific query patterns\n",
    "```python\n",
    "# Instead of just OPTIMIZE:\n",
    "OPTIMIZE table_name\n",
    "\n",
    "# Use OPTIMIZE with ZORDER:\n",
    "OPTIMIZE table_name ZORDER BY (frequently_filtered_columns)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6d22f78-2445-487b-931a-e6eb94f7209c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "Opcjonalnie usuÅ„ tabele demo utworzone podczas Ä‡wiczeÅ„:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5b1ec66-98c3-49a5-a03a-8a180a2958a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup - usuÅ„ tabele demo utworzone w tym notebooku\n",
    "\n",
    "# Odkomentuj poniÅ¼sze linie aby usunÄ…Ä‡ tabele demo:\n",
    "\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {ORDERS_OPT}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CUSTOMERS_OPT}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {ORDERS_PARTITIONED}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {SMALL_FILES_TABLE}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {LIQUID_TABLE}\")\n",
    "\n",
    "# print(\"âœ… Wszystkie tabele demo usuniÄ™te\")\n",
    "\n",
    "print(\"â„¹ï¸ Cleanup wyÅ‚Ä…czony (odkomentuj kod aby usunÄ…Ä‡ tabele demo)\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_optimization_best_practices",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
