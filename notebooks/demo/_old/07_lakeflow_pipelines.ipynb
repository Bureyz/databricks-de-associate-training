{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1297fb2-7aa7-454e-884e-7d100f753890",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Lakeflow Spark Declarative Pipelines\n",
    "\n",
    "**Training Objective:** Understanding Lakeflow declarative framework for building batch and streaming pipelines, plus practical implementation of Bronze→Silver→Gold with SQL API.\n",
    "\n",
    "**Topics Covered:**\n",
    "- Lakeflow concepts: declarative approach to pipeline definition\n",
    "- SQL vs Python API (focus on SQL)\n",
    "- Materialized views / streaming tables\n",
    "- Expectations: warn / drop / fail (data quality)\n",
    "- Event log and lineage per table\n",
    "- Automatic orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38b63fab-c778-461e-8e12-2bb40d6703e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Context and Requirements\n",
    "\n",
    "- **Training Day**: Day 3 - Transformation & Governance\n",
    "- **Notebook Type**: Demo\n",
    "- **Technical Requirements**:\n",
    " - Databricks Runtime 16.4 LTS or newer (recommended: 17.3 LTS)\n",
    " - Unity Catalog enabled\n",
    " - Permissions: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    " - Cluster: Standard or Serverless Compute\n",
    " \n",
    "**Note:** This notebook demonstrates **SQL API** for Lakeflow SDP. Python API (`create_streaming_table()`, `table()`) is an alternative with the same functionality.\n",
    "\n",
    "> **Update (June 2025):** The product name changed from \"Delta Live Tables (DLT)\" to \"Lakeflow Spark Declarative Pipelines (SDP)\". The functionality remains the same. Additionally, \"Databricks Jobs\" is now \"Lakeflow Jobs\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76ef371e-3db2-4942-b0e4-9690415b4d02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Theoretical Introduction - Lakeflow Spark Declarative Pipelines\n",
    "\n",
    "**Section Objective:** Understanding what Lakeflow SDP is and how it revolutionizes ETL/ELT pipeline building.\n",
    "\n",
    "---\n",
    "\n",
    "### What is Lakeflow Spark Declarative Pipelines?\n",
    "\n",
    "**Lakeflow Spark Declarative Pipelines (SDP)** is a declarative framework for creating batch and streaming data pipelines in SQL and Python. It extends Apache Spark Declarative Pipelines, running on the optimized Databricks Runtime.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│ TRADITIONAL APPROACH (Procedural) │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│ 1. Write code: df = spark.read.table(...) │\n",
    "│ 2. Write transformations: df.filter().groupBy()... │\n",
    "│ 3. Write orchestration: if/else, try/catch, retry logic │\n",
    "│ 4. Write monitoring: log metrics, track failures │\n",
    "│ 5. Write quality checks: manual assertions │\n",
    "│ 6. Deploy: schedule in Jobs, manage dependencies │\n",
    "│ │\n",
    "│ = Hundreds of lines of code, manual orchestration, error handling │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "\n",
    " \n",
    "\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│ LAKEFLOW SDP (Declarative) │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│ 1. Declare WHAT you want (WHAT): │\n",
    "│ CREATE OR REFRESH STREAMING TABLE bronze AS ... │\n",
    "│ CREATE OR REFRESH MATERIALIZED VIEW silver AS ... │\n",
    "│ CREATE OR REFRESH MATERIALIZED VIEW gold AS ... │\n",
    "│ │\n",
    "│ 2. Lakeflow automatically: │\n",
    "│ Orchestrates order (dependency DAG) │\n",
    "│ Retry on failures │\n",
    "│ Incremental processing │\n",
    "│ Monitoring (Event Log) │\n",
    "│ Data quality (expectations) │\n",
    "│ Lineage tracking │\n",
    "│ │\n",
    "│ = A dozen lines of SQL, zero orchestration code │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Benefits of Lakeflow SDP\n",
    "\n",
    "**1. Automatic Orchestration**\n",
    "\n",
    "Lakeflow automatically:\n",
    "- Analyzes dependencies between tables (who reads from whom)\n",
    "- Builds DAG (Directed Acyclic Graph)\n",
    "- Executes in correct order with maximum parallelization\n",
    "- Retry at levels: task → flow → pipeline\n",
    "\n",
    "```sql\n",
    "-- Just declare:\n",
    "CREATE OR REFRESH MATERIALIZED VIEW silver AS \n",
    " SELECT * FROM bronze; -- Lakeflow knows: silver depends on bronze\n",
    "\n",
    "CREATE OR REFRESH MATERIALIZED VIEW gold AS \n",
    " SELECT * FROM silver; -- Lakeflow knows: gold depends on silver\n",
    "\n",
    "-- Execution order: bronze → silver → gold (automatic!)\n",
    "```\n",
    "\n",
    "**2. Declarative Processing**\n",
    "\n",
    "Declarative API reduces hundreds of lines of code to a few:\n",
    "\n",
    "```sql\n",
    "-- Traditional (procedural):\n",
    "-- 1. Read source\n",
    "-- 2. Apply transformations\n",
    "-- 3. Handle schema evolution\n",
    "-- 4. Write to Delta\n",
    "-- 5. Error handling\n",
    "-- 6. Retry logic\n",
    "-- 7. Metrics logging\n",
    "-- = ~100+ lines of code\n",
    "\n",
    "-- Lakeflow (declarative):\n",
    "CREATE OR REFRESH STREAMING TABLE orders AS\n",
    " SELECT * FROM STREAM read_files('/path/to/orders');\n",
    "-- = 2 lines, all of the above is automatic!\n",
    "```\n",
    "\n",
    "**3. Incremental Processing**\n",
    "\n",
    "Lakeflow processes only new/changed data:\n",
    "\n",
    "- **Streaming tables**: Append-only, each record once\n",
    "- **Materialized views**: Incremental refresh (Databricks detects changes in source)\n",
    "- **AUTO CDC**: Out-of-order events handling, SCD Type 1/2\n",
    "\n",
    "**4. Built-in Data Quality**\n",
    "\n",
    "Expectations = SQL constraints with flexible handling:\n",
    "\n",
    "```sql\n",
    "CREATE OR REFRESH STREAMING TABLE orders (\n",
    " CONSTRAINT valid_amount EXPECT (total_amount > 0) ON VIOLATION DROP ROW,\n",
    " CONSTRAINT valid_date EXPECT (order_date IS NOT NULL) ON VIOLATION FAIL UPDATE\n",
    ")\n",
    "AS SELECT * FROM ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Basic Concepts\n",
    "\n",
    "- **Lakeflow SDP**: Declarative framework for batch + streaming pipelines\n",
    "- **Flow**: Processing unit (Append, AUTO CDC, Materialized View)\n",
    "- **STREAMING TABLE**: Delta table for streaming/incremental data (append-only, low-latency)\n",
    "- **MATERIALIZED VIEW**: Delta table with incremental refresh (batch, cache results)\n",
    "- **VIEW (temporary)**: Ephemeral, no persist, always recompute\n",
    "- **SINK**: Streaming target (Delta, Kafka, EventHub, custom Python)\n",
    "- **Pipeline**: Collection of flows + tables + views + sinks (unit of deployment)\n",
    "- **Expectations**: Data quality constraints (warn/drop/fail)\n",
    "- **Event Log**: Delta table with metrics, lineage, quality metrics\n",
    "\n",
    "**Why is this important?**\n",
    "\n",
    "Lakeflow SDP eliminates boilerplate code and lets you focus on business logic instead of orchestration. The declarative model provides:\n",
    "- **Separation of concerns**: WHAT (declaration) vs HOW (execution engine)\n",
    "- **Reusability**: Same declarations in dev/test/prod\n",
    "- **Observability**: Event Log out-of-the-box\n",
    "- **Reliability**: Automatic retry and error handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f44a05c2-05f0-4095-aa47-7994f88c0419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Per-User Isolation\n",
    "\n",
    "Run the initialization script for per-user catalog and schema isolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21b89975-086f-4205-ba28-b584620ec1de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba85972d-9ddb-4cc5-8a69-dde4ef27651e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuration\n",
    "\n",
    "Library imports and environment variable setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84b6bb78-bd75-4c53-b4be-25e2e1af8f2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "# Set catalog as default\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "\n",
    "# Source data paths\n",
    "ORDERS_JSON = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "CUSTOMERS_CSV = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "PRODUCTS_PARQUET = f\"{DATASET_BASE_PATH}/products/products.parquet\"\n",
    "\n",
    "# Display user context\n",
    "display(spark.createDataFrame([\n",
    "    (\"Catalog\", CATALOG),\n",
    "    (\"Schema Bronze\", BRONZE_SCHEMA),\n",
    "    (\"Schema Silver\", SILVER_SCHEMA),\n",
    "    (\"Schema Gold\", GOLD_SCHEMA),\n",
    "    (\"User\", raw_user),\n",
    "    (\"Orders path\", ORDERS_JSON),\n",
    "    (\"Customers path\", CUSTOMERS_CSV),\n",
    "    (\"Products path\", PRODUCTS_PARQUET)\n",
    "], [\"Parameter\", \"Value\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a93c5e5-5bac-45a3-802a-d3683fa75269",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Section 1: Lakeflow Concepts - Flows, Tables, Views\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Lakeflow SDP operates on three key concepts: **Flows** (how data flows), **Streaming Tables** (append-only targets), and **Materialized Views** (batch targets with incremental refresh).\n",
    "\n",
    "---\n",
    "\n",
    "### Flow Types (Data Flow Types)\n",
    "\n",
    "**Flow** is a data processing unit in Lakeflow - it defines HOW data flows from source to target.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│ FLOW TYPES │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│ 1. APPEND FLOW │\n",
    "│ • Source: Append-only (files, Kafka, Kinesis, Delta) │\n",
    "│ • Semantics: Streaming (continuous processing) │\n",
    "│ • Guarantee: Exactly-once per record │\n",
    "│ • Latency: Low (seconds) │\n",
    "│ • Use case: Real-time ingest, log streaming │\n",
    "│ • Target: STREAMING TABLE │\n",
    "│ │\n",
    "│ SQL Example: │\n",
    "│ CREATE OR REFRESH STREAMING TABLE orders AS │\n",
    "│ SELECT * FROM STREAM read_files('/path'); │\n",
    "│ │\n",
    "│ 2. AUTO CDC FLOW │\n",
    "│ • Source: Change Data Capture (CDF-enabled Delta) │\n",
    "│ • Semantics: Streaming with CDC operations │\n",
    "│ • Operations: INSERT, UPDATE, DELETE, TRUNCATE │\n",
    "│ • Sequencing: Out-of-order handling (automatic) │\n",
    "│ • SCD: Type 1 (update) or Type 2 (history tracking) │\n",
    "│ • Use case: Sync with transactional DB, audit trail │\n",
    "│ • Target: STREAMING TABLE │\n",
    "│ │\n",
    "│ SQL Example: │\n",
    "│ AUTO CDC INTO target_table │\n",
    "│ FROM source_table │\n",
    "│ KEYS (user_id) │\n",
    "│ SEQUENCE BY timestamp │\n",
    "│ APPLY AS DELETE WHEN operation = 'DELETE'; │\n",
    "│ │\n",
    "│ 3. MATERIALIZED VIEW FLOW │\n",
    "│ • Source: Batch read (Delta tables, views) │\n",
    "│ • Semantics: Batch (scheduled/triggered) │\n",
    "│ • Refresh: Incremental (only changed partitions) │\n",
    "│ • Cache: Results persisted (performance) │\n",
    "│ • Recompute: Full on schema changes or explicit │\n",
    "│ • Use case: Aggregations, slow queries, BI dashboards │\n",
    "│ • Target: MATERIALIZED VIEW │\n",
    "│ │\n",
    "│ SQL Example: │\n",
    "│ CREATE OR REFRESH MATERIALIZED VIEW daily_summary AS │\n",
    "│ SELECT date, SUM(amount) FROM orders GROUP BY date; │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Key Differences:**\n",
    "\n",
    "| Flow Type | Processing | Source | Latency | Incremental | Use Case |\n",
    "|-----------|------------|--------|---------|-------------|----------|\n",
    "| **Append** | Streaming | Append-only | Seconds | (watermarks) | Real-time ingest |\n",
    "| **AUTO CDC** | Streaming | CDC events | Seconds | (sequencing) | DB sync, SCD |\n",
    "| **Materialized View** | Batch | Any Delta | Minutes | (smart refresh) | Aggregations, BI |\n",
    "\n",
    "---\n",
    "\n",
    "### STREAMING TABLE vs MATERIALIZED VIEW\n",
    "\n",
    "| Aspect | STREAMING TABLE | MATERIALIZED VIEW |\n",
    "|--------|-----------------|-------------------|\n",
    "| **Semantics** | Streaming (continuous) | Batch (scheduled/triggered) |\n",
    "| **Processing** | Exactly-once per record | Incremental refresh (changed data) |\n",
    "| **Source** | `STREAM` keyword required | Batch read (no STREAM) |\n",
    "| **Latency** | Low (seconds) | Higher (minutes) |\n",
    "| **State** | Bounded (watermarks) | Stateless (recompute) |\n",
    "| **Joins** | Stream-snapshot (static dims) | Full recompute (always correct) |\n",
    "| **Use case** | Real-time ingest, CDC | Aggregations, slow queries |\n",
    "| **Schema evolution** | Limited (full refresh) | Flexible |\n",
    "\n",
    "**When to use:**\n",
    "- **STREAMING TABLE**: Ingest from files/Kafka, CDC, low-latency transformations\n",
    "- **MATERIALIZED VIEW**: Aggregations, joins with frequent dimension changes, pre-compute slow queries\n",
    "\n",
    "---\n",
    "\n",
    "### VIEW (temporary)\n",
    "\n",
    "**VIEW** is an ephemeral object - no persist, always recompute on query.\n",
    "\n",
    "**Use cases:**\n",
    "- Intermediate transformations (reusable logic)\n",
    "- Data quality checks (don't publish to catalog)\n",
    "- Testing (don't save to Delta)\n",
    "\n",
    "```sql\n",
    "-- VIEW: doesn't save to Delta\n",
    "CREATE OR REFRESH VIEW temp_filtered AS\n",
    " SELECT * FROM bronze WHERE status = 'ACTIVE';\n",
    "\n",
    "-- Use in downstream table\n",
    "CREATE OR REFRESH MATERIALIZED VIEW silver AS\n",
    " SELECT * FROM temp_filtered;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Automatic Dependency Resolution (DAG)\n",
    "\n",
    "Lakeflow automatically builds DAG from dependencies:\n",
    "\n",
    "```sql\n",
    "-- Declarations (you don't specify order):\n",
    "CREATE OR REFRESH STREAMING TABLE bronze AS ...;\n",
    "CREATE OR REFRESH MATERIALIZED VIEW silver AS SELECT * FROM bronze;\n",
    "CREATE OR REFRESH MATERIALIZED VIEW gold AS SELECT * FROM silver;\n",
    "\n",
    "-- Lakeflow execution order (automatic):\n",
    "-- 1. bronze (no dependencies)\n",
    "-- 2. silver (depends on bronze) [parallel if multiple silvers]\n",
    "-- 3. gold (depends on silver) [parallel if multiple golds]\n",
    "```\n",
    "\n",
    "**Key point:** You declare WHAT, Lakeflow decides HOW and WHEN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d221bda7-7e69-4ae1-9980-c98afff01f9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 1.1: STREAMING TABLE - Bronze Layer Ingest\n",
    "\n",
    "**Objective:** Demonstrate STREAMING TABLE for real-time ingest with Auto Loader\n",
    "\n",
    "**Approach:**\n",
    "1. Use `read_files()` for Auto Loader (SQL API)\n",
    "2. `STREAM` keyword for streaming semantics\n",
    "3. Write to STREAMING TABLE (append-only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aabe110-6e8e-4cfc-9d4c-b3f1aba36489",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example 1.1 - Bronze Layer (traditional approach for demonstration)\n",
    "# In production pipeline, we would use Lakeflow CREATE OR REFRESH STREAMING TABLE\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "# Bronze layer: load raw data from JSON (batch for demo)\n",
    "orders_bronze_df = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(ORDERS_JSON)\n",
    "    .withColumn(\"_bronze_ingest_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"_bronze_source_file\", F.input_file_name())\n",
    "    .withColumn(\"_bronze_ingested_by\", F.lit(raw_user))\n",
    "    .withColumn(\"_bronze_version\", F.lit(1))\n",
    ")\n",
    "\n",
    "# Save to Delta (Bronze table)\n",
    "bronze_table = \"orders_bronze\"\n",
    "(\n",
    "    orders_bronze_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(bronze_table)\n",
    ")\n",
    "\n",
    "# Preview\n",
    "display(spark.table(bronze_table).limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "016f2855-b386-4fd5-8ea2-85a511dccde1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Section 2: Silver Layer - MATERIALIZED VIEW + Expectations\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Silver layer cleans and validates data from Bronze. MATERIALIZED VIEW provides incremental refresh - processes only changed data. Expectations are built-in data quality constraints.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **MATERIALIZED VIEW**: Batch processing with incremental refresh\n",
    "- **Expectations**: SQL constraints with actions: EXPECT (warn), DROP ROW, FAIL UPDATE\n",
    "- **Data Quality Gates**: Validations between layers\n",
    "\n",
    "**Practical Application:**\n",
    "- Deduplication by business key\n",
    "- Validation: NOT NULL, ranges, business rules\n",
    "- Standardization: dates, text formats, type casting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a15b0a84-920c-4e59-8716-bdbc3e8913b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 2.1: MATERIALIZED VIEW with Expectations (Silver Layer)\n",
    "\n",
    "**Objective:** Demonstrate MATERIALIZED VIEW for Silver with data quality constraints\n",
    "\n",
    "**Lakeflow SQL Syntax (Production):**\n",
    "\n",
    "```sql\n",
    "-- In production Lakeflow pipeline:\n",
    "CREATE OR REFRESH MATERIALIZED VIEW silver.orders_silver (\n",
    " -- Expectations: Data Quality Constraints\n",
    " CONSTRAINT valid_amount EXPECT (total_amount > 0) ON VIOLATION DROP ROW,\n",
    " CONSTRAINT valid_date EXPECT (order_datetime IS NOT NULL) ON VIOLATION DROP ROW,\n",
    " CONSTRAINT valid_ids EXPECT (order_id IS NOT NULL AND customer_id IS NOT NULL)\n",
    ")\n",
    "COMMENT 'Silver layer - cleaned orders with quality checks'\n",
    "AS\n",
    "SELECT\n",
    " order_id,\n",
    " customer_id,\n",
    " product_id,\n",
    " store_id,\n",
    " to_date(order_datetime) AS order_date,\n",
    " to_timestamp(order_datetime) AS order_timestamp,\n",
    " quantity,\n",
    " unit_price,\n",
    " CAST(total_amount AS DECIMAL(10,2)) AS total_amount,\n",
    " UPPER(TRIM(payment_method)) AS payment_method,\n",
    " CASE \n",
    " WHEN total_amount > 0 THEN 'COMPLETED'\n",
    " ELSE 'UNKNOWN'\n",
    " END AS order_status,\n",
    " current_timestamp() AS _silver_processed_timestamp,\n",
    " 'VALID' AS _data_quality_flag\n",
    "FROM bronze.orders_bronze;\n",
    "```\n",
    "\n",
    "**Expectations Explanation:**\n",
    "- **EXPECT (warn)**: Log violation, keep record (default)\n",
    "- **ON VIOLATION DROP ROW**: Remove invalid record\n",
    "- **ON VIOLATION FAIL UPDATE**: Abort pipeline on violation (strict mode)\n",
    "\n",
    "**Traditional Implementation (for notebook demo):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28c31e9d-dff0-45d2-8ab2-89e96d077364",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example 2.1 - Silver Layer with data quality (traditional approach for demo)\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {SILVER_SCHEMA}\")\n",
    "\n",
    "# Load data from Bronze\n",
    "orders_bronze_df = spark.table(f\"{BRONZE_SCHEMA}.{bronze_table}\")\n",
    "\n",
    "# Silver transformations with validation (Expectations simulation)\n",
    "orders_silver_df = (\n",
    "    orders_bronze_df\n",
    "    # Deduplication\n",
    "    .dropDuplicates([\"order_id\"])\n",
    "    # NOT NULL validation (DROP ROW equivalent)\n",
    "    .filter(F.col(\"order_id\").isNotNull())\n",
    "    .filter(F.col(\"customer_id\").isNotNull())\n",
    "    .filter(F.col(\"product_id\").isNotNull())\n",
    "    # Business rule validation\n",
    "    .filter(F.col(\"total_amount\") > 0)\n",
    "    .filter(F.col(\"order_datetime\").isNotNull())\n",
    "    # Standardization\n",
    "    .withColumn(\"order_date\", F.to_date(F.col(\"order_datetime\")))\n",
    "    .withColumn(\"order_timestamp\", F.to_timestamp(F.col(\"order_datetime\")))\n",
    "    .withColumn(\"total_amount\", F.col(\"total_amount\").cast(\"decimal(10,2)\"))\n",
    "    .withColumn(\"payment_method\", F.upper(F.trim(F.col(\"payment_method\"))))\n",
    "    # Derived columns\n",
    "    .withColumn(\"order_status\", \n",
    "                F.when(F.col(\"total_amount\") > 0, \"COMPLETED\").otherwise(\"UNKNOWN\"))\n",
    "    # Silver metadata\n",
    "    .withColumn(\"_silver_processed_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"_data_quality_flag\", F.lit(\"VALID\"))\n",
    ")\n",
    "\n",
    "# Quality metrics\n",
    "bronze_count = orders_bronze_df.count()\n",
    "silver_count = orders_silver_df.count()\n",
    "rejected_count = bronze_count - silver_count\n",
    "rejection_rate = (rejected_count / bronze_count * 100) if bronze_count > 0 else 0\n",
    "\n",
    "# Data Quality Metrics\n",
    "display(spark.createDataFrame([\n",
    "    (\"Bronze input\", bronze_count),\n",
    "    (\"Silver output\", silver_count),\n",
    "    (\"Rejected\", rejected_count),\n",
    "    (\"Rejection rate %\", round(rejection_rate, 2))\n",
    "], [\"Metric\", \"Value\"]))\n",
    "\n",
    "# Save to Silver schema\n",
    "silver_table = \"orders_silver\"\n",
    "(\n",
    "    orders_silver_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(silver_table)\n",
    ")\n",
    "\n",
    "# Sample Silver data\n",
    "display(spark.table(silver_table).limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "100b5e45-bc21-43d1-8da6-cf4287a6106d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Section 3: Gold Layer - Business Aggregates\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Gold layer contains pre-aggregated business metrics, denormalized tables, and KPIs. MATERIALIZED VIEW with incremental refresh ensures only affected partitions are recalculated.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Business-level aggregates**: Daily/Monthly summaries, KPIs\n",
    "- **Denormalization**: Pre-computed joins for performance\n",
    "- **Incremental refresh**: Only affected partitions\n",
    "\n",
    "**Practical Application:**\n",
    "- BI dashboards (Power BI, Tableau)\n",
    "- Executive reporting\n",
    "- ML feature stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddf6416f-11bc-4b6b-8a3b-7bd9c649a30f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 3.1: MATERIALIZED VIEW for Gold (Daily Aggregates)\n",
    "\n",
    "**Objective:** Demonstrate Gold layer with business aggregates and KPIs\n",
    "\n",
    "**Lakeflow SQL Syntax (Production):**\n",
    "\n",
    "```sql\n",
    "-- In production Lakeflow pipeline:\n",
    "CREATE OR REFRESH MATERIALIZED VIEW gold.daily_order_summary\n",
    "COMMENT 'Gold layer - daily order summary (KPI)'\n",
    "AS\n",
    "SELECT\n",
    " order_date,\n",
    " order_status,\n",
    " -- Volume metrics\n",
    " COUNT(order_id) AS total_orders,\n",
    " COUNT(DISTINCT customer_id) AS unique_customers,\n",
    " -- Revenue metrics\n",
    " SUM(total_amount) AS total_revenue,\n",
    " AVG(total_amount) AS avg_order_value,\n",
    " MIN(total_amount) AS min_order_value,\n",
    " MAX(total_amount) AS max_order_value,\n",
    " -- Derived KPIs\n",
    " ROUND(SUM(total_amount) / COUNT(DISTINCT customer_id), 2) AS revenue_per_customer,\n",
    " -- Gold metadata\n",
    " current_timestamp() AS _gold_created_timestamp,\n",
    " 'DAILY' AS _gold_aggregation_level\n",
    "FROM silver.orders_silver\n",
    "GROUP BY order_date, order_status\n",
    "ORDER BY order_date DESC, order_status;\n",
    "```\n",
    "\n",
    "**Automatic Dependency:** Lakeflow knows that `gold.daily_order_summary` depends on `silver.orders_silver` → automatic execution order!\n",
    "\n",
    "**Traditional Implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "431625e6-c7b6-4600-85d5-799bf7a4b9ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example 3.1 - Gold Layer (Daily Aggregates)\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {GOLD_SCHEMA}\")\n",
    "\n",
    "# Load data from Silver\n",
    "orders_silver_df = spark.table(f\"{SILVER_SCHEMA}.{silver_table}\")\n",
    "\n",
    "# Gold aggregation: Daily order summary with KPIs\n",
    "daily_summary_df = (\n",
    "    orders_silver_df\n",
    "    .groupBy(\"order_date\", \"order_status\")\n",
    "    .agg(\n",
    "        # Volume metrics\n",
    "        F.count(\"order_id\").alias(\"total_orders\"),\n",
    "        F.countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "        # Revenue metrics\n",
    "        F.sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "        F.avg(\"total_amount\").alias(\"avg_order_value\"),\n",
    "        F.min(\"total_amount\").alias(\"min_order_value\"),\n",
    "        F.max(\"total_amount\").alias(\"max_order_value\")\n",
    "    )\n",
    "    # Derived KPIs\n",
    "    .withColumn(\"revenue_per_customer\", \n",
    "                F.round(F.col(\"total_revenue\") / F.col(\"unique_customers\"), 2))\n",
    "    # Gold metadata\n",
    "    .withColumn(\"_gold_created_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"_gold_aggregation_level\", F.lit(\"DAILY\"))\n",
    "    .orderBy(\"order_date\", \"order_status\")\n",
    ")\n",
    "\n",
    "# Summary statistics\n",
    "total_days = daily_summary_df.select(\"order_date\").distinct().count()\n",
    "total_orders_gold = daily_summary_df.agg(F.sum(\"total_orders\")).collect()[0][0]\n",
    "total_revenue_gold = daily_summary_df.agg(F.sum(\"total_revenue\")).collect()[0][0]\n",
    "\n",
    "# Gold layer summary\n",
    "display(spark.createDataFrame([\n",
    "    (\"Total days aggregated\", str(total_days)),\n",
    "    (\"Total orders\", f\"{total_orders_gold:,}\"),\n",
    "    (\"Total revenue\", f\"${total_revenue_gold:,.2f}\")\n",
    "], [\"Metric\", \"Value\"]))\n",
    "\n",
    "# Save to Gold schema\n",
    "gold_table = \"daily_order_summary\"\n",
    "(\n",
    "    daily_summary_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(gold_table)\n",
    ")\n",
    "\n",
    "# Sample Gold data (daily KPIs)\n",
    "display(spark.table(gold_table).limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb5eaa33-42b4-42a5-849b-201efc47335e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Section 4: Event Log and Lineage\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Lakeflow automatically logs all operations to **Event Log** (Delta table). Event Log contains:\n",
    "- Flow progress (success/failure per table)\n",
    "- Data quality metrics (expectations violations)\n",
    "- Lineage tracking (source → target)\n",
    "- Performance metrics (duration, records processed)\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Event Log**: Delta table in `system/events` (per pipeline)\n",
    "- **Flow types**: `flow_definition`, `flow_progress`, `expectation`, `user_action`\n",
    "- **Lineage**: Automatic dependency tracking Bronze → Silver → Gold\n",
    "\n",
    "**Practical Application:**\n",
    "- Monitoring pipeline health\n",
    "- Debugging failures\n",
    "- Data quality reporting\n",
    "- Audit and compliance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c6f3181-889c-4ecf-97ad-58a61fee4bc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 4.1: Event Log - Monitoring and Lineage\n",
    "\n",
    "**Event Log Location:**\n",
    "```\n",
    "dbfs:/pipelines/<pipeline_id>/system/events\n",
    "```\n",
    "\n",
    "**Sample Event Log Queries (in production Lakeflow pipeline):**\n",
    "\n",
    "```python\n",
    "# 1. Query Event Log\n",
    "event_log_path = \"dbfs:/pipelines/<pipeline_id>/system/events\"\n",
    "event_log_df = spark.read.format(\"delta\").load(event_log_path)\n",
    "\n",
    "# 2. Flow progress per table\n",
    "flow_progress = (\n",
    " event_log_df\n",
    " .filter(\"event_type = 'flow_progress'\")\n",
    " .select(\"timestamp\", \"details.flow_name\", \"details.output_records\", \"details.status\")\n",
    ")\n",
    "\n",
    "# 3. Expectations violations (data quality metrics)\n",
    "expectations_df = (\n",
    " event_log_df\n",
    " .filter(\"event_type = 'expectation'\")\n",
    " .select(\n",
    " \"timestamp\",\n",
    " \"details.dataset\",\n",
    " \"details.name\",\n",
    " \"details.passed_records\",\n",
    " \"details.failed_records\"\n",
    " )\n",
    ")\n",
    "\n",
    "# 4. Lineage tracking\n",
    "lineage_df = (\n",
    " event_log_df\n",
    " .filter(\"event_type = 'flow_definition'\")\n",
    " .select(\"details.flow_name\", \"details.input_datasets\", \"details.output_dataset\")\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20b05740-734d-4a85-a826-2abb1a8004d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Section 5: SQL vs Python API\n",
    "\n",
    "**Introduction:**\n",
    "\n",
    "Lakeflow SDP offers two equivalent APIs: **SQL** and **Python**. The choice depends on team preferences and use case.\n",
    "\n",
    "### Syntax Comparison\n",
    "\n",
    "| Aspect | SQL | Python |\n",
    "|--------|-----|--------|\n",
    "| **STREAMING TABLE** | `CREATE OR REFRESH STREAMING TABLE` | `@dp.table()` |\n",
    "| **MATERIALIZED VIEW** | `CREATE OR REFRESH MATERIALIZED VIEW` | `@dp.materialized_view()` |\n",
    "| **VIEW** | `CREATE OR REFRESH VIEW` | `@dp.view()` / `@dp.temporary_view()` |\n",
    "| **Expectations** | `CONSTRAINT ... EXPECT ... ON VIOLATION` | `@dp.expect()`, `@dp.expect_or_drop()`, `@dp.expect_or_fail()` |\n",
    "| **Streaming read** | `FROM STREAM table` | `spark.readStream.table()` |\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Same Pipeline in SQL and Python\n",
    "\n",
    "**SQL Approach:**\n",
    "\n",
    "```sql\n",
    "-- Bronze\n",
    "CREATE OR REFRESH STREAMING TABLE bronze.orders AS\n",
    "SELECT * FROM STREAM read_files('/path/orders', format => 'json');\n",
    "\n",
    "-- Silver\n",
    "CREATE OR REFRESH MATERIALIZED VIEW silver.orders (\n",
    " CONSTRAINT valid_amount EXPECT (total_amount > 0) ON VIOLATION DROP ROW\n",
    ")\n",
    "AS SELECT \n",
    " order_id, \n",
    " customer_id, \n",
    " CAST(total_amount AS DECIMAL(10,2)) AS total_amount\n",
    "FROM bronze.orders;\n",
    "\n",
    "-- Gold\n",
    "CREATE OR REFRESH MATERIALIZED VIEW gold.daily_summary AS\n",
    "SELECT \n",
    " DATE(order_date) AS date,\n",
    " SUM(total_amount) AS revenue\n",
    "FROM silver.orders\n",
    "GROUP BY DATE(order_date);\n",
    "```\n",
    "\n",
    "**Python Approach (equivalent):**\n",
    "\n",
    "```python\n",
    "from pyspark import pipelines as dp\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Bronze\n",
    "@dp.table(comment=\"Bronze orders\")\n",
    "def orders_bronze():\n",
    " return (\n",
    " spark.readStream\n",
    " .format(\"cloudFiles\")\n",
    " .option(\"cloudFiles.format\", \"json\")\n",
    " .load(\"/path/orders\")\n",
    " )\n",
    "\n",
    "# Silver\n",
    "@dp.materialized_view(comment=\"Silver orders\")\n",
    "@dp.expect_or_drop(\"valid_amount\", \"total_amount > 0\")\n",
    "def orders_silver():\n",
    " return (\n",
    " spark.read.table(\"bronze.orders\")\n",
    " .select(\n",
    " \"order_id\",\n",
    " \"customer_id\",\n",
    " F.col(\"total_amount\").cast(\"decimal(10,2)\")\n",
    " )\n",
    " )\n",
    "\n",
    "# Gold\n",
    "@dp.materialized_view(comment=\"Gold daily summary\")\n",
    "def daily_summary():\n",
    " return (\n",
    " spark.read.table(\"silver.orders\")\n",
    " .groupBy(F.to_date(\"order_date\").alias(\"date\"))\n",
    " .agg(F.sum(\"total_amount\").alias(\"revenue\"))\n",
    " )\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use SQL vs Python?\n",
    "\n",
    "**Use SQL if:**\n",
    "- Team has strong SQL skills\n",
    "- Simple transformations (filters, aggregations)\n",
    "- Integration with BI tools (SQL-native workflows)\n",
    "- Less metaprogramming\n",
    "\n",
    "**Use Python if:**\n",
    "- You need loops / dynamic table creation\n",
    "- Complex transformations (UDFs, window functions)\n",
    "- Integration with ML workflows\n",
    "- Testing (unit tests for transformations)\n",
    "\n",
    "**Best practice:** Mix them! SQL for simple, Python for complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1176f1b-d38e-4a0b-8b7b-1d2d9390606a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Resource Cleanup\n",
    "\n",
    "Clean up resources created during the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9544be8f-4fdd-4b28-8705-db6415f13d2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optional cleanup of test resources\n",
    "# WARNING: Run only if you want to delete all created data\n",
    "\n",
    "# Delete Demo tables\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {BRONZE_SCHEMA}.{bronze_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {SILVER_SCHEMA}.{silver_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {GOLD_SCHEMA}.{gold_table}\")\n",
    "\n",
    "# Clear cache\n",
    "# spark.catalog.clearCache()\n",
    "\n",
    "displayHTML(\"<p> To delete tables, uncomment the code above and run the cell</p>\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_lakeflow_pipelines",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
