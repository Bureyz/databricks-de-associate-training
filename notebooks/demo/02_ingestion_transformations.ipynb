{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6bc86c4-58e5-4f4f-af86-f3311937bb70",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Introduction"
    }
   },
   "source": [
    "# 2. Data Ingestion, Transformations & Analysis\n",
    "\n",
    "## 2.1. The Story Continues...\n",
    "\n",
    "Remember our e-commerce company? The data team has just gained access to multiple source systems:\n",
    "\n",
    "* **CRM System** exports customers as CSV (daily dump)\n",
    "* **Order Management** sends JSON via API \n",
    "* **Product Catalog** is in Parquet (from a legacy Hadoop system)\n",
    "* **Extended Customer Data** arrives in Excel from the marketing team\n",
    "\n",
    "**Your mission:** Import data from all sources, transform it, analyze it, and prepare it for downstream analytics.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.2. Why This Matters (Real-World Context)\n",
    "\n",
    "### The \"inferSchema\" Trap\n",
    "\n",
    "Most tutorials show: `spark.read.option(\"inferSchema\", \"true\")`\n",
    "\n",
    "**In production, this is often a bad idea:**\n",
    "\n",
    "| Scenario         | With inferSchema         | With explicit schema      |\n",
    "|------------------|-------------------------|--------------------------|\n",
    "| 10 GB file       | Scans entire file first | Direct read              |\n",
    "| Column `\"123\"`   | Might be INT or STRING? | You control it           |\n",
    "| New column added | Schema changes silently | Fails fast (good!)       |\n",
    "| Null values      | Might guess wrong type  | Explicit nullable        |\n",
    "\n",
    "**Rule of thumb:** Use `inferSchema=true` for exploration, explicit schema for production.\n",
    "\n",
    "### The Bronze Layer Philosophy\n",
    "\n",
    "In the Medallion architecture, the Bronze layer should:\n",
    "* Keep data as STRING (preserve original values)\n",
    "* Add metadata (ingestion time, source file)\n",
    "* NOT apply business logic\n",
    "* Be idempotent (safe to re-run)\n",
    "\n",
    "---\n",
    "\n",
    "### Data Process Patterns\n",
    "\n",
    "Understanding how data flows is critical for designing robust pipelines.\n",
    "\n",
    "| Pattern | Description | Trigger Type | Use Case |\n",
    "|---------|-------------|--------------|----------|\n",
    "| **Full Load** | Reloads the entire dataset from scratch every time. | Batch Read | Small tables, full history refreshes. |\n",
    "| **Incremental Batch** | Processes only new/changed data but runs as a scheduled job. | `Trigger.AvailableNow` (or `Once`) | Daily/Hourly ETL. efficient and cost-effective. |\n",
    "| **Continuous** | Continuously processes data as it arrives to minimize latency. | `Trigger.ProcessingTime` | Real-time monitoring, alerts. |\n",
    "\n",
    "![Data Ingestion Architecture](../../assets/images/8tLJIrTog2MFJouQVDHtP.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### Workshop Content Overview\n",
    "\n",
    "| Section | Topics Covered |\n",
    "|---------|----------------|\n",
    "| **Part 1: Data Ingestion** | • **DataFrame Reader API** – support for CSV, JSON, Parquet, Excel <br> • **Schema Definition** – explicit vs inferred approach <br> • **Format Options** – handling delimiters, quotes, headers |\n",
    "| **Part 2: DataFrame Operations** | • **Transformations** – `select`, `withColumn`, `cast`, `rename`, `drop` <br> • **Filtering** – complex conditions, null handling <br> • **Aggregations** – `groupBy`, `agg`, statistical functions |\n",
    "| **Part 3: Advanced Techniques** | • **SQL Integration** – Temp Views, mixed Python/SQL logic <br> • **Complex Types** – `explode`, `struct`, JSON parsing <br> • **Joins** – combining multiple datasets efficiently |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cf6c95b-2145-4ffe-a23d-8118983d4dd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.4. Environment Initialization\n",
    "\n",
    "Run the central configuration script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea9ed832-63c8-4788-8c7f-256fcf2ac6e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9981cb88-642b-4f84-8a87-ebf5d572e504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.4.1. Notebook Configuration\n",
    "\n",
    "Define variables specific to this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f11c5fa-8097-4ebe-89cf-93833a9e6f72",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Notebook Configuration"
    }
   },
   "outputs": [],
   "source": [
    "# Paths to data directories (subdirectories in DATASET_BASE_PATH from 00_setup)\n",
    "CUSTOMERS_PATH = f\"{DATASET_BASE_PATH}/customers\"\n",
    "ORDERS_PATH = f\"{DATASET_BASE_PATH}/orders\"\n",
    "PRODUCTS_PATH = f\"{DATASET_BASE_PATH}/products\"\n",
    "\n",
    "# Paths to specific files\n",
    "CUSTOMERS_CSV = f\"{CUSTOMERS_PATH}/customers.csv\"\n",
    "ORDERS_JSON = f\"{ORDERS_PATH}/orders_batch.json\"\n",
    "PRODUCTS_PARQUET = f\"{PRODUCTS_PATH}/products.parquet\"\n",
    "EXCEL_PATH = \"/Volumes/ecommerce_platform_trainer/default/datasets/customers/customers_extended.xlsx\"\n",
    "\n",
    "print(f\"Path to customers CSV file: {CUSTOMERS_CSV}\")\n",
    "print(f\"Path to orders JSON file: {ORDERS_JSON}\")\n",
    "print(f\"Path to products Parquet file: {PRODUCTS_PARQUET}\")\n",
    "print(f\"Path to Excel file: {EXCEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82bbd0b5-8936-432c-bbab-21e9f46ec084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.4.2. Import Libraries\n",
    "\n",
    "Import all necessary libraries for data ingestion and transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e438f20-ee94-4e47-aa8f-85f56d553a45",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "# Import PySpark types for schema definition\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, \n",
    "    DoubleType, TimestampType, DateType, ArrayType\n",
    ")\n",
    "\n",
    "# Import PySpark functions for transformations and aggregations\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, concat, upper, lower, year, month, day,\n",
    "    sum, avg, min, max, count, stddev, desc, asc,\n",
    "    explode, explode_outer, struct, array,\n",
    "    get_json_object, from_json, to_json,\n",
    "    to_date, current_date, datediff\n",
    ")\n",
    "\n",
    "# Import Python libraries\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "print(\"All imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "889facc3-12b1-46b1-ac58-004aef6b0971",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.5. CSV Data Import (Customers)\n",
    "\n",
    "### 2.5.1. Loading CSV with Automatic Schema Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e44f411-bfe5-4692-8552-458c1dcfad50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load CSV data with automatic schema inference\n",
    "customers_auto_df = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")       # First line contains column names\n",
    "    .option(\"inferSchema\", \"true\")  # Automatic data type inference\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bf57a69-f1da-43f1-ba1c-7adec2725f80",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CSV Row Count"
    }
   },
   "outputs": [],
   "source": [
    "# Count rows\n",
    "print(\"CSV loaded with inferSchema\")\n",
    "print(f\"Row count: {customers_auto_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e53bbb95-b397-4aaf-8b30-725f7e99dd0d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CSV Schema"
    }
   },
   "outputs": [],
   "source": [
    "# Display schema\n",
    "print(\"Schema detected automatically:\")\n",
    "customers_auto_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2cf3f4f-c94c-4b55-955a-4b04f4be829d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CSV Sample"
    }
   },
   "outputs": [],
   "source": [
    "# Display sample\n",
    "print(\"Data sample:\")\n",
    "display(customers_auto_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10f0f50b-3fa1-452a-85fd-f0df5773f08c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.5.4. Extended Reader Options – Typical Production Issues\n",
    "\n",
    "In a production environment, we often encounter CSV files with:\n",
    "\n",
    "- different separator (`;` instead of `,`),\n",
    "- quotes inside fields,\n",
    "- corrupted rows.\n",
    "\n",
    "Key options:\n",
    "\n",
    "- `delimiter` – custom column separator,\n",
    "- `quote` – character opening/closing text fields,\n",
    "- `escape` – way to \"escape\" special characters,\n",
    "- `mode` – handling of malformed records (`PERMISSIVE`, `DROPMALFORMED`, `FAILFAST`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5071f0b-5611-4d63-9e21-3f649dae5ee6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.5.5. Manual Schema Definition for CSV\n",
    "\n",
    "**Best Practice:** Defining schema manually ensures control and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1cdaa60-b1b1-46de-8604-7ae9aaa4c320",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Manual Schema Definition"
    }
   },
   "outputs": [],
   "source": [
    "# Schema definition for customers\n",
    "# Structure: customer_id (int), first_name (string), last_name (string), email (string), city (string), country (string), registration_date (timestamp)\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), nullable=False),\n",
    "    StructField(\"first_name\", StringType(), nullable=True),\n",
    "    StructField(\"last_name\", StringType(), nullable=True),\n",
    "    StructField(\"city\", StringType(), nullable=True),\n",
    "    StructField(\"email\", StringType(), nullable=True),\n",
    "    StructField(\"country\", StringType(), nullable=True),\n",
    "    StructField(\"registration_date\", TimestampType(), nullable=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bd147be-df3f-4274-8228-bf8cf6a626ba",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load CSV with Manual Schema"
    }
   },
   "outputs": [],
   "source": [
    "# Load CSV data with manually defined schema\n",
    "customers_df = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(customers_schema)\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")\n",
    "\n",
    "print(\"CSV loaded with manual schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2d3c9db-842b-4790-bce1-cda4d2363c26",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CSV Manual Count"
    }
   },
   "outputs": [],
   "source": [
    "# Count rows\n",
    "print(f\"Row count: {customers_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b45eb539-4abe-42d3-a71e-5f26ed0dcfee",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CSV Manual Schema"
    }
   },
   "outputs": [],
   "source": [
    "# Display schema\n",
    "print(\"Schema:\")\n",
    "customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66080f8a-f4fd-4cf6-b44e-4cd8277298d3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CSV Manual Sample"
    }
   },
   "outputs": [],
   "source": [
    "# Display sample\n",
    "print(\"Data sample:\")\n",
    "display(customers_df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df406dfc-49d8-4a2d-8823-053f287e7b45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load CSV data with automatic schema inference - load again\n",
    "customers_auto_df = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")       # First line contains column names\n",
    "    .option(\"inferSchema\", \"true\")  # Automatic data type inference\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2599e27-eda1-4c27-885d-3e96326d9a6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.6. JSON Data Import (Orders)\n",
    "\n",
    "### 2.6.1. Loading JSON with Automatic Schema Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "574e1299-3b2b-4ee2-93bb-b2a4e136776e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load JSON with inferSchema"
    }
   },
   "outputs": [],
   "source": [
    "# Load JSON data with automatic schema inference\n",
    "orders_auto_df = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(ORDERS_JSON)\n",
    ")\n",
    "\n",
    "print(\"JSON loaded with inferSchema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "377398e3-7ca6-4cab-a1f6-a14be9729118",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "JSON Schema"
    }
   },
   "outputs": [],
   "source": [
    "# Display schema\n",
    "print(\"Schema detected automatically:\")\n",
    "orders_auto_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c88352a4-bff1-4e76-b694-f5168a44c79b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "JSON Sample"
    }
   },
   "outputs": [],
   "source": [
    "# Display sample\n",
    "print(\"Data sample:\")\n",
    "display(orders_auto_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37d607c2-8aa5-4bbe-ad9c-1af6495d66a6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Manual Schema Header"
    }
   },
   "source": [
    "### 2.6.2. Manual Schema Definition for JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c826f976-b255-4e73-83fc-bcc6b3f5238b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Orders Schema Definition"
    }
   },
   "outputs": [],
   "source": [
    "# Schema definition for orders\n",
    "# Actual structure: order_id, customer_id, product_id, store_id, order_datetime, quantity, unit_price, discount_percent, total_amount, payment_method\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), nullable=True),  # Can be null in data\n",
    "    StructField(\"customer_id\", StringType(), nullable=False),\n",
    "    StructField(\"product_id\", StringType(), nullable=False),\n",
    "    StructField(\"store_id\", StringType(), nullable=False),\n",
    "    StructField(\"order_datetime\", StringType(), nullable=True),  # String, will convert to timestamp later\n",
    "    StructField(\"quantity\", IntegerType(), nullable=False),\n",
    "    StructField(\"unit_price\", DoubleType(), nullable=False),\n",
    "    StructField(\"discount_percent\", IntegerType(), nullable=False),\n",
    "    StructField(\"total_amount\", DoubleType(), nullable=False),\n",
    "    StructField(\"payment_method\", StringType(), nullable=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a928b826-9d57-43ae-826a-f50d97830be8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load JSON with Manual Schema"
    }
   },
   "outputs": [],
   "source": [
    "# Load JSON data with manually defined schema\n",
    "orders_df = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .schema(orders_schema)\n",
    "    .load(ORDERS_JSON)\n",
    ")\n",
    "\n",
    "print(\"JSON loaded with manual schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16db6f47-8e13-46db-8b0e-bef8d911b45f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "JSON Manual Schema Count"
    }
   },
   "outputs": [],
   "source": [
    "# Count rows\n",
    "print(f\"Row count: {orders_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7907b760-6934-41e6-b64b-905f61aefaa7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "JSON Manual Schema Schema"
    }
   },
   "outputs": [],
   "source": [
    "# Display schema\n",
    "print(\"Schema:\")\n",
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb5a4f56-4c34-470c-a427-4fc09175d980",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "JSON Manual Schema Sample"
    }
   },
   "outputs": [],
   "source": [
    "# Display sample\n",
    "print(\"Data sample:\")\n",
    "display(orders_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9f2041e-010b-42d1-b577-0a31eb841d95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.7. Parquet Data Import (Products)\n",
    "\n",
    "### 2.7.1. Loading Parquet (built-in schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "607e760f-8e10-45a7-9341-59f7da46a630",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Parquet"
    }
   },
   "outputs": [],
   "source": [
    "# Parquet already contains built-in schema - no need to define it\n",
    "products_df = (\n",
    "    spark.read\n",
    "    .format(\"parquet\")\n",
    "    .load(PRODUCTS_PARQUET)\n",
    ")\n",
    "\n",
    "print(\"Parquet loaded (built-in schema)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d55bbc89-857d-4418-a3c0-59b3dee6b577",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Parquet Count"
    }
   },
   "outputs": [],
   "source": [
    "# Count rows\n",
    "print(f\"Row count: {products_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0f3314a-0d44-4234-83ca-fa637ab955df",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Parquet Schema"
    }
   },
   "outputs": [],
   "source": [
    "# Display schema\n",
    "print(\"Schema:\")\n",
    "products_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9dafa9d-b6d2-4c2c-977c-e94c51c66e2c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Parquet Sample"
    }
   },
   "outputs": [],
   "source": [
    "# Display sample\n",
    "print(\"Data sample:\")\n",
    "display(products_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42527a0a-eae2-424d-9a35-7ffb8e88363e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Excel Import Header"
    }
   },
   "source": [
    "## 2.8. Excel Data Import (Extended Customer Data)\n",
    "\n",
    "### 2.8.1. Loading Excel with Spark-Excel Library\n",
    "\n",
    "For Excel files, Spark provides native support through the **spark-excel** library.\n",
    "\n",
    "**Benefits:**\n",
    "* Native Spark integration (distributed processing)\n",
    "* Supports large Excel files\n",
    "* Consistent API with other formats\n",
    "* Works with Unity Catalog Volumes\n",
    "\n",
    "**Note:** Requires spark-excel library to be installed on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6324e2f4-bcac-438a-9a53-992820bea242",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Excel"
    }
   },
   "outputs": [],
   "source": [
    "# Load Excel file using spark-excel library\n",
    "print(f\"--- Reading Excel from: {EXCEL_PATH} ---\")\n",
    "\n",
    "customers_extended_df = (\n",
    "    spark.read\n",
    "    .format(\"excel\")\n",
    "    .option(\"dataAddress\", \"Arkusz1!D6:M26\")\n",
    "    .option(\"headerRows\", 1)\n",
    "    .load(EXCEL_PATH)\n",
    ")\n",
    "print(f\"Excel loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abc24416-c4c9-4eb8-bb88-ffd4de38bc8e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Excel Count"
    }
   },
   "outputs": [],
   "source": [
    "# Count rows\n",
    "if 'customers_extended_df' in dir() and customers_extended_df.count() > 0:\n",
    "    print(f\"Row count: {customers_extended_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "412f8ca1-8756-4a07-b422-b065c191cd76",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Excel Schema"
    }
   },
   "outputs": [],
   "source": [
    "# Display schema\n",
    "if 'customers_extended_df' in dir() and customers_extended_df.count() > 0:\n",
    "    print(\"Schema:\")\n",
    "    customers_extended_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "599c50a2-67fa-4e6a-b522-adf2b116763d",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768909247981}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Excel Sample"
    }
   },
   "outputs": [],
   "source": [
    "# Display sample\n",
    "if 'customers_extended_df' in dir() and customers_extended_df.count() > 0:\n",
    "    print(\"Data sample:\")\n",
    "    display(customers_extended_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bb09b67-4724-4fbc-8d4d-08cb6dad06c6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Performance Comparison Header"
    }
   },
   "source": [
    "## 2.9. Performance Comparison\n",
    "\n",
    "### 2.9.1. Loading CSV: inferSchema vs Manual Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21c1e752-420b-4f81-99ca-0943eca7cb6d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Benchmark - inferSchema"
    }
   },
   "outputs": [],
   "source": [
    "# Test 1: Automatic schema inference\n",
    "start_auto = time.time()\n",
    "df_auto = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")\n",
    "count_auto = df_auto.count()  # Action - forces execution\n",
    "time_auto = time.time() - start_auto\n",
    "\n",
    "print(f\"[BENCHMARK] Loading CSV with inferSchema: {time_auto:.3f} seconds\")\n",
    "print(f\"[BENCHMARK] Row count: {count_auto}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08647755-e8d0-4613-a186-11cba24c6844",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Benchmark - Manual Schema"
    }
   },
   "outputs": [],
   "source": [
    "# Test 2: Manual schema definition\n",
    "start_manual = time.time()\n",
    "df_manual = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(customers_schema)\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")\n",
    "count_manual = df_manual.count()  # Action - forces execution\n",
    "time_manual = time.time() - start_manual\n",
    "\n",
    "print(f\"[BENCHMARK] Loading CSV with manual schema: {time_manual:.3f} seconds\")\n",
    "print(f\"[BENCHMARK] Row count: {count_manual}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb4d139e-5c55-47d4-9cba-e1f111e4f072",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Benchmark - Comparison"
    }
   },
   "outputs": [],
   "source": [
    "# Comparison\n",
    "speedup = (time_auto - time_manual) / time_auto * 100\n",
    "print(f\"[RESULT] Speedup with manual schema: {speedup:.1f}%\")\n",
    "print(f\"\\nConclusion: Manual schema is faster by {speedup:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "190bf43a-2bda-4128-8b41-9b220af80c45",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DataFrame Transformations Header"
    }
   },
   "source": [
    "## 2.10. DataFrame Transformations\n",
    "\n",
    "Now that we have loaded data from multiple sources (CSV, JSON, Parquet, Excel), let's explore common DataFrame transformation operations.\n",
    "\n",
    "### 2.10.1. Select - Choosing Columns\n",
    "\n",
    "The `select()` operation allows you to choose specific columns from a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd7b57f2-1dee-451d-8e59-017817f241cd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Select - Specific Columns"
    }
   },
   "outputs": [],
   "source": [
    "# Example 1: Select specific columns\n",
    "customers_selected = customers_df.select(\"customer_id\", \"first_name\", \"last_name\", \"email\")\n",
    "display(customers_selected.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d09e6de-3a71-48c1-9e84-a4d1cc5f87b4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Select - With Transformations"
    }
   },
   "outputs": [],
   "source": [
    "# Example 2: Select with column expressions\n",
    "customers_transformed = customers_df.select(\n",
    "    col(\"customer_id\"),\n",
    "    upper(col(\"first_name\")).alias(\"first_name_upper\"),\n",
    "    col(\"email\")\n",
    ")\n",
    "display(customers_transformed.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfc32c6d-27a6-4c78-986b-32e3dda3e13c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "WithColumn Header"
    }
   },
   "source": [
    "### 2.10.2. WithColumn - Adding/Modifying Columns\n",
    "\n",
    "The `withColumn()` operation adds a new column or replaces an existing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "839fcdda-a327-4ee7-b8c5-391f9a066a8c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "WithColumn - Add Single Column"
    }
   },
   "outputs": [],
   "source": [
    "# Example 1: Add a new column\n",
    "customers_with_fullname = customers_df.withColumn(\n",
    "    \"full_name\", \n",
    "    concat(col(\"first_name\"), lit(\" \"), col(\"last_name\"))\n",
    ")\n",
    "display(customers_with_fullname.select(\"customer_id\", \"first_name\", \"last_name\", \"full_name\").limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f1e0c9a-ba51-4477-8034-0c75dbd375f5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "WithColumn - Add Multiple Columns"
    }
   },
   "outputs": [],
   "source": [
    "# Example 2: Add multiple columns\n",
    "customers_enriched = customers_df \\\n",
    "    .withColumn(\"email_lower\", lower(col(\"email\"))) \\\n",
    "    .withColumn(\"registration_year\", year(col(\"registration_date\")))\n",
    "    \n",
    "display(customers_enriched.select(\"customer_id\", \"email\", \"email_lower\", \"registration_date\", \"registration_year\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1edd3536-4285-49bc-a949-914b67931994",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cast Header"
    }
   },
   "source": [
    "### 2.10.3. Cast - Type Conversion\n",
    "\n",
    "The `cast()` operation converts column data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da2f1d63-1a32-4350-a7ac-02a64ef28cef",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cast - To String"
    }
   },
   "outputs": [],
   "source": [
    "# Example 1: Cast customer_id to string\n",
    "print(\"--- Cast customer_id to string ---\")\n",
    "customers_casted = customers_df.withColumn(\"customer_id_str\", col(\"customer_id\").cast(StringType()))\n",
    "print(\"Original schema:\")\n",
    "customers_df.select(\"customer_id\").printSchema()\n",
    "print(\"\\nAfter cast:\")\n",
    "customers_casted.select(\"customer_id_str\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d9df3ff-c58c-46dc-8e45-104d6ed04a78",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cast - To Date"
    }
   },
   "outputs": [],
   "source": [
    "# Example 2: Cast timestamp to date\n",
    "print(\"--- Cast timestamp to date ---\")\n",
    "customers_date = customers_df.withColumn(\"registration_date_only\", col(\"registration_date\").cast(DateType()))\n",
    "display(customers_date.select(\"customer_id\", \"registration_date\", \"registration_date_only\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b6eace4-a44e-4f1d-9935-c4fb71b141c8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Rename Header"
    }
   },
   "source": [
    "### 2.10.4. Rename - Changing Column Names\n",
    "\n",
    "The `withColumnRenamed()` operation renames columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41518697-d541-46a2-89bd-3572cb8ac74f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Rename - Single Column"
    }
   },
   "outputs": [],
   "source": [
    "# Example 1: Rename single column\n",
    "print(\"--- Rename single column ---\")\n",
    "customers_renamed = customers_df.withColumnRenamed(\"customer_id\", \"id\")\n",
    "print(\"Original columns:\", customers_df.columns)\n",
    "print(\"Renamed columns:\", customers_renamed.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53e4b79a-e7db-42f8-af37-04e41576c10a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Rename - Multiple Columns"
    }
   },
   "outputs": [],
   "source": [
    "# Example 2: Rename multiple columns using select with alias\n",
    "print(\"--- Rename multiple columns ---\")\n",
    "customers_multi_renamed = customers_df.select(\n",
    "    col(\"customer_id\").alias(\"id\"),\n",
    "    col(\"first_name\").alias(\"fname\"),\n",
    "    col(\"last_name\").alias(\"lname\"),\n",
    "    col(\"email\"),\n",
    "    col(\"city\")\n",
    ")\n",
    "display(customers_multi_renamed.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63fed83c-d1ce-4adb-8cf9-77c03a2b8342",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Drop Header"
    }
   },
   "source": [
    "### 2.10.5. Drop - Removing Columns\n",
    "\n",
    "The `drop()` operation removes columns from a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bf0fc69-0a5d-4fb9-b6d5-4027a95b4ee2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Drop - Single Column"
    }
   },
   "outputs": [],
   "source": [
    "# Example 1: Drop single column\n",
    "print(\"--- Drop single column ---\")\n",
    "customers_dropped = customers_df.drop(\"email\")\n",
    "print(\"Original columns:\", customers_df.columns)\n",
    "print(\"After drop:\", customers_dropped.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d44823b-f165-467c-99b6-245421be5329",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Drop - Multiple Columns"
    }
   },
   "outputs": [],
   "source": [
    "# Example 2: Drop multiple columns\n",
    "print(\"--- Drop multiple columns ---\")\n",
    "customers_minimal = customers_df.drop(\"email\", \"city\", \"country\")\n",
    "print(\"Remaining columns:\", customers_minimal.columns)\n",
    "display(customers_minimal.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d32f758d-5440-4a12-8bb1-85cc77bf11de",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Distinct Header"
    }
   },
   "source": [
    "### 2.10.6. Distinct - Unique Rows\n",
    "\n",
    "The `distinct()` operation returns unique rows, and `dropDuplicates()` allows column-specific deduplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa50d76b-6222-4fb1-be0d-910061b42081",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Distinct - Countries"
    }
   },
   "outputs": [],
   "source": [
    "# Example 1: Get distinct countries\n",
    "print(\"--- Distinct countries ---\")\n",
    "distinct_countries = customers_df.select(\"country\").distinct()\n",
    "print(f\"Total distinct countries: {distinct_countries.count()}\")\n",
    "display(distinct_countries.orderBy(\"country\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad74d923-7fbc-497b-b47b-980450aa685a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Distinct - Drop Duplicates"
    }
   },
   "outputs": [],
   "source": [
    "# Example 2: Drop duplicates based on specific columns\n",
    "print(\"--- Drop duplicates by city and country ---\")\n",
    "unique_locations = customers_df.select(\"city\", \"country\").dropDuplicates([\"city\", \"country\"])\n",
    "print(f\"Unique city-country combinations: {unique_locations.count()}\")\n",
    "display(unique_locations.orderBy(\"country\", \"city\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "340a5de4-d9a8-4861-a1e2-02030eb07a32",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "OrderBy Header"
    }
   },
   "source": [
    "### 2.10.7. OrderBy - Sorting Data\n",
    "\n",
    "The `orderBy()` or `sort()` operations sort DataFrame rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f48aeca-3878-47db-bd74-acabffb87972",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "OrderBy - Single Column"
    }
   },
   "outputs": [],
   "source": [
    "# Example 1: Sort by single column (ascending)\n",
    "print(\"--- Sort by registration_date (ascending) ---\")\n",
    "customers_sorted_asc = customers_df.orderBy(\"registration_date\")\n",
    "display(customers_sorted_asc.select(\"customer_id\", \"first_name\", \"last_name\", \"registration_date\").limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b68ceedd-cd4c-4856-87fa-2576a128ab73",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "OrderBy - Multiple Columns"
    }
   },
   "outputs": [],
   "source": [
    "# Example 2: Sort by multiple columns with different directions\n",
    "print(\"--- Sort by country (asc) and registration_date (desc) ---\")\n",
    "customers_sorted_multi = customers_df.orderBy(asc(\"country\"), desc(\"registration_date\"))\n",
    "display(customers_sorted_multi.select(\"customer_id\", \"first_name\", \"country\", \"registration_date\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bc7be71-ad70-445b-bace-b28ad05e0e91",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Filtering Section Header"
    }
   },
   "source": [
    "## 2.11. Filtering Data\n",
    "\n",
    "Filtering allows you to select rows that meet specific conditions.\n",
    "\n",
    "### 2.11.1. Simple Filter Conditions\n",
    "\n",
    "Use `filter()` or `where()` (they are equivalent) to apply conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a32900c-f9dd-4f2b-b8f4-a34c2bb9590e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Filter - By Country"
    }
   },
   "outputs": [],
   "source": [
    "# Example 1: Filter by country\n",
    "print(\"--- Customers from USA ---\")\n",
    "usa_customers = customers_df.filter(col(\"country\") == \"USA\")\n",
    "print(f\"Count: {usa_customers.count()}\")\n",
    "display(usa_customers.select(\"customer_id\", \"first_name\", \"last_name\", \"country\").limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "415ca31e-b554-420c-b2bd-fb5bf0b9c8a4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Filter - Using Where"
    }
   },
   "outputs": [],
   "source": [
    "# Example 2: Filter using where (equivalent to filter)\n",
    "print(\"--- Customers from specific city ---\")\n",
    "nyc_customers = customers_df.where(col(\"city\") == \"New York\")\n",
    "print(f\"Count: {nyc_customers.count()}\")\n",
    "display(nyc_customers.select(\"customer_id\", \"first_name\", \"city\", \"country\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e76ad1c8-597a-4de8-a38f-1a12b4412c44",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Multiple Conditions Header"
    }
   },
   "source": [
    "### 2.11.2. Multiple Conditions\n",
    "\n",
    "Combine conditions using `&` (AND) and `|` (OR). Remember to wrap each condition in parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f93de5a-e920-4354-b542-ac0231e8c08a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Filter - AND Condition"
    }
   },
   "outputs": [],
   "source": [
    "# Example 1: AND condition\n",
    "print(\"--- Customers from USA AND registered in 2023 ---\")\n",
    "usa_2023 = customers_df.filter(\n",
    "    (col(\"country\") == \"USA\") & (year(col(\"registration_date\")) == 2023)\n",
    ")\n",
    "print(f\"Count: {usa_2023.count()}\")\n",
    "display(usa_2023.select(\"customer_id\", \"first_name\", \"country\", \"registration_date\").limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76117267-e85d-4d3a-adbe-b0befa8d7f1a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Filter - OR Condition"
    }
   },
   "outputs": [],
   "source": [
    "# Example 2: OR condition\n",
    "print(\"--- Customers from USA OR UK ---\")\n",
    "usa_or_uk = customers_df.filter(\n",
    "    (col(\"country\") == \"USA\") | (col(\"country\") == \"UK\")\n",
    ")\n",
    "print(f\"Count: {usa_or_uk.count()}\")\n",
    "display(usa_or_uk.select(\"customer_id\", \"first_name\", \"country\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6cbe8c5-d5b2-4a3b-baec-e0685570aaca",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "isin() Header"
    }
   },
   "source": [
    "### 2.11.3. isin() - Filtering by List of Values\n",
    "\n",
    "The `isin()` method checks if a column value is in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cab9015-fca4-463b-b9fa-9431f5e6d416",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "isin() Examples"
    }
   },
   "outputs": [],
   "source": [
    "# Example 1: Filter by list of countries\n",
    "print(\"--- Customers from specific countries ---\")\n",
    "selected_countries = [\"USA\", \"UK\", \"Germany\", \"France\"]\n",
    "customers_selected_countries = customers_df.filter(col(\"country\").isin(selected_countries))\n",
    "print(f\"Count: {customers_selected_countries.count()}\")\n",
    "\n",
    "# Show distribution by country\n",
    "print(\"\\nDistribution by country:\")\n",
    "display(customers_selected_countries.groupBy(\"country\").count().orderBy(desc(\"count\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "758bbc53-ffdd-40bc-b313-889f30ec896f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Null Handling Header"
    }
   },
   "source": [
    "### 2.11.4. Null Handling\n",
    "\n",
    "Use `isNull()` and `isNotNull()` to filter based on null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "674ccd4d-535c-4199-897c-14a26db09a1e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Null - isNotNull"
    }
   },
   "outputs": [],
   "source": [
    "# Example 1: Filter rows where email is NOT null\n",
    "print(\"--- Customers with valid email ---\")\n",
    "customers_with_email = customers_df.filter(col(\"email\").isNotNull())\n",
    "print(f\"Count with email: {customers_with_email.count()}\")\n",
    "print(f\"Total count: {customers_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8446af7b-61fd-42ff-b6ac-e6eddb10ada7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Null - isNull"
    }
   },
   "outputs": [],
   "source": [
    "# Example 2: Filter rows where city IS null\n",
    "print(\"--- Customers with missing city ---\")\n",
    "customers_no_city = customers_df.filter(col(\"city\").isNull())\n",
    "print(f\"Count without city: {customers_no_city.count()}\")\n",
    "if customers_no_city.count() > 0:\n",
    "    display(customers_no_city.select(\"customer_id\", \"first_name\", \"city\", \"country\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c481ab17-70d3-468d-8584-6537fcd40630",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "String Operations Header"
    }
   },
   "source": [
    "### 2.11.5. String Operations\n",
    "\n",
    "Use string functions like `like()`, `contains()`, `startswith()`, and `endswith()` for pattern matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "821a4eac-842c-40e5-ad53-ab03df1cbc98",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "String - Like Pattern"
    }
   },
   "outputs": [],
   "source": [
    "# Example 1: Filter using like (SQL-style pattern matching)\n",
    "print(\"--- Customers with email from gmail ---\")\n",
    "gmail_customers = customers_df.filter(col(\"email\").like(\"%@gmail.com\"))\n",
    "print(f\"Count: {gmail_customers.count()}\")\n",
    "display(gmail_customers.select(\"customer_id\", \"first_name\", \"email\").limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbed2084-a6e7-4306-a2a6-52d37cc7c995",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "String - Contains"
    }
   },
   "outputs": [],
   "source": [
    "# Example 2: Filter using contains\n",
    "print(\"--- Customers with 'New' in city name ---\")\n",
    "new_cities = customers_df.filter(col(\"city\").contains(\"New\"))\n",
    "print(f\"Count: {new_cities.count()}\")\n",
    "display(new_cities.select(\"customer_id\", \"first_name\", \"city\").limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d533e329-079f-4d0b-bd35-4ebea34d3d26",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "String - Startswith"
    }
   },
   "outputs": [],
   "source": [
    "# Example 3: Filter using startswith\n",
    "print(\"--- Customers with first name starting with 'J' ---\")\n",
    "j_names = customers_df.filter(col(\"first_name\").startswith(\"J\"))\n",
    "print(f\"Count: {j_names.count()}\")\n",
    "display(j_names.select(\"customer_id\", \"first_name\", \"last_name\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80b4c33f-a5e8-415c-9376-99f532a5087f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Aggregations Section Header"
    }
   },
   "source": [
    "## 2.12. Aggregations and Grouping\n",
    "\n",
    "Aggregations allow you to compute summary statistics on your data.\n",
    "\n",
    "### 2.12.1. Basic Aggregations\n",
    "\n",
    "Use `groupBy()` with aggregation functions like `count()`, `sum()`, `avg()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b747ce1-c590-46ea-ae32-1d7bb0b3641b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Aggregation - Count by Country"
    }
   },
   "outputs": [],
   "source": [
    "# Example 1: Count by country\n",
    "print(\"--- Customer count by country ---\")\n",
    "customers_by_country = customers_df.groupBy(\"country\").count().orderBy(desc(\"count\"))\n",
    "display(customers_by_country.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a845f7c8-a15a-4f69-a0d3-da7d3b3c2100",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Aggregation - Revenue by Payment"
    }
   },
   "outputs": [],
   "source": [
    "# Example 2: Sum and average on orders\n",
    "print(\"--- Total revenue and average order value by payment method ---\")\n",
    "revenue_by_payment = orders_df.groupBy(\"payment_method\").agg(\n",
    "    sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "    avg(\"total_amount\").alias(\"avg_order_value\"),\n",
    "    count(\"*\").alias(\"order_count\")\n",
    ").orderBy(desc(\"total_revenue\"))\n",
    "\n",
    "display(revenue_by_payment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38e785f5-094d-42ee-9e18-2e9a89405d14",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Min/Max Header"
    }
   },
   "source": [
    "### 2.12.2. Min/Max Aggregations\n",
    "\n",
    "Find minimum and maximum values in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee0468e5-bcae-4bde-a746-e11d9beab731",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Aggregation - Min/Max Stats"
    }
   },
   "outputs": [],
   "source": [
    "# Example 1: Min and max order amounts\n",
    "print(\"--- Order amount statistics ---\")\n",
    "order_stats = orders_df.agg(\n",
    "    min(\"total_amount\").alias(\"min_amount\"),\n",
    "    max(\"total_amount\").alias(\"max_amount\"),\n",
    "    avg(\"total_amount\").alias(\"avg_amount\")\n",
    ")\n",
    "display(order_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cdbae88-3b51-49dd-976b-99ee7b039dc1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Aggregation - Price Range by Product"
    }
   },
   "outputs": [],
   "source": [
    "# Example 2: Min/max by product\n",
    "print(\"--- Price range by product ---\")\n",
    "product_price_range = orders_df.groupBy(\"product_id\").agg(\n",
    "    min(\"unit_price\").alias(\"min_price\"),\n",
    "    max(\"unit_price\").alias(\"max_price\"),\n",
    "    count(\"*\").alias(\"order_count\")\n",
    ").orderBy(desc(\"order_count\"))\n",
    "\n",
    "display(product_price_range.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eacbd31c-cf05-45fb-a647-8f7eda3a1713",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Multiple Aggregations Header"
    }
   },
   "source": [
    "### 2.12.3. Multiple Aggregations with agg()\n",
    "\n",
    "Use `agg()` to apply multiple aggregation functions at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "942abb37-7e74-46dc-81d1-d7b4902bff7a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Multiple Aggregations Examples"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Multiple aggregations on orders by customer\n",
    "print(\"--- Customer order statistics ---\")\n",
    "customer_stats = orders_df.groupBy(\"customer_id\").agg(\n",
    "    count(\"*\").alias(\"total_orders\"),\n",
    "    sum(\"total_amount\").alias(\"total_spent\"),\n",
    "    avg(\"total_amount\").alias(\"avg_order_value\"),\n",
    "    min(\"total_amount\").alias(\"min_order\"),\n",
    "    max(\"total_amount\").alias(\"max_order\")\n",
    ").orderBy(desc(\"total_spent\"))\n",
    "\n",
    "print(f\"Total customers with orders: {customer_stats.count()}\")\n",
    "display(customer_stats.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba5b7611-5a85-426f-8ed6-f01edea8d408",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Having Clause Header"
    }
   },
   "source": [
    "### 2.12.4. Having Clause (Filter After Aggregation)\n",
    "\n",
    "In Spark, use `filter()` after `groupBy()` to implement SQL's HAVING clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41bdf5d0-ef44-42d1-b18f-a31c6dbfbd2d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Having - High Frequency Customers"
    }
   },
   "outputs": [],
   "source": [
    "# Example 1: Customers with more than 5 orders\n",
    "print(\"--- High-frequency customers (>5 orders) ---\")\n",
    "high_frequency = orders_df.groupBy(\"customer_id\").agg(\n",
    "    count(\"*\").alias(\"order_count\"),\n",
    "    sum(\"total_amount\").alias(\"total_spent\")\n",
    ").filter(col(\"order_count\") > 5).orderBy(desc(\"order_count\"))\n",
    "\n",
    "print(f\"Customers with >5 orders: {high_frequency.count()}\")\n",
    "display(high_frequency.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de2dbb83-6a87-4a88-985f-64e2b217b1f7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Having - Premium Products"
    }
   },
   "outputs": [],
   "source": [
    "# Example 2: Products with average price > 100\n",
    "print(\"--- Premium products (avg price > 100) ---\")\n",
    "premium_products = orders_df.groupBy(\"product_id\").agg(\n",
    "    avg(\"unit_price\").alias(\"avg_price\"),\n",
    "    count(\"*\").alias(\"times_ordered\")\n",
    ").filter(col(\"avg_price\") > 100).orderBy(desc(\"avg_price\"))\n",
    "\n",
    "print(f\"Premium products count: {premium_products.count()}\")\n",
    "display(premium_products.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9480eac2-231f-40c0-806a-c9bfcfe45cf4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Temp Views Section Header"
    }
   },
   "source": [
    "## 2.13. Temporary Views & SQL\n",
    "\n",
    "Bridge between DataFrame API and SQL by creating temporary views.\n",
    "\n",
    "### 2.13.1. Creating Temporary Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7358a04-9c35-4746-b379-0be9ff929f1d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Temp Views"
    }
   },
   "outputs": [],
   "source": [
    "# Create temporary views from our DataFrames\n",
    "print(\"--- Creating temporary views ---\")\n",
    "\n",
    "customers_df.createOrReplaceTempView(\"customers\")\n",
    "orders_df.createOrReplaceTempView(\"orders\")\n",
    "products_df.createOrReplaceTempView(\"products\")\n",
    "\n",
    "print(\"✅ Created temp views: customers, orders, products\")\n",
    "print(\"\\nThese views are available for SQL queries in this session.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e07705b-218e-4522-ba14-308234e1f13c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SQL Query Examples Header"
    }
   },
   "source": [
    "### 2.13.2. Running SQL Queries\n",
    "\n",
    "Use `spark.sql()` to run SQL queries against temporary views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "830d9f7d-d7eb-4639-82d6-7fb021ae4bcc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SQL Query Examples"
    }
   },
   "outputs": [],
   "source": [
    "# Example 1: Simple SELECT query\n",
    "print(\"--- SQL: Top 10 customers by order count ---\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        COUNT(*) as order_count,\n",
    "        SUM(total_amount) as total_spent\n",
    "    FROM orders\n",
    "    GROUP BY customer_id\n",
    "    ORDER BY order_count DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "display(result)\n",
    "\n",
    "# Example 2: SQL with WHERE clause\n",
    "print(\"\\n--- SQL: Orders with high discount ---\")\n",
    "high_discount = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        order_id,\n",
    "        customer_id,\n",
    "        total_amount,\n",
    "        discount_percent\n",
    "    FROM orders\n",
    "    WHERE discount_percent > 20\n",
    "    ORDER BY discount_percent DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "display(high_discount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae368127-a06a-497d-a9de-e454fe930a07",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "API vs SQL Header"
    }
   },
   "source": [
    "### 2.13.3. DataFrame API vs SQL - Same Result, Different Syntax\n",
    "\n",
    "Compare how the same operation looks in DataFrame API and SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "707d61a2-19a2-404f-9cf5-596991de1405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Same query - two approaches\n",
    "\n",
    "# Approach 1: DataFrame API\n",
    "print(\"--- DataFrame API approach ---\")\n",
    "df_api_result = customers_df  \\\n",
    "    .groupBy(\"city\") \\\n",
    "    .count() \\\n",
    "    .orderBy(desc(\"count\")) \\\n",
    "    .limit(5)\n",
    "\n",
    "display(df_api_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52784051-d3d6-49f7-983e-d66b18827ff8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "API vs SQL Examples"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Approach 2: SQL\n",
    "print(\"\\n--- SQL approach (same result) ---\")\n",
    "sql_result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        city,\n",
    "        COUNT(*) as count\n",
    "    FROM customers\n",
    "    GROUP BY city\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "display(sql_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c5be866-8b2f-4a6d-aedb-211733239a08",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Comparison - SQL"
    }
   },
   "outputs": [],
   "source": [
    "# Approach 2: SQL\n",
    "print(\"--- SQL approach (same result) ---\")\n",
    "sql_result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        city,\n",
    "        COUNT(*) as count\n",
    "    FROM customers\n",
    "    GROUP BY city\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "display(sql_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9651542e-5e1b-4a3b-b72e-9e157006f5e4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Global Temp View Header"
    }
   },
   "source": [
    "### 2.13.4. Global Temporary Views\n",
    "\n",
    "Global temp views are accessible across different notebooks in the same session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9346d9c0-3d2d-4160-91bf-6fdc3e1f3a73",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Global Temp View"
    }
   },
   "outputs": [],
   "source": [
    "# Create global temporary view\n",
    "print(\"--- Creating global temporary view ---\")\n",
    "customers_df.createOrReplaceGlobalTempView(\"global_customers\")\n",
    "\n",
    "print(\"✅ Created global temp view: global_customers\")\n",
    "print(\"Access it using: global_temp.global_customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ed1aca4-1221-4815-8af3-5e3726e25813",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Query Global Temp View"
    }
   },
   "outputs": [],
   "source": [
    "# Query global temp view\n",
    "print(\"--- Query global temp view ---\")\n",
    "global_result = spark.sql(\"\"\"\n",
    "    SELECT country, COUNT(*) as count\n",
    "    FROM global_temp.global_customers\n",
    "    GROUP BY country\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "display(global_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42f12c72-950f-427c-8c50-b65369d57a86",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Complex SQL Header"
    }
   },
   "source": [
    "### 2.13.5. Complex SQL Queries\n",
    "\n",
    "Combine multiple operations in SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e988b0ed-788a-4a83-b5c8-cb0412d1f3f7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Complex SQL Examples"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Complex query with JOIN and aggregation\n",
    "print(\"--- Customer spending analysis with SQL ---\")\n",
    "\n",
    "complex_query = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        c.customer_id,\n",
    "        c.first_name,\n",
    "        c.last_name,\n",
    "        c.country,\n",
    "        COUNT(o.order_id) as total_orders,\n",
    "        SUM(o.total_amount) as total_spent,\n",
    "        AVG(o.total_amount) as avg_order_value,\n",
    "        MAX(o.total_amount) as largest_order\n",
    "    FROM customers c\n",
    "    LEFT JOIN orders o ON c.customer_id = try_cast(o.customer_id AS STRING)\n",
    "    GROUP BY c.customer_id, c.first_name, c.last_name, c.country\n",
    "    HAVING COUNT(o.order_id) > 0\n",
    "    ORDER BY total_spent DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "display(complex_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e98e047-e1e4-4d9f-991e-f0509a698aee",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SQL Best Practices Note"
    }
   },
   "source": [
    "### 2.13.6. When to Use SQL vs DataFrame API?\n",
    "\n",
    "**Use SQL when:**\n",
    "* Team is more familiar with SQL\n",
    "* Complex queries with multiple JOINs\n",
    "* Ad-hoc analysis and exploration\n",
    "\n",
    "**Use DataFrame API when:**\n",
    "* Building reusable data pipelines\n",
    "* Need type safety and compile-time checks\n",
    "* Complex transformations with custom logic\n",
    "* Better IDE support and autocomplete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90caa8a9-a806-41e7-a292-d1527cdd543b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "JSON Operations Header"
    }
   },
   "source": [
    "## 2.14. JSON Operations\n",
    "\n",
    "Working with semi-structured JSON data requires special operations.\n",
    "\n",
    "### 2.14.1. Explode - Flattening Arrays\n",
    "\n",
    "The `explode()` function creates a new row for each element in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8416de6-f130-4465-a0e9-15428cdca3a9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Explode Example Setup"
    }
   },
   "outputs": [],
   "source": [
    "# Create sample data with arrays to demonstrate explode\n",
    "print(\"--- Creating sample data with arrays ---\")\n",
    "sample_data = [\n",
    "    (1, \"Customer A\", [\"product_1\", \"product_2\", \"product_3\"]),\n",
    "    (2, \"Customer B\", [\"product_1\"]),\n",
    "    (3, \"Customer C\", []),  # Empty array\n",
    "    (4, \"Customer D\", None)  # Null array\n",
    "]\n",
    "\n",
    "sample_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"customer_name\", StringType(), False),\n",
    "    StructField(\"purchased_products\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "sample_df = spark.createDataFrame(sample_data, schema=sample_schema)\n",
    "print(\"\\nOriginal data with arrays:\")\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21849177-5530-463a-8646-9390fc779a00",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Explode - Standard"
    }
   },
   "outputs": [],
   "source": [
    "# Example 1: explode() - skips null and empty arrays\n",
    "print(\"--- Using explode() ---\")\n",
    "exploded_df = sample_df.select(\n",
    "    \"customer_id\",\n",
    "    \"customer_name\",\n",
    "    explode(\"purchased_products\").alias(\"product\")\n",
    ")\n",
    "print(\"Note: Customers 3 and 4 are missing (empty/null arrays)\")\n",
    "display(exploded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13fcec77-db23-4391-a7da-60d7b8574649",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Explode - Outer"
    }
   },
   "outputs": [],
   "source": [
    "# Example 2: explode_outer() - keeps null and empty arrays\n",
    "print(\"--- Using explode_outer() ---\")\n",
    "exploded_outer_df = sample_df.select(\n",
    "    \"customer_id\",\n",
    "    \"customer_name\",\n",
    "    explode_outer(\"purchased_products\").alias(\"product\")\n",
    ")\n",
    "print(\"Note: All customers are present, with null for empty/null arrays\")\n",
    "display(exploded_outer_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "030b26b0-7d94-418b-b410-bb88b211f7f8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Nested JSON Header"
    }
   },
   "source": [
    "### 2.14.2. Nested JSON Structures\n",
    "\n",
    "Access nested fields using dot notation or `getField()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f47194fe-d289-46b5-b7fa-9d3fdb89496a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Nested JSON - Create Structure"
    }
   },
   "outputs": [],
   "source": [
    "# Create sample data with nested structures\n",
    "print(\"--- Creating nested structure ---\")\n",
    "nested_data = customers_df.select(\n",
    "    col(\"customer_id\"),\n",
    "    struct(\n",
    "        col(\"first_name\"),\n",
    "        col(\"last_name\"),\n",
    "        col(\"email\")\n",
    "    ).alias(\"personal_info\"),\n",
    "    struct(\n",
    "        col(\"city\"),\n",
    "        col(\"country\")\n",
    "    ).alias(\"location\")\n",
    ")\n",
    "\n",
    "print(\"\\nNested structure:\")\n",
    "nested_data.printSchema()\n",
    "display(nested_data.limit(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b7dd764-0c49-455f-9e86-37ef67c69110",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Nested JSON - Access Fields"
    }
   },
   "outputs": [],
   "source": [
    "# Access nested fields\n",
    "print(\"--- Accessing nested fields ---\")\n",
    "flattened = nested_data.select(\n",
    "    \"customer_id\",\n",
    "    col(\"personal_info.first_name\").alias(\"first_name\"),\n",
    "    col(\"personal_info.email\").alias(\"email\"),\n",
    "    col(\"location.country\").alias(\"country\")\n",
    ")\n",
    "display(flattened.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7621b1e-76b8-4a44-a5eb-1775fd5c3370",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "JSON String Parsing Header"
    }
   },
   "source": [
    "### 2.14.3. Parsing JSON Strings\n",
    "\n",
    "Use `get_json_object()` or `from_json()` to parse JSON stored as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4923769c-60c2-4009-84df-522f0022ad28",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "JSON String - Create Sample"
    }
   },
   "outputs": [],
   "source": [
    "# Create sample data with JSON strings\n",
    "print(\"--- Creating sample data with JSON strings ---\")\n",
    "json_string_data = [\n",
    "    (1, '{\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}'),\n",
    "    (2, '{\"name\": \"Jane\", \"age\": 25, \"city\": \"London\"}'),\n",
    "    (3, '{\"name\": \"Bob\", \"age\": 35, \"city\": \"Paris\"}')\n",
    "]\n",
    "\n",
    "json_df = spark.createDataFrame(json_string_data, [\"id\", \"json_data\"])\n",
    "display(json_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24ab4c19-918f-4d76-9ebd-25a6c9fd19fe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "JSON String - Parse Fields"
    }
   },
   "outputs": [],
   "source": [
    "# Extract fields using get_json_object\n",
    "print(\"--- Extract fields with get_json_object ---\")\n",
    "parsed_df = json_df.select(\n",
    "    \"id\",\n",
    "    get_json_object(\"json_data\", \"$.name\").alias(\"name\"),\n",
    "    get_json_object(\"json_data\", \"$.age\").alias(\"age\"),\n",
    "    get_json_object(\"json_data\", \"$.city\").alias(\"city\")\n",
    ")\n",
    "display(parsed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c863689-e99b-4d26-91fc-459d99e6784a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Joins Section Header"
    }
   },
   "source": [
    "## 2.15. Joins - Combining Datasets\n",
    "\n",
    "Joins allow you to combine data from multiple DataFrames based on common keys.\n",
    "\n",
    "### 2.15.1. Inner Join\n",
    "\n",
    "Inner join returns only matching rows from both DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e157b941-ba44-4f4c-a980-65f8a6c0ce56",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Inner Join Example"
    }
   },
   "outputs": [],
   "source": [
    "# Example 1: Inner join - customers with their orders\n",
    "print(\"--- Inner join: Customers with orders ---\")\n",
    "\n",
    "customers_with_orders = customers_df.join(\n",
    "    orders_df,\n",
    "    customers_df.customer_id == orders_df.customer_id,\n",
    "    \"inner\"\n",
    ").select(\n",
    "    customers_df.customer_id,\n",
    "    customers_df.first_name,\n",
    "    customers_df.last_name,\n",
    "    orders_df.order_id,\n",
    "    orders_df.total_amount,\n",
    "    orders_df.payment_method\n",
    ")\n",
    "\n",
    "print(f\"Total rows: {customers_with_orders.count()}\")\n",
    "display(customers_with_orders.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c991c155-59fa-471b-858f-fd615092d685",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Left Join Header"
    }
   },
   "source": [
    "### 2.15.2. Left Join\n",
    "\n",
    "Left join returns all rows from the left DataFrame and matching rows from the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e221573-16ac-4469-9d72-ef990233ae76",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Left Join Example"
    }
   },
   "outputs": [],
   "source": [
    "# Example 2: Left join - all orders with product details (if available)\n",
    "print(\"--- Left join: Orders with product information ---\")\n",
    "\n",
    "orders_with_products = orders_df.join(\n",
    "    products_df,\n",
    "    orders_df.product_id == products_df.product_id,\n",
    "    \"left\"\n",
    ").select(\n",
    "    orders_df.order_id,\n",
    "    orders_df.customer_id,\n",
    "    orders_df.product_id,\n",
    "    products_df.product_name,\n",
    "    orders_df.total_amount\n",
    ")\n",
    "\n",
    "print(f\"Total orders: {orders_with_products.count()}\")\n",
    "display(orders_with_products.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e24acfa7-998d-4e6f-b31a-986256fbb385",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Right Join Header"
    }
   },
   "source": [
    "### 2.15.3. Right Join\n",
    "\n",
    "Right join returns all rows from the right DataFrame and matching rows from the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72f7d9b0-c21d-46cc-a5dd-371e70540eca",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Right Join Example"
    }
   },
   "outputs": [],
   "source": [
    "# Example 3: Right join - all products with their orders (if any)\n",
    "print(\"--- Right join: Products with orders ---\")\n",
    "\n",
    "products_with_orders = orders_df.join(\n",
    "    products_df,\n",
    "    orders_df.product_id == products_df.product_id,\n",
    "    \"right\"\n",
    ").groupBy(\n",
    "    products_df.product_id,\n",
    "    products_df.product_name,\n",
    ").agg(\n",
    "    count(orders_df.order_id).alias(\"order_count\")\n",
    ")\n",
    "\n",
    "print(f\"Total products: {products_with_orders.count()}\")\n",
    "display(products_with_orders.orderBy(desc(\"order_count\")).limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b345bd9f-c785-448e-bc17-baa38b10e239",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Full Outer Join Header"
    }
   },
   "source": [
    "### 2.15.4. Full Outer Join\n",
    "\n",
    "Full outer join returns all rows from both DataFrames, with nulls where there's no match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f282a05f-aed1-484f-85d0-d130609d6d90",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Full Outer Join Example"
    }
   },
   "outputs": [],
   "source": [
    "# Example 4: Full outer join - all customers and all orders\n",
    "print(\"--- Full outer join: All customers and orders ---\")\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "full_join = customers_df.join(\n",
    "    orders_df,\n",
    "    customers_df.customer_id == orders_df.customer_id,\n",
    "    \"outer\"\n",
    ").select(\n",
    "    customers_df.customer_id.alias(\"cust_id\"),\n",
    "    orders_df.customer_id.alias(\"order_cust_id\"),\n",
    "    customers_df.first_name,\n",
    "    col(\"order_id\").alias(\"order_id\"),\n",
    "    orders_df.total_amount\n",
    ")\n",
    "\n",
    "print(\"\\nSample with potential nulls:\")\n",
    "display(full_join.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5586a4d-0977-42ea-b968-3a6a4f9994a2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Multi-Table Join Header"
    }
   },
   "source": [
    "### 2.15.5 Complex Multi-Table Join\n",
    "\n",
    "Join three tables together: customers, orders, and products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0971465-7875-4596-846a-6c9fa5acfac2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Join - Three Tables"
    }
   },
   "outputs": [],
   "source": [
    "# Example 6: Three-table join - complete order information\n",
    "print(\"--- Three-table join: Customers + Orders + Products ---\")\n",
    "\n",
    "complete_orders = customers_df \\\n",
    "    .join(orders_df, customers_df.customer_id == orders_df.customer_id, \"left\") \\\n",
    "    .join(products_df, orders_df.product_id == products_df.product_id, \"left\") \\\n",
    "    .select(\n",
    "        customers_df.customer_id,\n",
    "        customers_df.first_name,\n",
    "        customers_df.last_name,\n",
    "        customers_df.country,\n",
    "        orders_df.order_id,\n",
    "        orders_df.total_amount,\n",
    "        products_df.product_name\n",
    "    )\n",
    "\n",
    "print(f\"Total complete order records: {complete_orders.count()}\")\n",
    "\n",
    "\n",
    "print(\"\\nSample complete order data:\")\n",
    "display(complete_orders.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d052d92a-3ce7-4dd9-95db-4d21e078a81c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Join - Revenue by Category"
    }
   },
   "outputs": [],
   "source": [
    "# Bonus: Analyze revenue by category\n",
    "print(\"--- Revenue by product category ---\")\n",
    "country_revenue = complete_orders.groupBy(\"country\").agg(\n",
    "    sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "    count(\"*\").alias(\"order_count\")\n",
    ").orderBy(desc(\"total_revenue\"))\n",
    "\n",
    "display(country_revenue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37afcf22-f2ca-4357-9191-f612b098b4fb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Final Summary"
    }
   },
   "source": [
    "## 2.16. Summary\n",
    "\n",
    "In this comprehensive notebook you learned:\n",
    "\n",
    "**Data Ingestion (Sections 2.4-2.9)**\n",
    "* Loading CSV with inferSchema vs manual schema\n",
    "* Loading JSON with automatic schema detection\n",
    "* Loading Parquet (built-in schema)\n",
    "* Loading Excel using spark-excel library\n",
    "* Performance comparison and best practices\n",
    "\n",
    "**DataFrame Transformations (Section 2.10)**\n",
    "* `select()` - choosing columns\n",
    "* `withColumn()` - adding/modifying columns\n",
    "* `cast()` - type conversion\n",
    "* `withColumnRenamed()` - renaming columns\n",
    "* `drop()` - removing columns\n",
    "* `distinct()` and `dropDuplicates()` - unique rows\n",
    "* `orderBy()` - sorting data\n",
    "\n",
    "**Filtering Data (Section 2.11)**\n",
    "* Simple filter conditions with `filter()` and `where()`\n",
    "* Multiple conditions with `&` (AND) and `|` (OR)\n",
    "* `isin()` - filtering by list of values\n",
    "* `isNull()` and `isNotNull()` - null handling\n",
    "* String operations: `like()`, `contains()`, `startswith()`\n",
    "\n",
    "**Aggregations (Section 2.12)**\n",
    "* `groupBy()` with `count()`, `sum()`, `avg()`\n",
    "* `min()` and `max()` aggregations\n",
    "* Multiple aggregations with `agg()`\n",
    "* HAVING clause equivalent with `filter()` after groupBy\n",
    "\n",
    "**Temporary Views & SQL (Section 2.13)**\n",
    "* Creating temp views with `createOrReplaceTempView()`\n",
    "* Running SQL queries with `spark.sql()`\n",
    "* DataFrame API vs SQL comparison\n",
    "* Global temporary views\n",
    "* Complex SQL queries with JOINs\n",
    "\n",
    "**JSON Operations (Section 2.14)**\n",
    "* `explode()` and `explode_outer()` for arrays\n",
    "* Accessing nested structures with dot notation\n",
    "* `struct()` for creating nested structures\n",
    "* `get_json_object()` for parsing JSON strings\n",
    "\n",
    "**Joins (Section 2.15)**\n",
    "* Inner join - matching rows only\n",
    "* Left join - all left + matching right\n",
    "* Right join - all right + matching left\n",
    "* Full outer join - all rows from both\n",
    "* Joining with extended data sources\n",
    "* Complex multi-table joins (3+ tables)\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Schema Definition**: Use explicit schemas in production for performance and data quality\n",
    "2. **Format Selection**: CSV for compatibility, Parquet for performance, JSON for flexibility\n",
    "3. **Transformations**: Chain operations for readable, maintainable code\n",
    "4. **SQL Bridge**: Use temp views to leverage SQL when appropriate\n",
    "5. **Joins**: Choose the right join type based on your data requirements\n",
    "\n",
    "**Next Steps**: Apply these techniques to build ingestion pipelines! 🚀"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_ingestion_transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
