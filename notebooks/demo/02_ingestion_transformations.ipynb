{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6bc86c4-58e5-4f4f-af86-f3311937bb70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Import and Exploration\n",
    "\n",
    "## The Story Continues...\n",
    "\n",
    "Remember our e-commerce company? The data team just got access to the source systems:\n",
    "\n",
    "- **CRM System** exports customers to CSV (daily dump)\n",
    "- **Order Management** sends JSON via API \n",
    "- **Product Catalog** is in Parquet (from legacy Hadoop system)\n",
    "\n",
    "**Your first task:** Import this data and understand its structure before building pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Matters (Real-World Context)\n",
    "\n",
    "### The \"inferSchema\" Trap\n",
    "\n",
    "Most tutorials show: `spark.read.option(\"inferSchema\", \"true\")`\n",
    "\n",
    "**In production, this is often a bad idea:**\n",
    "\n",
    "| Scenario | With inferSchema | With explicit schema |\n",
    "|----------|------------------|---------------------|\n",
    "| 10 GB file | Scans entire file first | Direct read |\n",
    "| Column `\"123\"` | Might be INT or STRING? | You control it |\n",
    "| New column added | Schema changes silently | Fails fast (good!) |\n",
    "| Null values | Might guess wrong type | Explicit nullable |\n",
    "\n",
    "**Rule of thumb:** Use `inferSchema=true` for exploration, explicit schema for production.\n",
    "\n",
    "### The Bronze Layer Philosophy\n",
    "\n",
    "In Medallion architecture, Bronze layer should:\n",
    "- Keep data as STRING (preserve original values)\n",
    "- Add metadata (ingestion time, source file)\n",
    "- NOT apply business logic\n",
    "- Be idempotent (re-run safe)\n",
    "\n",
    "*We'll explore this more in the ingestion workshop.*\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **DataFrame Reader API** - unified interface for all formats\n",
    "2. **Schema definition** - explicit vs inferred, when to use which\n",
    "3. **Exploration operations** - quick data profiling\n",
    "4. **Format-specific options** - CSV, JSON, Parquet gotchas\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cf6c95b-2145-4ffe-a23d-8118983d4dd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Environment Initialization\n",
    "\n",
    "Run the central configuration script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea9ed832-63c8-4788-8c7f-256fcf2ac6e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9981cb88-642b-4f84-8a87-ebf5d572e504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Notebook Configuration\n",
    "\n",
    "Define variables specific to this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f11c5fa-8097-4ebe-89cf-93833a9e6f72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paths to data directories (subdirectories in DATASET_BASE_PATH from 00_setup)\n",
    "CUSTOMERS_PATH = f\"{DATASET_BASE_PATH}/customers\"\n",
    "ORDERS_PATH = f\"{DATASET_BASE_PATH}/orders\"\n",
    "PRODUCTS_PATH = f\"{DATASET_BASE_PATH}/products\"\n",
    "\n",
    "# Paths to specific files\n",
    "CUSTOMERS_CSV = f\"{CUSTOMERS_PATH}/customers.csv\"\n",
    "ORDERS_JSON = f\"{ORDERS_PATH}/orders_batch.json\"\n",
    "PRODUCTS_PARQUET = f\"{PRODUCTS_PATH}/products.parquet\"\n",
    "\n",
    "print(f\"Path to customers CSV file: {CUSTOMERS_CSV}\")\n",
    "print(f\"Path to orders JSON file: {ORDERS_JSON}\")\n",
    "print(f\"Path to products Parquet file: {PRODUCTS_PARQUET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "889facc3-12b1-46b1-ac58-004aef6b0971",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Part 1: CSV Data Import (Customers)\n",
    "\n",
    "### 1.1. Loading CSV with Automatic Schema Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e44f411-bfe5-4692-8552-458c1dcfad50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load CSV data with automatic schema inference\n",
    "customers_auto_df = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")       # First line contains column names\n",
    "    .option(\"inferSchema\", \"true\")  # Automatic data type inference\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bf57a69-f1da-43f1-ba1c-7adec2725f80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display schema\n",
    "print(\" Automatically detected schema:\")\n",
    "customers_auto_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44eb5392-ef2f-41d4-bf26-057fcd3d1654",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n Data sample (5 rows):\")\n",
    "customers_auto_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9442b0e9-3cad-4495-984d-e7e7f7b73e99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display data sample\n",
    "print(\"\\n Data sample :\")\n",
    "display(customers_auto_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b54f9b85-8dc4-4446-9164-cf1b07518eee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.2. CSV Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bb59e98-a08e-46c0-9a35-7cddc6bce458",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of column names\n",
    "print(\" DataFrame Columns:\")\n",
    "customers_auto_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8274f51b-13c1-4a57-bfd6-2b921ff6eda3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of data types\n",
    "print(\"\\n Column data types:\")\n",
    "customers_auto_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3287a960-4d7a-498f-aebc-31cc03abfebd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Row count\n",
    "row_count = customers_auto_df.count()\n",
    "print(f\"\\nRow count: {row_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7437bcb1-5c86-400a-b99b-009f7ceeaec0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.3. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c0ea429-e348-41c1-a2c1-275d16952a03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Schema definition for customers\n",
    "# Actual structure: customer_id, first_name, last_name, email, phone, city, state, country, registration_date, customer_segment\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), nullable=False),\n",
    "    StructField(\"first_name\", StringType(), nullable=True),\n",
    "    StructField(\"last_name\", StringType(), nullable=True), \n",
    "    StructField(\"email\", StringType(), nullable=True),\n",
    "    StructField(\"phone\", StringType(), nullable=True),\n",
    "    StructField(\"city\", StringType(), nullable=True),\n",
    "    StructField(\"state\", StringType(), nullable=True),\n",
    "    StructField(\"country\", StringType(), nullable=True),\n",
    "    StructField(\"registration_date\", StringType(), nullable=True),  # Date as string, will convert later\n",
    "    StructField(\"customer_segment\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Descriptive statistics (count, mean, stddev, min, max)\n",
    "print(\" Descriptive statistics (describe):\")\n",
    "customers_auto_df.describe().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbd046cf-ee66-4ecb-9ba8-d8291c194052",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extended statistics (+ percentiles)\n",
    "print(\"\\n Extended statistics (summary):\")\n",
    "customers_auto_df.summary().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10f0f50b-3fa1-452a-85fd-f0df5773f08c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Extended Reader Options – Typical Production Issues\n",
    "\n",
    "In a production environment, we often encounter CSV files with:\n",
    "\n",
    "- different separator (`;` instead of `,`),\n",
    "- quotes inside fields,\n",
    "- corrupted rows.\n",
    "\n",
    "Key options:\n",
    "\n",
    "- `delimiter` – custom column separator,\n",
    "- `quote` – character opening/closing text fields,\n",
    "- `escape` – way to \"escape\" special characters,\n",
    "- `mode` – handling of malformed records (`PERMISSIVE`, `DROPMALFORMED`, `FAILFAST`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5071f0b-5611-4d63-9e21-3f649dae5ee6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.4. Manual Schema Definition for CSV\n",
    "\n",
    "**Best Practice:** Defining schema manually ensures control and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0a84ce7-93e5-4da7-8492-e6ca35479b17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1cdaa60-b1b1-46de-8604-7ae9aaa4c320",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Schema definition for customers\n",
    "# Structure: customer_id (int), first_name (string), last_name (string), email (string), city (string), country (string), registration_date (timestamp)\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"first_name\", StringType(), nullable=True),\n",
    "    StructField(\"last_name\", StringType(), nullable=True),\n",
    "    StructField(\"city\", StringType(), nullable=True),\n",
    "    StructField(\"email\", StringType(), nullable=True),\n",
    "    StructField(\"country\", StringType(), nullable=True),\n",
    "    StructField(\"registration_date\", TimestampType(), nullable=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bd147be-df3f-4274-8228-bf8cf6a626ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load CSV data with manually defined schema\n",
    "customers_df = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(customers_schema)  # Use defined schema\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")\n",
    "customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5468f78b-50f0-4032-94af-b1dcba5dd4df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n Data sample with manual schema:\")\n",
    "customers_df.display(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2599e27-eda1-4c27-885d-3e96326d9a6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Part 2: JSON Data Import (Orders)\n",
    "\n",
    "### 2.1. Loading JSON with Automatic Schema Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "574e1299-3b2b-4ee2-93bb-b2a4e136776e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load JSON data with automatic schema inference\n",
    "orders_auto_df = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(ORDERS_JSON)\n",
    ")\n",
    "\n",
    "print(\" JSON schema detected automatically:\")\n",
    "orders_auto_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed55989b-08d3-4660-a0cc-02cbbfddee90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n JSON data sample:\")\n",
    "orders_auto_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "438958ac-583a-469e-aea4-aafe0780a54e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.2. Eksploracja danych JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17c7d1a3-7296-463d-916b-dfecac27c277",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Number of columns and rows\n",
    "print(f\" Number of columns: {len(orders_auto_df.columns)}\")\n",
    "print(f\" Column names: {orders_auto_df.columns}\")\n",
    "print(f\" Number of rows: {orders_auto_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "151a08c4-7463-4c4d-9870-3bafbba9a101",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data types\n",
    "print(\"\\n Data types:\")\n",
    "for col_name, col_type in orders_auto_df.dtypes:\n",
    "    print(f\"  - {col_name}: {col_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37d607c2-8aa5-4bbe-ad9c-1af6495d66a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.3. Manual Schema Definition for JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c826f976-b255-4e73-83fc-bcc6b3f5238b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType, StringType, StructField, StructType, TimestampType, IntegerType\n",
    "\n",
    "# Schema definition for orders\n",
    "# Actual structure: order_id, customer_id, product_id, store_id, order_datetime, quantity, unit_price, discount_percent, total_amount, payment_method\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), nullable=True),  # Can be null in data\n",
    "    StructField(\"customer_id\", StringType(), nullable=False),\n",
    "    StructField(\"product_id\", StringType(), nullable=False),\n",
    "    StructField(\"store_id\", StringType(), nullable=False),\n",
    "    StructField(\"order_datetime\", StringType(), nullable=True),  # String, will convert to timestamp later\n",
    "    StructField(\"quantity\", IntegerType(), nullable=False),\n",
    "    StructField(\"unit_price\", DoubleType(), nullable=False),\n",
    "    StructField(\"discount_percent\", IntegerType(), nullable=False),\n",
    "    StructField(\"total_amount\", DoubleType(), nullable=False),\n",
    "    StructField(\"payment_method\", StringType(), nullable=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "630d2a6d-8307-47ea-8de2-abacfe368d11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), nullable=False),\n",
    "    StructField(\"customer_id\", StringType(), nullable=True),\n",
    "    StructField(\"order_datetime\", TimestampType(), nullable=True),\n",
    "    StructField(\"total_amount\", DoubleType(), nullable=True),\n",
    "    StructField(\"status\", StringType(), nullable=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a928b826-9d57-43ae-826a-f50d97830be8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load JSON data with manually defined schema\n",
    "orders_df = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .schema(orders_schema)\n",
    "    .load(ORDERS_JSON)\n",
    ")\n",
    "\n",
    "print(\" JSON schema defined manually:\")\n",
    "orders_df.printSchema()\n",
    "\n",
    "print(\"\\n Data sample with manual schema:\")\n",
    "orders_df.display(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69067b3e-a06e-4e93-bdfa-b9bd4cc945cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.4. Descriptive Statistics for JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f74cfb11-9531-4779-9b4a-2cb6fa8ae265",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Statistics for numerical columns\n",
    "print(\" Statistics for orders:\")\n",
    "orders_df.select(\"order_id\", \"customer_id\", \"total_amount\").describe().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74f09cf3-b798-4341-b2a8-8a22b1ca9038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Order distribution by payment method\n",
    "print(\"\\n Order distribution by payment method (payment_method):\")\n",
    "orders_df.groupBy(\"payment_method\").count().orderBy(\"count\", ascending=False).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f286bb44-3d18-4746-a701-aaea6e6fdaaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Top 10 customers with the highest number of orders\n",
    "print(\"\\n Top 10 customers with the highest number of orders:\")\n",
    "orders_auto_df.groupBy(\"customer_id\").count().orderBy(\"count\", ascending=False).limit(10).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9f2041e-010b-42d1-b577-0a31eb841d95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Part 3: Parquet Data Import (Products)\n",
    "\n",
    "### 3.1. Loading Parquet (built-in schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "607e760f-8e10-45a7-9341-59f7da46a630",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parquet already contains built-in schema - no need to define it\n",
    "products_df = (\n",
    "    spark.read\n",
    "    .format(\"parquet\")\n",
    "    .load(PRODUCTS_PARQUET)\n",
    ")\n",
    "\n",
    "print(\" Parquet Schema (built-in):\")\n",
    "products_df.printSchema()\n",
    "\n",
    "print(\"\\n Parquet data sample:\")\n",
    "products_df.display(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08f2a68f-7205-49c4-a823-18d44f4d50d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.2. Parquet Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da237eab-76fe-4a9a-9e89-c98b159d5c96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Basic information\n",
    "print(f\" Number of columns: {len(products_df.columns)}\")\n",
    "print(f\" Column names: {products_df.columns}\")\n",
    "print(f\" Number of products: {products_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3483321-4da9-4cd9-975b-5504dd4d9d2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data types\n",
    "print(\"\\n Data types:\")\n",
    "for col_name, col_type in products_df.dtypes:\n",
    "    print(f\"  - {col_name}: {col_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88d9b0fd-684e-4db2-9b60-95b243b5896d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.3. Descriptive Statistics for Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c333b1b-91a9-4da1-b891-c7c650f55e3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Statistics for numerical columns\n",
    "print(\" Statistics for products:\")\n",
    "products_df.describe().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36e56f2d-d3d2-4ca5-9304-2c75230d6cc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extended statistics\n",
    "print(\"\\n Extended statistics:\")\n",
    "products_df.summary().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bb09b67-4724-4fbc-8d4d-08cb6dad06c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Part 4: Performance Comparison\n",
    "\n",
    "### 4.1. Loading CSV: inferSchema vs Manual Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21c1e752-420b-4f81-99ca-0943eca7cb6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Test 1: Automatic schema inference\n",
    "start_auto = time.time()\n",
    "df_auto = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")\n",
    "count_auto = df_auto.count()  # Action - forces execution\n",
    "time_auto = time.time() - start_auto\n",
    "\n",
    "print(f\"[BENCHMARK] Loading CSV with inferSchema: {time_auto:.3f} seconds\")\n",
    "print(f\"[BENCHMARK] Row count: {count_auto}\")\n",
    "\n",
    "# Test 2: Manual schema definition\n",
    "start_manual = time.time()\n",
    "df_manual = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(customers_schema)\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")\n",
    "count_manual = df_manual.count()  # Action - forces execution\n",
    "time_manual = time.time() - start_manual\n",
    "\n",
    "print(f\"\\n[BENCHMARK] Loading CSV with manual schema: {time_manual:.3f} seconds\")\n",
    "print(f\"[BENCHMARK] Row count: {count_manual}\")\n",
    "\n",
    "# Comparison\n",
    "speedup = (time_auto - time_manual) / time_auto * 100\n",
    "print(f\"\\n[RESULT] Speedup: {speedup:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3d4e576-2385-455d-bdce-d782b705729f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Part 5: Best Practices\n",
    "\n",
    "### Recommendations for Data Import:\n",
    "\n",
    "1. **Always define schema manually**\n",
    "   - Faster loading\n",
    "   - Data type control\n",
    "   - Avoiding type errors\n",
    "\n",
    "2. **Choose the right format**\n",
    "   - **Parquet** - best for analytics (columnar, compression)\n",
    "   - **CSV** - easy to debug, but slower\n",
    "   - **JSON** - flexible for semi-structured data\n",
    "\n",
    "3. **Use partitioning**\n",
    "   - Speed up filtering queries\n",
    "   - Example: partitioning orders by date\n",
    "\n",
    "4. **Check data quality immediately**\n",
    "   - `count()` - check row count\n",
    "   - `describe()` - check value distribution\n",
    "   - `printSchema()` - verify types\n",
    "\n",
    "5. **Use `limit()` when experimenting**\n",
    "   - Speeds up code iterations\n",
    "   - Example: `df.limit(1000).display()`\n",
    "\n",
    "6. **Document schemas**\n",
    "   - Facilitates code maintenance\n",
    "   - Example: comments at StructType definition\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f118114-afa2-4aa5-b224-caffe767626f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook you learned:\n",
    "\n",
    "✅ **DataFrame Reader API**\n",
    "- Loading data from CSV, JSON, Parquet\n",
    "- Configuring options (header, inferSchema, delimiter)\n",
    "\n",
    "✅ **Manual Schema Definition**\n",
    "- Creating schemas using StructType and StructField\n",
    "- Performance comparison: inferSchema vs manual schema\n",
    "\n",
    "✅ **Exploration Operations**\n",
    "- Basic operations: columns, dtypes, count\n",
    "- Statistics: describe(), summary()\n",
    "- Grouping and aggregation\n",
    "\n",
    "✅ **Best Practices**\n",
    "- Recommendation for manual schema definition\n",
    "- Choosing data format for different scenarios\n",
    "- Checking data quality\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Databricks - Reading Data](https://docs.databricks.com/ingestion/index.html)\n",
    "- [PySpark DataFrame API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_data_import_exploration",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
