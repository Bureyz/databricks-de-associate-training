{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e3b4724",
   "metadata": {},
   "source": [
    "# Workshop: Lakeflow Declarative Pipelines\n",
    "\n",
    "**Training Objective:** Build a complete ETL pipeline using Lakeflow Declarative Pipelines (formerly Delta Live Tables) with Bronze-Silver-Gold medallion architecture.\n",
    "\n",
    "**Topics covered:**\n",
    "- Lakeflow pipeline creation and configuration\n",
    "- STREAMING TABLE and MATERIALIZED VIEW concepts\n",
    "- Auto Loader for streaming ingestion\n",
    "- Data quality expectations (EXPECT constraints)\n",
    "- SCD Type 2 implementation\n",
    "- Star schema in Gold layer\n",
    "\n",
    "**Duration:** 45 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1812c982",
   "metadata": {},
   "source": [
    "## Context and Requirements\n",
    "\n",
    "- **Training Day**: Day 1 - Lakeflow Pipelines\n",
    "- **Notebook Type**: Workshop\n",
    "- **Technical Requirements**:\n",
    "  - Databricks Runtime 13.0+ (recommended: 14.3 LTS)\n",
    "  - Unity Catalog enabled\n",
    "  - Permissions: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY, Workflows access\n",
    "  - Cluster: Standard with minimum 2 workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d668e0",
   "metadata": {},
   "source": [
    "## Theoretical Introduction\n",
    "\n",
    "**Section Objective:** Understanding Lakeflow Declarative Pipelines fundamentals\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Lakeflow Pipeline**: Declarative ETL framework that automates orchestration, error handling, and data quality\n",
    "- **STREAMING TABLE**: Append-only table for incremental ingestion with exactly-once semantics\n",
    "- **MATERIALIZED VIEW**: Cached aggregation that updates incrementally based on source changes\n",
    "- **Auto Loader**: Streaming file ingestion using `read_files()` function\n",
    "- **SCD Type 2**: Slowly Changing Dimension pattern for tracking historical changes\n",
    "\n",
    "**Why Lakeflow?**\n",
    "Traditional ETL requires managing dependencies, retries, and checkpoints manually. Lakeflow handles this automatically - you declare WHAT you want, not HOW to build it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58964eda",
   "metadata": {},
   "source": [
    "## Environment Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c68213",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7c8b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display paths for Lakeflow pipeline configuration\n",
    "print(\"=\"*60)\n",
    "print(\"PATHS FOR LAKEFLOW PIPELINE CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nCustomers CSV:  {DATASET_BASE_PATH}/customers/\")\n",
    "print(f\"Orders JSON:    {DATASET_BASE_PATH}/orders/stream/\")\n",
    "print(f\"Products CSV:   {DATASET_BASE_PATH}/products/csv/\")\n",
    "print(f\"\\nCatalog:        {CATALOG}\")\n",
    "print(f\"Schema Bronze:  {BRONZE_SCHEMA}\")\n",
    "print(f\"Schema Silver:  {SILVER_SCHEMA}\")\n",
    "print(f\"Schema Gold:    {GOLD_SCHEMA}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de09a61",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Creating Lakeflow Pipeline\n",
    "\n",
    "### Task 1.1: Create Pipeline in Databricks UI\n",
    "\n",
    "**Objective:** Create a new Lakeflow pipeline in Databricks workspace.\n",
    "\n",
    "**Instructions:**\n",
    "1. Navigate to **Workflows → Lakeflow Pipelines**\n",
    "2. Click **Create Pipeline**\n",
    "3. Configure:\n",
    "   - **Name:** `ecommerce_workshop_pipeline`\n",
    "   - **Product edition:** Advanced\n",
    "   - **Pipeline mode:** Triggered\n",
    "   - **Target catalog:** Use your catalog from setup\n",
    "   - **Target schema:** `lakeflow_workshop`\n",
    "\n",
    "**Hints:**\n",
    "- Use \"Triggered\" mode for workshop (not Continuous)\n",
    "- Development mode allows full refresh for testing\n",
    "- You can add SQL files later from source code section\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdda7a4",
   "metadata": {},
   "source": [
    "### Task 1.2: Configure Pipeline Parameters\n",
    "\n",
    "**Objective:** Add configuration parameters for source data paths.\n",
    "\n",
    "**Instructions:**\n",
    "1. In pipeline settings, find **Configuration** section\n",
    "2. Add the following key-value pairs:\n",
    "\n",
    "| Key | Value |\n",
    "|-----|-------|\n",
    "| `customers_path` | `/Volumes/.../customers/` |\n",
    "| `orders_path` | `/Volumes/.../orders/stream/` |\n",
    "| `products_path` | `/Volumes/.../products/csv/` |\n",
    "\n",
    "**Hints:**\n",
    "- Use paths displayed in the setup cell above\n",
    "- Parameters are referenced in SQL as `${parameter_name}`\n",
    "- You can also set parameters in JSON pipeline definition\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f9bdd2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Bronze Layer - Streaming Ingestion\n",
    "\n",
    "### Task 2.1: Create Bronze Customers Table\n",
    "\n",
    "**Objective:** Create a STREAMING TABLE to ingest customer CSV files using Auto Loader.\n",
    "\n",
    "**Instructions:**\n",
    "1. Create a new SQL file in the pipeline\n",
    "2. Use `CREATE OR REFRESH STREAMING TABLE`\n",
    "3. Use `STREAM read_files()` for Auto Loader\n",
    "4. Add metadata columns for lineage tracking\n",
    "\n",
    "**Hints:**\n",
    "- `read_files()` options: `format => 'csv'`, `header => true`\n",
    "- `_metadata.file_path` gives source file path\n",
    "- `_metadata.file_modification_time` gives file timestamp\n",
    "- `current_timestamp()` for ingestion time\n",
    "\n",
    "**TODO:** Complete the SQL below:\n",
    "\n",
    "```sql\n",
    "-- Bronze Customers - Auto Loader ingestion\n",
    "CREATE OR REFRESH STREAMING TABLE bronze_customers\n",
    "COMMENT 'Raw customer data from CSV files'\n",
    "AS\n",
    "SELECT\n",
    "    customer_id,\n",
    "    first_name,\n",
    "    last_name,\n",
    "    email,\n",
    "    phone,\n",
    "    city,\n",
    "    country,\n",
    "    CAST(registration_date AS DATE) AS registration_date,\n",
    "    customer_segment,\n",
    "    -- TODO: Add metadata columns\n",
    "    _metadata.___ AS _source_file,\n",
    "    _metadata.___ AS _file_modified_at,\n",
    "    ___() AS _ingested_at\n",
    "FROM STREAM read_files(\n",
    "    '${customers_path}',\n",
    "    format => '___',\n",
    "    header => ___,\n",
    "    inferColumnTypes => true\n",
    ");\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4c9f02",
   "metadata": {},
   "source": [
    "### Task 2.2: Create Bronze Orders Table\n",
    "\n",
    "**Objective:** Create a STREAMING TABLE for JSON order files.\n",
    "\n",
    "**Instructions:**\n",
    "1. Ingest JSON files from orders path\n",
    "2. Cast order_date to TIMESTAMP\n",
    "3. Add metadata columns\n",
    "\n",
    "**Hints:**\n",
    "- JSON format: `format => 'json'`\n",
    "- No header option needed for JSON\n",
    "- Cast types explicitly for consistency\n",
    "\n",
    "**TODO:** Complete the SQL:\n",
    "\n",
    "```sql\n",
    "-- Bronze Orders - JSON streaming ingestion\n",
    "CREATE OR REFRESH STREAMING TABLE bronze_orders\n",
    "COMMENT 'Raw order data from JSON stream'\n",
    "AS\n",
    "SELECT\n",
    "    order_id,\n",
    "    customer_id,\n",
    "    product_id,\n",
    "    CAST(order_date AS ___) AS order_date,  -- TODO: What type?\n",
    "    quantity,\n",
    "    unit_price,\n",
    "    total_amount,\n",
    "    status,\n",
    "    payment_method,\n",
    "    _metadata.file_path AS _source_file,\n",
    "    current_timestamp() AS _ingested_at\n",
    "FROM STREAM read_files(\n",
    "    '${___}',  -- TODO: Which parameter?\n",
    "    format => '___'  -- TODO: What format?\n",
    ");\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffd052f",
   "metadata": {},
   "source": [
    "### Task 2.3: Create Bronze Products Table\n",
    "\n",
    "**Objective:** Create a STREAMING TABLE for product CSV files.\n",
    "\n",
    "**Instructions:**\n",
    "1. Ingest CSV files from products path\n",
    "2. Cast price to DECIMAL and stock_quantity to INT\n",
    "3. Add metadata columns\n",
    "\n",
    "**Hints:**\n",
    "- `CAST(price AS DECIMAL(10,2))` for price precision\n",
    "- `CAST(stock_quantity AS INT)` for integer quantity\n",
    "\n",
    "**TODO:** Write the complete SQL for bronze_products table.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6767c6fa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Silver Layer - Data Quality & SCD Type 2\n",
    "\n",
    "### Task 3.1: Create Silver Customers with SCD Type 2\n",
    "\n",
    "**Objective:** Implement SCD Type 2 for customer dimension to track history of changes.\n",
    "\n",
    "**Instructions:**\n",
    "1. Define the target STREAMING TABLE with SCD2 columns\n",
    "2. Create an AUTO CDC FLOW with KEYS and SEQUENCE BY\n",
    "3. Use `STORED AS SCD TYPE 2`\n",
    "\n",
    "**Hints:**\n",
    "- `__START_AT` and `__END_AT` are auto-managed by Lakeflow\n",
    "- `KEYS` defines the business key for matching records\n",
    "- `SEQUENCE BY` determines which record is newer (conflict resolution)\n",
    "- Active records have `__END_AT IS NULL`\n",
    "\n",
    "**TODO:** Complete the SQL:\n",
    "\n",
    "```sql\n",
    "-- Silver Customers - SCD Type 2 with history tracking\n",
    "CREATE OR REFRESH STREAMING TABLE silver_customers (\n",
    "    customer_id STRING,\n",
    "    first_name STRING,\n",
    "    last_name STRING,\n",
    "    email STRING,\n",
    "    phone STRING,\n",
    "    city STRING,\n",
    "    country STRING,\n",
    "    registration_date DATE,\n",
    "    customer_segment STRING,\n",
    "    _source_file STRING,\n",
    "    _ingested_at TIMESTAMP,\n",
    "    -- SCD2 columns (auto-managed)\n",
    "    __START_AT TIMESTAMP,\n",
    "    __END_AT TIMESTAMP\n",
    ")\n",
    "COMMENT 'Silver customers with SCD Type 2 history';\n",
    "\n",
    "-- AUTO CDC Flow for SCD Type 2\n",
    "CREATE FLOW silver_customers_scd2\n",
    "AS AUTO CDC INTO silver_customers\n",
    "FROM STREAM ___  -- TODO: Source table?\n",
    "KEYS (___)  -- TODO: Business key?\n",
    "SEQUENCE BY ___  -- TODO: Ordering column?\n",
    "STORED AS SCD TYPE ___; -- TODO: Which SCD type?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b86d5c",
   "metadata": {},
   "source": [
    "### Task 3.2: Create Silver Orders with Data Quality\n",
    "\n",
    "**Objective:** Apply data quality expectations to validate orders.\n",
    "\n",
    "**Instructions:**\n",
    "1. Add CONSTRAINT ... EXPECT for validation rules\n",
    "2. Choose violation action: DROP ROW or FAIL UPDATE\n",
    "3. Add calculated fields (gross_amount, discount_amount)\n",
    "4. Standardize text fields (UPPER for status, payment_method)\n",
    "\n",
    "**Hints:**\n",
    "- `ON VIOLATION DROP ROW` - silently drops invalid records\n",
    "- `ON VIOLATION FAIL UPDATE` - stops pipeline on violation\n",
    "- Dropped records are tracked in Event Log\n",
    "- Use business-meaningful constraint names\n",
    "\n",
    "**TODO:** Complete the SQL:\n",
    "\n",
    "```sql\n",
    "-- Silver Orders - with Data Quality expectations\n",
    "CREATE OR REFRESH STREAMING TABLE silver_orders (\n",
    "    -- Data Quality Constraints\n",
    "    CONSTRAINT valid_order_id EXPECT (order_id IS NOT ___) ON VIOLATION ___ ROW,\n",
    "    CONSTRAINT valid_amount EXPECT (total_amount ___ 0) ON VIOLATION DROP ROW,\n",
    "    CONSTRAINT valid_quantity EXPECT (quantity > 0) ON VIOLATION DROP ROW,\n",
    "    CONSTRAINT valid_date EXPECT (order_date IS NOT NULL) ON VIOLATION ___ UPDATE\n",
    ")\n",
    "COMMENT 'Validated orders with data quality checks'\n",
    "AS\n",
    "SELECT\n",
    "    order_id,\n",
    "    customer_id,\n",
    "    product_id,\n",
    "    order_date,\n",
    "    DATE(order_date) AS order_date_key,\n",
    "    quantity,\n",
    "    unit_price,\n",
    "    total_amount,\n",
    "    -- TODO: Calculate gross and discount amounts\n",
    "    (unit_price * ___) AS gross_amount,\n",
    "    (total_amount - (unit_price * quantity)) AS discount_amount,\n",
    "    -- TODO: Standardize text fields\n",
    "    ___(status) AS status,\n",
    "    UPPER(payment_method) AS payment_method,\n",
    "    _source_file,\n",
    "    _ingested_at,\n",
    "    current_timestamp() AS _processed_at\n",
    "FROM STREAM bronze_orders;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387b9bf7",
   "metadata": {},
   "source": [
    "### Task 3.3: Create Silver Products with Validation\n",
    "\n",
    "**Objective:** Create validated product table with price tier classification.\n",
    "\n",
    "**Instructions:**\n",
    "1. Add expectations for product_id, price, category\n",
    "2. Apply INITCAP to product_name, UPPER to category\n",
    "3. Calculate price_tier based on price ranges\n",
    "\n",
    "**Hints:**\n",
    "- `INITCAP()` capitalizes first letter of each word\n",
    "- Use CASE WHEN for price_tier: PREMIUM (≥1000), STANDARD (≥100), BUDGET (<100)\n",
    "\n",
    "**TODO:** Write the complete SQL for silver_products table.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cb12cf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Gold Layer - Star Schema\n",
    "\n",
    "### Task 4.1: Create dim_customer Dimension\n",
    "\n",
    "**Objective:** Create customer dimension from SCD2 table (current records only).\n",
    "\n",
    "**Instructions:**\n",
    "1. Use MATERIALIZED VIEW (batch refresh, cached)\n",
    "2. Filter only current/active records from SCD2\n",
    "3. Add calculated column: days_since_registration\n",
    "\n",
    "**Hints:**\n",
    "- `WHERE __END_AT IS NULL` filters only active records\n",
    "- `DATEDIFF(current_date(), registration_date)` for days calculation\n",
    "- `CONCAT(first_name, ' ', last_name)` for full_name\n",
    "\n",
    "**TODO:** Complete the SQL:\n",
    "\n",
    "```sql\n",
    "-- Dimension: Customer (current version from SCD2)\n",
    "CREATE OR REFRESH MATERIALIZED VIEW dim_customer\n",
    "COMMENT 'Customer dimension - current active records only'\n",
    "AS\n",
    "SELECT\n",
    "    customer_id,\n",
    "    first_name,\n",
    "    last_name,\n",
    "    CONCAT(first_name, ' ', last_name) AS full_name,\n",
    "    email,\n",
    "    phone,\n",
    "    city,\n",
    "    country,\n",
    "    registration_date,\n",
    "    customer_segment,\n",
    "    ___(current_date(), registration_date) AS days_since_registration\n",
    "FROM silver_customers\n",
    "WHERE ___; -- TODO: Filter only active records\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b22fbb",
   "metadata": {},
   "source": [
    "### Task 4.2: Create dim_product Dimension\n",
    "\n",
    "**Objective:** Create product dimension with stock status.\n",
    "\n",
    "**Instructions:**\n",
    "1. Use MATERIALIZED VIEW\n",
    "2. Add stock_status: OUT_OF_STOCK (0), LOW_STOCK (<10), IN_STOCK (≥10)\n",
    "\n",
    "**TODO:** Complete the SQL:\n",
    "\n",
    "```sql\n",
    "-- Dimension: Product with stock status\n",
    "CREATE OR REFRESH MATERIALIZED VIEW dim_product\n",
    "COMMENT 'Product dimension with price tiers and stock status'\n",
    "AS\n",
    "SELECT\n",
    "    product_id,\n",
    "    product_name,\n",
    "    category,\n",
    "    price,\n",
    "    stock_quantity,\n",
    "    price_tier,\n",
    "    CASE \n",
    "        WHEN stock_quantity = 0 THEN '___'\n",
    "        WHEN stock_quantity < 10 THEN '___'\n",
    "        ELSE 'IN_STOCK'\n",
    "    END AS stock_status\n",
    "FROM silver_products;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9ed98a",
   "metadata": {},
   "source": [
    "### Task 4.3: Create dim_date Dimension\n",
    "\n",
    "**Objective:** Generate a date dimension with calendar attributes.\n",
    "\n",
    "**Instructions:**\n",
    "1. Use SEQUENCE to generate date range (2020-2025)\n",
    "2. Add: year, quarter, month, month_name, week_of_year, day_of_week, day_name, is_weekend\n",
    "\n",
    "**Hints:**\n",
    "- `EXPLODE(SEQUENCE(DATE('2020-01-01'), DATE('2025-12-31'), INTERVAL 1 DAY))` generates dates\n",
    "- `DATE_FORMAT(date, 'yyyyMMdd')` for date key\n",
    "- `DAYOFWEEK() IN (1, 7)` for weekend check (1=Sunday, 7=Saturday)\n",
    "\n",
    "**TODO:** Write the complete SQL for dim_date."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5b608a",
   "metadata": {},
   "source": [
    "### Task 4.4: Create fact_sales Fact Table\n",
    "\n",
    "**Objective:** Create streaming fact table for real-time sales data.\n",
    "\n",
    "**Instructions:**\n",
    "1. Use STREAMING TABLE for low-latency updates\n",
    "2. Include dimension keys: customer_id, product_id, order_date_key\n",
    "3. Include measures: quantity, unit_price, total_amount, gross_amount, discount_amount\n",
    "\n",
    "**Hints:**\n",
    "- `CAST(DATE_FORMAT(order_date, 'yyyyMMdd') AS INT)` for date key\n",
    "- Keep order_timestamp for detailed analysis\n",
    "- Include _processed_at for lineage\n",
    "\n",
    "**TODO:** Complete the SQL:\n",
    "\n",
    "```sql\n",
    "-- Fact: Sales (streaming for low-latency)\n",
    "CREATE OR REFRESH STREAMING TABLE fact_sales\n",
    "COMMENT 'Sales fact table - real-time updates'\n",
    "AS\n",
    "SELECT\n",
    "    -- Keys\n",
    "    order_id,\n",
    "    customer_id,\n",
    "    product_id,\n",
    "    CAST(DATE_FORMAT(order_date, '___') AS INT) AS order_date_key,\n",
    "    \n",
    "    -- Measures\n",
    "    quantity,\n",
    "    unit_price,\n",
    "    total_amount,\n",
    "    gross_amount,\n",
    "    discount_amount,\n",
    "    \n",
    "    -- Attributes\n",
    "    status,\n",
    "    payment_method,\n",
    "    order_date AS order_timestamp,\n",
    "    \n",
    "    -- Lineage\n",
    "    _processed_at\n",
    "FROM STREAM ___;  -- TODO: Source table?\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d7e751",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Running and Monitoring Pipeline\n",
    "\n",
    "### Task 5.1: Start Pipeline\n",
    "\n",
    "**Objective:** Run the pipeline and observe the execution.\n",
    "\n",
    "**Instructions:**\n",
    "1. Click **Start** in the pipeline UI\n",
    "2. Observe the DAG building automatically\n",
    "3. Watch tables being processed in order\n",
    "4. Note the parallel execution of independent tables\n",
    "\n",
    "**Questions to Consider:**\n",
    "- What is the execution order?\n",
    "- Which tables run in parallel?\n",
    "- How long does each table take?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2774608c",
   "metadata": {},
   "source": [
    "### Task 5.2: Explore Event Log\n",
    "\n",
    "**Objective:** Query the Event Log for pipeline metrics and data quality results.\n",
    "\n",
    "**Instructions:**\n",
    "1. After pipeline completes, query the Event Log\n",
    "2. Find data quality metrics\n",
    "3. Check processing statistics\n",
    "\n",
    "**Hints:**\n",
    "- Event Log is a Delta table accessible via SQL\n",
    "- Use `event_log('pipeline_id')` function\n",
    "- Filter by `event_type` for specific events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada6ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Query Event Log after pipeline runs\n",
    "# Replace 'your_pipeline_id' with actual pipeline ID\n",
    "\n",
    "# Uncomment after running pipeline:\n",
    "\n",
    "# event_log_query = \"\"\"\n",
    "# SELECT \n",
    "#     timestamp,\n",
    "#     event_type,\n",
    "#     message,\n",
    "#     details\n",
    "# FROM event_log('your_pipeline_id')\n",
    "# WHERE event_type IN ('flow_progress', 'data_quality')\n",
    "# ORDER BY timestamp DESC\n",
    "# LIMIT 20\n",
    "# \"\"\"\n",
    "# spark.sql(event_log_query).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4435d5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Query Data Quality metrics\n",
    "\n",
    "# Uncomment after running pipeline:\n",
    "\n",
    "# dq_query = \"\"\"\n",
    "# SELECT \n",
    "#     details:flow_name AS table_name,\n",
    "#     details:data_quality.expectation_name AS expectation,\n",
    "#     details:data_quality.passed_records AS passed,\n",
    "#     details:data_quality.failed_records AS failed,\n",
    "#     ROUND(details:data_quality.passed_records / \n",
    "#           NULLIF(details:data_quality.passed_records + details:data_quality.failed_records, 0) * 100, 2) AS pass_rate_pct\n",
    "# FROM event_log('your_pipeline_id')\n",
    "# WHERE event_type = 'data_quality'\n",
    "# ORDER BY timestamp DESC\n",
    "# \"\"\"\n",
    "# spark.sql(dq_query).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3385651e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Workshop Summary\n",
    "\n",
    "### Implemented Components\n",
    "\n",
    "| Layer | Table | Type | Key Feature |\n",
    "|-------|-------|------|-------------|\n",
    "| Bronze | bronze_customers | STREAMING TABLE | Auto Loader (CSV) |\n",
    "| Bronze | bronze_orders | STREAMING TABLE | Auto Loader (JSON) |\n",
    "| Bronze | bronze_products | STREAMING TABLE | Auto Loader (CSV) |\n",
    "| Silver | silver_customers | STREAMING TABLE | SCD Type 2 |\n",
    "| Silver | silver_orders | STREAMING TABLE | Data Quality |\n",
    "| Silver | silver_products | STREAMING TABLE | Validation + Enrichment |\n",
    "| Gold | dim_customer | MATERIALIZED VIEW | Current SCD2 snapshot |\n",
    "| Gold | dim_product | MATERIALIZED VIEW | Stock status |\n",
    "| Gold | dim_date | MATERIALIZED VIEW | Generated calendar |\n",
    "| Gold | fact_sales | STREAMING TABLE | Real-time fact |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Declarative = Less Code** - Focus on WHAT, not HOW\n",
    "2. **Automatic Orchestration** - No manual dependency management\n",
    "3. **Built-in Quality** - EXPECT constraints out-of-the-box\n",
    "4. **Incremental Processing** - Only process new/changed data\n",
    "5. **Observable** - Event Log, lineage, metrics included\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "| Practice | Description |\n",
    "|----------|-------------|\n",
    "| **Use STREAMING TABLE for ingest** | Append-only, exactly-once semantics |\n",
    "| **Use MATERIALIZED VIEW for aggregations** | Cached, incremental refresh |\n",
    "| **Add metadata columns** | Track source file, ingestion time |\n",
    "| **Use meaningful constraint names** | Easy debugging in Event Log |\n",
    "| **Separate Bronze/Silver/Gold** | Clear data quality progression |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cddd5d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solutions\n",
    "\n",
    "Below are the complete solutions for all workshop tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8608768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SOLUTIONS - Complete SQL for Lakeflow Pipeline\n",
    "# =============================================================================\n",
    "# These are the complete SQL statements. In practice, create separate .sql files\n",
    "# for each table and add them to the Lakeflow pipeline.\n",
    "\n",
    "bronze_customers_sql = \"\"\"\n",
    "-- Task 2.1: Bronze Customers\n",
    "CREATE OR REFRESH STREAMING TABLE bronze_customers\n",
    "COMMENT 'Raw customer data from CSV files'\n",
    "AS\n",
    "SELECT\n",
    "    customer_id,\n",
    "    first_name,\n",
    "    last_name,\n",
    "    email,\n",
    "    phone,\n",
    "    city,\n",
    "    country,\n",
    "    CAST(registration_date AS DATE) AS registration_date,\n",
    "    customer_segment,\n",
    "    _metadata.file_path AS _source_file,\n",
    "    _metadata.file_modification_time AS _file_modified_at,\n",
    "    current_timestamp() AS _ingested_at\n",
    "FROM STREAM read_files(\n",
    "    '${customers_path}',\n",
    "    format => 'csv',\n",
    "    header => true,\n",
    "    inferColumnTypes => true\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "bronze_orders_sql = \"\"\"\n",
    "-- Task 2.2: Bronze Orders\n",
    "CREATE OR REFRESH STREAMING TABLE bronze_orders\n",
    "COMMENT 'Raw order data from JSON stream'\n",
    "AS\n",
    "SELECT\n",
    "    order_id,\n",
    "    customer_id,\n",
    "    product_id,\n",
    "    CAST(order_date AS TIMESTAMP) AS order_date,\n",
    "    quantity,\n",
    "    unit_price,\n",
    "    total_amount,\n",
    "    status,\n",
    "    payment_method,\n",
    "    _metadata.file_path AS _source_file,\n",
    "    current_timestamp() AS _ingested_at\n",
    "FROM STREAM read_files(\n",
    "    '${orders_path}',\n",
    "    format => 'json'\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "bronze_products_sql = \"\"\"\n",
    "-- Task 2.3: Bronze Products\n",
    "CREATE OR REFRESH STREAMING TABLE bronze_products\n",
    "COMMENT 'Raw product catalog from CSV'\n",
    "AS\n",
    "SELECT\n",
    "    product_id,\n",
    "    product_name,\n",
    "    category,\n",
    "    CAST(price AS DECIMAL(10,2)) AS price,\n",
    "    CAST(stock_quantity AS INT) AS stock_quantity,\n",
    "    _metadata.file_path AS _source_file,\n",
    "    current_timestamp() AS _ingested_at\n",
    "FROM STREAM read_files(\n",
    "    '${products_path}',\n",
    "    format => 'csv',\n",
    "    header => true,\n",
    "    inferColumnTypes => true\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "print(\"Bronze layer SQL solutions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f44bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silver layer solutions\n",
    "\n",
    "silver_customers_sql = \"\"\"\n",
    "-- Task 3.1: Silver Customers with SCD Type 2\n",
    "CREATE OR REFRESH STREAMING TABLE silver_customers (\n",
    "    customer_id STRING,\n",
    "    first_name STRING,\n",
    "    last_name STRING,\n",
    "    email STRING,\n",
    "    phone STRING,\n",
    "    city STRING,\n",
    "    country STRING,\n",
    "    registration_date DATE,\n",
    "    customer_segment STRING,\n",
    "    _source_file STRING,\n",
    "    _ingested_at TIMESTAMP,\n",
    "    __START_AT TIMESTAMP,\n",
    "    __END_AT TIMESTAMP\n",
    ")\n",
    "COMMENT 'Silver customers with SCD Type 2 history';\n",
    "\n",
    "CREATE FLOW silver_customers_scd2\n",
    "AS AUTO CDC INTO silver_customers\n",
    "FROM STREAM bronze_customers\n",
    "KEYS (customer_id)\n",
    "SEQUENCE BY _ingested_at\n",
    "STORED AS SCD TYPE 2;\n",
    "\"\"\"\n",
    "\n",
    "silver_orders_sql = \"\"\"\n",
    "-- Task 3.2: Silver Orders with Data Quality\n",
    "CREATE OR REFRESH STREAMING TABLE silver_orders (\n",
    "    CONSTRAINT valid_order_id EXPECT (order_id IS NOT NULL) ON VIOLATION DROP ROW,\n",
    "    CONSTRAINT valid_amount EXPECT (total_amount > 0) ON VIOLATION DROP ROW,\n",
    "    CONSTRAINT valid_quantity EXPECT (quantity > 0) ON VIOLATION DROP ROW,\n",
    "    CONSTRAINT valid_date EXPECT (order_date IS NOT NULL) ON VIOLATION FAIL UPDATE\n",
    ")\n",
    "COMMENT 'Validated orders with data quality checks'\n",
    "AS\n",
    "SELECT\n",
    "    order_id,\n",
    "    customer_id,\n",
    "    product_id,\n",
    "    order_date,\n",
    "    DATE(order_date) AS order_date_key,\n",
    "    quantity,\n",
    "    unit_price,\n",
    "    total_amount,\n",
    "    (unit_price * quantity) AS gross_amount,\n",
    "    (total_amount - (unit_price * quantity)) AS discount_amount,\n",
    "    UPPER(status) AS status,\n",
    "    UPPER(payment_method) AS payment_method,\n",
    "    _source_file,\n",
    "    _ingested_at,\n",
    "    current_timestamp() AS _processed_at\n",
    "FROM STREAM bronze_orders;\n",
    "\"\"\"\n",
    "\n",
    "silver_products_sql = \"\"\"\n",
    "-- Task 3.3: Silver Products\n",
    "CREATE OR REFRESH STREAMING TABLE silver_products (\n",
    "    CONSTRAINT valid_product_id EXPECT (product_id IS NOT NULL) ON VIOLATION DROP ROW,\n",
    "    CONSTRAINT valid_price EXPECT (price > 0) ON VIOLATION DROP ROW,\n",
    "    CONSTRAINT valid_category EXPECT (category IS NOT NULL) ON VIOLATION DROP ROW\n",
    ")\n",
    "COMMENT 'Validated product catalog'\n",
    "AS\n",
    "SELECT\n",
    "    product_id,\n",
    "    INITCAP(product_name) AS product_name,\n",
    "    UPPER(category) AS category,\n",
    "    price,\n",
    "    stock_quantity,\n",
    "    CASE \n",
    "        WHEN price >= 1000 THEN 'PREMIUM'\n",
    "        WHEN price >= 100 THEN 'STANDARD'\n",
    "        ELSE 'BUDGET'\n",
    "    END AS price_tier,\n",
    "    _source_file,\n",
    "    _ingested_at\n",
    "FROM STREAM bronze_products;\n",
    "\"\"\"\n",
    "\n",
    "print(\"Silver layer SQL solutions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056022b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gold layer solutions\n",
    "\n",
    "dim_customer_sql = \"\"\"\n",
    "-- Task 4.1: dim_customer\n",
    "CREATE OR REFRESH MATERIALIZED VIEW dim_customer\n",
    "COMMENT 'Customer dimension - current active records only'\n",
    "AS\n",
    "SELECT\n",
    "    customer_id,\n",
    "    first_name,\n",
    "    last_name,\n",
    "    CONCAT(first_name, ' ', last_name) AS full_name,\n",
    "    email,\n",
    "    phone,\n",
    "    city,\n",
    "    country,\n",
    "    registration_date,\n",
    "    customer_segment,\n",
    "    DATEDIFF(current_date(), registration_date) AS days_since_registration\n",
    "FROM silver_customers\n",
    "WHERE __END_AT IS NULL;\n",
    "\"\"\"\n",
    "\n",
    "dim_product_sql = \"\"\"\n",
    "-- Task 4.2: dim_product\n",
    "CREATE OR REFRESH MATERIALIZED VIEW dim_product\n",
    "COMMENT 'Product dimension with price tiers and stock status'\n",
    "AS\n",
    "SELECT\n",
    "    product_id,\n",
    "    product_name,\n",
    "    category,\n",
    "    price,\n",
    "    stock_quantity,\n",
    "    price_tier,\n",
    "    CASE \n",
    "        WHEN stock_quantity = 0 THEN 'OUT_OF_STOCK'\n",
    "        WHEN stock_quantity < 10 THEN 'LOW_STOCK'\n",
    "        ELSE 'IN_STOCK'\n",
    "    END AS stock_status\n",
    "FROM silver_products;\n",
    "\"\"\"\n",
    "\n",
    "dim_date_sql = \"\"\"\n",
    "-- Task 4.3: dim_date\n",
    "CREATE OR REFRESH MATERIALIZED VIEW dim_date\n",
    "COMMENT 'Date dimension with calendar attributes'\n",
    "AS\n",
    "WITH date_range AS (\n",
    "    SELECT EXPLODE(SEQUENCE(\n",
    "        DATE('2020-01-01'), \n",
    "        DATE('2025-12-31'), \n",
    "        INTERVAL 1 DAY\n",
    "    )) AS date_value\n",
    ")\n",
    "SELECT\n",
    "    CAST(DATE_FORMAT(date_value, 'yyyyMMdd') AS INT) AS date_key,\n",
    "    date_value AS full_date,\n",
    "    YEAR(date_value) AS year,\n",
    "    QUARTER(date_value) AS quarter,\n",
    "    MONTH(date_value) AS month,\n",
    "    DATE_FORMAT(date_value, 'MMMM') AS month_name,\n",
    "    WEEKOFYEAR(date_value) AS week_of_year,\n",
    "    DAYOFWEEK(date_value) AS day_of_week,\n",
    "    DATE_FORMAT(date_value, 'EEEE') AS day_name,\n",
    "    CASE WHEN DAYOFWEEK(date_value) IN (1, 7) THEN TRUE ELSE FALSE END AS is_weekend\n",
    "FROM date_range;\n",
    "\"\"\"\n",
    "\n",
    "fact_sales_sql = \"\"\"\n",
    "-- Task 4.4: fact_sales\n",
    "CREATE OR REFRESH STREAMING TABLE fact_sales\n",
    "COMMENT 'Sales fact table - real-time updates'\n",
    "AS\n",
    "SELECT\n",
    "    order_id,\n",
    "    customer_id,\n",
    "    product_id,\n",
    "    CAST(DATE_FORMAT(order_date, 'yyyyMMdd') AS INT) AS order_date_key,\n",
    "    quantity,\n",
    "    unit_price,\n",
    "    total_amount,\n",
    "    gross_amount,\n",
    "    discount_amount,\n",
    "    status,\n",
    "    payment_method,\n",
    "    order_date AS order_timestamp,\n",
    "    _processed_at\n",
    "FROM STREAM silver_orders;\n",
    "\"\"\"\n",
    "\n",
    "print(\"Gold layer SQL solutions loaded!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"All SQL solutions are available in variables:\")\n",
    "print(\"- bronze_customers_sql, bronze_orders_sql, bronze_products_sql\")\n",
    "print(\"- silver_customers_sql, silver_orders_sql, silver_products_sql\")\n",
    "print(\"- dim_customer_sql, dim_product_sql, dim_date_sql, fact_sales_sql\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f247495",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resource Cleanup (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ad7a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: Run only if you want to delete the pipeline and all created tables\n",
    "\n",
    "# To clean up:\n",
    "# 1. Go to Workflows → Lakeflow Pipelines\n",
    "# 2. Find your pipeline\n",
    "# 3. Click Delete (this will remove all tables created by the pipeline)\n",
    "\n",
    "# Or manually drop tables:\n",
    "# spark.sql(f\"DROP SCHEMA IF EXISTS {CATALOG}.lakeflow_workshop CASCADE\")\n",
    "\n",
    "print(\"Resource cleanup instructions above. Uncomment to execute.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
