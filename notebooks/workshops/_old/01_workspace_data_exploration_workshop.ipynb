{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8060371b-0939-4f55-a509-89cd691f2a22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Workshop: Workspace Setup, Data Import & Exploration\n",
    "\n",
    "**Training Objective:** Practical mastery of workspace configuration, data import from various formats, and basic exploration operations.\n",
    "\n",
    "**Topics covered:**\n",
    "- Workspace and cluster configuration\n",
    "- Loading various data formats (CSV, JSON, Parquet)\n",
    "- Basic data exploration\n",
    "- Manual schema construction\n",
    "- Analysis of missing data and unique values\n",
    "\n",
    "**Duration:** 30 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b0c559c-962d-4c80-bed9-adf06ab250e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Context and Requirements\n",
    "\n",
    "- **Training Day**: Day 1 - Fundamentals & Exploration\n",
    "- **Notebook Type**: Workshop\n",
    "- **Technical Requirements**:\n",
    " - Databricks Runtime 13.0+ (recommended: 14.3 LTS)\n",
    " - Unity Catalog enabled\n",
    " - Permissions: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    " - Cluster: Standard with minimum 2 workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df0cb0e5-4641-4bea-b869-58c4944f69cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Workshop Introduction\n",
    "\n",
    "In this workshop you will work with real KION data:\n",
    "- **Customers** (customers.csv) - customer data\n",
    "- **Orders** (orders_batch.json) - orders\n",
    "- **Products** (products.parquet) - products\n",
    "\n",
    "### Tasks to complete:\n",
    "1. Environment and variable configuration\n",
    "2. Data import from CSV, JSON, Parquet\n",
    "3. Manual schema construction\n",
    "4. Data exploration: statistics, missing values, unique values\n",
    "5. Data quality analysis\n",
    "\n",
    "### Success criteria:\n",
    "- All 3 datasets correctly loaded\n",
    "- Schemas defined manually and applied\n",
    "- Complete exploratory analysis conducted\n",
    "- Data quality issues identified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f20c2fc5-c9ff-435f-8204-00027193aa9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Theoretical Introduction\n",
    "\n",
    "**Section Objective:** Understanding the basics of working with data in Databricks Lakehouse\n",
    "\n",
    "**Basic Concepts:**\n",
    "- **Workspace**: Databricks environment containing notebooks, clusters, and data\n",
    "- **Cluster**: Set of virtual machines processing data\n",
    "- **DataFrame**: Distributed collection of data organized in columns\n",
    "- **Schema**: Data structure defining column names and data types\n",
    "- **Data Format**: CSV (text), JSON (semi-structured), Parquet (columnar binary)\n",
    "\n",
    "**Why is this important?**\n",
    "Correct data loading and exploration is the foundation of every ETL pipeline. Understanding schemas, formats, and exploration methods allows early detection of data quality issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da5ac3d6-0897-4c7a-bc46-abd7edfc3d8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Environment Initialization\n",
    "\n",
    "Run the initialization script for per-user catalog and schema isolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0e035ba-e5ef-4d12-8de4-f01acd72ee06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "903908c9-c847-46a1-8fca-49842d81b0bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuration\n",
    "\n",
    "Define workshop-specific variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d29c33a-ff2b-48d2-92b3-d26cb2062f1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# ÅšcieÅ¼ki do plikÃ³w danych (juÅ¼ zdefiniowane w 00_setup)\n",
    "CUSTOMERS_CSV = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "ORDERS_JSON = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "PRODUCTS_PARQUET = f\"{DATASET_BASE_PATH}/products/products.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f838156-7a4a-4907-8d9e-fb5a1a1f99cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Configuration Context**\n",
    "\n",
    "Paths to actual data files have been configured:\n",
    "- **Customers CSV**: customers.csv (~10,000 customer records)\n",
    "- **Orders JSON**: orders_batch.json (~100,000 orders) \n",
    "- **Products Parquet**: products.parquet (product catalog)\n",
    "\n",
    "**Actual data structure:**\n",
    "- **Customers**: `customer_id`, `first_name`, `last_name`, `email`, `phone`, `city`, `state`, `country`, `registration_date`, `customer_segment`\n",
    "- **Orders**: `order_id`, `customer_id`, `product_id`, `store_id`, `order_datetime`, `quantity`, `unit_price`, `discount_percent`, `total_amount`, `payment_method`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2adab892-b60e-4d80-9cab-b762263f5c75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Task 1: CSV Data Import\n",
    "\n",
    "### Objective:\n",
    "Load customer data from CSV file, first with automatic schema detection, then with manually defined schema.\n",
    "\n",
    "### Instructions:\n",
    "1. Load `customers.csv` with `inferSchema=True` option\n",
    "2. Display schema and first 5 records\n",
    "3. Count the number of records\n",
    "4. Define schema manually (StructType)\n",
    "5. Reload using manual schema\n",
    "6. Compare schemas\n",
    "\n",
    "### Expected result:\n",
    "- DataFrame with customer data\n",
    "- Schema containing columns: customer_id (int), first_name (string), last_name (string), email (string), city (string), country (string), registration_date (timestamp)\n",
    "\n",
    "### Hints:\n",
    "- Use `spark.read.format(\"csv\").option(\"header\", \"true\")`\n",
    "- For manual schema use: `StructType`, `StructField`, `IntegerType`, `StringType`, `TimestampType`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68e06abc-0be9-4adc-8a55-60b7bf487f6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load CSV file with customers (automatic schema detection)\n",
    "customers_df = (\n",
    " spark.read\n",
    " .format(\"csv\")\n",
    " .option(\"header\", \"true\")\n",
    " .option(\"inferSchema\", \"true\")\n",
    " .load(CUSTOMERS_CSV)\n",
    ")\n",
    "\n",
    "# Display schema and sample data\n",
    "customers_df.printSchema()\n",
    "display(customers_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9a9b175-7604-4f62-bc61-2fe5e20e7b7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Customer data analysis after loading**\n",
    "\n",
    "Spark automatically detected the schema. Note the actual columns in the CSV file:\n",
    "- **Identification**: `customer_id` (CUST000001...)\n",
    "- **Personal data**: `first_name`, `last_name`, `email`, `phone`\n",
    "- **Location**: `city`, `state`, `country`\n",
    "- **Metadata**: `registration_date`, `customer_segment`\n",
    "\n",
    "**Next step**: Define schema manually for better data type control and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f666671-9e58-4956-ab71-15fa1ae84d9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define schema manually based on actual data\n",
    "# Complete data types for each field\n",
    "customers_schema = StructType([\n",
    " StructField(\"customer_id\", ____, False),\n",
    " StructField(\"first_name\", ____, True),\n",
    " StructField(\"last_name\", StringType(), True),\n",
    " StructField(\"email\", ____, True),\n",
    " StructField(\"phone\", StringType(), True),\n",
    " StructField(\"city\", ____, True),\n",
    " StructField(\"state\", StringType(), True),\n",
    " StructField(\"country\", ____, True),\n",
    " StructField(\"registration_date\", ____, True),\n",
    " StructField(\"customer_segment\", ____, True)\n",
    "])\n",
    "\n",
    "# Reload with manual schema\n",
    "customers_df_manual = (\n",
    " spark.read\n",
    " .format(\"____\")\n",
    " .option(\"header\", \"____\")\n",
    " .schema(____)\n",
    " .load(CUSTOMERS_CSV)\n",
    ")\n",
    "\n",
    "# Display schema\n",
    "print(\"[INFO] Schema with manual definition:\")\n",
    "customers_df_manual.printSchema()\n",
    "\n",
    "# Display sample data\n",
    "display(customers_df_manual.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "999d8555-2c6d-4ebb-8299-61a0536725a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Manual customers schema defined**\n",
    "\n",
    " **Best practice**: Always define schema manually instead of using `inferSchema=True`\n",
    "\n",
    "**Benefits of manual schema:**\n",
    "- **Performance**: No need to scan entire file to determine types\n",
    "- **Predictability**: Control over data types (String vs Integer)\n",
    "- **Safety**: Validation of data conformance to schema\n",
    "- **Documentation**: Schema serves as data structure documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c4bd244-51b5-4f4a-8dc7-16f3eeb4555e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Task 2: JSON Data Import\n",
    "\n",
    "### Objective:\n",
    "Load order data from JSON file and define schema manually.\n",
    "\n",
    "### Instructions:\n",
    "1. Load `orders_batch.json` with `inferSchema=True`\n",
    "2. Examine data structure (schema, types)\n",
    "3. Define schema manually\n",
    "4. Reload with manual schema\n",
    "\n",
    "### Expected result:\n",
    "- DataFrame with orders\n",
    "- Schema: order_id (int), customer_id (int), order_date (timestamp), total_amount (double), status (string)\n",
    "\n",
    "### Hints:\n",
    "- JSON doesn't require `header` option\n",
    "- Use `DoubleType` for monetary amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "412ccc42-659c-44bc-a8fe-2ccd8c314803",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load JSON file with orders (automatic schema detection)\n",
    "orders_df = (\n",
    " spark.read\n",
    " .format(\"json\")\n",
    " .option(\"multiLine\", \"true\")\n",
    " .load(ORDERS_JSON)\n",
    ")\n",
    "\n",
    "display(orders_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63c79ae2-999b-40c7-a851-c491e9d460e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Order data analysis after loading from JSON**\n",
    "\n",
    "Note the actual order data structure:\n",
    "- **Identifiers**: `order_id`, `customer_id`, `product_id`, `store_id`\n",
    "- **Transaction details**: `order_datetime`, `quantity`, `unit_price`, `discount_percent`\n",
    "- **Summary**: `total_amount`, `payment_method`\n",
    "\n",
    "**Data quality issues:**\n",
    "- Some records have `NULL` in `order_id` or `order_datetime`\n",
    "- Future dates in `order_datetime` (2026)\n",
    "- Data consistency check required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ed2904e-9dd6-4ed2-8180-3c95e36708f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define orders schema based on actual data\n",
    "orders_schema = StructType([\n",
    " StructField(\"order_id\", ____, True),\n",
    " StructField(\"customer_id\", ____, True),\n",
    " StructField(\"product_id\", StringType(), True),\n",
    " StructField(\"store_id\", ____, True),\n",
    " StructField(\"order_datetime\", ____, True),\n",
    " StructField(\"quantity\", ____, True),\n",
    " StructField(\"unit_price\", DoubleType(), True),\n",
    " StructField(\"discount_percent\", ____, True),\n",
    " StructField(\"total_amount\", ____, True),\n",
    " StructField(\"payment_method\", ____, True)\n",
    "])\n",
    "\n",
    "# Load with manual schema\n",
    "orders_df_manual = (\n",
    " spark.read\n",
    " .format(\"____\")\n",
    " .option(\"multiLine\", \"____\")\n",
    " .schema(____)\n",
    " .load(ORDERS_JSON)\n",
    ")\n",
    "\n",
    "orders_df_manual.printSchema()\n",
    "display(orders_df_manual.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5ca31f7-ccbc-4d79-9807-8aa8c7c4c572",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Task 3: Parquet Data Import\n",
    "\n",
    "### Objective:\n",
    "Load product data from Parquet file (schema is embedded).\n",
    "\n",
    "### Instructions:\n",
    "1. Load `products.parquet`\n",
    "2. Check schema (Parquet contains embedded schema)\n",
    "3. Display data\n",
    "4. Count records\n",
    "\n",
    "### Expected result:\n",
    "- DataFrame with products\n",
    "- Schema automatically loaded from Parquet file\n",
    "\n",
    "### Hints:\n",
    "- Parquet doesn't require `inferSchema` or manual schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80d4bf49-867d-47e7-a473-3afbb7e6a968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load products.parquet\n",
    "products_df = (\n",
    " spark.read\n",
    " .format(\"____\")\n",
    " .load(PRODUCTS_PARQUET)\n",
    ")\n",
    "\n",
    "# Display schema (Parquet contains embedded schema)\n",
    "products_df.printSchema()\n",
    "\n",
    "# Display data and count records\n",
    "display(products_df.limit(5))\n",
    "product_count = products_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea185a69-0d16-4ded-bb02-a177a994db9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Parquet - format with embedded schema**\n",
    "\n",
    " **Benefits of Parquet format:**\n",
    "- **Embedded schema**: No need to define schema manually\n",
    "- **Columnar compression**: Space savings and faster analytical queries\n",
    "- **Performance**: Best format for Big Data and analytics in lakehouse\n",
    "- **Compatibility**: Universal standard for analytical systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95f537b1-0441-4828-9046-54077bda7711",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Save Data to Delta Lake\n",
    "\n",
    "### Objective:\n",
    "Save loaded DataFrames to Delta Lake tables for further use.\n",
    "\n",
    "### Instructions:\n",
    "1. Save customers to table `bronze.customers_workshop`\n",
    "2. Save orders to table `bronze.orders_workshop`\n",
    "3. Save products to table `bronze.products_workshop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7694b3d9-1a3d-4d1c-b5f2-a894f49d3a80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example: Save customers to Delta Lake\n",
    "# This cell is ready - analyze the code and run it\n",
    "\n",
    "customers_table = f\"{BRONZE_SCHEMA}.customers_workshop\"\n",
    "\n",
    "(\n",
    " customers_df_manual\n",
    " .write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .option(\"overwriteSchema\", \"true\")\n",
    " .saveAsTable(customers_table)\n",
    ")\n",
    "\n",
    "# Check saved table structure\n",
    "spark.sql(f\"DESCRIBE TABLE {customers_table}\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b78ed5c0-1d28-4844-97c3-f552e14119bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Customers saved to Delta Lake**\n",
    "\n",
    " **Delta Lake - ACID format for lakehouse:**\n",
    "- **ACID transactions**: Atomic data operations\n",
    "- **Schema evolution**: Ability to change schema over time\n",
    "- **Time travel**: Access to historical data versions\n",
    "- **Optimize & Vacuum**: Performance optimization\n",
    "\n",
    "Table saved: `{customers_table}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2fbeb31-be1f-40d6-9045-e14d3789d8c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save orders to Delta Lake\n",
    "orders_table = f\"{BRONZE_SCHEMA}.orders_workshop\"\n",
    "\n",
    "(\n",
    " orders_df_manual\n",
    " .write\n",
    " .format(\"____\")\n",
    " .mode(\"____\")\n",
    " .option(\"overwriteSchema\", \"true\")\n",
    " .saveAsTable(____)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "145cbbb1-55c1-4331-a87e-c9211cd1573a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Orders saved to Delta Lake**\n",
    "\n",
    "After completing the blanks above, the orders table will be saved in Delta Lake format. \n",
    "Note the use of parameters:\n",
    "- **format(\"delta\")**: Specifies Delta Lake format\n",
    "- **mode(\"overwrite\")**: Replaces existing data\n",
    "- **overwriteSchema**: Allows table schema changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e887243-2829-4e78-aa88-31d3e179a9c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save products to Delta Lake\n",
    "products_table = f\"{BRONZE_SCHEMA}.products_workshop\"\n",
    "\n",
    "(\n",
    " ____\n",
    " .write\n",
    " .format(\"____\")\n",
    " .mode(\"____\")\n",
    " .option(\"overwriteSchema\", \"true\")\n",
    " .saveAsTable(____)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8c86e5f-3a96-4b47-82ea-66e350f5dd4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Products saved to Delta Lake**\n",
    "\n",
    "Complete the blanks above to save the products table. All three tables (customers, orders, products) will be available in the bronze schema for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f43982a5-ed74-452a-9315-7853905c3c0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Task 4: Data Exploration - Customers\n",
    "\n",
    "### Objective:\n",
    "Conduct detailed customer data exploration.\n",
    "\n",
    "### Instructions:\n",
    "1. Display list of columns and their types\n",
    "2. Count unique customers\n",
    "3. Find customer count by country\n",
    "4. Check for NULL values in columns\n",
    "5. Generate descriptive statistics (`describe()`)\n",
    "\n",
    "### Expected result:\n",
    "- Complete exploratory analysis\n",
    "- Identified data gaps\n",
    "- Geographic distribution of customers\n",
    "\n",
    "### Hints:\n",
    "- Use: `columns`, `dtypes`, `count()`, `distinct()`, `groupBy()`, `describe()`\n",
    "- To check NULL: `.filter(col(\"column_name\").isNull())`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0287bf6e-9cdf-487f-9800-e0269a65d455",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Basic information about data structure\n",
    "customers_df_manual.columns\n",
    "customers_df_manual.dtypes\n",
    "\n",
    "# Count unique customer_id values\n",
    "unique_customers = customers_df_manual.select(\"____\").distinct().count()\n",
    "total_customers = customers_df_manual.count()\n",
    "\n",
    "# Display statistics\n",
    "display(spark.createDataFrame([\n",
    " (total_customers, unique_customers, total_customers - unique_customers)\n",
    "], [\"total_records\", \"unique_customers\", \"duplicates\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "797d95e5-f35f-48cb-809c-d5e110542096",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Basic customers statistics**\n",
    "\n",
    "We checked:\n",
    "- **Columns and types**: Data structure and types of each column\n",
    "- **Duplicates**: Whether `customer_id` is unique (primary key)\n",
    "- **Completeness**: Total record count vs unique identifiers\n",
    "\n",
    "**Next step**: Check geographic distribution of customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9de8a409-133f-4cd7-bfea-ecf1cf82c966",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count customers by country\n",
    "customers_by_country = (\n",
    " customers_df_manual\n",
    " .groupBy(\"____\")\n",
    " .count()\n",
    " .orderBy(\"count\", ascending=____)\n",
    ")\n",
    "\n",
    "display(customers_by_country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27a8c8f7-dda4-4c2a-b3ad-c533e000394e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Geographic distribution of customers**\n",
    "\n",
    "The analysis shows customer distribution by country, sorted descending.\n",
    "This allows identifying:\n",
    "- **Main markets**: Countries with the largest customer base\n",
    "- **Expansion potential**: Countries with few customers\n",
    "- **Geographic concentration**: Does the company have global reach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13256e61-af3e-476e-befa-c377c60138f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check NULL values in each column\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "null_counts = customers_df_manual.select([\n",
    " spark_sum(col(c).____().____(\"int\")).alias(c)\n",
    " for c in customers_df_manual.columns\n",
    "])\n",
    "\n",
    "display(null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca2e8468-9151-4b72-a610-31c1c1bf711b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**NULL value analysis**\n",
    "\n",
    "Checking for missing data in each column:\n",
    "- **0 = Complete data** in column\n",
    "- **>0 = Missing data** requiring handling\n",
    "\n",
    "**Important for data quality**: Missing values in key fields (`customer_id`, `email`) are critical for business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d66afe98-3c5e-4ace-8188-4f3468446e32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate descriptive statistics\n",
    "display(customers_df_manual.____())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6de360ed-a57a-4a54-962b-8601df51d04a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Descriptive statistics for customers**\n",
    "\n",
    "The `describe()` method shows:\n",
    "- **count**: Number of non-null values\n",
    "- **mean/stddev**: Mean and standard deviation (for numeric columns)\n",
    "- **min/max**: Extreme values\n",
    "- For string columns: most frequent values\n",
    "\n",
    "**Usage**: Identifying outliers and general data distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52f4c1d3-d675-4290-aa9f-05c95a3df3b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Task 5: Data Exploration - Orders\n",
    "\n",
    "### Objective:\n",
    "Conduct order analysis.\n",
    "\n",
    "### Instructions:\n",
    "1. Count orders by status\n",
    "2. Calculate total order value\n",
    "3. Find average, min, max order value\n",
    "4. Check for missing data\n",
    "5. Find top 10 most expensive orders\n",
    "\n",
    "### Expected result:\n",
    "- Order business statistics\n",
    "- Data issues identified\n",
    "- Top 10 orders\n",
    "\n",
    "### Hints:\n",
    "- Use `.agg()` with functions: `sum`, `avg`, `min`, `max`\n",
    "- For sorting: `.orderBy(col(\"column\").desc())`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef1f9d64-6c54-428b-b579-e78d0d41b922",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count orders by payment method\n",
    "orders_by_payment = (\n",
    " orders_df_manual\n",
    " .groupBy(\"____\")\n",
    " .____()\n",
    " .orderBy(\"count\", ascending=False)\n",
    ")\n",
    "\n",
    "display(orders_by_payment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb072bcc-1419-47fe-822e-b6dd11b94eb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Payment method distribution**\n",
    "\n",
    "The analysis shows customer payment preferences:\n",
    "- **Cash, Credit Card, Debit Card, PayPal**: Main methods\n",
    "- **Business insights**: Which methods are most popular?\n",
    "- **Planning**: Are additional payment methods needed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "498faee6-df10-4523-a21f-6b0fa8c12e78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate statistics for total_amount\n",
    "from pyspark.sql.functions import sum, avg, min, max, count, round as spark_round\n",
    "\n",
    "orders_stats = orders_df_manual.select(\n",
    " count(\"*\").alias(\"total_orders\"),\n",
    " spark_round(____(\"total_amount\"), 2).alias(\"total_revenue\"),\n",
    " spark_round(____(\"total_amount\"), 2).alias(\"avg_order_value\"),\n",
    " ____(\"total_amount\").alias(\"min_order\"),\n",
    " ____(\"total_amount\").alias(\"max_order\")\n",
    ")\n",
    "\n",
    "display(orders_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d400ab4b-dd36-44cb-bc6e-6a8a28223295",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Key orders business metrics**\n",
    "\n",
    "Calculated statistics show:\n",
    "- **Total Orders**: Total number of orders\n",
    "- **Total Revenue**: Sum of all orders (revenue)\n",
    "- **Average Order Value (AOV)**: Average order value\n",
    "- **Min/Max Order**: Order value range\n",
    "\n",
    "**Business application**: Benchmarking, budget planning, trend analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e26c835-d516-487d-bc0d-cd7a63418fec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find top 10 most expensive orders\n",
    "top_orders = (\n",
    " orders_df_manual\n",
    " .orderBy(col(\"total_amount\").____())\n",
    " .limit(____)\n",
    ")\n",
    "\n",
    "display(top_orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4937168f-9894-451d-a891-e608969331a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Top 10 most expensive orders**\n",
    "\n",
    "Analysis of largest transactions shows:\n",
    "- **VIP customers**: Who generates the highest revenue?\n",
    "- **Premium products**: Which products in the most expensive orders?\n",
    "- **Patterns**: Do high-value orders share common characteristics?\n",
    "\n",
    "**Business application**: Customer segmentation, retention strategies, cross-selling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "becbb193-d702-4f2a-9316-de70f0d358be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Check missing data in orders\n",
    "null_counts_orders = orders_df_manual.select([\n",
    " spark_sum(col(c).isNull().cast(\"int\")).alias(c) \n",
    " for c in orders_df_manual.columns\n",
    "])\n",
    "\n",
    "display(null_counts_orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56e445e5-34b0-43ee-aef2-a5f07769edb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Missing data analysis in orders**\n",
    "\n",
    "Key gaps to check:\n",
    "- **order_id NULL**: Orders without identifier (system issue?)\n",
    "- **order_datetime NULL**: Missing transaction time (affects reporting)\n",
    "- **customer_id NULL**: Cannot associate with customer\n",
    "\n",
    "**Remediation actions**: Delete or fill missing values in subsequent ETL steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20c4faf5-cce1-4715-91e9-c2738fbc69ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Task 6: Data Exploration - Products\n",
    "\n",
    "### Objective:\n",
    "Conduct product analysis.\n",
    "\n",
    "### Instructions:\n",
    "1. Check schema and columns\n",
    "2. Count products by category (if column exists)\n",
    "3. Find price statistics (if price column exists)\n",
    "4. Display top 10 most expensive products\n",
    "\n",
    "### Expected result:\n",
    "- Complete product analysis\n",
    "- Category distribution\n",
    "- Price statistics\n",
    "\n",
    "### Hints:\n",
    "- Check available columns before analysis: `products_df.columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c077cc02-299d-4167-acc8-50c216f56b3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check schema and columns\n",
    "products_df.columns\n",
    "products_df.printSchema()\n",
    "\n",
    "# Display sample data\n",
    "display(products_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "529d3e82-8ddd-419f-b9a1-4728c37d82fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Products structure analysis**\n",
    "\n",
    "Checking the schema helps understand:\n",
    "- **Available columns**: What product information is available?\n",
    "- **Data types**: Parquet automatically preserves proper types\n",
    "- **Sample data**: Content and value format\n",
    "\n",
    "**Next steps**: Check if `category` and `price` columns exist for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b74a65b6-bcc2-4276-980f-5882af60c94c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(products_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9007e77a-d26e-4117-a93a-406058a9fc4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if 'category' column exists, count products by category\n",
    "if \"subcategory_code\" in products_df.columns:\n",
    " products_by_category = (\n",
    " products_df\n",
    " .groupBy(\"____\")\n",
    " .count()\n",
    " .orderBy(\"____\", ascending=False)\n",
    " )\n",
    " \n",
    " display(products_by_category)\n",
    "else:\n",
    " display(spark.createDataFrame([(\"Column 'category' does not exist in data\",)], [\"info\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4a09fe5-407f-450d-baa8-cc75440e46ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Product category distribution**\n",
    "\n",
    "If `subcategory_code` column exists:\n",
    "- **Product portfolio**: What categories are available?\n",
    "- **Concentration**: Which categories dominate the offering?\n",
    "- **Cross-selling opportunities**: Related categories\n",
    "\n",
    "If it doesn't exist - data enrichment with product categorization may be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81d007c8-86ac-4856-9782-a0a584a569be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if 'price' column exists, calculate statistics\n",
    "if \"price\" in products_df.columns:\n",
    " products_stats = products_df.select(\n",
    " ____(\"*\").alias(\"total_products\"),\n",
    " spark_round(avg(\"____\"), 2).alias(\"avg_price\"),\n",
    " min(\"price\").alias(\"min_price\"),\n",
    " ____(\"price\").alias(\"max_price\")\n",
    " )\n",
    " \n",
    " display(products_stats)\n",
    "else:\n",
    " display(spark.createDataFrame([(\"Column 'price' does not exist in data\",)], [\"info\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a8ea951-fff5-4fe7-af9b-908dbe7ad434",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1969457d-04c3-484e-85ac-cacff6ced00e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Product price statistics**\n",
    "\n",
    "If `price` column exists, we analyze:\n",
    "- **Average price**: Overall price level\n",
    "- **Price range**: min/max for portfolio understanding\n",
    "- **Segmentation**: Budget vs premium products\n",
    "\n",
    "**Application**: Pricing strategy, competitive analysis, product segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8528d360-9d75-4a8a-9792-b3de7b0a6bfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What was achieved:\n",
    "- You configured Databricks environment and per-user variables\n",
    "- You loaded data from three different formats (CSV, JSON, Parquet)\n",
    "- You defined schemas manually (best practice)\n",
    "- You conducted detailed exploration of all datasets\n",
    "- You identified data quality issues\n",
    "- You generated complete data quality report\n",
    "- You saved data to Delta Lake tables\n",
    "\n",
    "### Key takeaways:\n",
    "1. **Manual schemas > inferSchema**: Faster, safer, and predictable data types\n",
    "2. **Exploration before transformation**: Always analyze data before processing\n",
    "3. **Quality requires monitoring**: NULL, duplicates, outliers must be detected systematically\n",
    "4. **Parquet is the lakehouse standard**: Embedded schema, best compression and performance\n",
    "\n",
    "### Quick Reference - Most important commands:\n",
    "\n",
    "| Operation | PySpark | Notes |\n",
    "|----------|---------|---------|\n",
    "| Load CSV | `spark.read.format(\"csv\").option(\"header\",\"true\").schema(schema).load(path)` | Always use manual schema |\n",
    "| Load JSON | `spark.read.format(\"json\").schema(schema).load(path)` | `multiLine=true` option for JSON arrays |\n",
    "| Load Parquet | `spark.read.format(\"parquet\").load(path)` | Embedded schema |\n",
    "| Save to Delta | `df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table)` | mode: overwrite/append |\n",
    "| Check NULL | `df.select([sum(col(c).isNull()).alias(c) for c in df.columns])` | For each column |\n",
    "| Find duplicates | `df.count() - df.distinct().count()` | Difference = duplicate count |\n",
    "| Statistics | `df.describe()` | For numeric columns |\n",
    "| Grouping | `df.groupBy(\"col\").count()` | Aggregations |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89c0f178-b535-4e6f-ad82-23a07b163a01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## ðŸ“‹ Solutions\n",
    "\n",
    "Below are the complete solutions for all workshop tasks. Use them to verify your work or if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47caf9ee-331a-4283-aea1-898518d32a28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e1629fc-f3fb-464f-9375-77e3185a3b51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SOLUTIONS - Task 1: CSV Data Import\n",
    "# =============================================================================\n",
    "\n",
    "# Define schema manually based on actual data\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), False),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"phone\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"registration_date\", TimestampType(), True),\n",
    "    StructField(\"customer_segment\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Reload with manual schema\n",
    "customers_df_manual = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(customers_schema)\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# SOLUTIONS - Task 2: JSON Data Import\n",
    "# =============================================================================\n",
    "\n",
    "# Define orders schema based on actual data\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"store_id\", StringType(), True),\n",
    "    StructField(\"order_datetime\", TimestampType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True),\n",
    "    StructField(\"discount_percent\", IntegerType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Load with manual schema\n",
    "orders_df_manual = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .schema(orders_schema)\n",
    "    .load(ORDERS_JSON)\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# SOLUTIONS - Task 3: Parquet Data Import\n",
    "# =============================================================================\n",
    "\n",
    "# Load products.parquet\n",
    "products_df = (\n",
    "    spark.read\n",
    "    .format(\"parquet\")\n",
    "    .load(PRODUCTS_PARQUET)\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# SOLUTIONS - Save Data to Delta Lake\n",
    "# =============================================================================\n",
    "\n",
    "# Save orders to Delta Lake\n",
    "orders_table = f\"{BRONZE_SCHEMA}.orders_workshop\"\n",
    "(\n",
    "    orders_df_manual\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(orders_table)\n",
    ")\n",
    "\n",
    "# Save products to Delta Lake\n",
    "products_table = f\"{BRONZE_SCHEMA}.products_workshop\"\n",
    "(\n",
    "    products_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(products_table)\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# SOLUTIONS - Task 4: Data Exploration - Customers\n",
    "# =============================================================================\n",
    "\n",
    "# Count unique customer_id values\n",
    "unique_customers = customers_df_manual.select(\"customer_id\").distinct().count()\n",
    "total_customers = customers_df_manual.count()\n",
    "\n",
    "# Count customers by country\n",
    "customers_by_country = (\n",
    "    customers_df_manual\n",
    "    .groupBy(\"country\")\n",
    "    .count()\n",
    "    .orderBy(\"count\", ascending=False)\n",
    ")\n",
    "\n",
    "# Check NULL values in each column\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "null_counts = customers_df_manual.select([\n",
    "    spark_sum(col(c).isNull().cast(\"int\")).alias(c)\n",
    "    for c in customers_df_manual.columns\n",
    "])\n",
    "\n",
    "# Generate descriptive statistics\n",
    "customers_df_manual.describe()\n",
    "\n",
    "# =============================================================================\n",
    "# SOLUTIONS - Task 5: Data Exploration - Orders\n",
    "# =============================================================================\n",
    "\n",
    "# Count orders by payment method\n",
    "orders_by_payment = (\n",
    "    orders_df_manual\n",
    "    .groupBy(\"payment_method\")\n",
    "    .count()\n",
    "    .orderBy(\"count\", ascending=False)\n",
    ")\n",
    "\n",
    "# Calculate statistics for total_amount\n",
    "from pyspark.sql.functions import sum, avg, min, max, count, round as spark_round\n",
    "orders_stats = orders_df_manual.select(\n",
    "    count(\"*\").alias(\"total_orders\"),\n",
    "    spark_round(sum(\"total_amount\"), 2).alias(\"total_revenue\"),\n",
    "    spark_round(avg(\"total_amount\"), 2).alias(\"avg_order_value\"),\n",
    "    min(\"total_amount\").alias(\"min_order\"),\n",
    "    max(\"total_amount\").alias(\"max_order\")\n",
    ")\n",
    "\n",
    "# Find top 10 most expensive orders\n",
    "top_orders = (\n",
    "    orders_df_manual\n",
    "    .orderBy(col(\"total_amount\").desc())\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# SOLUTIONS - Task 6: Data Exploration - Products\n",
    "# =============================================================================\n",
    "\n",
    "# Count products by category\n",
    "if \"subcategory_code\" in products_df.columns:\n",
    "    products_by_category = (\n",
    "        products_df\n",
    "        .groupBy(\"subcategory_code\")\n",
    "        .count()\n",
    "        .orderBy(\"count\", ascending=False)\n",
    "    )\n",
    "\n",
    "# Calculate price statistics\n",
    "if \"price\" in products_df.columns:\n",
    "    products_stats = products_df.select(\n",
    "        count(\"*\").alias(\"total_products\"),\n",
    "        spark_round(avg(\"price\"), 2).alias(\"avg_price\"),\n",
    "        min(\"price\").alias(\"min_price\"),\n",
    "        max(\"price\").alias(\"max_price\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9945374c-c1b4-4351-9111-1a61c765baa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Resource Cleanup\n",
    "\n",
    "Optional - delete tables created during the workshop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56bee403-2364-4a3f-9894-a6e551f3a7a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# WARNING: Run only if you want to delete all created tables\n",
    "# These tables may be needed in subsequent workshops!\n",
    "\n",
    "# Uncomment the lines below to delete tables:\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {BRONZE_SCHEMA}.customers_workshop\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {BRONZE_SCHEMA}.orders_workshop\") \n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {BRONZE_SCHEMA}.products_workshop\")\n",
    "# spark.catalog.clearCache()\n",
    "\n",
    "display(spark.createDataFrame([\n",
    " (\"Resource cleanup is commented out\",),\n",
    " (\"Uncomment the code above to delete tables\",),\n",
    " (\"WARNING: Tables may be needed in subsequent workshops!\",)\n",
    "], [\"Info\"]))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_workspace_data_exploration_workshop",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
