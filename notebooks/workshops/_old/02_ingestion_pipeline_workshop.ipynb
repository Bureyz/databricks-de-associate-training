{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "030f8a50",
   "metadata": {},
   "source": [
    "## Context and Requirements\n",
    "\n",
    "- **Training Day**: Day 1 - Data Ingestion\n",
    "- **Notebook Type**: Workshop\n",
    "- **Technical Requirements**:\n",
    "  - Databricks Runtime 13.0+ (recommended: 14.3 LTS)\n",
    "  - Unity Catalog enabled\n",
    "  - Permissions: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Cluster: Standard with minimum 2 workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7f676",
   "metadata": {},
   "source": [
    "## Theoretical Introduction\n",
    "\n",
    "**Section Objective:** Understanding data ingestion methods in Databricks Lakehouse\n",
    "\n",
    "### COPY INTO - Batch Ingestion\n",
    "- **Purpose**: Load data from external files into Delta tables\n",
    "- **Idempotency**: Automatically tracks processed files, preventing duplicates\n",
    "- **Use case**: Scheduled batch jobs, one-time data migrations\n",
    "- **Supported formats**: CSV, JSON, Parquet, Avro, ORC, TEXT\n",
    "\n",
    "### Auto Loader - Streaming Ingestion\n",
    "- **Purpose**: Incrementally process new files as they arrive\n",
    "- **cloudFiles format**: Uses `.format(\"cloudFiles\")` for streaming read\n",
    "- **Schema inference**: Automatically detects and evolves schema\n",
    "- **Use case**: Near real-time processing, continuous data pipelines\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "| Feature | COPY INTO | Auto Loader |\n",
    "|---------|-----------|-------------|\n",
    "| Processing | Batch | Streaming |\n",
    "| File tracking | Built-in | Checkpoint-based |\n",
    "| Schema evolution | Manual | Automatic |\n",
    "| Scalability | Medium | High |\n",
    "| Cost | Per execution | Per file |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3499afe",
   "metadata": {},
   "source": [
    "## Environment Initialization\n",
    "\n",
    "Run the initialization script for per-user catalog and schema isolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469af8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8c8865",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Define workshop-specific variables and paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b831d5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workshop configuration\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Paths to data files (defined in 00_setup)\n",
    "CUSTOMERS_CSV = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "ORDERS_JSON = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "ORDERS_STREAM_PATH = f\"{DATASET_BASE_PATH}/orders/stream/\"\n",
    "PRODUCTS_PARQUET = f\"{DATASET_BASE_PATH}/products/products.parquet\"\n",
    "\n",
    "# Checkpoint path for Auto Loader\n",
    "CHECKPOINT_PATH = f\"{DATASET_BASE_PATH}/_checkpoints/workshop_02\"\n",
    "\n",
    "# Display configuration\n",
    "print(\"=\"*60)\n",
    "print(\"WORKSHOP CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Catalog:          {CATALOG}\")\n",
    "print(f\"Bronze Schema:    {BRONZE_SCHEMA}\")\n",
    "print(f\"Customers CSV:    {CUSTOMERS_CSV}\")\n",
    "print(f\"Orders JSON:      {ORDERS_JSON}\")\n",
    "print(f\"Orders Stream:    {ORDERS_STREAM_PATH}\")\n",
    "print(f\"Products Parquet: {PRODUCTS_PARQUET}\")\n",
    "print(f\"Checkpoint Path:  {CHECKPOINT_PATH}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b10079",
   "metadata": {},
   "source": [
    "**Data Schema Reference:**\n",
    "\n",
    "**Customers (CSV):**\n",
    "- `customer_id` (STRING): CUST000001\n",
    "- `first_name`, `last_name` (STRING): Customer name\n",
    "- `email` (STRING): Email address\n",
    "- `phone` (STRING): Phone number\n",
    "- `city`, `state`, `country` (STRING): Location\n",
    "- `registration_date` (DATE): Registration date\n",
    "- `customer_segment` (STRING): Basic, Premium, etc.\n",
    "\n",
    "**Orders (JSON):**\n",
    "- `order_id` (STRING): ORD00000001\n",
    "- `customer_id`, `product_id`, `store_id` (STRING): Foreign keys\n",
    "- `order_datetime` (TIMESTAMP): Order timestamp\n",
    "- `quantity` (INT), `unit_price`, `discount_percent`, `total_amount` (DOUBLE)\n",
    "- `payment_method` (STRING): Cash, Credit Card, etc.\n",
    "\n",
    "**Products (Parquet):**\n",
    "- `product_id` (STRING): PROD000001\n",
    "- `product_name` (STRING): Product name\n",
    "- `subcategory_code` (STRING): Category code\n",
    "- `brand` (STRING): Brand name\n",
    "- `unit_cost`, `list_price` (DOUBLE): Prices\n",
    "- `weight_kg` (DOUBLE): Weight\n",
    "- `status` (STRING): Active, Inactive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ee223e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: COPY INTO - Batch Ingestion\n",
    "\n",
    "### Task 1.1: CSV File Ingestion (Customers)\n",
    "\n",
    "**Objective:** Load customer data from CSV file into Delta table using COPY INTO.\n",
    "\n",
    "**Instructions:**\n",
    "1. Create target table `bronze_customers_batch` with ALL columns as STRING (bronze layer principle)\n",
    "2. Use `COPY INTO` to load data from `customers.csv`\n",
    "3. Add `_ingestion_timestamp` metadata column\n",
    "4. Verify loaded records\n",
    "\n",
    "**Hints:**\n",
    "- Use `USING DELTA` for table format\n",
    "- Use `FILEFORMAT = CSV` with `FORMAT_OPTIONS ('header' = 'true')`\n",
    "- Use `current_timestamp()` for ingestion timestamp\n",
    "- Do NOT use inferSchema - keep all data as STRING in bronze layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25db1793",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {CATALOG}.{BRONZE_SCHEMA}.bronze_customers_batch (\n",
    "        customer_id STRING,\n",
    "        first_name STRING,\n",
    "        last_name STRING,\n",
    "        email STRING,\n",
    "        phone STRING,\n",
    "        city STRING,\n",
    "        state STRING,\n",
    "        country STRING,\n",
    "        registration_date STRING,\n",
    "        customer_segment STRING,\n",
    "        _ingestion_timestamp TIMESTAMP\n",
    "    )\n",
    "    USING ____\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d9bb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    ____ INTO {CATALOG}.{BRONZE_SCHEMA}.bronze_customers_batch\n",
    "    FROM (\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            first_name,\n",
    "            last_name,\n",
    "            email,\n",
    "            phone,\n",
    "            city,\n",
    "            state,\n",
    "            country,\n",
    "            registration_date,\n",
    "            customer_segment,\n",
    "            ____() as _ingestion_timestamp\n",
    "        FROM '{CUSTOMERS_CSV}'\n",
    "    )\n",
    "    FILEFORMAT = ____\n",
    "    FORMAT_OPTIONS ('header' = '____')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2f5986",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_result = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as total_records \n",
    "    FROM {CATALOG}.{BRONZE_SCHEMA}.bronze_customers_batch\n",
    "\"\"\").collect()[0]['total_records']\n",
    "\n",
    "print(f\"Total customers loaded: {count_result}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT customer_id, first_name, last_name, email, customer_segment, _ingestion_timestamp\n",
    "    FROM {CATALOG}.{BRONZE_SCHEMA}.bronze_customers_batch\n",
    "    LIMIT 5\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20c462c",
   "metadata": {},
   "source": [
    "### Task 1.2: JSON File Ingestion (Orders)\n",
    "\n",
    "**Objective:** Load order data from JSON file into Delta table.\n",
    "\n",
    "**Instructions:**\n",
    "1. Create target table `bronze_orders_batch` with ALL columns as STRING\n",
    "2. Use `COPY INTO` with `FILEFORMAT = JSON`\n",
    "3. Add ingestion timestamp\n",
    "\n",
    "**Hints:**\n",
    "- JSON format does not need `header` option\n",
    "- All columns remain STRING in bronze layer\n",
    "- Type casting happens in silver layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61500b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {CATALOG}.{BRONZE_SCHEMA}.bronze_orders_batch (\n",
    "        order_id STRING,\n",
    "        customer_id STRING,\n",
    "        product_id STRING,\n",
    "        store_id STRING,\n",
    "        order_datetime STRING,\n",
    "        quantity STRING,\n",
    "        unit_price STRING,\n",
    "        discount_percent STRING,\n",
    "        total_amount STRING,\n",
    "        payment_method STRING,\n",
    "        _ingestion_timestamp TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e46bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    COPY INTO {CATALOG}.{BRONZE_SCHEMA}.bronze_orders_batch\n",
    "    FROM (\n",
    "        SELECT \n",
    "            order_id,\n",
    "            customer_id,\n",
    "            product_id,\n",
    "            store_id,\n",
    "            ____,\n",
    "            quantity,\n",
    "            unit_price,\n",
    "            discount_percent,\n",
    "            total_amount,\n",
    "            payment_method,\n",
    "            current_timestamp() as _ingestion_timestamp\n",
    "        FROM '{____}'\n",
    "    )\n",
    "    FILEFORMAT = ____\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aac9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_result = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as total_records \n",
    "    FROM {CATALOG}.{BRONZE_SCHEMA}.bronze_orders_batch\n",
    "\"\"\").collect()[0]['total_records']\n",
    "\n",
    "print(f\"Total orders loaded: {count_result}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT order_id, customer_id, order_datetime, total_amount, payment_method\n",
    "    FROM {CATALOG}.{BRONZE_SCHEMA}.bronze_orders_batch\n",
    "    LIMIT 5\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7961a3",
   "metadata": {},
   "source": [
    "### Task 1.3: Parquet File Ingestion (Products)\n",
    "\n",
    "**Objective:** Load product data from Parquet file with source file metadata.\n",
    "\n",
    "**Instructions:**\n",
    "1. Create target table `bronze_products_batch` with ALL columns as STRING\n",
    "2. Use `COPY INTO` with `FILEFORMAT = PARQUET`\n",
    "3. Add `_source_file` column using `_metadata.file_path`\n",
    "\n",
    "**Hints:**\n",
    "- Use `_metadata.file_path` to capture source file path\n",
    "- Even for Parquet, keep STRING types in bronze for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6bd704",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {CATALOG}.{BRONZE_SCHEMA}.bronze_products_batch (\n",
    "        product_id STRING,\n",
    "        product_name STRING,\n",
    "        subcategory_code STRING,\n",
    "        brand STRING,\n",
    "        unit_cost STRING,\n",
    "        list_price STRING,\n",
    "        weight_kg STRING,\n",
    "        status STRING,\n",
    "        _source_file STRING,\n",
    "        _ingestion_timestamp TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32c6e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    COPY INTO {CATALOG}.{BRONZE_SCHEMA}.bronze_products_batch\n",
    "    FROM (\n",
    "        SELECT \n",
    "            product_id,\n",
    "            product_name,\n",
    "            subcategory_code,\n",
    "            brand,\n",
    "            CAST(unit_cost AS STRING) as unit_cost,\n",
    "            CAST(list_price AS STRING) as list_price,\n",
    "            CAST(weight_kg AS STRING) as weight_kg,\n",
    "            status,\n",
    "            ____.____ as _source_file,\n",
    "            current_timestamp() as _ingestion_timestamp\n",
    "        FROM '{PRODUCTS_PARQUET}'\n",
    "    )\n",
    "    FILEFORMAT = ____\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fdf95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_result = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as total_records \n",
    "    FROM {CATALOG}.{BRONZE_SCHEMA}.bronze_products_batch\n",
    "\"\"\").collect()[0]['total_records']\n",
    "\n",
    "print(f\"Total products loaded: {count_result}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT product_id, product_name, brand, list_price, status, _source_file\n",
    "    FROM {CATALOG}.{BRONZE_SCHEMA}.bronze_products_batch\n",
    "    LIMIT 5\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1843d6fc",
   "metadata": {},
   "source": [
    "### Task 1.4: Testing COPY INTO Idempotency\n",
    "\n",
    "**Objective:** Verify that COPY INTO is idempotent and doesn't duplicate data.\n",
    "\n",
    "**Instructions:**\n",
    "1. Check record count before re-run\n",
    "2. Execute COPY INTO again\n",
    "3. Verify record count unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ccb1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "before_count = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as count \n",
    "    FROM {CATALOG}.{BRONZE_SCHEMA}.bronze_customers_batch\n",
    "\"\"\").collect()[0]['count']\n",
    "\n",
    "print(f\"Records BEFORE re-run: {before_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec9d46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    COPY INTO {CATALOG}.{BRONZE_SCHEMA}.bronze_customers_batch\n",
    "    FROM (\n",
    "        SELECT \n",
    "            customer_id, first_name, last_name, email, phone,\n",
    "            city, state, country, registration_date, customer_segment,\n",
    "            current_timestamp() as _ingestion_timestamp\n",
    "        FROM '{CUSTOMERS_CSV}'\n",
    "    )\n",
    "    FILEFORMAT = CSV\n",
    "    FORMAT_OPTIONS ('header' = 'true')\n",
    "\"\"\")\n",
    "\n",
    "print(\"COPY INTO executed again...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a032b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_count = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as count \n",
    "    FROM {CATALOG}.{BRONZE_SCHEMA}.bronze_customers_batch\n",
    "\"\"\").collect()[0]['count']\n",
    "\n",
    "print(f\"Records AFTER re-run: {after_count}\")\n",
    "print(f\"\\nIdempotency test: {'PASSED - No duplicates!' if before_count == after_count else 'FAILED - Data was duplicated!'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d8480b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Auto Loader - Streaming Ingestion\n",
    "\n",
    "### Task 2.1: Auto Loader for CSV Files\n",
    "\n",
    "**Objective:** Set up streaming ingestion for customer data using Auto Loader.\n",
    "\n",
    "**Instructions:**\n",
    "1. Configure `cloudFiles` format for streaming read\n",
    "2. Set schema location for schema evolution\n",
    "3. Add metadata columns (`_ingestion_timestamp`, `_source_file`)\n",
    "4. Write stream to Delta table\n",
    "\n",
    "**Hints:**\n",
    "- Format: `cloudFiles`\n",
    "- Option `cloudFiles.format`: `csv`\n",
    "- Use `current_timestamp()` and `input_file_name()` for metadata\n",
    "- Output mode: `append`\n",
    "- Set `checkpointLocation` for streaming state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbe560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "customers_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"____\")\n",
    "    .option(\"cloudFiles.format\", \"____\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{CHECKPOINT_PATH}/customers_schema\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78e00f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_enriched = (\n",
    "    customers_stream\n",
    "    .withColumn(\"_ingestion_timestamp\", ____)\n",
    "    .withColumn(\"_source_file\", ____)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a278970",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_customers = (\n",
    "    customers_enriched.writeStream\n",
    "    .format(\"____\")\n",
    "    .outputMode(\"____\")\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_PATH}/customers_stream\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .table(f\"{CATALOG}.{BRONZE_SCHEMA}.bronze_customers_stream\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12bb2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(15)\n",
    "\n",
    "count_result = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as total_records \n",
    "    FROM {CATALOG}.{BRONZE_SCHEMA}.bronze_customers_stream\n",
    "\"\"\").collect()[0]['total_records']\n",
    "\n",
    "print(f\"Records ingested via Auto Loader: {count_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73480f8c",
   "metadata": {},
   "source": [
    "### Task 2.2: Auto Loader for JSON Stream\n",
    "\n",
    "**Objective:** Set up streaming ingestion for order files from stream folder.\n",
    "\n",
    "**Instructions:**\n",
    "1. Configure Auto Loader for JSON format\n",
    "2. Configure rescued data column for invalid records\n",
    "\n",
    "**Hints:**\n",
    "- Option `cloudFiles.format`: `json`\n",
    "- Option `cloudFiles.rescuedDataColumn`: `_rescued_data`\n",
    "- Set `checkpointLocation` to `orders_stream` subfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3b167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"____\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{CHECKPOINT_PATH}/orders_schema\")\n",
    "    .option(\"cloudFiles.rescuedDataColumn\", \"____\")\n",
    "    .load(ORDERS_STREAM_PATH)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f0ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_orders = (\n",
    "    orders_stream.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"____\")\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_PATH}/____\")\n",
    "    .table(f\"{CATALOG}.{BRONZE_SCHEMA}.bronze_orders_stream\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed1fdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(15)\n",
    "\n",
    "count_result = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as total_records \n",
    "    FROM {CATALOG}.{BRONZE_SCHEMA}.bronze_orders_stream\n",
    "\"\"\").collect()[0]['total_records']\n",
    "\n",
    "print(f\"Orders ingested via Auto Loader: {count_result}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT order_id, customer_id, order_datetime, total_amount\n",
    "    FROM {CATALOG}.{BRONZE_SCHEMA}.bronze_orders_stream\n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efed156f",
   "metadata": {},
   "source": [
    "### Task 2.3: Monitoring Streaming Queries\n",
    "\n",
    "**Objective:** Learn to monitor active streaming queries.\n",
    "\n",
    "**Instructions:**\n",
    "1. List all active streams\n",
    "2. Check stream status and progress\n",
    "3. View metrics (processed records, batch duration)\n",
    "\n",
    "**Hints:**\n",
    "- Use `spark.streams.active` to get list of streams\n",
    "- Use `stream.lastProgress` for metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe35d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_streams = spark.streams.____\n",
    "\n",
    "print(f\"Number of active streams: {len(active_streams)}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for stream in active_streams:\n",
    "    print(f\"\\nStream ID: {stream.id}\")\n",
    "    print(f\"Name: {stream.name}\")\n",
    "    print(f\"Is Active: {stream.isActive}\")\n",
    "    print(f\"Status: {stream.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6efd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(active_streams) > 0:\n",
    "    last_progress = active_streams[0].____\n",
    "    \n",
    "    if last_progress:\n",
    "        print(\"Last Progress Metrics:\")\n",
    "        print(f\"  Batch ID: {last_progress.get('batchId', 'N/A')}\")\n",
    "        print(f\"  Num Input Rows: {last_progress.get('numInputRows', 'N/A')}\")\n",
    "        print(f\"  Input Rows/sec: {last_progress.get('inputRowsPerSecond', 'N/A')}\")\n",
    "        print(f\"  Processed Rows/sec: {last_progress.get('processedRowsPerSecond', 'N/A')}\")\n",
    "    else:\n",
    "        print(\"No progress data available yet.\")\n",
    "else:\n",
    "    print(\"No active streams.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5311ca30",
   "metadata": {},
   "source": [
    "### Task 2.4: Stopping Streaming Queries\n",
    "\n",
    "**Objective:** Gracefully stop all active streaming queries.\n",
    "\n",
    "**Instructions:**\n",
    "1. Iterate over active streams\n",
    "2. Stop each stream\n",
    "3. Verify all streams stopped\n",
    "\n",
    "**Hints:**\n",
    "- Use `stream.stop()` to stop a stream\n",
    "- Iterate over `spark.streams.active`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d935938",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stream in spark.streams.active:\n",
    "    print(f\"Stopping stream: {stream.name or stream.id}\")\n",
    "    stream.____()\n",
    "\n",
    "print(\"\\nAll streams stopped!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ed2a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Active streams remaining: {len(spark.streams.active)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a3686b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Comparison and Analysis\n",
    "\n",
    "### Task 3.1: Compare COPY INTO vs Auto Loader\n",
    "\n",
    "**Objective:** Analyze and compare results from both ingestion methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09baa652",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = spark.sql(f\"\"\"\n",
    "    SELECT 'COPY INTO (batch)' as method, COUNT(*) as records \n",
    "    FROM {CATALOG}.{BRONZE_SCHEMA}.bronze_customers_batch\n",
    "    UNION ALL\n",
    "    SELECT 'Auto Loader (stream)' as method, COUNT(*) as records \n",
    "    FROM {CATALOG}.{BRONZE_SCHEMA}.bronze_customers_stream\n",
    "\"\"\")\n",
    "\n",
    "comparison.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5936eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    ____ HISTORY {CATALOG}.{BRONZE_SCHEMA}.bronze_customers_batch\n",
    "\"\"\").select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a8fd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    DESCRIBE HISTORY {CATALOG}.{BRONZE_SCHEMA}.bronze_customers_stream\n",
    "\"\"\").select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2845898",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Workshop Summary\n",
    "\n",
    "**Achieved Objectives:**\n",
    "- Implemented batch ingestion with COPY INTO for CSV, JSON, Parquet\n",
    "- Configured Auto Loader for streaming ingestion\n",
    "- Added metadata columns for lineage tracking\n",
    "- Verified COPY INTO idempotency\n",
    "- Monitored and managed streaming queries\n",
    "\n",
    "**When to Use COPY INTO:**\n",
    "- Scheduled batch processing\n",
    "- Known, stable data structure\n",
    "- Need for explicit control over loading\n",
    "- Built-in idempotency without checkpoints\n",
    "\n",
    "**When to Use Auto Loader:**\n",
    "- Near real-time processing requirements\n",
    "- Schema evolution expected\n",
    "- Continuous monitoring for new files\n",
    "- High scalability needs\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "| Command | Usage |\n",
    "|---------|-------|\n",
    "| `COPY INTO table FROM path` | Batch load from files |\n",
    "| `.format(\"cloudFiles\")` | Auto Loader streaming read |\n",
    "| `.option(\"cloudFiles.format\", \"csv\")` | Specify source format |\n",
    "| `.option(\"checkpointLocation\", path)` | Set checkpoint for streaming |\n",
    "| `spark.streams.active` | List active streams |\n",
    "| `stream.stop()` | Stop a streaming query |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046a28bf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solutions\n",
    "\n",
    "Below are the complete solutions for all workshop tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ae739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {CATALOG}.{BRONZE_SCHEMA}.bronze_customers_batch (\n",
    "        customer_id STRING,\n",
    "        first_name STRING,\n",
    "        last_name STRING,\n",
    "        email STRING,\n",
    "        phone STRING,\n",
    "        city STRING,\n",
    "        state STRING,\n",
    "        country STRING,\n",
    "        registration_date STRING,\n",
    "        customer_segment STRING,\n",
    "        _ingestion_timestamp TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    COPY INTO {CATALOG}.{BRONZE_SCHEMA}.bronze_customers_batch\n",
    "    FROM (\n",
    "        SELECT \n",
    "            customer_id, first_name, last_name, email, phone,\n",
    "            city, state, country, registration_date, customer_segment,\n",
    "            current_timestamp() as _ingestion_timestamp\n",
    "        FROM '{CUSTOMERS_CSV}'\n",
    "    )\n",
    "    FILEFORMAT = CSV\n",
    "    FORMAT_OPTIONS ('header' = 'true')\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {CATALOG}.{BRONZE_SCHEMA}.bronze_orders_batch (\n",
    "        order_id STRING,\n",
    "        customer_id STRING,\n",
    "        product_id STRING,\n",
    "        store_id STRING,\n",
    "        order_datetime STRING,\n",
    "        quantity STRING,\n",
    "        unit_price STRING,\n",
    "        discount_percent STRING,\n",
    "        total_amount STRING,\n",
    "        payment_method STRING,\n",
    "        _ingestion_timestamp TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    COPY INTO {CATALOG}.{BRONZE_SCHEMA}.bronze_orders_batch\n",
    "    FROM (\n",
    "        SELECT \n",
    "            order_id, customer_id, product_id, store_id,\n",
    "            order_datetime, quantity, unit_price, discount_percent, \n",
    "            total_amount, payment_method,\n",
    "            current_timestamp() as _ingestion_timestamp\n",
    "        FROM '{ORDERS_JSON}'\n",
    "    )\n",
    "    FILEFORMAT = JSON\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {CATALOG}.{BRONZE_SCHEMA}.bronze_products_batch (\n",
    "        product_id STRING,\n",
    "        product_name STRING,\n",
    "        subcategory_code STRING,\n",
    "        brand STRING,\n",
    "        unit_cost STRING,\n",
    "        list_price STRING,\n",
    "        weight_kg STRING,\n",
    "        status STRING,\n",
    "        _source_file STRING,\n",
    "        _ingestion_timestamp TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    COPY INTO {CATALOG}.{BRONZE_SCHEMA}.bronze_products_batch\n",
    "    FROM (\n",
    "        SELECT \n",
    "            product_id, product_name, subcategory_code, brand,\n",
    "            CAST(unit_cost AS STRING) as unit_cost,\n",
    "            CAST(list_price AS STRING) as list_price,\n",
    "            CAST(weight_kg AS STRING) as weight_kg,\n",
    "            status,\n",
    "            _metadata.file_path as _source_file,\n",
    "            current_timestamp() as _ingestion_timestamp\n",
    "        FROM '{PRODUCTS_PARQUET}'\n",
    "    )\n",
    "    FILEFORMAT = PARQUET\n",
    "\"\"\")\n",
    "\n",
    "print(\"Part 1 Solutions executed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0519b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "customers_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{CHECKPOINT_PATH}/customers_schema\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")\n",
    "\n",
    "customers_enriched = (\n",
    "    customers_stream\n",
    "    .withColumn(\"_ingestion_timestamp\", current_timestamp())\n",
    "    .withColumn(\"_source_file\", input_file_name())\n",
    ")\n",
    "\n",
    "query_customers = (\n",
    "    customers_enriched.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_PATH}/customers_stream\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .table(f\"{CATALOG}.{BRONZE_SCHEMA}.bronze_customers_stream\")\n",
    ")\n",
    "\n",
    "orders_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{CHECKPOINT_PATH}/orders_schema\")\n",
    "    .option(\"cloudFiles.rescuedDataColumn\", \"_rescued_data\")\n",
    "    .load(ORDERS_STREAM_PATH)\n",
    ")\n",
    "\n",
    "query_orders = (\n",
    "    orders_stream.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_PATH}/orders_stream\")\n",
    "    .table(f\"{CATALOG}.{BRONZE_SCHEMA}.bronze_orders_stream\")\n",
    ")\n",
    "\n",
    "active_streams = spark.streams.active\n",
    "print(f\"Active streams: {len(active_streams)}\")\n",
    "\n",
    "for stream in active_streams:\n",
    "    print(f\"  - {stream.name or stream.id}: {stream.status}\")\n",
    "    if stream.lastProgress:\n",
    "        print(f\"    Last batch: {stream.lastProgress.get('batchId')}\")\n",
    "\n",
    "print(\"\\nPart 2 Solutions executed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159d1f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = spark.sql(f\"\"\"\n",
    "    SELECT 'COPY INTO' as method, COUNT(*) as records \n",
    "    FROM {CATALOG}.{BRONZE_SCHEMA}.bronze_customers_batch\n",
    "    UNION ALL\n",
    "    SELECT 'Auto Loader' as method, COUNT(*) as records \n",
    "    FROM {CATALOG}.{BRONZE_SCHEMA}.bronze_customers_stream\n",
    "\"\"\")\n",
    "comparison.show()\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    DESCRIBE HISTORY {CATALOG}.{BRONZE_SCHEMA}.bronze_customers_batch\n",
    "\"\"\").select(\"version\", \"operation\").show()\n",
    "\n",
    "print(\"Part 3 Solutions executed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1486d64a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resource Cleanup (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a4789a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stream in spark.streams.active:\n",
    "    stream.stop()\n",
    "\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{BRONZE_SCHEMA}.bronze_customers_batch\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{BRONZE_SCHEMA}.bronze_orders_batch\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{BRONZE_SCHEMA}.bronze_products_batch\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{BRONZE_SCHEMA}.bronze_customers_stream\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{BRONZE_SCHEMA}.bronze_orders_stream\")\n",
    "# dbutils.fs.rm(CHECKPOINT_PATH, recurse=True)\n",
    "\n",
    "print(\"Streams stopped. Uncomment DROP statements to delete tables.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
