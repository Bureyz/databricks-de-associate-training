{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be4dc6bc",
   "metadata": {},
   "source": [
    "# Workshop 2: Delta Lake Optimization\n",
    "\n",
    "## The Story\n",
    "\n",
    "We have an IoT ingestion system that receives data from thousands of sensors. The system writes data in very small batches (simulating continuous streaming or frequent small inserts).\n",
    "\n",
    "**The Problem:**\n",
    "This behavior has created the \"Small Files Problem\". The `iot_events_silver` table contains thousands of tiny files, which kills read performance because Spark has to open and close too many files to read a small amount of data.\n",
    "\n",
    "**Your Mission:**\n",
    "1.  **Diagnose**: Prove that we have a \"Small Files Problem\" by inspecting the table metadata and physical files.\n",
    "2.  **Optimize**: Use `OPTIMIZE` to compact small files into larger ones.\n",
    "3.  **Index**: Use `ZORDER` to co-locate data by `device_id` for faster filtering.\n",
    "4.  **Clean**: Use `VACUUM` to remove the old, fragmented files.\n",
    "\n",
    "**Time:** 30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4aaa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../00_setup\n",
    "from pyspark.sql.functions import col, rand, current_timestamp, expr\n",
    "\n",
    "# --- INDEPENDENT SETUP ---\n",
    "# We will create a synthetic table to simulate the \"Small Files Problem\"\n",
    "table_name = f\"{catalog}.{SILVER_SCHEMA}.iot_events_silver\"\n",
    "print(f\"Preparing simulation table: {table_name}...\")\n",
    "\n",
    "# 1. Generate synthetic data (50k rows)\n",
    "# Simulating IoT events: EventID, DeviceID, Temperature, Timestamp\n",
    "df_synthetic = spark.range(50000).select(\n",
    "    col(\"id\").alias(\"event_id\"),\n",
    "    (rand() * 100).cast(\"int\").alias(\"device_id\"),\n",
    "    (rand() * 30 + 20).alias(\"temperature\"),\n",
    "    current_timestamp().alias(\"event_time\"),\n",
    "    expr(\"case when rand() > 0.9 then 'ERROR' else 'OK' end\").alias(\"status\")\n",
    ")\n",
    "\n",
    "# 2. Write with EXTREME fragmentation\n",
    "# We use repartition(10000) to force Spark to write 10000 separate files.\n",
    "# This simulates the effect of 10000 separate small INSERT operations.\n",
    "print(\"Simulating 10000 small inserts (creating 1000 small files)...\")\n",
    "df_synthetic.repartition(10000).write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "print(f\"âœ… Table {table_name} created with ~10000 small files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0118686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = f\"{catalog}.{SILVER_SCHEMA}.iot_events_silver\"\n",
    "print(f\"Working with table: {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ce099b",
   "metadata": {},
   "source": [
    "## Step 1: Problem Diagnosis\n",
    "\n",
    "### Task 1.1: Check table details\n",
    "\n",
    "Use `DESCRIBE DETAIL` to see the number of files and total size.\n",
    "Also, let's look at the **physical files** on the storage to see how messy it is.\n",
    "\n",
    "**Hint:**\n",
    "```python\n",
    "# Metadata\n",
    "display(spark.sql(f\"DESCRIBE DETAIL {table_name}\"))\n",
    "\n",
    "# Physical files (requires getting the location first)\n",
    "# location = ...\n",
    "# display(dbutils.fs.ls(location))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b8a8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get table details\n",
    "# df_detail = spark.sql(f\"DESCRIBE DETAIL {table_name}\")\n",
    "# display(df_detail)\n",
    "\n",
    "# TODO: Get the location path from details and list files\n",
    "# location = df_detail.collect()[0]['location']\n",
    "# print(f\"Table Location: {location}\")\n",
    "# display(dbutils.fs.ls(location))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073c60cd",
   "metadata": {},
   "source": [
    "### Analysis Questions:\n",
    "\n",
    "1.  Look at `numFiles`. Is it high? (Should be ~1000)\n",
    "2.  Look at `sizeInBytes`. Calculate average file size (`size` / `numFiles`).\n",
    "    *   Example: 10 MB / 10000 files = 0.001 MB per file.\n",
    "    *   **Ideal size** for Delta/Parquet is usually 100MB - 1GB.\n",
    "3.  Look at the file list. Do you see many `part-000...` files?\n",
    "\n",
    "This confirms the **Small Files Problem**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c743afc",
   "metadata": {},
   "source": [
    "## Step 2: Time Travel - Change History\n",
    "\n",
    "### Task 2.1: Display table history\n",
    "\n",
    "Delta Lake records every change. We can travel through time!\n",
    "\n",
    "**Hint:**\n",
    "```sql\n",
    "DESCRIBE HISTORY catalog.schema.table_name\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8cf01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display table change history\n",
    "# display(spark.sql(f\"DESCRIBE HISTORY ...\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4d3f35",
   "metadata": {},
   "source": [
    "### Task 2.2: Read an older version of the table\n",
    "\n",
    "You can read data from a specific version or point in time.\n",
    "\n",
    "**Hint:**\n",
    "```python\n",
    "# By version\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", 0).table(\"name\")\n",
    "\n",
    "# By time\n",
    "spark.read.format(\"delta\").option(\"timestampAsOf\", \"2024-01-01\").table(\"name\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d89d869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Read version 0 of the table (first version)\n",
    "# df_v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).table(table_name)\n",
    "# print(f\"Version 0 had {df_v0.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135d0f25",
   "metadata": {},
   "source": [
    "## Step 3: Optimization\n",
    "\n",
    "### 3.1: Small Files Problem\n",
    "\n",
    "Many small files = many I/O operations = slow queries.\n",
    "\n",
    "**Solution:** `OPTIMIZE` combines small files into larger ones (default ~1GB).\n",
    "\n",
    "### Task 3.1: Run OPTIMIZE\n",
    "\n",
    "**Hint:**\n",
    "```sql\n",
    "OPTIMIZE catalog.schema.table_name\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be4b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Optimize the table\n",
    "# display(spark.sql(f\"OPTIMIZE {table_name}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b86cb5",
   "metadata": {},
   "source": [
    "### 3.2: Z-Ordering - Data Colocation\n",
    "\n",
    "Our analysts frequently filter by `device_id`.\n",
    "Currently, data for `device_id = 42` might be scattered across all 1000 files.\n",
    "Z-ORDER will reorganize data so all records for `device_id = 42` are in the same file (or few files).\n",
    "\n",
    "### Task 3.2: Run OPTIMIZE with Z-ORDER\n",
    "\n",
    "**Hint:**\n",
    "```sql\n",
    "OPTIMIZE catalog.schema.table_name ZORDER BY (column_name)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e59bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Optimize with Z-ORDER on device_id\n",
    "# display(spark.sql(f\"OPTIMIZE {table_name} ZORDER BY (device_id)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62575a45",
   "metadata": {},
   "source": [
    "### Task 3.3: Check optimization metrics\n",
    "\n",
    "After OPTIMIZE, check in history:\n",
    "- How many files were added/removed?\n",
    "- How did the file count change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8070b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check history after optimization\n",
    "display(spark.sql(f\"DESCRIBE HISTORY {table_name} LIMIT 5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acf23f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if file count decreased\n",
    "display(spark.sql(f\"DESCRIBE DETAIL {table_name}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a6d224",
   "metadata": {},
   "source": [
    "## Step 4: Vacuum - Cleaning Old Files\n",
    "\n",
    "### Problem: Old Files Take Up Space\n",
    "\n",
    "After OPTIMIZE, old small files still exist on disk (for Time Travel). `VACUUM` removes them.\n",
    "\n",
    "**WARNING:** After VACUUM, you can no longer travel to older versions!\n",
    "\n",
    "### Task 4.1: Run VACUUM (DRY RUN mode)\n",
    "\n",
    "First, check what will be deleted:\n",
    "\n",
    "**Hint:**\n",
    "```sql\n",
    "VACUUM catalog.schema.table_name RETAIN 168 HOURS DRY RUN\n",
    "```\n",
    "(168 hours = 7 days - default safety threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36770109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check what will be deleted (DRY RUN)\n",
    "# display(spark.sql(f\"VACUUM {table_name} RETAIN 168 HOURS DRY RUN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c942d9a4",
   "metadata": {},
   "source": [
    "### Task 4.2: Run VACUUM (for workshop with shorter time)\n",
    "\n",
    "In a development environment, we can force a shorter retention time.\n",
    "\n",
    "**Never do this in production!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89050277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable retention time check (DEV ONLY!)\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "\n",
    "# TODO: Run VACUUM with short retention time\n",
    "# display(spark.sql(f\"VACUUM {table_name} RETAIN 0 HOURS\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bbd046",
   "metadata": {},
   "source": [
    "## Step 5: Optimization Verification\n",
    "\n",
    "### Task 5.1: Compare performance\n",
    "\n",
    "Run a query filtering by city and check if it's faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a68872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query: Filter by a specific device\n",
    "# This should be faster now (Data Skipping)\n",
    "%%timeit -n 1 -r 1\n",
    "spark.table(table_name).filter(\"device_id = 50\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c498736e",
   "metadata": {},
   "source": [
    "##  Liquid Clustering \n",
    "\n",
    "A newer alternative to Z-ORDER - automatically maintains optimal data layout.\n",
    "\n",
    "```sql\n",
    "-- When creating a table\n",
    "CREATE TABLE ... CLUSTER BY (City)\n",
    "\n",
    "-- Or modifying an existing table\n",
    "ALTER TABLE ... CLUSTER BY (City)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb27bb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: Enable Liquid Clustering (optional)\n",
    "# spark.sql(f\"ALTER TABLE {table_name} CLUSTER BY (device_id)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5779931f",
   "metadata": {},
   "source": [
    "# Solution\n",
    "\n",
    "The complete code is below. Try to solve it yourself first!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6aa608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FULL SOLUTION - Workshop 2: Delta Lake Optimization\n",
    "# ============================================================\n",
    "\n",
    "table_name = f\"{catalog}.{SILVER_SCHEMA}.iot_events_silver\"\n",
    "\n",
    "# --- Step 1: Diagnosis ---\n",
    "print(\"TABLE DETAILS:\")\n",
    "df_detail = spark.sql(f\"DESCRIBE DETAIL {table_name}\")\n",
    "display(df_detail)\n",
    "\n",
    "print(\"\\nPHYSICAL FILES:\")\n",
    "location = df_detail.collect()[0]['location']\n",
    "display(dbutils.fs.ls(location))\n",
    "\n",
    "# --- Step 2: History ---\n",
    "print(\"\\nCHANGE HISTORY:\")\n",
    "display(spark.sql(f\"DESCRIBE HISTORY {table_name}\"))\n",
    "\n",
    "# --- Step 3: Optimization ---\n",
    "print(\"\\nOPTIMIZE WITH Z-ORDER:\")\n",
    "display(spark.sql(f\"OPTIMIZE {table_name} ZORDER BY (device_id)\"))\n",
    "\n",
    "# --- Step 4: Vacuum ---\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "print(\"\\nVACUUM:\")\n",
    "display(spark.sql(f\"VACUUM {table_name} RETAIN 0 HOURS\"))\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"\\nAFTER OPTIMIZATION:\")\n",
    "display(spark.sql(f\"DESCRIBE DETAIL {table_name}\"))\n",
    "\n",
    "print(\"\\nOptimization completed!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
