{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3907de28-40e1-4eee-8730-c3c38dc3aa2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Workshop 2: Delta Lake Optimization\n",
    "\n",
    "## The Story\n",
    "\n",
    "We have an IoT ingestion system that receives data from thousands of sensors. The system writes data in very small batches (simulating continuous streaming or frequent small inserts).\n",
    "\n",
    "**The Problem:**\n",
    "This behavior has created the \"Small Files Problem\". The `iot_events_silver` table contains thousands of tiny files, which kills read performance because Spark has to open and close too many files to read a small amount of data.\n",
    "\n",
    "**Your Mission:**\n",
    "1.  **Diagnose**: Prove that we have a \"Small Files Problem\" by inspecting the table metadata and physical files.\n",
    "2.  **Optimize**: Use `OPTIMIZE` to compact small files into larger ones.\n",
    "3.  **Index**: Use `ZORDER` to co-locate data by `device_id` for faster filtering.\n",
    "4.  **Clean**: Use `VACUUM` to remove the old, fragmented files.\n",
    "\n",
    "**Time:** 30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6274d691-1750-402d-baf6-ec666083efd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaac1c64-839d-46c5-ad0d-bc3d7f58ad0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, rand, current_timestamp, expr\n",
    "\n",
    "# --- INDEPENDENT SETUP ---\n",
    "# We will create a synthetic table to simulate the \"Small Files Problem\"\n",
    "table_name = f\"{CATALOG}.{SILVER_SCHEMA}.iot_events_silver\"\n",
    "print(f\"Preparing simulation table: {table_name}...\")\n",
    "\n",
    "# 1. Generate synthetic data (50k rows)\n",
    "# Simulating IoT events: EventID, DeviceID, Temperature, Timestamp\n",
    "df_synthetic = spark.range(50000).select(\n",
    "    col(\"id\").alias(\"event_id\"),\n",
    "    (rand() * 100).cast(\"int\").alias(\"device_id\"),\n",
    "    (rand() * 30 + 20).alias(\"temperature\"),\n",
    "    current_timestamp().alias(\"event_time\"),\n",
    "    expr(\"case when rand() > 0.9 then 'ERROR' else 'OK' end\").alias(\"status\")\n",
    ")\n",
    "\n",
    "# 2. Write with EXTREME fragmentation\n",
    "# We use repartition(10000) to force Spark to write 10000 separate files.\n",
    "# This simulates the effect of 10000 separate small INSERT operations.\n",
    "print(\"Simulating 10000 small inserts (creating 1000 small files)...\")\n",
    "df_synthetic.repartition(10000).write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "print(f\"âœ… Table {table_name} created with ~10000 small files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a88ee170-70f0-4f52-8e65-38aa57c30139",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_name = f\"{CATALOG}.{SILVER_SCHEMA}.iot_events_silver\"\n",
    "print(f\"Working with table: {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9b10b4e-81ae-4dfd-a00e-1d40bb95948e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Problem Diagnosis\n",
    "\n",
    "### Task 1.1: Check table details\n",
    "\n",
    "Use `DESCRIBE DETAIL` to see the number of files and total size.\n",
    "Also, let's look at the **physical files** on the storage to see how messy it is.\n",
    "\n",
    "**Hint:**\n",
    "Use `DESCRIBE DETAIL table_name` to get metadata.\n",
    "To list files, you need the `location` path from the details, then use `dbutils.fs.ls(location)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05c82dfb-5022-4226-9d78-088e8f56e719",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(f\"DESCRIBE DETAIL {table_name}\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbafed81-5625-42aa-b859-7ed04f366a31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Get table details\n",
    "\n",
    "\n",
    "# TODO: Get the location path from details and list files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cdf0286-bddf-4480-92e4-a880a0458517",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Analysis Questions:\n",
    "\n",
    "1.  Look at `numFiles`. Is it high? (Should be ~1000)\n",
    "2.  Look at `sizeInBytes`. Calculate average file size (`size` / `numFiles`).\n",
    "    *   Example: 10 MB / 10000 files = 0.001 MB per file.\n",
    "    *   **Ideal size** for Delta/Parquet is usually 100MB - 1GB.\n",
    "3.  Look at the file list. Do you see many `part-000...` files?\n",
    "\n",
    "This confirms the **Small Files Problem**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cc1e197-e37b-4f84-b410-07a5f0a8a06e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Time Travel - Change History\n",
    "\n",
    "### Task 2.1: Display table history\n",
    "\n",
    "Delta Lake records every change. We can travel through time!\n",
    "\n",
    "**Hint:**\n",
    "```sql\n",
    "DESCRIBE HISTORY catalog.schema.table_name\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fa22d9f-ac18-4ccb-8e40-2de829484343",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Display table change history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbd2d138-60bf-496a-bb8c-40166d6fa90d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 2.2: Read an older version of the table\n",
    "\n",
    "You can read data from a specific version or point in time.\n",
    "\n",
    "**Hint:**\n",
    "```python\n",
    "# By version\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", 0).table(\"name\")\n",
    "\n",
    "# By time\n",
    "spark.read.format(\"delta\").option(\"timestampAsOf\", \"2024-01-01\").table(\"name\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "213603d5-f289-4580-ab3d-f3cff92aee2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Read version 0 of the table (first version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34063c51-e5b0-4472-9b0c-52301f228a51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Change Data Feed (CDF)\n",
    "\n",
    "### Task 3.1: Enable CDF\n",
    "\n",
    "Delta Lake can track row-level changes (INSERT, UPDATE, DELETE). This is called **Change Data Feed**.\n",
    "It allows us to process only changed data instead of full reloads.\n",
    "\n",
    "**Hint:**\n",
    "```sql\n",
    "ALTER TABLE table_name SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67116cde-6608-42cc-a232-c3da3dfd4497",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Enable Change Data Feed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "308e9ce5-ad74-4816-8fb1-b57743547701",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 3.2: Modify Data\n",
    "\n",
    "Let's simulate some data corrections.\n",
    "1.  Fix 'ERROR' status to 'FIXED'.\n",
    "2.  Delete some records.\n",
    "\n",
    "These operations will be recorded in the CDF.\n",
    "\n",
    "**Hint:**\n",
    "```sql\n",
    "UPDATE table_name SET col = 'val' WHERE col = 'old_val'\n",
    "DELETE FROM table_name WHERE col = 'val'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "792c21be-6428-4002-a8b3-d64828ff311c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Update some records\n",
    "\n",
    "\n",
    "# TODO: Delete some records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe910816-1d9d-4ace-bb80-b12aaa11144c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 3.3: Read Changes (CDF)\n",
    "\n",
    "Now we can query the **Change Data Feed** to see exactly what happened.\n",
    "This is critical for ETL pipelines to propagate only changes to downstream tables.\n",
    "\n",
    "**Hint:**\n",
    "```python\n",
    "spark.read.format(\"delta\") \\\n",
    "    .option(\"readChangeFeed\", \"true\") \\\n",
    "    .option(\"startingVersion\", 1) \\\n",
    "    .table(\"name\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1e9b5c8-ba55-4a91-bfb6-a71aaaf23a64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Read changes from the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc12767a-1383-4348-b9a4-7de6b6dc67b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Optimization\n",
    "\n",
    "### 4.1: Small Files Problem\n",
    "\n",
    "Many small files = many I/O operations = slow queries.\n",
    "\n",
    "**Solution:** `OPTIMIZE` combines small files into larger ones (default ~1GB).\n",
    "\n",
    "### Task 4.1: Run OPTIMIZE\n",
    "\n",
    "**Hint:**\n",
    "```sql\n",
    "OPTIMIZE catalog.schema.table_name\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31ceef9e-a00f-4ed5-9c80-7756da5c092c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Optimize the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "158d35eb-f7ee-4a3d-a2f4-a337d76e93c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.2: Z-Ordering - Data Colocation\n",
    "\n",
    "Our analysts frequently filter by `device_id`.\n",
    "Currently, data for `device_id = 42` might be scattered across all 1000 files.\n",
    "\n",
    "**How it works (Data Skipping):**\n",
    "Delta Lake stores min/max statistics for each column in every file.\n",
    "*   **Without Z-Order:** File 1 (IDs 1-100), File 2 (IDs 1-100)... Spark has to read all files.\n",
    "*   **With Z-Order:** File 1 (IDs 1-10), File 2 (IDs 11-20)... Spark can **skip** files that don't contain the requested ID.\n",
    "\n",
    "### Task 4.2: Run OPTIMIZE with Z-ORDER\n",
    "\n",
    "**Hint:**\n",
    "```sql\n",
    "OPTIMIZE catalog.schema.table_name ZORDER BY (column_name)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "836a1018-c32f-4685-bc8f-26d7c19f5424",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Optimize with Z-ORDER on device_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb52e3bb-0703-4b0c-9b70-bb953fbc3fc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 3.3: Check optimization metrics\n",
    "\n",
    "After OPTIMIZE, check in history:\n",
    "- How many files were added/removed?\n",
    "- How did the file count change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1bbcd97-cec8-4aea-b240-2d9f17b5392e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {table_name}_lc AS \n",
    "SELECT * FROM {table_name}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bba8b89a-83f2-4c60-91ab-2d391a126a26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check history after optimization\n",
    "display(spark.sql(f\"DESCRIBE HISTORY {table_name} LIMIT 5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cb29ba0-c601-4c27-b35d-db99440e403a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if file count decreased\n",
    "display(spark.sql(f\"DESCRIBE DETAIL {table_name}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "062f772e-6fdc-46b4-a16b-6d037567a47a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.3: Liquid Clustering \n",
    "\n",
    "A newer alternative to Z-ORDER is **Liquid Clustering**.\n",
    "It automatically maintains optimal data layout without needing to run `OPTIMIZE ZORDER BY` manually every time.\n",
    "\n",
    "**How to use:**\n",
    "```sql\n",
    "-- When creating a table\n",
    "CREATE TABLE ... CLUSTER BY (column)\n",
    "\n",
    "-- Or modifying an existing table\n",
    "ALTER TABLE ... CLUSTER BY (column)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e064e1c-1f8b-4d79-9409-efa9f0e1910a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "268aa99d-30da-4e88-9794-4cdff6c5d365",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Bonus: Enable Liquid Clustering (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6e71c48-6f10-4354-8334-1cb4c020a8b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Vacuum - Cleaning Old Files\n",
    "\n",
    "### Problem: Old Files Take Up Space\n",
    "\n",
    "After OPTIMIZE, old small files still exist on disk (for Time Travel). `VACUUM` removes them.\n",
    "\n",
    "**WARNING:** After VACUUM, you can no longer travel to older versions!\n",
    "\n",
    "### Task 5.1: Run VACUUM (DRY RUN mode)\n",
    "\n",
    "First, check what will be deleted:\n",
    "\n",
    "**Hint:**\n",
    "```sql\n",
    "VACUUM catalog.schema.table_name RETAIN 168 HOURS DRY RUN\n",
    "```\n",
    "(168 hours = 7 days - default safety threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cb226b5-46b0-493f-b7ce-ca79b7c27afa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Check what will be deleted (DRY RUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27f12f1c-edaf-442a-9f7b-66104928da97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 4.2: Run VACUUM (for workshop with shorter time)\n",
    "\n",
    "In a development environment, we can force a shorter retention time.\n",
    "\n",
    "**Never do this in production!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "081015ce-e7bc-48e2-867d-d3a0a6014a89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Disable retention time check (DEV ONLY!)\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "\n",
    "# TODO: Run VACUUM with short retention time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "006059c2-d47e-412c-be1d-91f2fa2d1c94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# OPTIMIZE original table with Z-ORDER on device_id\n",
    "display(spark.sql(f\"OPTIMIZE {table_name} ZORDER BY (device_id)\"))\n",
    "\n",
    "# OPTIMIZE the liquid clustered table\n",
    "spark.sql(f\"ALTER TABLE {table_name}_lc CLUSTER BY (device_id)\")\n",
    "display(spark.sql(f\"OPTIMIZE {table_name}_lc FULL\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9795f0b0-93ef-45ab-9aa3-a192f4e0bb92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Benchmark query to compare performance\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2508de6-8192-4bc1-9489-fa5f8d63f7c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Benchmark Z-ORDER table\n",
    "start = perf_counter()\n",
    "\n",
    "display(spark.sql(f\"\"\"SELECT * FROM {table_name} a\n",
    "                    WHERE a.device_id >= 90 \n",
    "                    order by a.device_id\"\"\"))\n",
    "\n",
    "zorder_time = perf_counter() - start\n",
    "\n",
    "# Benchmark Liquid Clustering table\n",
    "start = perf_counter()\n",
    "\n",
    "display(spark.sql(f\"\"\"SELECT * FROM {table_name}_lc a  \n",
    "                  \n",
    "                  WHERE a.device_id >= 90  \n",
    "                  order by a.device_id\"\"\"))\n",
    "\n",
    "liquid_time = perf_counter() - start\n",
    "\n",
    "print(f\"Z-ORDER query time: {zorder_time:.2f} seconds\")\n",
    "print(f\"Liquid Clustering query time: {liquid_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7390e61-8ba3-4257-876b-7a94d3c57d25",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769090105133}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Benchmark Z-ORDER table\n",
    "start = perf_counter()\n",
    "\n",
    "display(spark.sql(f\"\"\"SELECT * FROM {table_name} a\n",
    "                    LEFT JOIN {table_name} b  ON a.event_id = b.event_id \n",
    "                    WHERE a.device_id >= 90 \n",
    "                    order by a.device_id\"\"\"))\n",
    "\n",
    "zorder_time = perf_counter() - start\n",
    "\n",
    "# Benchmark Liquid Clustering table\n",
    "start = perf_counter()\n",
    "\n",
    "display(spark.sql(f\"\"\"SELECT * FROM {table_name}_lc a  \n",
    "                  LEFT JOIN {table_name}_lc b  ON a.event_id = b.event_id\n",
    "                  WHERE a.device_id >= 90  \n",
    "                  order by a.device_id\"\"\"))\n",
    "\n",
    "liquid_time = perf_counter() - start\n",
    "\n",
    "print(f\"Z-ORDER query time: {zorder_time:.2f} seconds\")\n",
    "print(f\"Liquid Clustering query time: {liquid_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5f1b6f9-e63b-4473-abe4-73fc7f272b9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Solution\n",
    "\n",
    "The complete code is below. Try to solve it yourself first!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20390757-dfa4-4f32-b5c7-75515256652f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "4": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"path\":1243},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765229304079}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 4
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FULL SOLUTION - Workshop 2: Delta Lake Optimization\n",
    "# ============================================================\n",
    "\n",
    "table_name = f\"{CATALOG}.{SILVER_SCHEMA}.iot_events_silver\"\n",
    "\n",
    "# --- Step 1: Diagnosis ---\n",
    "print(\"TABLE DETAILS:\")\n",
    "df_detail = spark.sql(f\"DESCRIBE DETAIL {table_name}\")\n",
    "display(df_detail)\n",
    "\n",
    "print(\"\\nPHYSICAL FILES:\")\n",
    "location = df_detail.collect()[0]['location']\n",
    "print(location)\n",
    "\n",
    "# --- Step 2: History ---\n",
    "print(\"\\nCHANGE HISTORY:\")\n",
    "display(spark.sql(f\"DESCRIBE HISTORY {table_name}\"))\n",
    "\n",
    "# --- Step 3: Change Data Feed ---\n",
    "print(\"\\nENABLING CDF:\")\n",
    "spark.sql(f\"ALTER TABLE {table_name} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")\n",
    "\n",
    "print(\"\\nMODIFYING DATA:\")\n",
    "spark.sql(f\"UPDATE {table_name} SET status = 'FIXED' WHERE status = 'ERROR'\")\n",
    "spark.sql(f\"DELETE FROM {table_name} WHERE device_id = -1\") # Dummy delete\n",
    "\n",
    "print(\"\\nREADING CHANGES:\")\n",
    "display(spark.read.format(\"delta\").option(\"readChangeFeed\", \"true\").option(\"startingVersion\", 1).table(table_name))\n",
    "\n",
    "# --- Step 4: Optimization ---\n",
    "print(\"\\nOPTIMIZE WITH Z-ORDER:\")\n",
    "display(spark.sql(f\"OPTIMIZE {table_name} ZORDER BY (device_id)\"))\n",
    "\n",
    "# --- Step 5: Vacuum ---\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "print(\"\\nVACUUM:\")\n",
    "display(spark.sql(f\"DESCRIBE HISTORY {table_name}\"))\n",
    "\n",
    "display(spark.sql(f\"VACUUM {table_name} RETAIN 0 HOURS\"))\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"\\nAFTER OPTIMIZATION:\")\n",
    "display(spark.sql(f\"DESCRIBE DETAIL {table_name}\"))\n",
    "\n",
    "print(\"\\nOptimization completed!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "W2_delta_optimization",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
