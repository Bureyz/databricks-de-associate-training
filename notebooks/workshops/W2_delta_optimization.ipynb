{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be4dc6bc",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß Step 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4aaa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0118686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table name from Workshop 1 (or we'll create a test one)\n",
    "table_name = f\"{catalog}.{schema}.customers_silver\"\n",
    "print(f\"üìä Working with table: {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ce099b",
   "metadata": {},
   "source": [
    "---\n",
    "## üîç Step 1: Problem Diagnosis\n",
    "\n",
    "### Task 1.1: Check table details\n",
    "\n",
    "Use the `DESCRIBE DETAIL` command to see:\n",
    "- Table size\n",
    "- Number of files\n",
    "- Location\n",
    "\n",
    "**Hint:**\n",
    "```sql\n",
    "DESCRIBE DETAIL catalog.schema.table_name\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b8a8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display table details\n",
    "# display(spark.sql(f\"DESCRIBE DETAIL ...\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073c60cd",
   "metadata": {},
   "source": [
    "### ü§î Analysis Questions:\n",
    "\n",
    "1. How many files does the table have? (`numFiles` column)\n",
    "2. What is the size? (`sizeInBytes` column)\n",
    "3. Is the table partitioned? (`partitionColumns` column)\n",
    "\n",
    "**Red flag:** If you have many small files (e.g., 100+ files of a few KB each), that's a problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c743afc",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚è∞ Step 2: Time Travel - Change History\n",
    "\n",
    "### Task 2.1: Display table history\n",
    "\n",
    "Delta Lake records every change. We can travel through time!\n",
    "\n",
    "**Hint:**\n",
    "```sql\n",
    "DESCRIBE HISTORY catalog.schema.table_name\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8cf01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display table change history\n",
    "# display(spark.sql(f\"DESCRIBE HISTORY ...\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4d3f35",
   "metadata": {},
   "source": [
    "### Task 2.2: Read an older version of the table\n",
    "\n",
    "You can read data from a specific version or point in time.\n",
    "\n",
    "**Hint:**\n",
    "```python\n",
    "# By version\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", 0).table(\"name\")\n",
    "\n",
    "# By time\n",
    "spark.read.format(\"delta\").option(\"timestampAsOf\", \"2024-01-01\").table(\"name\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d89d869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Read version 0 of the table (first version)\n",
    "# df_v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).table(table_name)\n",
    "# print(f\"Version 0 had {df_v0.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135d0f25",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚ö° Step 3: Optimization\n",
    "\n",
    "### 3.1: Small Files Problem\n",
    "\n",
    "Many small files = many I/O operations = slow queries.\n",
    "\n",
    "**Solution:** `OPTIMIZE` combines small files into larger ones (default ~1GB).\n",
    "\n",
    "### Task 3.1: Run OPTIMIZE\n",
    "\n",
    "**Hint:**\n",
    "```sql\n",
    "OPTIMIZE catalog.schema.table_name\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be4b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Optimize the table\n",
    "# display(spark.sql(f\"OPTIMIZE {table_name}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b86cb5",
   "metadata": {},
   "source": [
    "### 3.2: Z-Ordering - Data Colocation\n",
    "\n",
    "The BI team often filters by **city** (`City`). Z-ORDER arranges data so that rows with the same city are physically close together.\n",
    "\n",
    "**Effect:** Spark can skip entire files that don't contain the searched city!\n",
    "\n",
    "### Task 3.2: Run OPTIMIZE with Z-ORDER\n",
    "\n",
    "**Hint:**\n",
    "```sql\n",
    "OPTIMIZE catalog.schema.table_name ZORDER BY (column1, column2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e59bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Optimize with Z-ORDER on City column\n",
    "# display(spark.sql(f\"OPTIMIZE {table_name} ZORDER BY (City)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62575a45",
   "metadata": {},
   "source": [
    "### Task 3.3: Check optimization metrics\n",
    "\n",
    "After OPTIMIZE, check in history:\n",
    "- How many files were added/removed?\n",
    "- How did the file count change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8070b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check history after optimization\n",
    "display(spark.sql(f\"DESCRIBE HISTORY {table_name} LIMIT 5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acf23f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if file count decreased\n",
    "display(spark.sql(f\"DESCRIBE DETAIL {table_name}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a6d224",
   "metadata": {},
   "source": [
    "---\n",
    "## üßπ Step 4: Vacuum - Cleaning Old Files\n",
    "\n",
    "### Problem: Old Files Take Up Space\n",
    "\n",
    "After OPTIMIZE, old small files still exist on disk (for Time Travel). `VACUUM` removes them.\n",
    "\n",
    "‚ö†Ô∏è **WARNING:** After VACUUM, you can no longer travel to older versions!\n",
    "\n",
    "### Task 4.1: Run VACUUM (DRY RUN mode)\n",
    "\n",
    "First, check what will be deleted:\n",
    "\n",
    "**Hint:**\n",
    "```sql\n",
    "VACUUM catalog.schema.table_name RETAIN 168 HOURS DRY RUN\n",
    "```\n",
    "(168 hours = 7 days - default safety threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36770109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check what will be deleted (DRY RUN)\n",
    "# display(spark.sql(f\"VACUUM {table_name} RETAIN 168 HOURS DRY RUN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c942d9a4",
   "metadata": {},
   "source": [
    "### Task 4.2: Run VACUUM (for workshop with shorter time)\n",
    "\n",
    "In a development environment, we can force a shorter retention time.\n",
    "\n",
    "‚ö†Ô∏è **Never do this in production!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89050277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable retention time check (DEV ONLY!)\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "\n",
    "# TODO: Run VACUUM with short retention time\n",
    "# display(spark.sql(f\"VACUUM {table_name} RETAIN 0 HOURS\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bbd046",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Step 5: Optimization Verification\n",
    "\n",
    "### Task 5.1: Compare performance\n",
    "\n",
    "Run a query filtering by city and check if it's faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a68872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query after optimization\n",
    "%%timeit -n 1 -r 1\n",
    "spark.table(table_name).filter(\"City = 'Seattle'\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c498736e",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Bonus: Liquid Clustering (Databricks 13.3+)\n",
    "\n",
    "A newer alternative to Z-ORDER - automatically maintains optimal data layout.\n",
    "\n",
    "```sql\n",
    "-- When creating a table\n",
    "CREATE TABLE ... CLUSTER BY (City)\n",
    "\n",
    "-- Or modifying an existing table\n",
    "ALTER TABLE ... CLUSTER BY (City)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb27bb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: Enable Liquid Clustering (optional)\n",
    "# spark.sql(f\"ALTER TABLE {table_name} CLUSTER BY (City)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5779931f",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# üìã SOLUTION\n",
    "\n",
    "‚ö†Ô∏è **Don't look here until you've tried it yourself!** ‚ö†Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6aa608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üìã FULL SOLUTION - Workshop 2: Delta Lake Optimization\n",
    "# ============================================================\n",
    "\n",
    "table_name = f\"{catalog}.{schema}.customers_silver\"\n",
    "\n",
    "# --- Step 1: Diagnosis ---\n",
    "print(\"üìä TABLE DETAILS:\")\n",
    "display(spark.sql(f\"DESCRIBE DETAIL {table_name}\"))\n",
    "\n",
    "# --- Step 2: History ---\n",
    "print(\"\\n‚è∞ CHANGE HISTORY:\")\n",
    "display(spark.sql(f\"DESCRIBE HISTORY {table_name}\"))\n",
    "\n",
    "# --- Step 3: Optimization ---\n",
    "print(\"\\n‚ö° OPTIMIZE WITH Z-ORDER:\")\n",
    "display(spark.sql(f\"OPTIMIZE {table_name} ZORDER BY (City)\"))\n",
    "\n",
    "# --- Step 4: Vacuum ---\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "print(\"\\nüßπ VACUUM:\")\n",
    "display(spark.sql(f\"VACUUM {table_name} RETAIN 0 HOURS\"))\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"\\n‚úÖ AFTER OPTIMIZATION:\")\n",
    "display(spark.sql(f\"DESCRIBE DETAIL {table_name}\"))\n",
    "\n",
    "print(\"\\n‚úÖ Optimization completed!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
