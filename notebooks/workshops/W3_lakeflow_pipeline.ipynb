{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf439070",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß Krok 0: Import biblioteki DLT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14811ed7",
   "metadata": {},
   "source": [
    "---\n",
    "## üåü Star Schema Model - What We're Building and Why\n",
    "\n",
    "### Source Data (OLTP - Transactional Systems)\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   Customers     ‚îÇ     ‚îÇ SalesOrderHeader ‚îÇ     ‚îÇ    Products     ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§     ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§     ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ CustomerID      ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ CustomerID       ‚îÇ     ‚îÇ ProductID       ‚îÇ\n",
    "‚îÇ FirstName       ‚îÇ     ‚îÇ SalesOrderID     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Name            ‚îÇ\n",
    "‚îÇ LastName        ‚îÇ     ‚îÇ OrderDate        ‚îÇ     ‚îÇ ListPrice       ‚îÇ\n",
    "‚îÇ EmailAddress    ‚îÇ     ‚îÇ TotalDue         ‚îÇ     ‚îÇ ProductCategory ‚îÇ\n",
    "‚îÇ CompanyName     ‚îÇ     ‚îÇ Status           ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "‚îÇ ModifiedDate    ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ\n",
    "                                 ‚îÇ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ\n",
    "‚îÇSalesOrderDetail ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ SalesOrderID    ‚îÇ\n",
    "‚îÇ ProductID       ‚îÇ\n",
    "‚îÇ OrderQty        ‚îÇ\n",
    "‚îÇ UnitPrice       ‚îÇ\n",
    "‚îÇ LineTotal       ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### ‚ùå Problems with OLTP Data:\n",
    "- **Multiple JOINs** needed for simple reports\n",
    "- **Normalization** = data scattered across many tables\n",
    "- **Slow analytical queries**\n",
    "- **No change history** - we don't know what the customer looked like a year ago\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Solution: Star Schema\n",
    "\n",
    "```\n",
    "                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                    ‚îÇ    Dim_Date         ‚îÇ\n",
    "                    ‚îÇ (SCD Type 1)        ‚îÇ\n",
    "                    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "                    ‚îÇ date_key (SK)       ‚îÇ\n",
    "                    ‚îÇ full_date           ‚îÇ\n",
    "                    ‚îÇ year, month, day    ‚îÇ\n",
    "                    ‚îÇ quarter             ‚îÇ\n",
    "                    ‚îÇ day_of_week         ‚îÇ\n",
    "                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                               ‚îÇ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   Dim_Customer      ‚îÇ        ‚îÇ        ‚îÇ    Dim_Product      ‚îÇ\n",
    "‚îÇ   (SCD Type 2)      ‚îÇ        ‚îÇ        ‚îÇ    (SCD Type 1)     ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§        ‚îÇ        ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ customer_key (SK)   ‚îÇ        ‚îÇ        ‚îÇ product_key (SK)    ‚îÇ\n",
    "‚îÇ customer_id (NK)    ‚îÇ        ‚îÇ        ‚îÇ product_id (NK)     ‚îÇ\n",
    "‚îÇ full_name           ‚îÇ        ‚îÇ        ‚îÇ product_name        ‚îÇ\n",
    "‚îÇ email               ‚îÇ        ‚îÇ        ‚îÇ category_name       ‚îÇ\n",
    "‚îÇ company_name        ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ list_price          ‚îÇ\n",
    "‚îÇ is_current          ‚îÇ        ‚îÇ        ‚îÇ color               ‚îÇ\n",
    "‚îÇ valid_from          ‚îÇ        ‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "‚îÇ valid_to            ‚îÇ        ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ\n",
    "                               ‚îÇ\n",
    "              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "              ‚îÇ         FACT_SALES              ‚îÇ\n",
    "              ‚îÇ       (Fact Table)              ‚îÇ\n",
    "              ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "              ‚îÇ sales_key (SK)                  ‚îÇ\n",
    "              ‚îÇ customer_key (FK) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚ñ∫ Dim_Customer\n",
    "              ‚îÇ product_key (FK) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚ñ∫ Dim_Product\n",
    "              ‚îÇ date_key (FK) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚ñ∫ Dim_Date\n",
    "              ‚îÇ                                 ‚îÇ\n",
    "              ‚îÇ order_id (DD)                   ‚îÇ\n",
    "              ‚îÇ quantity                        ‚îÇ\n",
    "              ‚îÇ unit_price                      ‚îÇ\n",
    "              ‚îÇ line_total                      ‚îÇ\n",
    "              ‚îÇ total_due                       ‚îÇ\n",
    "              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "Legend:\n",
    "  SK = Surrogate Key (artificial key)\n",
    "  NK = Natural Key (business key)\n",
    "  FK = Foreign Key\n",
    "  DD = Degenerate Dimension\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Star Schema Benefits:\n",
    "\n",
    "| Aspect | OLTP (before) | Star Schema (after) |\n",
    "|--------|--------------|---------------------|\n",
    "| **Queries** | 5-6 JOINs | 1-2 JOINs |\n",
    "| **Performance** | Slow | Very fast |\n",
    "| **Understandability** | Complex | Intuitive |\n",
    "| **Change history** | None | SCD Type 2 |\n",
    "| **Aggregations** | Expensive | Pre-computed |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf4ad0d",
   "metadata": {},
   "source": [
    "---\n",
    "## üîÑ Slowly Changing Dimensions (SCD) - Theory\n",
    "\n",
    "### What is SCD?\n",
    "\n",
    "**SCD** (Slowly Changing Dimensions) are techniques for managing changes in data warehouse dimensions.\n",
    "\n",
    "Imagine customer **John Smith** changed his last name to **John Johnson**. What do we do?\n",
    "\n",
    "---\n",
    "\n",
    "### üìù SCD Type 1 - Overwrite\n",
    "\n",
    "**Rule:** Simply overwrite the old value with the new one.\n",
    "\n",
    "```\n",
    "BEFORE:                         AFTER:\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ CustomerID ‚îÇ LastName    ‚îÇ    ‚îÇ CustomerID ‚îÇ LastName    ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ 1          ‚îÇ Smith       ‚îÇ => ‚îÇ 1          ‚îÇ Johnson     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "- ‚úÖ Data where history **IS NOT important** (e.g., typo corrections)\n",
    "- ‚úÖ **Static** dimensions (e.g., product categories)\n",
    "- ‚úÖ When you want to **save space**\n",
    "\n",
    "**Examples in Adventure Works:**\n",
    "- `Dim_Product` - product price, name\n",
    "- `Dim_Date` - calendar doesn't change\n",
    "\n",
    "---\n",
    "\n",
    "### üìú SCD Type 2 - History Tracking\n",
    "\n",
    "**Rule:** Add a new row, keep the old one with validity dates.\n",
    "\n",
    "```\n",
    "BEFORE:\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ customer_key‚îÇ CustomerID ‚îÇ LastName    ‚îÇ is_current ‚îÇ valid_from ‚îÇ valid_to   ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ 1           ‚îÇ 1          ‚îÇ Smith       ‚îÇ true       ‚îÇ 2023-01-01 ‚îÇ 9999-12-31 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "AFTER CHANGE (2024-06-15):\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ customer_key‚îÇ CustomerID ‚îÇ LastName    ‚îÇ is_current ‚îÇ valid_from ‚îÇ valid_to   ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ 1           ‚îÇ 1          ‚îÇ Smith       ‚îÇ false      ‚îÇ 2023-01-01 ‚îÇ 2024-06-14 ‚îÇ\n",
    "‚îÇ 2           ‚îÇ 1          ‚îÇ Johnson     ‚îÇ true       ‚îÇ 2024-06-15 ‚îÇ 9999-12-31 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "- ‚úÖ Data where history **IS important** for analysis\n",
    "- ‚úÖ **Dynamic** dimensions (e.g., customer data, addresses)\n",
    "- ‚úÖ When you need **historical analysis** (\"what was the customer like a year ago?\")\n",
    "\n",
    "**Examples in Adventure Works:**\n",
    "- `Dim_Customer` - company name, email may change\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öñÔ∏è SCD Type 1 vs Type 2 Comparison\n",
    "\n",
    "| Aspect | SCD Type 1 | SCD Type 2 |\n",
    "|--------|------------|------------|\n",
    "| **History** | ‚ùå Lost | ‚úÖ Preserved |\n",
    "| **Complexity** | Simple | More complex |\n",
    "| **Data size** | Small | Grows over time |\n",
    "| **Queries** | Simple | Require `is_current = true` |\n",
    "| **DW usage** | Static dimensions | Dynamic dimensions |\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Implementation with Delta Lake MERGE\n",
    "\n",
    "Delta Lake offers powerful `MERGE` command for SCD implementation:\n",
    "\n",
    "```sql\n",
    "-- SCD Type 1 (overwrite)\n",
    "MERGE INTO dim_product AS target\n",
    "USING source_product AS source\n",
    "ON target.product_id = source.product_id\n",
    "WHEN MATCHED THEN UPDATE SET *\n",
    "WHEN NOT MATCHED THEN INSERT *\n",
    "\n",
    "-- SCD Type 2 (history) - requires 2 steps\n",
    "-- 1. Close old records\n",
    "-- 2. Insert new records\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfe76c3",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# üõ†Ô∏è HANDS-ON SECTION\n",
    "\n",
    "## ‚öôÔ∏è Step 0: Environment Setup\n",
    "\n",
    "**NOTE:** This workshop is **self-contained** - it doesn't require completing previous workshops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27db8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the setup notebook first to set variables!\n",
    "# %run ../00_setup\n",
    "\n",
    "# Configuration (will be overwritten by 00_setup)\n",
    "catalog = spark.conf.get(\"spark.databricks.unityCatalog.catalog\", \"training\")\n",
    "schema = spark.conf.get(\"spark.databricks.unityCatalog.schema\", \"workshop\")\n",
    "volume_path = f\"/Volumes/{catalog}/{schema}/data\"\n",
    "\n",
    "# Table prefix for this workshop (isolation from others)\n",
    "TABLE_PREFIX = \"w3_\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üè≠ WORKSHOP 3: LAKEFLOW PIPELINE - STAR SCHEMA\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÇ Catalog: {catalog}\")\n",
    "print(f\"üìÇ Schema: {schema}\")\n",
    "print(f\"üìÇ Volume Path: {volume_path}\")\n",
    "print(f\"üìÇ Table Prefix: {TABLE_PREFIX}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934fbd77",
   "metadata": {},
   "source": [
    "---\n",
    "## ü•â Step 1: Bronze Layer - Load Raw Data\n",
    "\n",
    "The first step in the Medallion Architecture is loading **raw data** into the Bronze layer.\n",
    "Data is saved **without transformations** - exactly as it came from the source.\n",
    "\n",
    "### Source Files:\n",
    "| File | Description | Records |\n",
    "|------|-------------|---------|\n",
    "| `Customers.csv` | Customer data | ~10,000 |\n",
    "| `Product.csv` | Products | ~500 |\n",
    "| `ProductCategory.csv` | Product categories | ~42 |\n",
    "| `SalesOrderHeader.csv` | Order headers | ~50,000 |\n",
    "| `SalesOrderDetail.csv` | Order line items | ~129,000 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a159caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ü•â BRONZE LAYER - Load Raw Data\n",
    "# ============================================================\n",
    "\n",
    "# List of source files\n",
    "source_files = [\n",
    "    \"Customers.csv\",\n",
    "    \"Product.csv\",\n",
    "    \"ProductCategory.csv\",\n",
    "    \"SalesOrderHeader.csv\",\n",
    "    \"SalesOrderDetail.csv\"\n",
    "]\n",
    "\n",
    "# Load all CSV files into Bronze Delta tables\n",
    "for file_name in source_files:\n",
    "    table_name = f\"{TABLE_PREFIX}bronze_{file_name.replace('.csv', '').lower()}\"\n",
    "    file_path = f\"{volume_path}/{file_name}\"\n",
    "    \n",
    "    print(f\"üì• Loading: {file_name} -> {table_name}\")\n",
    "    \n",
    "    # Read CSV\n",
    "    df = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(file_path)\n",
    "    \n",
    "    # Save as Delta table (Bronze)\n",
    "    df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(f\"{catalog}.{schema}.{table_name}\")\n",
    "    \n",
    "    print(f\"   ‚úÖ Saved {df.count()} records\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ BRONZE LAYER COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d19488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Bronze Layer\n",
    "print(\"üìä BRONZE LAYER SUMMARY:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for file_name in source_files:\n",
    "    table_name = f\"{TABLE_PREFIX}bronze_{file_name.replace('.csv', '').lower()}\"\n",
    "    count = spark.table(f\"{catalog}.{schema}.{table_name}\").count()\n",
    "    print(f\"  {table_name}: {count:,} records\")\n",
    "\n",
    "# Preview sample data\n",
    "print(\"\\nüìã Sample Data - Bronze Customers:\")\n",
    "display(spark.table(f\"{catalog}.{schema}.{TABLE_PREFIX}bronze_customers\").limit(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7878155",
   "metadata": {},
   "source": [
    "---\n",
    "## ü•à Step 2: Silver Layer - Clean Data\n",
    "\n",
    "The Silver layer contains **cleaned and normalized** data:\n",
    "- ‚úÖ Correct data types\n",
    "- ‚úÖ Removed duplicates\n",
    "- ‚úÖ Filled missing values\n",
    "- ‚úÖ Data quality validation\n",
    "\n",
    "### Task 2.1: Create Silver Customers\n",
    "\n",
    "**Requirements:**\n",
    "1. Select only needed columns\n",
    "2. Create `full_name` column (FirstName + LastName)\n",
    "3. Remove duplicates by `CustomerID`\n",
    "4. Filter records without `EmailAddress`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e461b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ü•à SILVER LAYER - Cleaned Data\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# --- SILVER CUSTOMERS ---\n",
    "print(\"üîß Creating: Silver Customers\")\n",
    "\n",
    "silver_customers = spark.table(f\"{catalog}.{schema}.{TABLE_PREFIX}bronze_customers\") \\\n",
    "    .select(\n",
    "        col(\"CustomerID\").cast(\"int\"),\n",
    "        \"FirstName\",\n",
    "        \"LastName\", \n",
    "        \"EmailAddress\",\n",
    "        \"Phone\",\n",
    "        \"CompanyName\",\n",
    "        col(\"ModifiedDate\").cast(\"timestamp\")\n",
    "    ) \\\n",
    "    .withColumn(\"full_name\", concat_ws(\" \", col(\"FirstName\"), col(\"LastName\"))) \\\n",
    "    .filter(col(\"EmailAddress\").isNotNull()) \\\n",
    "    .dropDuplicates([\"CustomerID\"])\n",
    "\n",
    "silver_customers.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(f\"{catalog}.{schema}.{TABLE_PREFIX}silver_customers\")\n",
    "\n",
    "print(f\"   ‚úÖ Silver Customers: {silver_customers.count()} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71de850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SILVER PRODUCTS (with category) ---\n",
    "print(\"üîß Creating: Silver Products\")\n",
    "\n",
    "bronze_products = spark.table(f\"{catalog}.{schema}.{TABLE_PREFIX}bronze_product\")\n",
    "bronze_categories = spark.table(f\"{catalog}.{schema}.{TABLE_PREFIX}bronze_productcategory\")\n",
    "\n",
    "silver_products = bronze_products \\\n",
    "    .join(bronze_categories, bronze_products.ProductCategoryID == bronze_categories.ProductCategoryID, \"left\") \\\n",
    "    .select(\n",
    "        bronze_products.ProductID.cast(\"int\"),\n",
    "        bronze_products.Name.alias(\"product_name\"),\n",
    "        bronze_products.ProductNumber,\n",
    "        bronze_products.Color,\n",
    "        bronze_products.ListPrice.cast(\"decimal(18,2)\"),\n",
    "        bronze_products.StandardCost.cast(\"decimal(18,2)\"),\n",
    "        bronze_categories.Name.alias(\"category_name\"),\n",
    "        bronze_products.ModifiedDate.cast(\"timestamp\")\n",
    "    ) \\\n",
    "    .filter(col(\"ProductID\").isNotNull()) \\\n",
    "    .dropDuplicates([\"ProductID\"])\n",
    "\n",
    "silver_products.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(f\"{catalog}.{schema}.{TABLE_PREFIX}silver_products\")\n",
    "\n",
    "print(f\"   ‚úÖ Silver Products: {silver_products.count()} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7ee166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SILVER ORDERS (Header + Detail) ---\n",
    "print(\"üîß Creating: Silver Orders\")\n",
    "\n",
    "bronze_orders = spark.table(f\"{catalog}.{schema}.{TABLE_PREFIX}bronze_salesorderheader\")\n",
    "bronze_details = spark.table(f\"{catalog}.{schema}.{TABLE_PREFIX}bronze_salesorderdetail\")\n",
    "\n",
    "# Silver Order Header\n",
    "silver_order_header = bronze_orders \\\n",
    "    .select(\n",
    "        col(\"SalesOrderID\").cast(\"int\"),\n",
    "        col(\"CustomerID\").cast(\"int\"),\n",
    "        col(\"ShipToAddressID\").cast(\"int\"),\n",
    "        col(\"OrderDate\").cast(\"date\"),\n",
    "        col(\"ShipDate\").cast(\"date\"),\n",
    "        col(\"TotalDue\").cast(\"decimal(18,2)\"),\n",
    "        col(\"Status\").cast(\"int\"),\n",
    "        \"ShipMethod\"\n",
    "    ) \\\n",
    "    .filter(col(\"SalesOrderID\").isNotNull())\n",
    "\n",
    "silver_order_header.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(f\"{catalog}.{schema}.{TABLE_PREFIX}silver_order_header\")\n",
    "\n",
    "# Silver Order Detail\n",
    "silver_order_detail = bronze_details \\\n",
    "    .select(\n",
    "        col(\"SalesOrderID\").cast(\"int\"),\n",
    "        col(\"SalesOrderDetailID\").cast(\"int\"),\n",
    "        col(\"ProductID\").cast(\"int\"),\n",
    "        col(\"OrderQty\").cast(\"int\"),\n",
    "        col(\"UnitPrice\").cast(\"decimal(18,2)\"),\n",
    "        col(\"UnitPriceDiscount\").cast(\"decimal(5,2)\"),\n",
    "        col(\"LineTotal\").cast(\"decimal(18,2)\")\n",
    "    ) \\\n",
    "    .filter(col(\"SalesOrderDetailID\").isNotNull())\n",
    "\n",
    "silver_order_detail.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(f\"{catalog}.{schema}.{TABLE_PREFIX}silver_order_detail\")\n",
    "\n",
    "print(f\"   ‚úÖ Silver Order Header: {silver_order_header.count()} records\")\n",
    "print(f\"   ‚úÖ Silver Order Detail: {silver_order_detail.count()} records\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ SILVER LAYER COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb1d95a",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## ü•á Step 3: Gold Layer - Dimensions with SCD Type 1\n",
    "\n",
    "### Dim_Product (SCD Type 1)\n",
    "\n",
    "The product dimension uses **SCD Type 1** - when the price or name changes, we simply **overwrite** the old value.\n",
    "\n",
    "**Why SCD Type 1?**\n",
    "- Products rarely change\n",
    "- We don't need product price history in the dimension (can track separately)\n",
    "- Simpler model = faster queries\n",
    "\n",
    "### Task 3.1: Create Dim_Product\n",
    "\n",
    "**PySpark Version:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e0e162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ü•á GOLD LAYER - Dim_Product (SCD Type 1) - PySpark\n",
    "# ============================================================\n",
    "\n",
    "print(\"üåü Creating: Dim_Product (SCD Type 1)\")\n",
    "\n",
    "# Load source data from Silver\n",
    "silver_products = spark.table(f\"{catalog}.{schema}.{TABLE_PREFIX}silver_products\")\n",
    "\n",
    "# Create dimension with Surrogate Key\n",
    "dim_product = silver_products \\\n",
    "    .withColumn(\"product_key\", monotonically_increasing_id() + 1) \\\n",
    "    .select(\n",
    "        col(\"product_key\"),\n",
    "        col(\"ProductID\").alias(\"product_id\"),  # Natural Key\n",
    "        col(\"product_name\"),\n",
    "        col(\"ProductNumber\").alias(\"product_number\"),\n",
    "        col(\"Color\").alias(\"color\"),\n",
    "        col(\"category_name\"),\n",
    "        col(\"ListPrice\").alias(\"list_price\"),\n",
    "        col(\"StandardCost\").alias(\"standard_cost\"),\n",
    "        current_timestamp().alias(\"load_timestamp\")\n",
    "    )\n",
    "\n",
    "# Save as Delta table\n",
    "dim_product.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(f\"{catalog}.{schema}.{TABLE_PREFIX}dim_product\")\n",
    "\n",
    "print(f\"   ‚úÖ Dim_Product: {dim_product.count()} records\")\n",
    "display(dim_product.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe7efdb",
   "metadata": {},
   "source": [
    "**SQL Version (equivalent):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37b3a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ü•á GOLD LAYER - Dim_Product (SCD Type 1) - SQL Version\n",
    "# ============================================================\n",
    "\n",
    "# Same logic in SQL\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {catalog}.{schema}.{TABLE_PREFIX}dim_product_sql AS\n",
    "SELECT \n",
    "    ROW_NUMBER() OVER (ORDER BY ProductID) AS product_key,\n",
    "    ProductID AS product_id,\n",
    "    product_name,\n",
    "    ProductNumber AS product_number,\n",
    "    Color AS color,\n",
    "    category_name,\n",
    "    ListPrice AS list_price,\n",
    "    StandardCost AS standard_cost,\n",
    "    current_timestamp() AS load_timestamp\n",
    "FROM {catalog}.{schema}.{TABLE_PREFIX}silver_products\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Dim_Product (SQL) created!\")\n",
    "\n",
    "# SCD Type 1 Update Pattern - when new data arrives\n",
    "print(\"\"\"\n",
    "üìù SCD Type 1 - Update Pattern (MERGE):\n",
    "\n",
    "MERGE INTO dim_product AS target\n",
    "USING silver_products AS source\n",
    "ON target.product_id = source.ProductID\n",
    "WHEN MATCHED AND (\n",
    "    target.product_name != source.product_name OR\n",
    "    target.list_price != source.ListPrice\n",
    ") THEN UPDATE SET\n",
    "    target.product_name = source.product_name,\n",
    "    target.list_price = source.ListPrice,\n",
    "    target.load_timestamp = current_timestamp()\n",
    "WHEN NOT MATCHED THEN INSERT (\n",
    "    product_key, product_id, product_name, product_number, \n",
    "    color, category_name, list_price, standard_cost, load_timestamp\n",
    ") VALUES (\n",
    "    (SELECT COALESCE(MAX(product_key), 0) + 1 FROM dim_product),\n",
    "    source.ProductID, source.product_name, source.ProductNumber,\n",
    "    source.Color, source.category_name, source.ListPrice, \n",
    "    source.StandardCost, current_timestamp()\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73db86a7",
   "metadata": {},
   "source": [
    "---\n",
    "### Dim_Date (SCD Type 1)\n",
    "\n",
    "The date dimension is a **static dimension** - the calendar doesn't change.\n",
    "We generate it once and use it repeatedly.\n",
    "\n",
    "### Task 3.2: Create Dim_Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f4487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ü•á GOLD LAYER - Dim_Date (SCD Type 1) - PySpark\n",
    "# ============================================================\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"üìÖ Creating: Dim_Date\")\n",
    "\n",
    "# Generate date range (2022-2026)\n",
    "start_date = datetime(2022, 1, 1)\n",
    "end_date = datetime(2026, 12, 31)\n",
    "date_range = [(start_date + timedelta(days=x)).strftime('%Y-%m-%d') \n",
    "              for x in range((end_date - start_date).days + 1)]\n",
    "\n",
    "# Create DataFrame with dates\n",
    "dim_date = spark.createDataFrame([(d,) for d in date_range], [\"full_date\"]) \\\n",
    "    .withColumn(\"full_date\", col(\"full_date\").cast(\"date\")) \\\n",
    "    .withColumn(\"date_key\", date_format(col(\"full_date\"), \"yyyyMMdd\").cast(\"int\")) \\\n",
    "    .withColumn(\"year\", year(col(\"full_date\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"full_date\"))) \\\n",
    "    .withColumn(\"day\", dayofmonth(col(\"full_date\"))) \\\n",
    "    .withColumn(\"quarter\", quarter(col(\"full_date\"))) \\\n",
    "    .withColumn(\"day_of_week\", dayofweek(col(\"full_date\"))) \\\n",
    "    .withColumn(\"day_name\", date_format(col(\"full_date\"), \"EEEE\")) \\\n",
    "    .withColumn(\"month_name\", date_format(col(\"full_date\"), \"MMMM\")) \\\n",
    "    .withColumn(\"is_weekend\", when(dayofweek(col(\"full_date\")).isin([1, 7]), True).otherwise(False)) \\\n",
    "    .select(\n",
    "        \"date_key\",\n",
    "        \"full_date\",\n",
    "        \"year\",\n",
    "        \"month\",\n",
    "        \"month_name\",\n",
    "        \"day\",\n",
    "        \"quarter\",\n",
    "        \"day_of_week\",\n",
    "        \"day_name\",\n",
    "        \"is_weekend\"\n",
    "    )\n",
    "\n",
    "# Save as Delta table\n",
    "dim_date.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(f\"{catalog}.{schema}.{TABLE_PREFIX}dim_date\")\n",
    "\n",
    "print(f\"   ‚úÖ Dim_Date: {dim_date.count()} records (5 years)\")\n",
    "display(dim_date.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf8eb02",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## ü•á Step 4: Gold Layer - Dimensions with SCD Type 2\n",
    "\n",
    "### Dim_Customer (SCD Type 2)\n",
    "\n",
    "The customer dimension uses **SCD Type 2** - when a customer changes data (e.g., company, email), we **preserve history**.\n",
    "\n",
    "**Why SCD Type 2?**\n",
    "- We want to know which company the customer belonged to **at the time of the order**\n",
    "- Historical analysis requires past state of data\n",
    "- Audit and compliance require tracking changes\n",
    "\n",
    "### SCD Type 2 Structure:\n",
    "| Column | Description |\n",
    "|--------|-------------|\n",
    "| `customer_key` | Surrogate Key (unique for each version) |\n",
    "| `customer_id` | Natural Key (business customer ID) |\n",
    "| `is_current` | Is this the current version? |\n",
    "| `valid_from` | Validity start date |\n",
    "| `valid_to` | Validity end date (9999-12-31 = current) |\n",
    "\n",
    "### Task 4.1: Create Dim_Customer with SCD Type 2\n",
    "\n",
    "**PySpark Version:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d0cb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ü•á GOLD LAYER - Dim_Customer (SCD Type 2) - PySpark\n",
    "# ============================================================\n",
    "\n",
    "print(\"üë§ Creating: Dim_Customer (SCD Type 2)\")\n",
    "\n",
    "# Load source data\n",
    "silver_customers = spark.table(f\"{catalog}.{schema}.{TABLE_PREFIX}silver_customers\")\n",
    "\n",
    "# Create dimension with SCD Type 2 fields\n",
    "dim_customer = silver_customers \\\n",
    "    .withColumn(\"customer_key\", monotonically_increasing_id() + 1) \\\n",
    "    .withColumn(\"is_current\", lit(True)) \\\n",
    "    .withColumn(\"valid_from\", current_date()) \\\n",
    "    .withColumn(\"valid_to\", lit(\"9999-12-31\").cast(\"date\")) \\\n",
    "    .select(\n",
    "        col(\"customer_key\"),\n",
    "        col(\"CustomerID\").alias(\"customer_id\"),  # Natural Key\n",
    "        col(\"full_name\"),\n",
    "        col(\"FirstName\").alias(\"first_name\"),\n",
    "        col(\"LastName\").alias(\"last_name\"),\n",
    "        col(\"EmailAddress\").alias(\"email\"),\n",
    "        col(\"Phone\").alias(\"phone\"),\n",
    "        col(\"CompanyName\").alias(\"company_name\"),\n",
    "        col(\"is_current\"),\n",
    "        col(\"valid_from\"),\n",
    "        col(\"valid_to\")\n",
    "    )\n",
    "\n",
    "# Save as Delta table\n",
    "dim_customer.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(f\"{catalog}.{schema}.{TABLE_PREFIX}dim_customer\")\n",
    "\n",
    "print(f\"   ‚úÖ Dim_Customer: {dim_customer.count()} records\")\n",
    "display(dim_customer.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e862d730",
   "metadata": {},
   "source": [
    "### üîÑ SCD Type 2 - Update with MERGE (Simulation)\n",
    "\n",
    "Now let's simulate a customer data update and show how SCD Type 2 works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f654fed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üîÑ SCD TYPE 2 - Update with Delta MERGE\n",
    "# ============================================================\n",
    "\n",
    "# Simulate customer data change - customer changed company\n",
    "print(\"üìù Simulating customer data change...\")\n",
    "\n",
    "# Get a sample customer\n",
    "sample_customer = spark.table(f\"{catalog}.{schema}.{TABLE_PREFIX}dim_customer\") \\\n",
    "    .filter(col(\"is_current\") == True) \\\n",
    "    .limit(1).collect()[0]\n",
    "\n",
    "print(f\"   Before change: {sample_customer['full_name']} - {sample_customer['company_name']}\")\n",
    "\n",
    "# Create \"new data\" - simulate company change\n",
    "updated_data = [(\n",
    "    sample_customer['customer_id'],\n",
    "    sample_customer['full_name'],\n",
    "    sample_customer['first_name'],\n",
    "    sample_customer['last_name'],\n",
    "    sample_customer['email'],\n",
    "    sample_customer['phone'],\n",
    "    \"NEW COMPANY INC.\"  # Changed company!\n",
    ")]\n",
    "\n",
    "updated_df = spark.createDataFrame(updated_data, [\n",
    "    \"customer_id\", \"full_name\", \"first_name\", \"last_name\", \n",
    "    \"email\", \"phone\", \"company_name\"\n",
    "])\n",
    "\n",
    "print(f\"   New company: NEW COMPANY INC.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ac4a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SCD TYPE 2 - MERGE in SQL\n",
    "# ============================================================\n",
    "\n",
    "# Register DataFrame as temp view\n",
    "updated_df.createOrReplaceTempView(\"updated_customers\")\n",
    "\n",
    "# SCD Type 2 requires 2 steps:\n",
    "# Step 1: Close old records (set valid_to and is_current = false)\n",
    "# Step 2: Insert new records\n",
    "\n",
    "dim_customer_table = f\"{catalog}.{schema}.{TABLE_PREFIX}dim_customer\"\n",
    "\n",
    "# STEP 1: Close old records\n",
    "spark.sql(f\"\"\"\n",
    "MERGE INTO {dim_customer_table} AS target\n",
    "USING updated_customers AS source\n",
    "ON target.customer_id = source.customer_id \n",
    "   AND target.is_current = true\n",
    "   AND target.company_name != source.company_name\n",
    "WHEN MATCHED THEN UPDATE SET\n",
    "    target.is_current = false,\n",
    "    target.valid_to = current_date() - INTERVAL 1 DAY\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Step 1: Closed old records\")\n",
    "\n",
    "# STEP 2: Insert new records\n",
    "spark.sql(f\"\"\"\n",
    "INSERT INTO {dim_customer_table}\n",
    "SELECT \n",
    "    (SELECT COALESCE(MAX(customer_key), 0) + 1 FROM {dim_customer_table}) AS customer_key,\n",
    "    source.customer_id,\n",
    "    source.full_name,\n",
    "    source.first_name,\n",
    "    source.last_name,\n",
    "    source.email,\n",
    "    source.phone,\n",
    "    source.company_name,\n",
    "    true AS is_current,\n",
    "    current_date() AS valid_from,\n",
    "    DATE '9999-12-31' AS valid_to\n",
    "FROM updated_customers source\n",
    "WHERE EXISTS (\n",
    "    SELECT 1 FROM {dim_customer_table} target \n",
    "    WHERE target.customer_id = source.customer_id \n",
    "    AND target.is_current = false\n",
    "    AND target.valid_to = current_date() - INTERVAL 1 DAY\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Step 2: Inserted new records\")\n",
    "\n",
    "# Verification\n",
    "print(\"\\nüìä Change history for customer:\")\n",
    "display(spark.sql(f\"\"\"\n",
    "SELECT customer_key, customer_id, full_name, company_name, \n",
    "       is_current, valid_from, valid_to\n",
    "FROM {dim_customer_table}\n",
    "WHERE customer_id = {sample_customer['customer_id']}\n",
    "ORDER BY valid_from\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee44ed5b",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Gold Layer - Fact Table\n",
    "\n",
    "### Fact_Sales - Central Fact Table\n",
    "\n",
    "Now we build the central fact table that connects all dimensions. This table contains:\n",
    "- **Measures**: quantity, unit_price, line_total, order_total\n",
    "- **Foreign Keys**: customer_key, product_key, date_key\n",
    "\n",
    "The fact table follows a **star schema pattern** where each foreign key points to a dimension table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c000d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GOLD LAYER: Fact_Sales\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, date_format, sha2, concat_ws, monotonically_increasing_id\n",
    ")\n",
    "\n",
    "# Read silver tables\n",
    "silver_orders = spark.table(\"silver_orders\")\n",
    "silver_order_details = spark.table(\"silver_order_details\")\n",
    "\n",
    "# Read dimension tables for key lookup\n",
    "dim_customer = spark.table(\"gold_dim_customer\").filter(col(\"is_current\") == True)\n",
    "dim_product = spark.table(\"gold_dim_product\")\n",
    "dim_date = spark.table(\"gold_dim_date\")\n",
    "\n",
    "# Build fact table by joining with dimensions\n",
    "fact_sales = (\n",
    "    silver_order_details\n",
    "    .join(silver_orders, \"SalesOrderID\")\n",
    "    # Join to get dimension keys\n",
    "    .join(\n",
    "        dim_customer.select(\"customer_key\", \"customer_id\"),\n",
    "        silver_orders[\"CustomerID\"] == dim_customer[\"customer_id\"],\n",
    "        \"left\"\n",
    "    )\n",
    "    .join(\n",
    "        dim_product.select(\"product_key\", \"product_id\"),\n",
    "        silver_order_details[\"ProductID\"] == dim_product[\"product_id\"],\n",
    "        \"left\"\n",
    "    )\n",
    "    .join(\n",
    "        dim_date.select(\"date_key\", \"full_date\"),\n",
    "        date_format(silver_orders[\"OrderDate\"], \"yyyy-MM-dd\") == date_format(dim_date[\"full_date\"], \"yyyy-MM-dd\"),\n",
    "        \"left\"\n",
    "    )\n",
    "    # Select final columns\n",
    "    .select(\n",
    "        monotonically_increasing_id().alias(\"sales_fact_id\"),\n",
    "        col(\"SalesOrderID\").alias(\"order_id\"),\n",
    "        col(\"SalesOrderDetailID\").alias(\"order_line_id\"),\n",
    "        col(\"customer_key\"),\n",
    "        col(\"product_key\"),\n",
    "        col(\"date_key\"),\n",
    "        col(\"OrderQty\").alias(\"quantity\"),\n",
    "        col(\"UnitPrice\").alias(\"unit_price\"),\n",
    "        col(\"LineTotal\").alias(\"line_total\"),\n",
    "        col(\"TotalDue\").alias(\"order_total\"),\n",
    "        col(\"OrderDate\").alias(\"order_date\"),\n",
    "        col(\"ShipDate\").alias(\"ship_date\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Save as Delta table\n",
    "fact_sales.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_fact_sales\")\n",
    "\n",
    "print(f\"‚úÖ Fact_Sales created with {fact_sales.count():,} records\")\n",
    "display(fact_sales.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb86325a",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- SQL Equivalent: Create Fact_Sales\n",
    "-- This shows the same logic using pure SQL\n",
    "\n",
    "CREATE OR REPLACE TABLE gold_fact_sales_sql AS\n",
    "SELECT \n",
    "    monotonically_increasing_id() as sales_fact_id,\n",
    "    sod.SalesOrderID as order_id,\n",
    "    sod.SalesOrderDetailID as order_line_id,\n",
    "    dc.customer_key,\n",
    "    dp.product_key,\n",
    "    dd.date_key,\n",
    "    sod.OrderQty as quantity,\n",
    "    sod.UnitPrice as unit_price,\n",
    "    sod.LineTotal as line_total,\n",
    "    so.TotalDue as order_total,\n",
    "    so.OrderDate as order_date,\n",
    "    so.ShipDate as ship_date\n",
    "FROM silver_order_details sod\n",
    "JOIN silver_orders so ON sod.SalesOrderID = so.SalesOrderID\n",
    "LEFT JOIN gold_dim_customer dc \n",
    "    ON so.CustomerID = dc.customer_id AND dc.is_current = true\n",
    "LEFT JOIN gold_dim_product dp \n",
    "    ON sod.ProductID = dp.product_id\n",
    "LEFT JOIN gold_dim_date dd \n",
    "    ON date_format(so.OrderDate, 'yyyy-MM-dd') = date_format(dd.full_date, 'yyyy-MM-dd');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3500775f",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. SCD Type 2 - Simulating Changes\n",
    "\n",
    "### Understanding the MERGE Pattern for SCD Type 2\n",
    "\n",
    "When new data arrives, we need to:\n",
    "1. **Identify changed records** - Compare incoming data with existing current records\n",
    "2. **Expire old versions** - Set `is_current = false` and `valid_to = current_timestamp`\n",
    "3. **Insert new versions** - Add new record with `is_current = true` and `version + 1`\n",
    "\n",
    "Below we simulate customer updates and show the MERGE pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc40d2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SIMULATING CUSTOMER UPDATES FOR SCD TYPE 2\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import col, lit, when, current_timestamp\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Simulate incoming customer updates (e.g., customer moved to new city)\n",
    "# In real scenarios, this would come from a source system\n",
    "updated_customers_data = [\n",
    "    (1, \"John\", \"Smith\", \"john.smith@newmail.com\", \"New York\"),  # Changed email and city\n",
    "    (2, \"Jane\", \"Doe\", \"jane.doe@email.com\", \"Los Angeles\"),     # Changed city\n",
    "    (99999, \"New\", \"Customer\", \"new@email.com\", \"Chicago\")       # New customer\n",
    "]\n",
    "\n",
    "updated_customers_df = spark.createDataFrame(\n",
    "    updated_customers_data, \n",
    "    [\"CustomerID\", \"FirstName\", \"LastName\", \"EmailAddress\", \"City\"]\n",
    ")\n",
    "\n",
    "print(\"üì• Incoming customer updates:\")\n",
    "display(updated_customers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a9dc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SCD TYPE 2 MERGE PATTERN (PySpark with Delta)\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, current_timestamp, sha2, concat_ws,\n",
    "    max as spark_max, coalesce\n",
    ")\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Get reference to existing dimension table\n",
    "dim_customer_table = DeltaTable.forName(spark, \"gold_dim_customer\")\n",
    "\n",
    "# Prepare incoming updates with the same structure\n",
    "incoming_customers = (updated_customers_df\n",
    "    .withColumn(\"customer_key\", sha2(col(\"CustomerID\").cast(\"string\"), 256))\n",
    "    .withColumn(\"full_name\", concat_ws(\" \", col(\"FirstName\"), col(\"LastName\")))\n",
    "    .withColumn(\"is_current\", lit(True))\n",
    "    .withColumn(\"valid_from\", current_timestamp())\n",
    "    .withColumn(\"valid_to\", lit(None).cast(\"timestamp\"))\n",
    "    .select(\n",
    "        col(\"customer_key\"),\n",
    "        col(\"CustomerID\").alias(\"customer_id\"),\n",
    "        col(\"FirstName\").alias(\"first_name\"),\n",
    "        col(\"LastName\").alias(\"last_name\"),\n",
    "        col(\"EmailAddress\").alias(\"email\"),\n",
    "        col(\"City\").alias(\"city\"),\n",
    "        col(\"full_name\"),\n",
    "        col(\"is_current\"),\n",
    "        col(\"valid_from\"),\n",
    "        col(\"valid_to\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Get max version for each customer to determine next version\n",
    "current_versions = (\n",
    "    spark.table(\"gold_dim_customer\")\n",
    "    .groupBy(\"customer_id\")\n",
    "    .agg(spark_max(\"version\").alias(\"max_version\"))\n",
    ")\n",
    "\n",
    "# Add version to incoming records\n",
    "incoming_with_version = (\n",
    "    incoming_customers\n",
    "    .join(current_versions, \"customer_id\", \"left\")\n",
    "    .withColumn(\"version\", coalesce(col(\"max_version\") + 1, lit(1)))\n",
    "    .drop(\"max_version\")\n",
    ")\n",
    "\n",
    "print(\"üìã Prepared incoming records with version:\")\n",
    "display(incoming_with_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cec4a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXECUTE SCD TYPE 2 MERGE\n",
    "# ============================================================\n",
    "\n",
    "# The MERGE statement for SCD Type 2:\n",
    "# 1. When matched AND values changed: expire old record, insert new version\n",
    "# 2. When not matched: insert as new record\n",
    "\n",
    "# First, expire the old current records for customers that have updates\n",
    "dim_customer_table.alias(\"target\").merge(\n",
    "    incoming_with_version.alias(\"source\"),\n",
    "    \"target.customer_id = source.customer_id AND target.is_current = true\"\n",
    ").whenMatchedUpdate(\n",
    "    condition = \"\"\"\n",
    "        target.email != source.email OR \n",
    "        target.city != source.city OR \n",
    "        target.first_name != source.first_name OR\n",
    "        target.last_name != source.last_name\n",
    "    \"\"\",\n",
    "    set = {\n",
    "        \"is_current\": lit(False),\n",
    "        \"valid_to\": current_timestamp()\n",
    "    }\n",
    ").execute()\n",
    "\n",
    "print(\"‚úÖ Step 1: Expired old current records\")\n",
    "\n",
    "# Now insert the new versions\n",
    "incoming_with_version.write.format(\"delta\").mode(\"append\").saveAsTable(\"gold_dim_customer\")\n",
    "\n",
    "print(\"‚úÖ Step 2: Inserted new versions\")\n",
    "\n",
    "# Verify the changes\n",
    "print(\"\\nüìä Customer history for CustomerID = 1:\")\n",
    "display(\n",
    "    spark.table(\"gold_dim_customer\")\n",
    "    .filter(col(\"customer_id\") == 1)\n",
    "    .orderBy(\"version\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f52fb0a",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- SQL Equivalent: SCD Type 2 MERGE Pattern\n",
    "-- This shows the same logic using pure SQL\n",
    "\n",
    "-- Step 1: Expire old records where values have changed\n",
    "MERGE INTO gold_dim_customer AS target\n",
    "USING (\n",
    "    SELECT \n",
    "        sha2(CAST(CustomerID AS STRING), 256) as customer_key,\n",
    "        CustomerID as customer_id,\n",
    "        FirstName as first_name,\n",
    "        LastName as last_name,\n",
    "        EmailAddress as email,\n",
    "        City as city\n",
    "    FROM updated_customers_temp\n",
    ") AS source\n",
    "ON target.customer_id = source.customer_id AND target.is_current = true\n",
    "WHEN MATCHED AND (\n",
    "    target.email != source.email OR \n",
    "    target.city != source.city OR\n",
    "    target.first_name != source.first_name OR\n",
    "    target.last_name != source.last_name\n",
    ") THEN\n",
    "    UPDATE SET \n",
    "        is_current = false,\n",
    "        valid_to = current_timestamp();\n",
    "\n",
    "-- Step 2: Insert new versions\n",
    "INSERT INTO gold_dim_customer\n",
    "SELECT \n",
    "    sha2(CAST(CustomerID AS STRING), 256) as customer_key,\n",
    "    CustomerID as customer_id,\n",
    "    FirstName as first_name,\n",
    "    LastName as last_name,\n",
    "    EmailAddress as email,\n",
    "    City as city,\n",
    "    concat(FirstName, ' ', LastName) as full_name,\n",
    "    true as is_current,\n",
    "    current_timestamp() as valid_from,\n",
    "    NULL as valid_to,\n",
    "    COALESCE(\n",
    "        (SELECT MAX(version) + 1 FROM gold_dim_customer WHERE customer_id = u.CustomerID),\n",
    "        1\n",
    "    ) as version\n",
    "FROM updated_customers_temp u;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62022107",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Validation & Analytics Queries\n",
    "\n",
    "### Validating the Star Schema\n",
    "\n",
    "Let's run some queries to validate our Star Schema is working correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d528cd",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Validation Query 1: Row counts across all tables\n",
    "SELECT 'bronze_customers' as table_name, COUNT(*) as row_count FROM bronze_customers\n",
    "UNION ALL\n",
    "SELECT 'bronze_products', COUNT(*) FROM bronze_products\n",
    "UNION ALL\n",
    "SELECT 'bronze_orders', COUNT(*) FROM bronze_orders\n",
    "UNION ALL\n",
    "SELECT 'silver_customers', COUNT(*) FROM silver_customers\n",
    "UNION ALL\n",
    "SELECT 'silver_products', COUNT(*) FROM silver_products\n",
    "UNION ALL\n",
    "SELECT 'silver_orders', COUNT(*) FROM silver_orders\n",
    "UNION ALL\n",
    "SELECT 'gold_dim_customer', COUNT(*) FROM gold_dim_customer\n",
    "UNION ALL\n",
    "SELECT 'gold_dim_product', COUNT(*) FROM gold_dim_product\n",
    "UNION ALL\n",
    "SELECT 'gold_dim_date', COUNT(*) FROM gold_dim_date\n",
    "UNION ALL\n",
    "SELECT 'gold_fact_sales', COUNT(*) FROM gold_fact_sales\n",
    "ORDER BY table_name;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c603ce",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Validation Query 2: Sample Star Schema query - Sales by Product Category\n",
    "SELECT \n",
    "    dp.category_name,\n",
    "    dd.year,\n",
    "    dd.month_name,\n",
    "    COUNT(DISTINCT f.order_id) as total_orders,\n",
    "    SUM(f.quantity) as total_quantity,\n",
    "    ROUND(SUM(f.line_total), 2) as total_revenue\n",
    "FROM gold_fact_sales f\n",
    "JOIN gold_dim_product dp ON f.product_key = dp.product_key\n",
    "JOIN gold_dim_date dd ON f.date_key = dd.date_key\n",
    "GROUP BY dp.category_name, dd.year, dd.month_name\n",
    "ORDER BY total_revenue DESC\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69801cd2",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Validation Query 3: Top customers by revenue\n",
    "SELECT \n",
    "    dc.full_name,\n",
    "    dc.email,\n",
    "    dc.city,\n",
    "    COUNT(DISTINCT f.order_id) as total_orders,\n",
    "    ROUND(SUM(f.line_total), 2) as total_spent\n",
    "FROM gold_fact_sales f\n",
    "JOIN gold_dim_customer dc ON f.customer_key = dc.customer_key\n",
    "WHERE dc.is_current = true\n",
    "GROUP BY dc.full_name, dc.email, dc.city\n",
    "ORDER BY total_spent DESC\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4c9c5f",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Validation Query 4: SCD Type 2 History Check\n",
    "-- Shows customers with multiple versions (history)\n",
    "SELECT \n",
    "    customer_id,\n",
    "    full_name,\n",
    "    email,\n",
    "    city,\n",
    "    version,\n",
    "    is_current,\n",
    "    valid_from,\n",
    "    valid_to\n",
    "FROM gold_dim_customer\n",
    "WHERE customer_id IN (\n",
    "    SELECT customer_id \n",
    "    FROM gold_dim_customer \n",
    "    GROUP BY customer_id \n",
    "    HAVING COUNT(*) > 1\n",
    ")\n",
    "ORDER BY customer_id, version;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4fbe81",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Complete Solutions\n",
    "\n",
    "This section provides complete end-to-end implementations of the Star Schema pipeline using both **PySpark** and **SQL** approaches.\n",
    "\n",
    "### Solution A: Complete PySpark Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2530e022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SOLUTION A: COMPLETE PYSPARK STAR SCHEMA PIPELINE\n",
    "# ============================================================\n",
    "# This is the full end-to-end implementation using PySpark DataFrame API\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, current_timestamp, date_format, year, month, dayofmonth,\n",
    "    dayofweek, quarter, weekofyear, sha2, concat_ws, monotonically_increasing_id,\n",
    "    when, coalesce, max as spark_max, trim, upper, lower, regexp_replace\n",
    ")\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Configuration\n",
    "WORKSHOP_PATH = \"/Volumes/main/default/workshop_data\"\n",
    "CATALOG = \"workshop_catalog\"\n",
    "SCHEMA = \"star_schema\"\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1: BRONZE LAYER - Raw Data Ingestion\n",
    "# ============================================================\n",
    "\n",
    "def load_bronze_tables():\n",
    "    \"\"\"Load all raw CSV files into Bronze Delta tables\"\"\"\n",
    "    \n",
    "    bronze_tables = {\n",
    "        \"bronze_customers\": f\"{WORKSHOP_PATH}/Customers.csv\",\n",
    "        \"bronze_products\": f\"{WORKSHOP_PATH}/Product.csv\",\n",
    "        \"bronze_product_category\": f\"{WORKSHOP_PATH}/ProductCategory.csv\",\n",
    "        \"bronze_orders\": f\"{WORKSHOP_PATH}/SalesOrderHeader.csv\",\n",
    "        \"bronze_order_details\": f\"{WORKSHOP_PATH}/SalesOrderDetail.csv\"\n",
    "    }\n",
    "    \n",
    "    for table_name, path in bronze_tables.items():\n",
    "        df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(path)\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "        print(f\"‚úÖ {table_name}: {df.count():,} records\")\n",
    "    \n",
    "    return bronze_tables\n",
    "\n",
    "# ============================================================\n",
    "# STEP 2: SILVER LAYER - Data Cleaning & Transformation\n",
    "# ============================================================\n",
    "\n",
    "def create_silver_customers():\n",
    "    \"\"\"Clean and transform customer data\"\"\"\n",
    "    return (\n",
    "        spark.table(\"bronze_customers\")\n",
    "        .withColumn(\"FirstName\", trim(col(\"FirstName\")))\n",
    "        .withColumn(\"LastName\", trim(col(\"LastName\")))\n",
    "        .withColumn(\"EmailAddress\", lower(trim(col(\"EmailAddress\"))))\n",
    "        .dropDuplicates([\"CustomerID\"])\n",
    "        .filter(col(\"CustomerID\").isNotNull())\n",
    "    )\n",
    "\n",
    "def create_silver_products():\n",
    "    \"\"\"Clean and enrich product data with category\"\"\"\n",
    "    products = spark.table(\"bronze_products\")\n",
    "    categories = spark.table(\"bronze_product_category\")\n",
    "    \n",
    "    return (\n",
    "        products\n",
    "        .join(categories, \"ProductCategoryID\", \"left\")\n",
    "        .withColumn(\"Name\", trim(col(\"Name\")))\n",
    "        .withColumn(\"ProductNumber\", upper(trim(col(\"ProductNumber\"))))\n",
    "        .dropDuplicates([\"ProductID\"])\n",
    "        .filter(col(\"ProductID\").isNotNull())\n",
    "    )\n",
    "\n",
    "def create_silver_orders():\n",
    "    \"\"\"Clean order header and details\"\"\"\n",
    "    orders = (\n",
    "        spark.table(\"bronze_orders\")\n",
    "        .filter(col(\"SalesOrderID\").isNotNull())\n",
    "        .dropDuplicates([\"SalesOrderID\"])\n",
    "    )\n",
    "    \n",
    "    order_details = (\n",
    "        spark.table(\"bronze_order_details\")\n",
    "        .filter(col(\"SalesOrderDetailID\").isNotNull())\n",
    "        .dropDuplicates([\"SalesOrderDetailID\"])\n",
    "    )\n",
    "    \n",
    "    return orders, order_details\n",
    "\n",
    "print(\"‚úÖ Silver layer functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd698bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 3: GOLD LAYER - Star Schema Dimensions\n",
    "# ============================================================\n",
    "\n",
    "def create_dim_date(start_date=\"2020-01-01\", end_date=\"2025-12-31\"):\n",
    "    \"\"\"Create date dimension with calendar attributes\"\"\"\n",
    "    from pyspark.sql.functions import explode, sequence, to_date\n",
    "    \n",
    "    date_df = spark.sql(f\"\"\"\n",
    "        SELECT explode(sequence(\n",
    "            to_date('{start_date}'), \n",
    "            to_date('{end_date}'), \n",
    "            interval 1 day\n",
    "        )) as full_date\n",
    "    \"\"\")\n",
    "    \n",
    "    return (date_df\n",
    "        .withColumn(\"date_key\", date_format(col(\"full_date\"), \"yyyyMMdd\").cast(\"int\"))\n",
    "        .withColumn(\"year\", year(col(\"full_date\")))\n",
    "        .withColumn(\"quarter\", quarter(col(\"full_date\")))\n",
    "        .withColumn(\"month\", month(col(\"full_date\")))\n",
    "        .withColumn(\"month_name\", date_format(col(\"full_date\"), \"MMMM\"))\n",
    "        .withColumn(\"day\", dayofmonth(col(\"full_date\")))\n",
    "        .withColumn(\"day_of_week\", dayofweek(col(\"full_date\")))\n",
    "        .withColumn(\"day_name\", date_format(col(\"full_date\"), \"EEEE\"))\n",
    "        .withColumn(\"week_of_year\", weekofyear(col(\"full_date\")))\n",
    "        .withColumn(\"is_weekend\", when(dayofweek(col(\"full_date\")).isin(1, 7), True).otherwise(False))\n",
    "    )\n",
    "\n",
    "def create_dim_product_scd1(silver_products):\n",
    "    \"\"\"Create product dimension with SCD Type 1 (overwrite)\"\"\"\n",
    "    return (silver_products\n",
    "        .withColumn(\"product_key\", sha2(col(\"ProductID\").cast(\"string\"), 256))\n",
    "        .select(\n",
    "            col(\"product_key\"),\n",
    "            col(\"ProductID\").alias(\"product_id\"),\n",
    "            col(\"Name\").alias(\"product_name\"),\n",
    "            col(\"ProductNumber\").alias(\"product_number\"),\n",
    "            col(\"Color\").alias(\"color\"),\n",
    "            col(\"StandardCost\").alias(\"standard_cost\"),\n",
    "            col(\"ListPrice\").alias(\"list_price\"),\n",
    "            col(\"Size\").alias(\"size\"),\n",
    "            col(\"Weight\").alias(\"weight\"),\n",
    "            coalesce(col(\"ProductCategoryName\"), lit(\"Unknown\")).alias(\"category_name\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "def create_dim_customer_scd2(silver_customers):\n",
    "    \"\"\"Create customer dimension with SCD Type 2 (history tracking)\"\"\"\n",
    "    return (silver_customers\n",
    "        .withColumn(\"customer_key\", sha2(col(\"CustomerID\").cast(\"string\"), 256))\n",
    "        .withColumn(\"full_name\", concat_ws(\" \", col(\"FirstName\"), col(\"LastName\")))\n",
    "        .withColumn(\"is_current\", lit(True))\n",
    "        .withColumn(\"valid_from\", current_timestamp())\n",
    "        .withColumn(\"valid_to\", lit(None).cast(\"timestamp\"))\n",
    "        .withColumn(\"version\", lit(1))\n",
    "        .select(\n",
    "            col(\"customer_key\"),\n",
    "            col(\"CustomerID\").alias(\"customer_id\"),\n",
    "            col(\"FirstName\").alias(\"first_name\"),\n",
    "            col(\"LastName\").alias(\"last_name\"),\n",
    "            col(\"EmailAddress\").alias(\"email\"),\n",
    "            col(\"CompanyName\").alias(\"company_name\"),\n",
    "            col(\"full_name\"),\n",
    "            col(\"is_current\"),\n",
    "            col(\"valid_from\"),\n",
    "            col(\"valid_to\"),\n",
    "            col(\"version\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Gold dimension functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e307ea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 4: GOLD LAYER - Fact Table\n",
    "# ============================================================\n",
    "\n",
    "def create_fact_sales(silver_orders, silver_order_details):\n",
    "    \"\"\"Create fact sales table joining all dimensions\"\"\"\n",
    "    \n",
    "    # Read dimension tables (current versions only for SCD2)\n",
    "    dim_customer = spark.table(\"gold_dim_customer\").filter(col(\"is_current\") == True)\n",
    "    dim_product = spark.table(\"gold_dim_product\")\n",
    "    dim_date = spark.table(\"gold_dim_date\")\n",
    "    \n",
    "    return (silver_order_details\n",
    "        .join(silver_orders, \"SalesOrderID\")\n",
    "        .join(dim_customer.select(\"customer_key\", \"customer_id\"),\n",
    "              silver_orders[\"CustomerID\"] == dim_customer[\"customer_id\"], \"left\")\n",
    "        .join(dim_product.select(\"product_key\", \"product_id\"),\n",
    "              silver_order_details[\"ProductID\"] == dim_product[\"product_id\"], \"left\")\n",
    "        .join(dim_date.select(\"date_key\", \"full_date\"),\n",
    "              date_format(silver_orders[\"OrderDate\"], \"yyyy-MM-dd\") == \n",
    "              date_format(dim_date[\"full_date\"], \"yyyy-MM-dd\"), \"left\")\n",
    "        .select(\n",
    "            monotonically_increasing_id().alias(\"sales_fact_id\"),\n",
    "            col(\"SalesOrderID\").alias(\"order_id\"),\n",
    "            col(\"SalesOrderDetailID\").alias(\"order_line_id\"),\n",
    "            col(\"customer_key\"),\n",
    "            col(\"product_key\"),\n",
    "            col(\"date_key\"),\n",
    "            col(\"OrderQty\").alias(\"quantity\"),\n",
    "            col(\"UnitPrice\").alias(\"unit_price\"),\n",
    "            col(\"LineTotal\").alias(\"line_total\"),\n",
    "            col(\"TotalDue\").alias(\"order_total\"),\n",
    "            col(\"OrderDate\").alias(\"order_date\"),\n",
    "            col(\"ShipDate\").alias(\"ship_date\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "# ============================================================\n",
    "# STEP 5: SCD TYPE 2 UPDATE FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "def apply_scd2_update(table_name, incoming_df, key_column, compare_columns):\n",
    "    \"\"\"\n",
    "    Apply SCD Type 2 update pattern to a dimension table\n",
    "    \n",
    "    Args:\n",
    "        table_name: Name of the target Delta table\n",
    "        incoming_df: DataFrame with new/updated records\n",
    "        key_column: Business key column name (e.g., 'customer_id')\n",
    "        compare_columns: List of columns to compare for changes\n",
    "    \"\"\"\n",
    "    from delta.tables import DeltaTable\n",
    "    \n",
    "    target_table = DeltaTable.forName(spark, table_name)\n",
    "    \n",
    "    # Build condition for detecting changes\n",
    "    change_condition = \" OR \".join([\n",
    "        f\"target.{c} != source.{c}\" for c in compare_columns\n",
    "    ])\n",
    "    \n",
    "    # Step 1: Expire old records\n",
    "    target_table.alias(\"target\").merge(\n",
    "        incoming_df.alias(\"source\"),\n",
    "        f\"target.{key_column} = source.{key_column} AND target.is_current = true\"\n",
    "    ).whenMatchedUpdate(\n",
    "        condition=change_condition,\n",
    "        set={\n",
    "            \"is_current\": lit(False),\n",
    "            \"valid_to\": current_timestamp()\n",
    "        }\n",
    "    ).execute()\n",
    "    \n",
    "    # Step 2: Insert new versions\n",
    "    # Get max version for each key\n",
    "    current_versions = (\n",
    "        spark.table(table_name)\n",
    "        .groupBy(key_column)\n",
    "        .agg(spark_max(\"version\").alias(\"max_version\"))\n",
    "    )\n",
    "    \n",
    "    new_records = (\n",
    "        incoming_df\n",
    "        .join(current_versions, key_column, \"left\")\n",
    "        .withColumn(\"version\", coalesce(col(\"max_version\") + 1, lit(1)))\n",
    "        .drop(\"max_version\")\n",
    "    )\n",
    "    \n",
    "    new_records.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)\n",
    "    \n",
    "    print(f\"‚úÖ SCD Type 2 update applied to {table_name}\")\n",
    "\n",
    "print(\"‚úÖ Fact table and SCD2 update functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d8f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXECUTE COMPLETE PYSPARK PIPELINE\n",
    "# ============================================================\n",
    "\n",
    "def run_complete_pyspark_pipeline():\n",
    "    \"\"\"Execute the complete Star Schema pipeline using PySpark\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"STARTING COMPLETE PYSPARK STAR SCHEMA PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Bronze Layer\n",
    "    print(\"\\nüì¶ STEP 1: Loading Bronze Layer...\")\n",
    "    load_bronze_tables()\n",
    "    \n",
    "    # Step 2: Silver Layer\n",
    "    print(\"\\nüîß STEP 2: Creating Silver Layer...\")\n",
    "    silver_customers = create_silver_customers()\n",
    "    silver_customers.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_customers\")\n",
    "    print(f\"   ‚úÖ silver_customers: {silver_customers.count():,} records\")\n",
    "    \n",
    "    silver_products = create_silver_products()\n",
    "    silver_products.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_products\")\n",
    "    print(f\"   ‚úÖ silver_products: {silver_products.count():,} records\")\n",
    "    \n",
    "    silver_orders, silver_order_details = create_silver_orders()\n",
    "    silver_orders.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_orders\")\n",
    "    silver_order_details.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_order_details\")\n",
    "    print(f\"   ‚úÖ silver_orders: {silver_orders.count():,} records\")\n",
    "    print(f\"   ‚úÖ silver_order_details: {silver_order_details.count():,} records\")\n",
    "    \n",
    "    # Step 3: Gold Dimensions\n",
    "    print(\"\\n‚≠ê STEP 3: Creating Gold Dimensions...\")\n",
    "    \n",
    "    dim_date = create_dim_date()\n",
    "    dim_date.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_dim_date\")\n",
    "    print(f\"   ‚úÖ gold_dim_date: {dim_date.count():,} records\")\n",
    "    \n",
    "    dim_product = create_dim_product_scd1(silver_products)\n",
    "    dim_product.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_dim_product\")\n",
    "    print(f\"   ‚úÖ gold_dim_product (SCD1): {dim_product.count():,} records\")\n",
    "    \n",
    "    dim_customer = create_dim_customer_scd2(silver_customers)\n",
    "    dim_customer.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_dim_customer\")\n",
    "    print(f\"   ‚úÖ gold_dim_customer (SCD2): {dim_customer.count():,} records\")\n",
    "    \n",
    "    # Step 4: Gold Fact Table\n",
    "    print(\"\\nüìä STEP 4: Creating Gold Fact Table...\")\n",
    "    fact_sales = create_fact_sales(silver_orders, silver_order_details)\n",
    "    fact_sales.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_fact_sales\")\n",
    "    print(f\"   ‚úÖ gold_fact_sales: {fact_sales.count():,} records\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ COMPLETE PYSPARK PIPELINE FINISHED SUCCESSFULLY!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Uncomment to run:\n",
    "# run_complete_pyspark_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f808cb",
   "metadata": {},
   "source": [
    "---\n",
    "### Solution B: Complete SQL Implementation\n",
    "\n",
    "The following SQL script implements the complete Star Schema pipeline using pure Spark SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9b7c44",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- ============================================================\n",
    "-- SOLUTION B: COMPLETE SQL STAR SCHEMA PIPELINE\n",
    "-- ============================================================\n",
    "\n",
    "-- ============================================================\n",
    "-- STEP 1: BRONZE LAYER - Raw Data Ingestion\n",
    "-- ============================================================\n",
    "\n",
    "-- Bronze Customers\n",
    "CREATE OR REPLACE TABLE bronze_customers_sql\n",
    "USING CSV\n",
    "OPTIONS (header = 'true', inferSchema = 'true')\n",
    "LOCATION '/Volumes/main/default/workshop_data/Customers.csv';\n",
    "\n",
    "-- Bronze Products\n",
    "CREATE OR REPLACE TABLE bronze_products_sql\n",
    "USING CSV\n",
    "OPTIONS (header = 'true', inferSchema = 'true')\n",
    "LOCATION '/Volumes/main/default/workshop_data/Product.csv';\n",
    "\n",
    "-- Bronze Product Category\n",
    "CREATE OR REPLACE TABLE bronze_product_category_sql\n",
    "USING CSV\n",
    "OPTIONS (header = 'true', inferSchema = 'true')\n",
    "LOCATION '/Volumes/main/default/workshop_data/ProductCategory.csv';\n",
    "\n",
    "-- Bronze Orders\n",
    "CREATE OR REPLACE TABLE bronze_orders_sql\n",
    "USING CSV\n",
    "OPTIONS (header = 'true', inferSchema = 'true')\n",
    "LOCATION '/Volumes/main/default/workshop_data/SalesOrderHeader.csv';\n",
    "\n",
    "-- Bronze Order Details\n",
    "CREATE OR REPLACE TABLE bronze_order_details_sql\n",
    "USING CSV\n",
    "OPTIONS (header = 'true', inferSchema = 'true')\n",
    "LOCATION '/Volumes/main/default/workshop_data/SalesOrderDetail.csv';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badd2256",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- ============================================================\n",
    "-- STEP 2: SILVER LAYER - Data Cleaning & Transformation\n",
    "-- ============================================================\n",
    "\n",
    "-- Silver Customers\n",
    "CREATE OR REPLACE TABLE silver_customers_sql AS\n",
    "SELECT DISTINCT\n",
    "    CustomerID,\n",
    "    TRIM(FirstName) as FirstName,\n",
    "    TRIM(LastName) as LastName,\n",
    "    LOWER(TRIM(EmailAddress)) as EmailAddress,\n",
    "    Phone,\n",
    "    CompanyName\n",
    "FROM bronze_customers_sql\n",
    "WHERE CustomerID IS NOT NULL;\n",
    "\n",
    "-- Silver Products (with category enrichment)\n",
    "CREATE OR REPLACE TABLE silver_products_sql AS\n",
    "SELECT DISTINCT\n",
    "    p.ProductID,\n",
    "    TRIM(p.Name) as Name,\n",
    "    UPPER(TRIM(p.ProductNumber)) as ProductNumber,\n",
    "    p.Color,\n",
    "    p.StandardCost,\n",
    "    p.ListPrice,\n",
    "    p.Size,\n",
    "    p.Weight,\n",
    "    p.ProductCategoryID,\n",
    "    COALESCE(c.Name, 'Unknown') as ProductCategoryName\n",
    "FROM bronze_products_sql p\n",
    "LEFT JOIN bronze_product_category_sql c ON p.ProductCategoryID = c.ProductCategoryID\n",
    "WHERE p.ProductID IS NOT NULL;\n",
    "\n",
    "-- Silver Orders\n",
    "CREATE OR REPLACE TABLE silver_orders_sql AS\n",
    "SELECT DISTINCT *\n",
    "FROM bronze_orders_sql\n",
    "WHERE SalesOrderID IS NOT NULL;\n",
    "\n",
    "-- Silver Order Details\n",
    "CREATE OR REPLACE TABLE silver_order_details_sql AS\n",
    "SELECT DISTINCT *\n",
    "FROM bronze_order_details_sql\n",
    "WHERE SalesOrderDetailID IS NOT NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69799a8e",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- ============================================================\n",
    "-- STEP 3: GOLD LAYER - Dimension Tables\n",
    "-- ============================================================\n",
    "\n",
    "-- Dim_Date (SCD Type 1 - Full refresh)\n",
    "CREATE OR REPLACE TABLE gold_dim_date_sql AS\n",
    "SELECT \n",
    "    CAST(date_format(full_date, 'yyyyMMdd') AS INT) as date_key,\n",
    "    full_date,\n",
    "    year(full_date) as year,\n",
    "    quarter(full_date) as quarter,\n",
    "    month(full_date) as month,\n",
    "    date_format(full_date, 'MMMM') as month_name,\n",
    "    dayofmonth(full_date) as day,\n",
    "    dayofweek(full_date) as day_of_week,\n",
    "    date_format(full_date, 'EEEE') as day_name,\n",
    "    weekofyear(full_date) as week_of_year,\n",
    "    CASE WHEN dayofweek(full_date) IN (1, 7) THEN true ELSE false END as is_weekend\n",
    "FROM (\n",
    "    SELECT explode(sequence(\n",
    "        to_date('2020-01-01'), \n",
    "        to_date('2025-12-31'), \n",
    "        interval 1 day\n",
    "    )) as full_date\n",
    ");\n",
    "\n",
    "-- Dim_Product (SCD Type 1 - Overwrite)\n",
    "CREATE OR REPLACE TABLE gold_dim_product_sql AS\n",
    "SELECT \n",
    "    sha2(CAST(ProductID AS STRING), 256) as product_key,\n",
    "    ProductID as product_id,\n",
    "    Name as product_name,\n",
    "    ProductNumber as product_number,\n",
    "    Color as color,\n",
    "    StandardCost as standard_cost,\n",
    "    ListPrice as list_price,\n",
    "    Size as size,\n",
    "    Weight as weight,\n",
    "    ProductCategoryName as category_name\n",
    "FROM silver_products_sql;\n",
    "\n",
    "-- Dim_Customer (SCD Type 2 - History tracking)\n",
    "CREATE OR REPLACE TABLE gold_dim_customer_sql AS\n",
    "SELECT \n",
    "    sha2(CAST(CustomerID AS STRING), 256) as customer_key,\n",
    "    CustomerID as customer_id,\n",
    "    FirstName as first_name,\n",
    "    LastName as last_name,\n",
    "    EmailAddress as email,\n",
    "    CompanyName as company_name,\n",
    "    concat(FirstName, ' ', LastName) as full_name,\n",
    "    true as is_current,\n",
    "    current_timestamp() as valid_from,\n",
    "    CAST(NULL AS TIMESTAMP) as valid_to,\n",
    "    1 as version\n",
    "FROM silver_customers_sql;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9eca00",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- ============================================================\n",
    "-- STEP 4: GOLD LAYER - Fact Table\n",
    "-- ============================================================\n",
    "\n",
    "CREATE OR REPLACE TABLE gold_fact_sales_sql AS\n",
    "SELECT \n",
    "    monotonically_increasing_id() as sales_fact_id,\n",
    "    sod.SalesOrderID as order_id,\n",
    "    sod.SalesOrderDetailID as order_line_id,\n",
    "    dc.customer_key,\n",
    "    dp.product_key,\n",
    "    dd.date_key,\n",
    "    sod.OrderQty as quantity,\n",
    "    sod.UnitPrice as unit_price,\n",
    "    sod.LineTotal as line_total,\n",
    "    so.TotalDue as order_total,\n",
    "    so.OrderDate as order_date,\n",
    "    so.ShipDate as ship_date\n",
    "FROM silver_order_details_sql sod\n",
    "JOIN silver_orders_sql so ON sod.SalesOrderID = so.SalesOrderID\n",
    "LEFT JOIN gold_dim_customer_sql dc \n",
    "    ON so.CustomerID = dc.customer_id AND dc.is_current = true\n",
    "LEFT JOIN gold_dim_product_sql dp \n",
    "    ON sod.ProductID = dp.product_id\n",
    "LEFT JOIN gold_dim_date_sql dd \n",
    "    ON date_format(so.OrderDate, 'yyyy-MM-dd') = date_format(dd.full_date, 'yyyy-MM-dd');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1def1d8",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- ============================================================\n",
    "-- SCD TYPE 2 UPDATE PATTERN (SQL)\n",
    "-- ============================================================\n",
    "\n",
    "-- Create temporary table with incoming updates\n",
    "CREATE OR REPLACE TEMP VIEW updated_customers_temp AS\n",
    "SELECT \n",
    "    1 as CustomerID, 'John' as FirstName, 'Smith' as LastName, \n",
    "    'john.smith@newmail.com' as EmailAddress, 'New Company Inc.' as CompanyName\n",
    "UNION ALL\n",
    "SELECT 2, 'Jane', 'Doe', 'jane.doe@email.com', 'Another Corp'\n",
    "UNION ALL\n",
    "SELECT 99999, 'New', 'Customer', 'new@email.com', 'Fresh Start LLC';\n",
    "\n",
    "-- Step 1: Expire old current records where values have changed\n",
    "MERGE INTO gold_dim_customer_sql AS target\n",
    "USING updated_customers_temp AS source\n",
    "ON target.customer_id = source.CustomerID AND target.is_current = true\n",
    "WHEN MATCHED AND (\n",
    "    target.email != source.EmailAddress OR \n",
    "    target.company_name != source.CompanyName OR\n",
    "    target.first_name != source.FirstName OR\n",
    "    target.last_name != source.LastName\n",
    ") THEN\n",
    "    UPDATE SET \n",
    "        is_current = false,\n",
    "        valid_to = current_timestamp();\n",
    "\n",
    "-- Step 2: Insert new versions\n",
    "INSERT INTO gold_dim_customer_sql\n",
    "SELECT \n",
    "    sha2(CAST(u.CustomerID AS STRING), 256) as customer_key,\n",
    "    u.CustomerID as customer_id,\n",
    "    u.FirstName as first_name,\n",
    "    u.LastName as last_name,\n",
    "    u.EmailAddress as email,\n",
    "    u.CompanyName as company_name,\n",
    "    concat(u.FirstName, ' ', u.LastName) as full_name,\n",
    "    true as is_current,\n",
    "    current_timestamp() as valid_from,\n",
    "    NULL as valid_to,\n",
    "    COALESCE(\n",
    "        (SELECT MAX(version) + 1 FROM gold_dim_customer_sql WHERE customer_id = u.CustomerID),\n",
    "        1\n",
    "    ) as version\n",
    "FROM updated_customers_temp u;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c4614c",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Summary & Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "| Topic | Key Concepts |\n",
    "|-------|--------------|\n",
    "| **Medallion Architecture** | Bronze (raw) ‚Üí Silver (cleaned) ‚Üí Gold (business-ready) |\n",
    "| **Star Schema** | Central fact table surrounded by dimension tables |\n",
    "| **SCD Type 1** | Overwrite existing data - no history preserved |\n",
    "| **SCD Type 2** | Track history with is_current, valid_from, valid_to, version |\n",
    "| **MERGE Pattern** | Delta Lake's MERGE for upsert and SCD operations |\n",
    "| **Lakeflow Pipeline** | Standard Spark/Delta approach for building pipelines |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Use surrogate keys** (hash-based) instead of natural keys for dimensions\n",
    "2. **Always filter by `is_current = true`** when joining SCD2 dimensions in analytics\n",
    "3. **Create separate validation queries** to verify data quality at each layer\n",
    "4. **Partition large fact tables** by date for better performance\n",
    "5. **Use Delta Lake's MERGE** for efficient incremental updates\n",
    "\n",
    "### Tables Created\n",
    "\n",
    "| Layer | Tables |\n",
    "|-------|--------|\n",
    "| **Bronze** | bronze_customers, bronze_products, bronze_product_category, bronze_orders, bronze_order_details |\n",
    "| **Silver** | silver_customers, silver_products, silver_orders, silver_order_details |\n",
    "| **Gold** | gold_dim_date, gold_dim_product (SCD1), gold_dim_customer (SCD2), gold_fact_sales |\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Challenge Exercise\n",
    "\n",
    "Try to extend the Star Schema by:\n",
    "1. Adding a **Dim_Store** dimension (you can simulate store data)\n",
    "2. Implementing **SCD Type 3** for one dimension (keeping only current and previous value)\n",
    "3. Creating additional fact table metrics like **profit margin** or **discount percentage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1641d1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Lakeflow SDP (dzia≈Ça tylko w kontek≈õcie potoku Lakeflow)\n",
    "from pyspark import pipelines as dp\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab15c5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfiguracja - ≈õcie≈ºki do danych\n",
    "# W rzeczywistym potoku te warto≈õci pochodzƒÖ z konfiguracji pipeline'u\n",
    "# spark.conf.get(\"mypipeline.source_path\")\n",
    "\n",
    "# Dla warsztatu u≈ºyjemy ≈õcie≈ºek hardcoded\n",
    "SOURCE_PATH = \"/Volumes/training/workshop/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142f94f2",
   "metadata": {},
   "source": [
    "---\n",
    "## ü•â Krok 1: Bronze Layer - Surowe dane\n",
    "\n",
    "### Zadanie 1.1: Utw√≥rz tabelƒô Bronze dla klient√≥w\n",
    "\n",
    "Bronze layer zawiera dane **dok≈Çadnie tak, jak przysz≈Çy ze ≈∫r√≥d≈Ça** - bez transformacji.\n",
    "\n",
    "**Wymagania:**\n",
    "- U≈ºyj `dp.create_streaming_table()` do utworzenia Streaming Table\n",
    "- Dodaj `@dp.append_flow` do przep≈Çywu danych\n",
    "- U≈ºyj Auto Loader (`cloudFiles`) do automatycznego wykrywania nowych plik√≥w\n",
    "\n",
    "**Podpowied≈∫ - Lakeflow SDP:**\n",
    "```python\n",
    "# Utw√≥rz streaming table\n",
    "dp.create_streaming_table(\n",
    "    name=\"bronze_customers\",\n",
    "    comment=\"Opis tabeli\"\n",
    ")\n",
    "\n",
    "# Dodaj append flow\n",
    "@dp.append_flow(target=\"bronze_customers\")\n",
    "def bronze_customers_flow():\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(path)\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae9ae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Utw√≥rz tabelƒô bronze_customers (Lakeflow SDP)\n",
    "\n",
    "# Krok 1: Utw√≥rz streaming table\n",
    "dp.create_streaming_table(\n",
    "    name=\"bronze_customers\",\n",
    "    comment=\"Surowe dane klient√≥w z plik√≥w CSV\"\n",
    ")\n",
    "\n",
    "# Krok 2: Dodaj append flow\n",
    "@dp.append_flow(target=\"bronze_customers\")\n",
    "def bronze_customers_flow():\n",
    "    return (\n",
    "        spark.readStream\n",
    "        # TODO: Uzupe≈Çnij kod\n",
    "        # .format(\"cloudFiles\")\n",
    "        # .option(\"cloudFiles.format\", \"csv\")\n",
    "        # .option(\"header\", \"true\")\n",
    "        # .load(f\"{SOURCE_PATH}/Customers.csv\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373655b0",
   "metadata": {},
   "source": [
    "### Zadanie 1.2: Utw√≥rz tabelƒô Bronze dla zam√≥wie≈Ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8f6e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Utw√≥rz tabelƒô bronze_orders (Lakeflow SDP)\n",
    "\n",
    "dp.create_streaming_table(\n",
    "    name=\"bronze_orders\",\n",
    "    comment=\"Surowe dane zam√≥wie≈Ñ z plik√≥w CSV\"\n",
    ")\n",
    "\n",
    "@dp.append_flow(target=\"bronze_orders\")\n",
    "def bronze_orders_flow():\n",
    "    return (\n",
    "        spark.readStream\n",
    "        # TODO: Uzupe≈Çnij kod\n",
    "        # .format(\"cloudFiles\")\n",
    "        # .option(\"cloudFiles.format\", \"csv\")\n",
    "        # .option(\"header\", \"true\")\n",
    "        # .load(f\"{SOURCE_PATH}/SalesOrderHeader.csv\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845d9a43",
   "metadata": {},
   "source": [
    "### Zadanie 1.3: Utw√≥rz tabelƒô Bronze dla produkt√≥w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84292c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Utw√≥rz tabelƒô bronze_products (Lakeflow SDP)\n",
    "\n",
    "dp.create_streaming_table(\n",
    "    name=\"bronze_products\",\n",
    "    comment=\"Surowe dane produkt√≥w z plik√≥w CSV\"\n",
    ")\n",
    "\n",
    "@dp.append_flow(target=\"bronze_products\")\n",
    "def bronze_products_flow():\n",
    "    return (\n",
    "        spark.readStream\n",
    "        # TODO: Uzupe≈Çnij kod\n",
    "        # .format(\"cloudFiles\")\n",
    "        # .option(\"cloudFiles.format\", \"csv\")\n",
    "        # .option(\"header\", \"true\")\n",
    "        # .load(f\"{SOURCE_PATH}/Product.csv\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa54ce30",
   "metadata": {},
   "source": [
    "---\n",
    "## ü•à Krok 2: Silver Layer - Data Quality\n",
    "\n",
    "### Data Quality z Lakeflow Expectations\n",
    "\n",
    "Lakeflow SDP u≈ºywa parametru `expect_all_or_drop` w definicji tabeli:\n",
    "\n",
    "```python\n",
    "@dp.materialized_view(\n",
    "    name=\"silver_customers\",\n",
    "    comment=\"Oczyszczeni klienci\",\n",
    "    expect_all_or_drop={\"valid_id\": \"CustomerID IS NOT NULL\"}\n",
    ")\n",
    "def silver_customers():\n",
    "    return spark.read.table(\"bronze_customers\")\n",
    "```\n",
    "\n",
    "**Rodzaje expectations:**\n",
    "- `expect_all` - loguj naruszenia, ale przepu≈õƒá dane\n",
    "- `expect_all_or_drop` - usu≈Ñ wiersze naruszajƒÖce regu≈Çƒô\n",
    "- `expect_all_or_fail` - zatrzymaj potok przy naruszeniu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2b5b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Utw√≥rz silver_customers z walidacjƒÖ (Lakeflow SDP)\n",
    "\n",
    "@dp.materialized_view(\n",
    "    name=\"silver_customers\",\n",
    "    comment=\"Oczyszczone dane klient√≥w z walidacjƒÖ\",\n",
    "    expect_all_or_drop={\"valid_customer_id\": \"CustomerID IS NOT NULL\"}\n",
    ")\n",
    "def silver_customers():\n",
    "    return (\n",
    "        spark.read.table(\"bronze_customers\")\n",
    "        .select(\n",
    "            col(\"CustomerID\").cast(\"int\"),\n",
    "            col(\"FirstName\"),\n",
    "            col(\"LastName\"),\n",
    "            col(\"EmailAddress\"),\n",
    "            col(\"Phone\"),\n",
    "            col(\"CompanyName\")\n",
    "        )\n",
    "        .withColumn(\"full_name\", concat_ws(\" \", col(\"FirstName\"), col(\"LastName\")))\n",
    "        .dropDuplicates([\"CustomerID\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa526ee",
   "metadata": {},
   "source": [
    "### Zadanie 2.2: Utw√≥rz tabelƒô Silver dla zam√≥wie≈Ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1b911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Utw√≥rz silver_orders z walidacjƒÖ (Lakeflow SDP)\n",
    "\n",
    "@dp.materialized_view(\n",
    "    name=\"silver_orders\",\n",
    "    comment=\"Oczyszczone dane zam√≥wie≈Ñ z walidacjƒÖ\",\n",
    "    expect_all_or_drop={\"valid_order\": \"SalesOrderID IS NOT NULL\"}\n",
    ")\n",
    "def silver_orders():\n",
    "    return (\n",
    "        spark.read.table(\"bronze_orders\")\n",
    "        .select(\n",
    "            col(\"SalesOrderID\").cast(\"int\"),\n",
    "            col(\"CustomerID\").cast(\"int\"),\n",
    "            col(\"OrderDate\").cast(\"date\"),\n",
    "            col(\"ShipDate\").cast(\"date\"),\n",
    "            col(\"TotalDue\").cast(\"decimal(18,2)\"),\n",
    "            col(\"Status\").cast(\"int\"),\n",
    "            col(\"ShipMethod\")\n",
    "        )\n",
    "        .filter(col(\"TotalDue\") > 0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf000981",
   "metadata": {},
   "source": [
    "### Zadanie 2.3: Utw√≥rz tabelƒô Silver dla produkt√≥w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8842de88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Utw√≥rz silver_products z walidacjƒÖ (Lakeflow SDP)\n",
    "\n",
    "@dp.materialized_view(\n",
    "    name=\"silver_products\",\n",
    "    comment=\"Oczyszczone dane produkt√≥w z walidacjƒÖ\",\n",
    "    expect_all_or_drop={\"valid_product\": \"ProductID IS NOT NULL\"}\n",
    ")\n",
    "def silver_products():\n",
    "    return (\n",
    "        spark.read.table(\"bronze_products\")\n",
    "        .select(\n",
    "            col(\"ProductID\").cast(\"int\"),\n",
    "            col(\"Name\").alias(\"product_name\"),\n",
    "            col(\"ProductNumber\"),\n",
    "            col(\"Color\"),\n",
    "            col(\"ListPrice\").cast(\"decimal(18,2)\"),\n",
    "            col(\"StandardCost\").cast(\"decimal(18,2)\")\n",
    "        )\n",
    "        .filter(col(\"ListPrice\") > 0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923fd759",
   "metadata": {},
   "source": [
    "---\n",
    "## ü•á Krok 3: Gold Layer - Star Schema z SCD\n",
    "\n",
    "### SCD Type 2 z Lakeflow AUTO CDC\n",
    "\n",
    "Lakeflow SDP oferuje wbudowane wsparcie dla SCD przez AUTO CDC:\n",
    "\n",
    "```python\n",
    "# Utw√≥rz streaming table dla wymiaru SCD2\n",
    "dp.create_streaming_table(\"dim_customer\", comment=\"SCD Type 2\")\n",
    "\n",
    "# Skonfiguruj AUTO CDC\n",
    "dp.create_auto_cdc_flow(\n",
    "    target=\"dim_customer\",\n",
    "    source=\"silver_customers\",\n",
    "    keys=[\"customer_id\"],\n",
    "    sequence_by=col(\"modified_date\"),\n",
    "    stored_as_scd_type=\"2\"\n",
    ")\n",
    "```\n",
    "\n",
    "Zalety AUTO CDC:\n",
    "- Automatyczne kolumny `__START_AT` i `__END_AT`\n",
    "- Obs≈Çuga zdarze≈Ñ out-of-order\n",
    "- Brak rƒôcznego kodu MERGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c421246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Utw√≥rz gold_customer_sales (Lakeflow SDP)\n",
    "\n",
    "@dp.materialized_view(\n",
    "    name=\"gold_customer_sales\",\n",
    "    comment=\"Agregacja sprzeda≈ºy per klient\"\n",
    ")\n",
    "def gold_customer_sales():\n",
    "    customers = spark.read.table(\"silver_customers\")\n",
    "    orders = spark.read.table(\"silver_orders\")\n",
    "    \n",
    "    return (\n",
    "        orders\n",
    "        .join(customers, \"CustomerID\")\n",
    "        .groupBy(\"CustomerID\", \"full_name\")\n",
    "        .agg(\n",
    "            sum(\"TotalDue\").alias(\"total_sales\"),\n",
    "            count(\"SalesOrderID\").alias(\"order_count\"),\n",
    "            avg(\"TotalDue\").alias(\"avg_order_value\")\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eb56a3",
   "metadata": {},
   "source": [
    "---\n",
    "## üöÄ Krok 4: Deployment\n",
    "\n",
    "### Instrukcja uruchomienia potoku:\n",
    "\n",
    "1. **Przejd≈∫ do Workflows** ‚Üí **Delta Live Tables**\n",
    "2. **Kliknij \"Create Pipeline\"**\n",
    "3. **Wype≈Çnij konfiguracjƒô:**\n",
    "   - **Pipeline name:** `workshop_w3_pipeline`\n",
    "   - **Source code:** Wybierz ten notebook\n",
    "   - **Target catalog:** Tw√≥j katalog\n",
    "   - **Target schema:** Tw√≥j schemat\n",
    "   - **Cluster mode:** `Enhanced autoscaling`\n",
    "\n",
    "4. **Dodaj konfiguracjƒô** (w sekcji Configuration):\n",
    "   ```\n",
    "   mypipeline.source_path = /Volumes/training/workshop/data\n",
    "   ```\n",
    "\n",
    "5. **Kliknij \"Start\"** i obserwuj wizualizacjƒô DAG!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf95b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ten kod NIE uruchomi DLT - to tylko informacja\n",
    "print(\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë  üìã INSTRUKCJA URUCHOMIENIA POTOKU DLT                       ‚ïë\n",
    "‚ïë                                                              ‚ïë\n",
    "‚ïë  1. Przejd≈∫ do: Workflows ‚Üí Delta Live Tables                ‚ïë\n",
    "‚ïë  2. Kliknij: Create Pipeline                                 ‚ïë\n",
    "‚ïë  3. Wybierz ten notebook jako ≈∫r√≥d≈Ço                         ‚ïë\n",
    "‚ïë  4. Skonfiguruj target catalog i schema                      ‚ïë\n",
    "‚ïë  5. Kliknij: Start                                           ‚ïë\n",
    "‚ïë                                                              ‚ïë\n",
    "‚ïë  üëÄ Obserwuj wizualizacjƒô DAG i metryki jako≈õci!            ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e15c77",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# üìã ROZWIƒÑZANIE\n",
    "\n",
    "‚ö†Ô∏è **Nie patrz tutaj, dop√≥ki nie spr√≥bujesz sam!** ‚ö†Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c472aa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# KOMPLETNE ROZWIƒÑZANIE: Lakeflow SDP Pipeline\n",
    "# ============================================================\n",
    "\n",
    "from pyspark import pipelines as dp\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# ============================================================\n",
    "# BRONZE LAYER - Streaming Tables\n",
    "# ============================================================\n",
    "\n",
    "# Bronze Customers\n",
    "dp.create_streaming_table(\"bronze_customers\", comment=\"Raw customer data\")\n",
    "\n",
    "@dp.append_flow(target=\"bronze_customers\")\n",
    "def bronze_customers_flow():\n",
    "    return spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\", \"csv\").option(\"header\", \"true\").load(f\"{SOURCE_PATH}/Customers.csv\")\n",
    "\n",
    "# Bronze Orders\n",
    "dp.create_streaming_table(\"bronze_orders\", comment=\"Raw order data\")\n",
    "\n",
    "@dp.append_flow(target=\"bronze_orders\")\n",
    "def bronze_orders_flow():\n",
    "    return spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\", \"csv\").option(\"header\", \"true\").load(f\"{SOURCE_PATH}/SalesOrderHeader.csv\")\n",
    "\n",
    "# Bronze Products\n",
    "dp.create_streaming_table(\"bronze_products\", comment=\"Raw product data\")\n",
    "\n",
    "@dp.append_flow(target=\"bronze_products\")\n",
    "def bronze_products_flow():\n",
    "    return spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\", \"csv\").option(\"header\", \"true\").load(f\"{SOURCE_PATH}/Product.csv\")\n",
    "\n",
    "# ============================================================\n",
    "# SILVER LAYER - Materialized Views\n",
    "# ============================================================\n",
    "\n",
    "@dp.materialized_view(\n",
    "    name=\"silver_customers\",\n",
    "    comment=\"Cleaned customer data\",\n",
    "    expect_all_or_drop={\"valid_id\": \"CustomerID IS NOT NULL\"}\n",
    ")\n",
    "def silver_customers():\n",
    "    return spark.read.table(\"bronze_customers\").select(\n",
    "        col(\"CustomerID\").cast(\"int\"),\n",
    "        concat_ws(\" \", col(\"FirstName\"), col(\"LastName\")).alias(\"full_name\"),\n",
    "        col(\"EmailAddress\"), col(\"Phone\"), col(\"CompanyName\")\n",
    "    ).dropDuplicates([\"CustomerID\"])\n",
    "\n",
    "@dp.materialized_view(name=\"silver_orders\", comment=\"Cleaned order data\")\n",
    "def silver_orders():\n",
    "    return spark.read.table(\"bronze_orders\").select(\n",
    "        col(\"SalesOrderID\").cast(\"int\"), col(\"CustomerID\").cast(\"int\"),\n",
    "        col(\"OrderDate\").cast(\"date\"), col(\"TotalDue\").cast(\"decimal(18,2)\")\n",
    "    )\n",
    "\n",
    "# ============================================================\n",
    "# GOLD LAYER - SCD z AUTO CDC\n",
    "# ============================================================\n",
    "\n",
    "# Dim_Customer SCD Type 2\n",
    "dp.create_streaming_table(\"dim_customer\", comment=\"Customer dimension SCD2\")\n",
    "dp.create_auto_cdc_flow(\n",
    "    target=\"dim_customer\",\n",
    "    source=\"silver_customers\",\n",
    "    keys=[\"CustomerID\"],\n",
    "    sequence_by=col(\"ModifiedDate\"),\n",
    "    stored_as_scd_type=\"2\"\n",
    ")\n",
    "\n",
    "# Fact Sales\n",
    "@dp.materialized_view(name=\"fact_sales\", comment=\"Sales fact table\")\n",
    "def fact_sales():\n",
    "    return (\n",
    "        spark.read.table(\"silver_orders\")\n",
    "        .join(spark.read.table(\"dim_customer\").filter(col(\"__END_AT\").isNull()), \"CustomerID\")\n",
    "        .select(\n",
    "            monotonically_increasing_id().alias(\"sales_key\"),\n",
    "            col(\"customer_key\"), col(\"SalesOrderID\").alias(\"order_id\"),\n",
    "            col(\"TotalDue\").alias(\"total_due\")\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0518b3",
   "metadata": {},
   "source": [
    "### üîÑ SCD Type 2 - Aktualizacja z MERGE (Symulacja)\n",
    "\n",
    "Teraz zasymulujemy aktualizacjƒô danych klienta i poka≈ºemy jak dzia≈Ça SCD Type 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb08c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üîÑ SCD TYPE 2 - Aktualizacja z Delta MERGE\n",
    "# ============================================================\n",
    "\n",
    "# Symulujemy zmianƒô danych klienta - klient zmieni≈Ç firmƒô\n",
    "print(\"üìù Symulacja zmiany danych klienta...\")\n",
    "\n",
    "# Pobierz przyk≈Çadowego klienta\n",
    "sample_customer = spark.table(f\"{catalog}.{schema}.{TABLE_PREFIX}dim_customer\") \\\n",
    "    .filter(col(\"is_current\") == True) \\\n",
    "    .limit(1).collect()[0]\n",
    "\n",
    "print(f\"   Przed zmianƒÖ: {sample_customer['full_name']} - {sample_customer['company_name']}\")\n",
    "\n",
    "# Utw√≥rz \"nowe dane\" - symulacja zmiany firmy\n",
    "from pyspark.sql import Row\n",
    "updated_data = [(\n",
    "    sample_customer['customer_id'],\n",
    "    sample_customer['full_name'],\n",
    "    sample_customer['first_name'],\n",
    "    sample_customer['last_name'],\n",
    "    sample_customer['email'],\n",
    "    sample_customer['phone'],\n",
    "    \"NOWA FIRMA SP. Z O.O.\"  # Zmieniona firma!\n",
    ")]\n",
    "\n",
    "updated_df = spark.createDataFrame(updated_data, [\n",
    "    \"customer_id\", \"full_name\", \"first_name\", \"last_name\", \n",
    "    \"email\", \"phone\", \"company_name\"\n",
    "])\n",
    "\n",
    "print(f\"   Nowa firma: NOWA FIRMA SP. Z O.O.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d21f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SCD TYPE 2 - MERGE w SQL\n",
    "# ============================================================\n",
    "\n",
    "# Zarejestruj DataFrame jako temp view\n",
    "updated_df.createOrReplaceTempView(\"updated_customers\")\n",
    "\n",
    "# SCD Type 2 wymaga 2 krok√≥w:\n",
    "# Krok 1: Zamknij stare rekordy (ustaw valid_to i is_current = false)\n",
    "# Krok 2: Wstaw nowe rekordy\n",
    "\n",
    "dim_customer_table = f\"{catalog}.{schema}.{TABLE_PREFIX}dim_customer\"\n",
    "\n",
    "# KROK 1: Zamknij stare rekordy\n",
    "spark.sql(f\"\"\"\n",
    "MERGE INTO {dim_customer_table} AS target\n",
    "USING updated_customers AS source\n",
    "ON target.customer_id = source.customer_id \n",
    "   AND target.is_current = true\n",
    "   AND target.company_name != source.company_name\n",
    "WHEN MATCHED THEN UPDATE SET\n",
    "    target.is_current = false,\n",
    "    target.valid_to = current_date() - INTERVAL 1 DAY\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Krok 1: Zamkniƒôto stare rekordy\")\n",
    "\n",
    "# KROK 2: Wstaw nowe rekordy\n",
    "spark.sql(f\"\"\"\n",
    "INSERT INTO {dim_customer_table}\n",
    "SELECT \n",
    "    (SELECT COALESCE(MAX(customer_key), 0) + 1 FROM {dim_customer_table}) AS customer_key,\n",
    "    source.customer_id,\n",
    "    source.full_name,\n",
    "    source.first_name,\n",
    "    source.last_name,\n",
    "    source.email,\n",
    "    source.phone,\n",
    "    source.company_name,\n",
    "    true AS is_current,\n",
    "    current_date() AS valid_from,\n",
    "    DATE '9999-12-31' AS valid_to\n",
    "FROM updated_customers source\n",
    "WHERE EXISTS (\n",
    "    SELECT 1 FROM {dim_customer_table} target \n",
    "    WHERE target.customer_id = source.customer_id \n",
    "    AND target.is_current = false\n",
    "    AND target.valid_to = current_date() - INTERVAL 1 DAY\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Krok 2: Wstawiono nowe rekordy\")\n",
    "\n",
    "# Weryfikacja\n",
    "print(\"\\nüìä Historia zmian dla klienta:\")\n",
    "display(spark.sql(f\"\"\"\n",
    "SELECT customer_key, customer_id, full_name, company_name, \n",
    "       is_current, valid_from, valid_to\n",
    "FROM {dim_customer_table}\n",
    "WHERE customer_id = {sample_customer['customer_id']}\n",
    "ORDER BY valid_from\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efceafe5",
   "metadata": {},
   "source": [
    "---\n",
    "### Wymiary Gotowe!\n",
    "\n",
    "Po utworzeniu wszystkich trzech wymiar√≥w (Dim_Customer, Dim_Product, Dim_Date), mo≈ºemy przej≈õƒá do tworzenia tabeli fakt√≥w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc5df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ‚úÖ WYMIARY GOLD LAYER GOTOWE!\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ WSZYSTKIE WYMIARY GOLD LAYER GOTOWE!\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"üìä Utworzone wymiary:\")\n",
    "print(\"  ‚Ä¢ Dim_Customer (SCD Type 2) - z historiƒÖ zmian\")\n",
    "print(\"  ‚Ä¢ Dim_Product (SCD Type 1) - bez historii\")\n",
    "print(\"  ‚Ä¢ Dim_Date (SCD Type 1) - wymiar daty\")\n",
    "print()\n",
    "print(\"üéØ Nastƒôpny krok: Fact_Sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5de874",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## ü•á Krok 5: Gold Layer - Fact_Sales (Tabela Fakt√≥w)\n",
    "\n",
    "### Tabela Fakt√≥w\n",
    "\n",
    "Tabela fakt√≥w to **serce Star Schema** - zawiera:\n",
    "- **Klucze obce (FK)** do wszystkich wymiar√≥w\n",
    "- **Miary (Measures)** - warto≈õci liczbowe do analizy\n",
    "- **Degenerate Dimensions** - np. numer zam√≥wienia\n",
    "\n",
    "### Struktura Fact_Sales:\n",
    "\n",
    "| Kolumna | Typ | Opis |\n",
    "|---------|-----|------|\n",
    "| `sales_key` | SK | Surrogate Key |\n",
    "| `customer_key` | FK | Klucz do Dim_Customer |\n",
    "| `product_key` | FK | Klucz do Dim_Product |\n",
    "| `date_key` | FK | Klucz do Dim_Date |\n",
    "| `order_id` | DD | Numer zam√≥wienia |\n",
    "| `quantity` | Measure | Ilo≈õƒá |\n",
    "| `unit_price` | Measure | Cena jednostkowa |\n",
    "| `line_total` | Measure | Warto≈õƒá linii |\n",
    "\n",
    "### Zadanie 5.1: Utw√≥rz Fact_Sales\n",
    "\n",
    "**Wersja PySpark:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2450a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ü•á GOLD LAYER - Fact_Sales - PySpark\n",
    "# ============================================================\n",
    "\n",
    "print(\"üìä Tworzƒô: Fact_Sales\")\n",
    "\n",
    "# Wczytaj tabele ≈∫r√≥d≈Çowe (Silver) i wymiary (Gold)\n",
    "silver_order_header = spark.table(f\"{catalog}.{schema}.{TABLE_PREFIX}silver_order_header\")\n",
    "silver_order_detail = spark.table(f\"{catalog}.{schema}.{TABLE_PREFIX}silver_order_detail\")\n",
    "dim_customer = spark.table(f\"{catalog}.{schema}.{TABLE_PREFIX}dim_customer\").filter(col(\"is_current\") == True)\n",
    "dim_product = spark.table(f\"{catalog}.{schema}.{TABLE_PREFIX}dim_product\")\n",
    "dim_date = spark.table(f\"{catalog}.{schema}.{TABLE_PREFIX}dim_date\")\n",
    "\n",
    "# Z≈ÇƒÖcz Order Header z Order Detail\n",
    "orders_combined = silver_order_detail.alias(\"detail\") \\\n",
    "    .join(silver_order_header.alias(\"header\"), \"SalesOrderID\")\n",
    "\n",
    "# Z≈ÇƒÖcz z wymiarami i utw√≥rz Fact_Sales\n",
    "fact_sales = orders_combined \\\n",
    "    .join(dim_customer, orders_combined[\"CustomerID\"] == dim_customer[\"customer_id\"], \"left\") \\\n",
    "    .join(dim_product, orders_combined[\"ProductID\"] == dim_product[\"product_id\"], \"left\") \\\n",
    "    .withColumn(\"date_key\", date_format(col(\"OrderDate\"), \"yyyyMMdd\").cast(\"int\")) \\\n",
    "    .withColumn(\"sales_key\", monotonically_increasing_id() + 1) \\\n",
    "    .select(\n",
    "        col(\"sales_key\"),\n",
    "        col(\"customer_key\"),\n",
    "        col(\"product_key\"),\n",
    "        col(\"date_key\"),\n",
    "        col(\"SalesOrderID\").alias(\"order_id\"),\n",
    "        col(\"SalesOrderDetailID\").alias(\"order_detail_id\"),\n",
    "        col(\"OrderQty\").alias(\"quantity\"),\n",
    "        col(\"UnitPrice\").alias(\"unit_price\"),\n",
    "        col(\"UnitPriceDiscount\").alias(\"discount\"),\n",
    "        col(\"LineTotal\").alias(\"line_total\"),\n",
    "        col(\"TotalDue\").alias(\"total_due\"),\n",
    "        col(\"ShipMethod\").alias(\"ship_method\")\n",
    "    )\n",
    "\n",
    "# Zapisz jako tabelƒô Delta\n",
    "fact_sales.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(f\"{catalog}.{schema}.{TABLE_PREFIX}fact_sales\")\n",
    "\n",
    "print(f\"   ‚úÖ Fact_Sales: {fact_sales.count()} rekord√≥w\")\n",
    "display(fact_sales.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099c4533",
   "metadata": {},
   "source": [
    "**Wersja SQL (r√≥wnowa≈ºna):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0100cdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ü•á GOLD LAYER - Fact_Sales - SQL Version\n",
    "# ============================================================\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {catalog}.{schema}.{TABLE_PREFIX}fact_sales_sql AS\n",
    "SELECT \n",
    "    ROW_NUMBER() OVER (ORDER BY d.SalesOrderID, d.SalesOrderDetailID) AS sales_key,\n",
    "    dc.customer_key,\n",
    "    dp.product_key,\n",
    "    CAST(DATE_FORMAT(h.OrderDate, 'yyyyMMdd') AS INT) AS date_key,\n",
    "    d.SalesOrderID AS order_id,\n",
    "    d.SalesOrderDetailID AS order_detail_id,\n",
    "    d.OrderQty AS quantity,\n",
    "    d.UnitPrice AS unit_price,\n",
    "    d.UnitPriceDiscount AS discount,\n",
    "    d.LineTotal AS line_total,\n",
    "    h.TotalDue AS total_due,\n",
    "    h.ShipMethod AS ship_method\n",
    "FROM {catalog}.{schema}.{TABLE_PREFIX}silver_order_detail d\n",
    "JOIN {catalog}.{schema}.{TABLE_PREFIX}silver_order_header h \n",
    "    ON d.SalesOrderID = h.SalesOrderID\n",
    "LEFT JOIN {catalog}.{schema}.{TABLE_PREFIX}dim_customer dc \n",
    "    ON h.CustomerID = dc.customer_id AND dc.is_current = true\n",
    "LEFT JOIN {catalog}.{schema}.{TABLE_PREFIX}dim_product dp \n",
    "    ON d.ProductID = dp.product_id\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Fact_Sales (SQL) utworzony!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bf8d99",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## üéØ Krok 6: Weryfikacja Star Schema\n",
    "\n",
    "Sprawd≈∫my, czy nasz model Star Schema dzia≈Ça poprawnie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950212d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üéØ WERYFIKACJA STAR SCHEMA\n",
    "# ============================================================\n",
    "\n",
    "print(\"üìä PODSUMOWANIE STAR SCHEMA:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Podsumowanie tabel - 3 wymiary + 1 fakt\n",
    "tables = [\n",
    "    (\"Dim_Customer\", f\"{TABLE_PREFIX}dim_customer\"),\n",
    "    (\"Dim_Product\", f\"{TABLE_PREFIX}dim_product\"),\n",
    "    (\"Dim_Date\", f\"{TABLE_PREFIX}dim_date\"),\n",
    "    (\"Fact_Sales\", f\"{TABLE_PREFIX}fact_sales\")\n",
    "]\n",
    "\n",
    "for name, table in tables:\n",
    "    count = spark.table(f\"{catalog}.{schema}.{table}\").count()\n",
    "    print(f\"  {name}: {count:,} rekord√≥w\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde6adb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üìà PRZYK≈ÅADOWE ZAPYTANIA ANALITYCZNE\n",
    "# ============================================================\n",
    "\n",
    "print(\"üìà PRZYK≈ÅADOWE ZAPYTANIA NA STAR SCHEMA:\")\n",
    "print()\n",
    "\n",
    "# Zapytanie 1: Sprzeda≈º per klient\n",
    "print(\"1Ô∏è‚É£ TOP 10 klient√≥w wg warto≈õci sprzeda≈ºy:\")\n",
    "display(spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    c.full_name,\n",
    "    c.company_name,\n",
    "    SUM(f.line_total) AS total_sales,\n",
    "    COUNT(DISTINCT f.order_id) AS order_count\n",
    "FROM {catalog}.{schema}.{TABLE_PREFIX}fact_sales f\n",
    "JOIN {catalog}.{schema}.{TABLE_PREFIX}dim_customer c \n",
    "    ON f.customer_key = c.customer_key\n",
    "GROUP BY c.full_name, c.company_name\n",
    "ORDER BY total_sales DESC\n",
    "LIMIT 10\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc2444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zapytanie 2: Sprzeda≈º per kategoria produktu\n",
    "print(\"2Ô∏è‚É£ Sprzeda≈º per kategoria produktu:\")\n",
    "display(spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    p.category_name,\n",
    "    SUM(f.line_total) AS total_sales,\n",
    "    SUM(f.quantity) AS total_quantity,\n",
    "    AVG(f.unit_price) AS avg_price\n",
    "FROM {catalog}.{schema}.{TABLE_PREFIX}fact_sales f\n",
    "JOIN {catalog}.{schema}.{TABLE_PREFIX}dim_product p \n",
    "    ON f.product_key = p.product_key\n",
    "WHERE p.category_name IS NOT NULL\n",
    "GROUP BY p.category_name\n",
    "ORDER BY total_sales DESC\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece896a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zapytanie 3: Sprzeda≈º per miesiƒÖc (z Dim_Date)\n",
    "print(\"3Ô∏è‚É£ Trend sprzeda≈ºy miesiƒôcznej:\")\n",
    "display(spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    d.year,\n",
    "    d.month,\n",
    "    d.month_name,\n",
    "    SUM(f.line_total) AS total_sales,\n",
    "    COUNT(DISTINCT f.order_id) AS order_count\n",
    "FROM {catalog}.{schema}.{TABLE_PREFIX}fact_sales f\n",
    "JOIN {catalog}.{schema}.{TABLE_PREFIX}dim_date d \n",
    "    ON f.date_key = d.date_key\n",
    "GROUP BY d.year, d.month, d.month_name\n",
    "ORDER BY d.year, d.month\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4905d3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zapytanie 4: Analiza wielowymiarowa (CUBE)\n",
    "print(\"4Ô∏è‚É£ Analiza wielowymiarowa - kategorie i lata:\")\n",
    "display(spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    p.category_name,\n",
    "    d.year,\n",
    "    SUM(f.line_total) AS total_sales,\n",
    "    COUNT(DISTINCT f.order_id) AS order_count,\n",
    "    AVG(f.line_total) AS avg_order_value\n",
    "FROM {catalog}.{schema}.{TABLE_PREFIX}fact_sales f\n",
    "JOIN {catalog}.{schema}.{TABLE_PREFIX}dim_product p \n",
    "    ON f.product_key = p.product_key\n",
    "JOIN {catalog}.{schema}.{TABLE_PREFIX}dim_date d \n",
    "    ON f.date_key = d.date_key\n",
    "WHERE p.category_name IS NOT NULL\n",
    "GROUP BY p.category_name, d.year\n",
    "ORDER BY p.category_name, d.year\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b9493a",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# üìã ROZWIƒÑZANIE KOMPLETNE\n",
    "\n",
    "‚ö†Ô∏è **Nie patrz tutaj, dop√≥ki nie spr√≥bujesz sam!** ‚ö†Ô∏è\n",
    "\n",
    "Poni≈ºej znajdziesz kompletny kod do budowy Star Schema w dw√≥ch wersjach:\n",
    "1. **PySpark** - programistyczne podej≈õcie\n",
    "2. **SQL** - deklaratywne podej≈õcie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a939e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "# ‚ïë                                                                              ‚ïë\n",
    "# ‚ïë   üìã FULL SOLUTION - PYSPARK VERSION                                         ‚ïë\n",
    "# ‚ïë   Workshop 3: Lakeflow Pipeline - Star Schema with SCD                       ‚ïë\n",
    "# ‚ïë                                                                              ‚ïë\n",
    "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "catalog = spark.conf.get(\"spark.databricks.unityCatalog.catalog\", \"training\")\n",
    "schema = spark.conf.get(\"spark.databricks.unityCatalog.schema\", \"workshop\")\n",
    "volume_path = f\"/Volumes/{catalog}/{schema}/data\"\n",
    "TABLE_PREFIX = \"w3_\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üè≠ FULL SOLUTION - STAR SCHEMA WITH SCD (PySpark)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================\n",
    "# ü•â BRONZE LAYER\n",
    "# ============================================================\n",
    "print(\"\\nü•â BRONZE LAYER\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "source_files = [\"Customers.csv\", \"Product.csv\", \n",
    "                \"ProductCategory.csv\", \"SalesOrderHeader.csv\", \"SalesOrderDetail.csv\"]\n",
    "\n",
    "for file_name in source_files:\n",
    "    table_name = f\"{TABLE_PREFIX}bronze_{file_name.replace('.csv', '').lower()}\"\n",
    "    df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(f\"{volume_path}/{file_name}\")\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.{table_name}\")\n",
    "    print(f\"  ‚úÖ {table_name}: {df.count()} records\")\n",
    "\n",
    "# ============================================================\n",
    "# ü•à SILVER LAYER\n",
    "# ============================================================\n",
    "print(\"\\nü•à SILVER LAYER\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Silver Customers\n",
    "silver_customers = spark.table(f\"{catalog}.{schema}.{TABLE_PREFIX}bronze_customers\") \\\n",
    "    .select(col(\"CustomerID\").cast(\"int\"), \"FirstName\", \"LastName\", \"EmailAddress\", \"Phone\", \"CompanyName\", col(\"ModifiedDate\").cast(\"timestamp\")) \\\n",
    "    .withColumn(\"full_name\", concat_ws(\" \", col(\"FirstName\"), col(\"LastName\"))) \\\n",
    "    .filter(col(\"EmailAddress\").isNotNull()).dropDuplicates([\"CustomerID\"])\n",
    "silver_customers.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.{TABLE_PREFIX}silver_customers\")\n",
    "print(f\"  ‚úÖ silver_customers: {silver_customers.count()} records\")\n",
    "\n",
    "# Silver Products (with category)\n",
    "bronze_products = spark.table(f\"{catalog}.{schema}.{TABLE_PREFIX}bronze_product\")\n",
    "bronze_categories = spark.table(f\"{catalog}.{schema}.{TABLE_PREFIX}bronze_productcategory\")\n",
    "silver_products = bronze_products.join(bronze_categories, bronze_products.ProductCategoryID == bronze_categories.ProductCategoryID, \"left\") \\\n",
    "    .select(bronze_products.ProductID.cast(\"int\"), bronze_products.Name.alias(\"product_name\"), bronze_products.ProductNumber, \n",
    "            bronze_products.Color, bronze_products.ListPrice.cast(\"decimal(18,2)\"), bronze_products.StandardCost.cast(\"decimal(18,2)\"),\n",
    "            bronze_categories.Name.alias(\"category_name\"), bronze_products.ModifiedDate.cast(\"timestamp\")) \\\n",
    "    .filter(col(\"ProductID\").isNotNull()).dropDuplicates([\"ProductID\"])\n",
    "silver_products.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.{TABLE_PREFIX}silver_products\")\n",
    "print(f\"  ‚úÖ silver_products: {silver_products.count()} records\")\n",
    "\n",
    "# Silver Orders\n",
    "bronze_orders = spark.table(f\"{catalog}.{schema}.{TABLE_PREFIX}bronze_salesorderheader\")\n",
    "bronze_details = spark.table(f\"{catalog}.{schema}.{TABLE_PREFIX}bronze_salesorderdetail\")\n",
    "\n",
    "silver_order_header = bronze_orders.select(col(\"SalesOrderID\").cast(\"int\"), col(\"CustomerID\").cast(\"int\"),\n",
    "    col(\"OrderDate\").cast(\"date\"), col(\"ShipDate\").cast(\"date\"), col(\"TotalDue\").cast(\"decimal(18,2)\"), col(\"Status\").cast(\"int\"), \"ShipMethod\") \\\n",
    "    .filter(col(\"SalesOrderID\").isNotNull())\n",
    "silver_order_header.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.{TABLE_PREFIX}silver_order_header\")\n",
    "print(f\"  ‚úÖ silver_order_header: {silver_order_header.count()} records\")\n",
    "\n",
    "silver_order_detail = bronze_details.select(col(\"SalesOrderID\").cast(\"int\"), col(\"SalesOrderDetailID\").cast(\"int\"), col(\"ProductID\").cast(\"int\"),\n",
    "    col(\"OrderQty\").cast(\"int\"), col(\"UnitPrice\").cast(\"decimal(18,2)\"), col(\"UnitPriceDiscount\").cast(\"decimal(5,2)\"), col(\"LineTotal\").cast(\"decimal(18,2)\")) \\\n",
    "    .filter(col(\"SalesOrderDetailID\").isNotNull())\n",
    "silver_order_detail.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.{TABLE_PREFIX}silver_order_detail\")\n",
    "print(f\"  ‚úÖ silver_order_detail: {silver_order_detail.count()} records\")\n",
    "\n",
    "# ============================================================\n",
    "# ü•á GOLD LAYER - DIMENSIONS\n",
    "# ============================================================\n",
    "print(\"\\nü•á GOLD LAYER - DIMENSIONS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Dim_Product (SCD Type 1)\n",
    "dim_product = spark.table(f\"{catalog}.{schema}.{TABLE_PREFIX}silver_products\") \\\n",
    "    .withColumn(\"product_key\", monotonically_increasing_id() + 1) \\\n",
    "    .select(col(\"product_key\"), col(\"ProductID\").alias(\"product_id\"), col(\"product_name\"), col(\"ProductNumber\").alias(\"product_number\"),\n",
    "            col(\"Color\").alias(\"color\"), col(\"category_name\"), col(\"ListPrice\").alias(\"list_price\"), col(\"StandardCost\").alias(\"standard_cost\"),\n",
    "            current_timestamp().alias(\"load_timestamp\"))\n",
    "dim_product.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.{TABLE_PREFIX}dim_product\")\n",
    "print(f\"  ‚úÖ dim_product (SCD1): {dim_product.count()} records\")\n",
    "\n",
    "# Dim_Date (SCD Type 1)\n",
    "start_date, end_date = datetime(2022, 1, 1), datetime(2026, 12, 31)\n",
    "date_range = [(start_date + timedelta(days=x)).strftime('%Y-%m-%d') for x in range((end_date - start_date).days + 1)]\n",
    "dim_date = spark.createDataFrame([(d,) for d in date_range], [\"full_date\"]) \\\n",
    "    .withColumn(\"full_date\", col(\"full_date\").cast(\"date\")) \\\n",
    "    .withColumn(\"date_key\", date_format(col(\"full_date\"), \"yyyyMMdd\").cast(\"int\")) \\\n",
    "    .withColumn(\"year\", year(col(\"full_date\"))).withColumn(\"month\", month(col(\"full_date\"))).withColumn(\"day\", dayofmonth(col(\"full_date\"))) \\\n",
    "    .withColumn(\"quarter\", quarter(col(\"full_date\"))).withColumn(\"day_of_week\", dayofweek(col(\"full_date\"))) \\\n",
    "    .withColumn(\"day_name\", date_format(col(\"full_date\"), \"EEEE\")).withColumn(\"month_name\", date_format(col(\"full_date\"), \"MMMM\")) \\\n",
    "    .withColumn(\"is_weekend\", when(dayofweek(col(\"full_date\")).isin([1, 7]), True).otherwise(False)) \\\n",
    "    .select(\"date_key\", \"full_date\", \"year\", \"month\", \"month_name\", \"day\", \"quarter\", \"day_of_week\", \"day_name\", \"is_weekend\")\n",
    "dim_date.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.{TABLE_PREFIX}dim_date\")\n",
    "print(f\"  ‚úÖ dim_date (SCD1): {dim_date.count()} records\")\n",
    "\n",
    "# Dim_Customer (SCD Type 2)\n",
    "dim_customer = spark.table(f\"{catalog}.{schema}.{TABLE_PREFIX}silver_customers\") \\\n",
    "    .withColumn(\"customer_key\", monotonically_increasing_id() + 1) \\\n",
    "    .withColumn(\"is_current\", lit(True)).withColumn(\"valid_from\", current_date()).withColumn(\"valid_to\", lit(\"9999-12-31\").cast(\"date\")) \\\n",
    "    .select(col(\"customer_key\"), col(\"CustomerID\").alias(\"customer_id\"), col(\"full_name\"), col(\"FirstName\").alias(\"first_name\"),\n",
    "            col(\"LastName\").alias(\"last_name\"), col(\"EmailAddress\").alias(\"email\"), col(\"Phone\").alias(\"phone\"),\n",
    "            col(\"CompanyName\").alias(\"company_name\"), col(\"is_current\"), col(\"valid_from\"), col(\"valid_to\"))\n",
    "dim_customer.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.{TABLE_PREFIX}dim_customer\")\n",
    "print(f\"  ‚úÖ dim_customer (SCD2): {dim_customer.count()} records\")\n",
    "\n",
    "# ============================================================\n",
    "# ü•á GOLD LAYER - FACT_SALES\n",
    "# ============================================================\n",
    "print(\"\\nü•á GOLD LAYER - FACT TABLE\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "silver_order_header = spark.table(f\"{catalog}.{schema}.{TABLE_PREFIX}silver_order_header\")\n",
    "silver_order_detail = spark.table(f\"{catalog}.{schema}.{TABLE_PREFIX}silver_order_detail\")\n",
    "dim_customer = spark.table(f\"{catalog}.{schema}.{TABLE_PREFIX}dim_customer\").filter(col(\"is_current\") == True)\n",
    "dim_product = spark.table(f\"{catalog}.{schema}.{TABLE_PREFIX}dim_product\")\n",
    "\n",
    "orders_combined = silver_order_detail.alias(\"d\").join(silver_order_header.alias(\"h\"), \"SalesOrderID\")\n",
    "\n",
    "fact_sales = orders_combined \\\n",
    "    .join(dim_customer, orders_combined[\"CustomerID\"] == dim_customer[\"customer_id\"], \"left\") \\\n",
    "    .join(dim_product, orders_combined[\"ProductID\"] == dim_product[\"product_id\"], \"left\") \\\n",
    "    .withColumn(\"date_key\", date_format(col(\"OrderDate\"), \"yyyyMMdd\").cast(\"int\")) \\\n",
    "    .withColumn(\"sales_key\", monotonically_increasing_id() + 1) \\\n",
    "    .select(col(\"sales_key\"), col(\"customer_key\"), col(\"product_key\"), col(\"date_key\"),\n",
    "            col(\"SalesOrderID\").alias(\"order_id\"), col(\"SalesOrderDetailID\").alias(\"order_detail_id\"),\n",
    "            col(\"OrderQty\").alias(\"quantity\"), col(\"UnitPrice\").alias(\"unit_price\"), col(\"UnitPriceDiscount\").alias(\"discount\"),\n",
    "            col(\"LineTotal\").alias(\"line_total\"), col(\"TotalDue\").alias(\"total_due\"), col(\"ShipMethod\").alias(\"ship_method\"))\n",
    "\n",
    "fact_sales.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.{TABLE_PREFIX}fact_sales\")\n",
    "print(f\"  ‚úÖ fact_sales: {fact_sales.count()} records\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéâ STAR SCHEMA COMPLETE!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eef585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "# ‚ïë                                                                              ‚ïë\n",
    "# ‚ïë   üìã FULL SOLUTION - SQL VERSION                                             ‚ïë\n",
    "# ‚ïë   Workshop 3: Lakeflow Pipeline - Star Schema with SCD                       ‚ïë\n",
    "# ‚ïë                                                                              ‚ïë\n",
    "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "catalog = spark.conf.get(\"spark.databricks.unityCatalog.catalog\", \"training\")\n",
    "schema = spark.conf.get(\"spark.databricks.unityCatalog.schema\", \"workshop\")\n",
    "volume_path = f\"/Volumes/{catalog}/{schema}/data\"\n",
    "TABLE_PREFIX = \"w3_sql_\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üè≠ FULL SOLUTION - STAR SCHEMA WITH SCD (SQL)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================\n",
    "# ü•â BRONZE LAYER (SQL)\n",
    "# ============================================================\n",
    "print(\"\\nü•â BRONZE LAYER\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "source_files = {\n",
    "    \"Customers\": \"Customers.csv\",\n",
    "    \"Product\": \"Product.csv\",\n",
    "    \"ProductCategory\": \"ProductCategory.csv\",\n",
    "    \"SalesOrderHeader\": \"SalesOrderHeader.csv\",\n",
    "    \"SalesOrderDetail\": \"SalesOrderDetail.csv\"\n",
    "}\n",
    "\n",
    "for table_name, file_name in source_files.items():\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE {catalog}.{schema}.{TABLE_PREFIX}bronze_{table_name.lower()}\n",
    "        USING CSV\n",
    "        OPTIONS (\n",
    "            path '{volume_path}/{file_name}',\n",
    "            header 'true',\n",
    "            inferSchema 'true'\n",
    "        )\n",
    "    \"\"\")\n",
    "    count = spark.table(f\"{catalog}.{schema}.{TABLE_PREFIX}bronze_{table_name.lower()}\").count()\n",
    "    print(f\"  ‚úÖ bronze_{table_name.lower()}: {count} records\")\n",
    "\n",
    "# ============================================================\n",
    "# ü•à SILVER LAYER (SQL)\n",
    "# ============================================================\n",
    "print(\"\\nü•à SILVER LAYER\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Silver Customers\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {catalog}.{schema}.{TABLE_PREFIX}silver_customers AS\n",
    "SELECT \n",
    "    CAST(CustomerID AS INT) AS CustomerID,\n",
    "    FirstName,\n",
    "    LastName,\n",
    "    CONCAT_WS(' ', FirstName, LastName) AS full_name,\n",
    "    EmailAddress,\n",
    "    Phone,\n",
    "    CompanyName,\n",
    "    CAST(ModifiedDate AS TIMESTAMP) AS ModifiedDate\n",
    "FROM {catalog}.{schema}.{TABLE_PREFIX}bronze_customers\n",
    "WHERE EmailAddress IS NOT NULL\n",
    "\"\"\")\n",
    "print(f\"  ‚úÖ silver_customers: {spark.table(f'{catalog}.{schema}.{TABLE_PREFIX}silver_customers').count()} records\")\n",
    "\n",
    "# Silver Products (with category)\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {catalog}.{schema}.{TABLE_PREFIX}silver_products AS\n",
    "SELECT \n",
    "    CAST(p.ProductID AS INT) AS ProductID,\n",
    "    p.Name AS product_name,\n",
    "    p.ProductNumber,\n",
    "    p.Color,\n",
    "    CAST(p.ListPrice AS DECIMAL(18,2)) AS ListPrice,\n",
    "    CAST(p.StandardCost AS DECIMAL(18,2)) AS StandardCost,\n",
    "    c.Name AS category_name,\n",
    "    CAST(p.ModifiedDate AS TIMESTAMP) AS ModifiedDate\n",
    "FROM {catalog}.{schema}.{TABLE_PREFIX}bronze_product p\n",
    "LEFT JOIN {catalog}.{schema}.{TABLE_PREFIX}bronze_productcategory c \n",
    "    ON p.ProductCategoryID = c.ProductCategoryID\n",
    "WHERE p.ProductID IS NOT NULL\n",
    "\"\"\")\n",
    "print(f\"  ‚úÖ silver_products: {spark.table(f'{catalog}.{schema}.{TABLE_PREFIX}silver_products').count()} records\")\n",
    "\n",
    "# Silver Order Header\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {catalog}.{schema}.{TABLE_PREFIX}silver_order_header AS\n",
    "SELECT \n",
    "    CAST(SalesOrderID AS INT) AS SalesOrderID,\n",
    "    CAST(CustomerID AS INT) AS CustomerID,\n",
    "    CAST(OrderDate AS DATE) AS OrderDate,\n",
    "    CAST(ShipDate AS DATE) AS ShipDate,\n",
    "    CAST(TotalDue AS DECIMAL(18,2)) AS TotalDue,\n",
    "    CAST(Status AS INT) AS Status,\n",
    "    ShipMethod\n",
    "FROM {catalog}.{schema}.{TABLE_PREFIX}bronze_salesorderheader\n",
    "WHERE SalesOrderID IS NOT NULL\n",
    "\"\"\")\n",
    "print(f\"  ‚úÖ silver_order_header: {spark.table(f'{catalog}.{schema}.{TABLE_PREFIX}silver_order_header').count()} records\")\n",
    "\n",
    "# Silver Order Detail\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {catalog}.{schema}.{TABLE_PREFIX}silver_order_detail AS\n",
    "SELECT \n",
    "    CAST(SalesOrderID AS INT) AS SalesOrderID,\n",
    "    CAST(SalesOrderDetailID AS INT) AS SalesOrderDetailID,\n",
    "    CAST(ProductID AS INT) AS ProductID,\n",
    "    CAST(OrderQty AS INT) AS OrderQty,\n",
    "    CAST(UnitPrice AS DECIMAL(18,2)) AS UnitPrice,\n",
    "    CAST(UnitPriceDiscount AS DECIMAL(5,2)) AS UnitPriceDiscount,\n",
    "    CAST(LineTotal AS DECIMAL(18,2)) AS LineTotal\n",
    "FROM {catalog}.{schema}.{TABLE_PREFIX}bronze_salesorderdetail\n",
    "WHERE SalesOrderDetailID IS NOT NULL\n",
    "\"\"\")\n",
    "print(f\"  ‚úÖ silver_order_detail: {spark.table(f'{catalog}.{schema}.{TABLE_PREFIX}silver_order_detail').count()} records\")\n",
    "\n",
    "# ============================================================\n",
    "# ü•á GOLD LAYER - DIMENSIONS (SQL)\n",
    "# ============================================================\n",
    "print(\"\\nü•á GOLD LAYER - DIMENSIONS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Dim_Product (SCD Type 1)\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {catalog}.{schema}.{TABLE_PREFIX}dim_product AS\n",
    "SELECT \n",
    "    ROW_NUMBER() OVER (ORDER BY ProductID) AS product_key,\n",
    "    ProductID AS product_id,\n",
    "    product_name,\n",
    "    ProductNumber AS product_number,\n",
    "    Color AS color,\n",
    "    category_name,\n",
    "    ListPrice AS list_price,\n",
    "    StandardCost AS standard_cost,\n",
    "    current_timestamp() AS load_timestamp\n",
    "FROM {catalog}.{schema}.{TABLE_PREFIX}silver_products\n",
    "\"\"\")\n",
    "print(f\"  ‚úÖ dim_product (SCD1): {spark.table(f'{catalog}.{schema}.{TABLE_PREFIX}dim_product').count()} records\")\n",
    "\n",
    "# Dim_Date (SCD Type 1)\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {catalog}.{schema}.{TABLE_PREFIX}dim_date AS\n",
    "SELECT \n",
    "    CAST(DATE_FORMAT(date_col, 'yyyyMMdd') AS INT) AS date_key,\n",
    "    date_col AS full_date,\n",
    "    YEAR(date_col) AS year,\n",
    "    MONTH(date_col) AS month,\n",
    "    DATE_FORMAT(date_col, 'MMMM') AS month_name,\n",
    "    DAY(date_col) AS day,\n",
    "    QUARTER(date_col) AS quarter,\n",
    "    DAYOFWEEK(date_col) AS day_of_week,\n",
    "    DATE_FORMAT(date_col, 'EEEE') AS day_name,\n",
    "    CASE WHEN DAYOFWEEK(date_col) IN (1, 7) THEN true ELSE false END AS is_weekend\n",
    "FROM (\n",
    "    SELECT EXPLODE(SEQUENCE(DATE '2022-01-01', DATE '2026-12-31', INTERVAL 1 DAY)) AS date_col\n",
    ")\n",
    "\"\"\")\n",
    "print(f\"  ‚úÖ dim_date (SCD1): {spark.table(f'{catalog}.{schema}.{TABLE_PREFIX}dim_date').count()} records\")\n",
    "\n",
    "# Dim_Customer (SCD Type 2)\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {catalog}.{schema}.{TABLE_PREFIX}dim_customer AS\n",
    "SELECT \n",
    "    ROW_NUMBER() OVER (ORDER BY CustomerID) AS customer_key,\n",
    "    CustomerID AS customer_id,\n",
    "    full_name,\n",
    "    FirstName AS first_name,\n",
    "    LastName AS last_name,\n",
    "    EmailAddress AS email,\n",
    "    Phone AS phone,\n",
    "    CompanyName AS company_name,\n",
    "    true AS is_current,\n",
    "    current_date() AS valid_from,\n",
    "    DATE '9999-12-31' AS valid_to\n",
    "FROM {catalog}.{schema}.{TABLE_PREFIX}silver_customers\n",
    "\"\"\")\n",
    "print(f\"  ‚úÖ dim_customer (SCD2): {spark.table(f'{catalog}.{schema}.{TABLE_PREFIX}dim_customer').count()} records\")\n",
    "\n",
    "# ============================================================\n",
    "# ü•á GOLD LAYER - FACT_SALES (SQL)\n",
    "# ============================================================\n",
    "print(\"\\nü•á GOLD LAYER - FACT TABLE\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {catalog}.{schema}.{TABLE_PREFIX}fact_sales AS\n",
    "SELECT \n",
    "    ROW_NUMBER() OVER (ORDER BY d.SalesOrderID, d.SalesOrderDetailID) AS sales_key,\n",
    "    dc.customer_key,\n",
    "    dp.product_key,\n",
    "    CAST(DATE_FORMAT(h.OrderDate, 'yyyyMMdd') AS INT) AS date_key,\n",
    "    d.SalesOrderID AS order_id,\n",
    "    d.SalesOrderDetailID AS order_detail_id,\n",
    "    d.OrderQty AS quantity,\n",
    "    d.UnitPrice AS unit_price,\n",
    "    d.UnitPriceDiscount AS discount,\n",
    "    d.LineTotal AS line_total,\n",
    "    h.TotalDue AS total_due,\n",
    "    h.ShipMethod AS ship_method\n",
    "FROM {catalog}.{schema}.{TABLE_PREFIX}silver_order_detail d\n",
    "JOIN {catalog}.{schema}.{TABLE_PREFIX}silver_order_header h \n",
    "    ON d.SalesOrderID = h.SalesOrderID\n",
    "LEFT JOIN {catalog}.{schema}.{TABLE_PREFIX}dim_customer dc \n",
    "    ON h.CustomerID = dc.customer_id AND dc.is_current = true\n",
    "LEFT JOIN {catalog}.{schema}.{TABLE_PREFIX}dim_product dp \n",
    "    ON d.ProductID = dp.product_id\n",
    "\"\"\")\n",
    "print(f\"  ‚úÖ fact_sales: {spark.table(f'{catalog}.{schema}.{TABLE_PREFIX}fact_sales').count()} records\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéâ STAR SCHEMA (SQL VERSION) COMPLETE!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49a76e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "# ‚ïë                                                                              ‚ïë\n",
    "# ‚ïë   üìã BONUS: SCD TYPE 2 MERGE - AKTUALIZACJA WYMIARU                          ‚ïë\n",
    "# ‚ïë                                                                              ‚ïë\n",
    "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "print(\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë  SCD TYPE 2 - PATTERN AKTUALIZACJI Z DELTA MERGE                             ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "Gdy przychodzƒÖ nowe dane klient√≥w (np. klient zmieni≈Ç firmƒô), wykonujemy \n",
    "dwuetapowy proces MERGE:\n",
    "\n",
    "KROK 1: Zamknij stare rekordy (ustaw is_current=false, valid_to=data-1)\n",
    "========================================================================\n",
    "\n",
    "MERGE INTO dim_customer AS target\n",
    "USING (\n",
    "    SELECT * FROM updated_customers\n",
    "    WHERE EXISTS (\n",
    "        SELECT 1 FROM dim_customer d \n",
    "        WHERE d.customer_id = updated_customers.customer_id \n",
    "        AND d.is_current = true\n",
    "        AND (d.company_name != updated_customers.company_name \n",
    "             OR d.email != updated_customers.email)\n",
    "    )\n",
    ") AS source\n",
    "ON target.customer_id = source.customer_id \n",
    "   AND target.is_current = true\n",
    "WHEN MATCHED THEN UPDATE SET\n",
    "    target.is_current = false,\n",
    "    target.valid_to = current_date() - INTERVAL 1 DAY\n",
    "\n",
    "\n",
    "KROK 2: Wstaw nowe wersje rekord√≥w\n",
    "==================================\n",
    "\n",
    "INSERT INTO dim_customer\n",
    "SELECT \n",
    "    (SELECT MAX(customer_key) + ROW_NUMBER() OVER (ORDER BY customer_id) \n",
    "     FROM dim_customer) AS customer_key,\n",
    "    customer_id,\n",
    "    full_name,\n",
    "    first_name,\n",
    "    last_name,\n",
    "    email,\n",
    "    phone,\n",
    "    company_name,\n",
    "    true AS is_current,\n",
    "    current_date() AS valid_from,\n",
    "    DATE '9999-12-31' AS valid_to\n",
    "FROM updated_customers source\n",
    "WHERE EXISTS (\n",
    "    SELECT 1 FROM dim_customer target \n",
    "    WHERE target.customer_id = source.customer_id \n",
    "    AND target.is_current = false\n",
    "    AND target.valid_to = current_date() - INTERVAL 1 DAY\n",
    ")\n",
    "\n",
    "\n",
    "ALTERNATYWNIE - JEDEN MERGE Z WIELOMA AKCJAMI (Databricks):\n",
    "===========================================================\n",
    "\n",
    "MERGE INTO dim_customer AS target\n",
    "USING updated_customers AS source\n",
    "ON target.customer_id = source.customer_id AND target.is_current = true\n",
    "\n",
    "-- Zamknij stary rekord gdy sƒÖ zmiany\n",
    "WHEN MATCHED AND (target.company_name != source.company_name) THEN\n",
    "    UPDATE SET \n",
    "        is_current = false,\n",
    "        valid_to = current_date() - INTERVAL 1 DAY\n",
    "\n",
    "-- Wstaw nowy rekord (jako second pass)\n",
    "WHEN NOT MATCHED THEN\n",
    "    INSERT (customer_key, customer_id, full_name, first_name, last_name, \n",
    "            email, phone, company_name, is_current, valid_from, valid_to)\n",
    "    VALUES (\n",
    "        (SELECT COALESCE(MAX(customer_key), 0) + 1 FROM dim_customer),\n",
    "        source.customer_id, source.full_name, source.first_name, source.last_name,\n",
    "        source.email, source.phone, source.company_name,\n",
    "        true, current_date(), DATE '9999-12-31'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n‚úÖ Workshop 3 zako≈Ñczony!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"üìä Stworzy≈Çe≈õ kompletny model Star Schema z:\")\n",
    "print(\"   ‚Ä¢ 3 wymiarami (Dim_Customer, Dim_Product, Dim_Date)\")\n",
    "print(\"   ‚Ä¢ 1 tabelƒÖ fakt√≥w (Fact_Sales)\")\n",
    "print(\"   ‚Ä¢ SCD Type 1 (Dim_Product, Dim_Date)\")\n",
    "print(\"   ‚Ä¢ SCD Type 2 (Dim_Customer)\")\n",
    "print(\"   ‚Ä¢ ImplementacjƒÖ w PySpark i SQL\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}