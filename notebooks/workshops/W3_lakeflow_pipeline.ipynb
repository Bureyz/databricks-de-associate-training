{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "461d7c31",
   "metadata": {},
   "source": [
    "# Workshop 3: Lakeflow Pipelines\n",
    "\n",
    "## The Story\n",
    "\n",
    "Our retail company, \"BikeSuperstore\", is modernizing its data platform. We have raw CSV files landing in our data lake (Orders, Customers, Products), and the business intelligence team needs a reliable, up-to-date **Star Schema** to report on sales performance.\n",
    "\n",
    "**The Challenge:**\n",
    "Traditional ETL pipelines are brittle and hard to maintain. We need to use **Databricks Lakeflow (Spark Declarative Pipelines)** to build a robust system that handles:\n",
    "1.  **Ingestion**: Automatically loading new files.\n",
    "2.  **History**: Tracking changes in customer data (SCD Type 2).\n",
    "3.  **Quality**: Enforcing data quality rules.\n",
    "4.  **Modeling**: Creating a Gold layer with a Star Schema.\n",
    "\n",
    "**Your Mission:**\n",
    "You will implement the pipeline definition. You can choose to work in **SQL** or **Python** (or both!). The code provided has some \"blanks\" that you need to fill in to make the pipeline work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e56be4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da289265",
   "metadata": {},
   "source": [
    "## Business Logic & Architecture\n",
    "\n",
    "We are following the **Medallion Architecture**:\n",
    "\n",
    "1.  **Bronze Layer (Raw)**:\n",
    "    *   Ingest CSV files from `Customers`, `Product`, `ProductCategory`, `SalesOrderHeader`, `SalesOrderDetail`.\n",
    "    *   Use **Auto Loader** (`cloud_files`) for efficient incremental ingestion.\n",
    "\n",
    "2.  **Silver Layer (Cleaned & Enriched)**:\n",
    "    *   **Customers**: Apply **SCD Type 2** (Slowly Changing Dimensions) to track history based on `ModifiedDate`.\n",
    "    *   **Products & Categories**: Apply **SCD Type 1** (Overwrite) to keep the latest info.\n",
    "    *   **Orders**: Clean data and apply **Data Quality Expectations** (e.g., `TotalDue > 0`).\n",
    "\n",
    "3.  **Gold Layer (Star Schema)**:\n",
    "    *   **Fact Table**: `fact_sales` (Transactions).\n",
    "    *   **Dimensions**:\n",
    "        *   `dim_customer`: Current customer details.\n",
    "        *   `dim_product`: Product details enriched with Category names.\n",
    "        *   `dim_date`: A generated calendar dimension for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3965b249",
   "metadata": {},
   "source": [
    "![assets/images/ibDi2xO_FDNG-IhAmH5CO.png](../../assets/images/ibDi2xO_FDNG-IhAmH5CO.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdc1f4d",
   "metadata": {},
   "source": [
    "## Choose Your Weapon: SQL or Python?\n",
    "\n",
    "Lakeflow SDP supports both languages.\n",
    "*   **Section A**: SQL Implementation (Standard for analysts/engineers).\n",
    "*   **Section B**: Python Implementation (Great for complex logic and metaprogramming).\n",
    "\n",
    "\n",
    "*   **source_path** : /Volumes/ecommerce_platform_<your_user_name>/default/datasets/workshop\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18008564",
   "metadata": {},
   "source": [
    "# SECTION A: SQL Implementation\n",
    "\n",
    "In this section, you will complete the SQL DDL statements to define the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d70ab4",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- ============================================================\n",
    "-- 1. BRONZE LAYER: Ingestion\n",
    "-- ============================================================\n",
    "-- TODO: Complete the Auto Loader syntax to read CSV files.\n",
    "-- Hint: Use cloud_files function. We need to infer schema and read headers.\n",
    "\n",
    "-- Bronze Customers\n",
    "CREATE OR REFRESH STREAMING TABLE bronze_customers\n",
    "COMMENT 'Raw customers data from CSV'\n",
    "AS SELECT * FROM ___('${source_path}/Customers', 'csv', map(\"header\", \"true\", \"inferSchema\", \"___\"));\n",
    "\n",
    "-- Bronze Products\n",
    "CREATE OR REFRESH STREAMING TABLE bronze_products\n",
    "COMMENT 'Raw products data from CSV'\n",
    "AS SELECT * FROM cloud_files('${source_path}/___', 'csv', map(\"header\", \"true\", \"inferSchema\", \"___\"));\n",
    "\n",
    "-- [TASK] Complete the code for Product Categories\n",
    "-- Bronze Product Categories\n",
    "CREATE OR REFRESH STREAMING TABLE bronze_product_categories\n",
    "COMMENT 'Raw product categories data from CSV'\n",
    "AS SELECT * FROM cloud_files('${source_path}/ProductCategory', '___', map(___));\n",
    "\n",
    "-- [TASK] Complete the code for Orders Header\n",
    "-- Bronze Orders Header\n",
    "CREATE OR REFRESH STREAMING TABLE bronze_orders_header\n",
    "COMMENT 'Raw orders data from CSV'\n",
    "AS SELECT * FROM cloud_files('${source_path}/SalesOrderHeader', '___', map(___));\n",
    "\n",
    "-- [TASK] Complete the code for Orders Detail\n",
    "-- Bronze Orders Detail\n",
    "CREATE OR REFRESH STREAMING TABLE bronze_orders_detail\n",
    "COMMENT 'Raw orders data from CSV'\n",
    "AS SELECT * FROM ___('${source_path}/___', 'csv', map(\"header\", \"true\", \"inferSchema\", \"true\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6c8350",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- ============================================================\n",
    "-- 2. SILVER LAYER: SCD & Quality\n",
    "-- ============================================================\n",
    "\n",
    "-- [TASK] Implement SCD Type 2 for Customers\n",
    "-- We need to track history using 'ModifiedDate'\n",
    "CREATE OR REFRESH STREAMING TABLE silver_customers;\n",
    "\n",
    "AUTO CDC INTO silver_customers\n",
    "FROM bronze_customers\n",
    "KEYS (CustomerID)\n",
    "SEQUENCE BY ___\n",
    "STORED AS SCD TYPE ___;\n",
    "\n",
    "-- [TASK] Implement SCD Type 1 for Products\n",
    "CREATE OR REFRESH STREAMING TABLE silver_products;\n",
    "\n",
    "AUTO CDC INTO silver_products\n",
    "FROM bronze_products\n",
    "KEYS (___)\n",
    "SEQUENCE BY ___\n",
    "STORED AS SCD TYPE 1;\n",
    "\n",
    "-- [TASK] Implement SCD Type 1 for Product Categories\n",
    "CREATE OR REFRESH STREAMING TABLE silver_product_categories;\n",
    "\n",
    "AUTO CDC INTO silver_product_categories\n",
    "FROM bronze_product_categories\n",
    "KEYS (ProductCategoryID)\n",
    "SEQUENCE BY ___\n",
    "STORED AS SCD TYPE ___;\n",
    "\n",
    "-- [TASK] Add Data Quality Expectations for Orders\n",
    "-- Ensure TotalDue is greater than 0 (DROP ROW on violation)\n",
    "-- Ensure CustomerID is not null (FAIL UPDATE on violation)\n",
    "CREATE OR REFRESH STREAMING TABLE silver_orders\n",
    "(\n",
    "  CONSTRAINT valid_amount EXPECT (___) ON VIOLATION DROP ROW,\n",
    "  CONSTRAINT valid_customer EXPECT (CustomerID IS NOT NULL) ON VIOLATION ___\n",
    ")\n",
    "AS SELECT \n",
    "  SalesOrderID, CustomerID, TotalDue, OrderDate, Status, current_timestamp() as processed_at\n",
    "FROM STREAM(bronze_orders_header);\n",
    "\n",
    "-- [TASK] Create Silver Order Details\n",
    "-- Just a simple pass-through with some cleaning if needed\n",
    "CREATE OR REFRESH STREAMING TABLE silver_order_details\n",
    "AS SELECT \n",
    "  SalesOrderID, ___, OrderQty, ProductID, UnitPrice, LineTotal, ModifiedDate\n",
    "FROM STREAM(___);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c193b9",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- ============================================================\n",
    "-- 3. GOLD LAYER: Star Schema\n",
    "-- ============================================================\n",
    "\n",
    "-- [TASK] Create the Customer Dimension\n",
    "-- Filter out old records (SCD Type 2) using __END_AT\n",
    "CREATE OR REFRESH MATERIALIZED VIEW dim_customer\n",
    "AS SELECT \n",
    "  CustomerID, FirstName, LastName, EmailAddress, Phone\n",
    "FROM ___\n",
    "WHERE ___ IS NULL;\n",
    "\n",
    "-- [TASK] Create the Product Dimension\n",
    "-- Join silver_products with silver_product_categories to get CategoryName\n",
    "CREATE OR REFRESH MATERIALIZED VIEW dim_product\n",
    "AS SELECT \n",
    "  p.ProductID,\n",
    "  p.Name as ProductName,\n",
    "  pc.Name as CategoryName\n",
    "FROM silver_products p\n",
    "LEFT JOIN ___ pc ON p.ProductCategoryID = ___;\n",
    "\n",
    "-- [TASK] Create the Date Dimension\n",
    "-- Extract Year, Month, Day from OrderDate in silver_orders\n",
    "CREATE OR REFRESH MATERIALIZED VIEW dim_date\n",
    "AS SELECT ___\n",
    "  cast(OrderDate as date) as DateKey,\n",
    "  year(OrderDate) as Year,\n",
    "  month(OrderDate) as Month,\n",
    "  ___(OrderDate) as Day\n",
    "FROM silver_orders;\n",
    "\n",
    "-- [TASK] Create the Fact Table\n",
    "-- Join Order Headers and Details\n",
    "CREATE OR REFRESH MATERIALIZED VIEW fact_sales\n",
    "AS SELECT \n",
    "  od.SalesOrderID,\n",
    "  oh.OrderDate,\n",
    "  oh.CustomerID,\n",
    "  od.ProductID,\n",
    "  od.LineTotal\n",
    "FROM silver_order_details od\n",
    "JOIN silver_orders oh ON ___ = ___;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dc0e58",
   "metadata": {},
   "source": [
    "# SECTION B: Python Implementation\n",
    "\n",
    "In this section, you will use the `pyspark.pipelines` (dp) API to define the same logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b06e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.pipelines as dp\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "source_path = spark.conf.get(\"source_path\")\n",
    "\n",
    "# ============================================================\n",
    "# 1. BRONZE LAYER\n",
    "# ============================================================\n",
    "\n",
    "# Bronze Customers\n",
    "dp.create_streaming_table(name=\"bronze_customers\")\n",
    "\n",
    "@dp.append_flow(target=\"bronze_customers\")\n",
    "def bronze_customers_flow():\n",
    "    return (\n",
    "        spark.readStream.format(\"___\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(f\"{source_path}/___\")\n",
    "    )\n",
    "\n",
    "# [TASK] Bronze Products\n",
    "dp.create_streaming_table(name=\"bronze_products\")\n",
    "\n",
    "@dp.append_flow(target=\"bronze_products\")\n",
    "def bronze_products_flow():\n",
    "    return (\n",
    "        spark.readStream.format(\"___\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(f\"{source_path}/___\")\n",
    "    )\n",
    "\n",
    "# [TASK] Bronze Product Categories\n",
    "dp.create_streaming_table(name=\"bronze_product_categories\")\n",
    "\n",
    "@dp.append_flow(target=\"bronze_product_categories\")\n",
    "def bronze_product_categories_flow():\n",
    "    return (\n",
    "        spark.readStream.format(\"___\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(f\"{source_path}/___\")\n",
    "    )\n",
    "\n",
    "# [TASK] Complete the Bronze Orders Header flow\n",
    "dp.create_streaming_table(name=\"bronze_orders_header\")\n",
    "\n",
    "@dp.append_flow(target=\"bronze_orders_header\")\n",
    "def bronze_orders_header_flow():\n",
    "    return (\n",
    "        spark.readStream.format(\"___\") # Hint: Auto Loader format\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(f\"{source_path}/___\")\n",
    "    )\n",
    "\n",
    "# [TASK] Bronze Orders Detail\n",
    "dp.create_streaming_table(name=\"bronze_orders_detail\")\n",
    "\n",
    "@dp.append_flow(target=\"bronze_orders_detail\")\n",
    "def bronze_orders_detail_flow():\n",
    "    return (\n",
    "        spark.readStream.format(\"___\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(f\"{source_path}/___\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916f32fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2. SILVER LAYER\n",
    "# ============================================================\n",
    "\n",
    "# [TASK] Define SCD Type 2 for Customers\n",
    "dp.create_streaming_table(name=\"silver_customers\")\n",
    "\n",
    "dp.create_auto_cdc_flow(\n",
    "    target=\"silver_customers\",\n",
    "    source=\"bronze_customers\",\n",
    "    keys=[\"___\"],\n",
    "    sequence_by=col(\"___\"), # What column tracks the change time?\n",
    "    stored_as_scd_type=\"___\" # Type 1 or 2?\n",
    ")\n",
    "\n",
    "# [TASK] Define SCD Type 1 for Products\n",
    "dp.create_streaming_table(name=\"silver_products\")\n",
    "\n",
    "dp.create_auto_cdc_flow(\n",
    "    target=\"silver_products\",\n",
    "    source=\"bronze_products\",\n",
    "    keys=[\"___\"],\n",
    "    sequence_by=col(\"___\"),\n",
    "    stored_as_scd_type=\"1\"\n",
    ")\n",
    "\n",
    "# [TASK] Define SCD Type 1 for Product Categories\n",
    "dp.create_streaming_table(name=\"silver_product_categories\")\n",
    "\n",
    "dp.create_auto_cdc_flow(\n",
    "    target=\"silver_product_categories\",\n",
    "    source=\"bronze_product_categories\",\n",
    "    keys=[\"___\"],\n",
    "    sequence_by=col(\"___\"),\n",
    "    stored_as_scd_type=\"1\"\n",
    ")\n",
    "\n",
    "# [TASK] Define Data Quality for Orders\n",
    "@dp.table(\n",
    "    name=\"silver_orders\",\n",
    "    expect_all_or_drop={\"valid_amount\": \"___ > 0\"},\n",
    "    expect_all_or_fail={\"valid_customer\": \"___\"} # Check for not null\n",
    ")\n",
    "def silver_orders():\n",
    "    return (\n",
    "        spark.readStream.table(\"bronze_orders_header\")\n",
    "        .select(\"SalesOrderID\", \"CustomerID\", \"TotalDue\", \"OrderDate\", \"Status\")\n",
    "    )\n",
    "\n",
    "# [TASK] Silver Order Details\n",
    "@dp.table(name=\"silver_order_details\")\n",
    "def silver_order_details():\n",
    "    return (\n",
    "        spark.readStream.table(\"___\")\n",
    "        .select(\"SalesOrderID\", \"___\", \"OrderQty\", \"ProductID\", \"UnitPrice\", \"LineTotal\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b667f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3. GOLD LAYER\n",
    "# ============================================================\n",
    "\n",
    "# [TASK] Create the Customer Dimension\n",
    "@dp.materialized_view(name=\"dim_customer\")\n",
    "def dim_customer():\n",
    "    return (\n",
    "        spark.read.table(\"___\")\n",
    "        .filter(col(\"___\").isNull()) # Filter for current records\n",
    "        .select(\"CustomerID\", \"FirstName\", \"LastName\", \"EmailAddress\")\n",
    "    )\n",
    "\n",
    "# [TASK] Create the Product Dimension\n",
    "@dp.materialized_view(name=\"dim_product\")\n",
    "def dim_product():\n",
    "    p = spark.read.table(\"___\")\n",
    "    pc = spark.read.table(\"silver_product_categories\")\n",
    "    \n",
    "    return (\n",
    "        p.join(pc, \"___\", \"left\")\n",
    "        .select(\n",
    "            p[\"ProductID\"],\n",
    "            p[\"Name\"].alias(\"ProductName\"),\n",
    "            pc[\"Name\"].alias(\"CategoryName\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "# [TASK] Create the Date Dimension\n",
    "@dp.materialized_view(name=\"dim_date\")\n",
    "def dim_date():\n",
    "    return (\n",
    "        spark.read.table(\"___\")\n",
    "        .select(col(\"OrderDate\").cast(\"date\").alias(\"DateKey\"))\n",
    "        .___()\n",
    "        .select(\n",
    "            col(\"DateKey\"),\n",
    "            year(\"DateKey\").alias(\"Year\"),\n",
    "            month(\"DateKey\").alias(\"Month\"),\n",
    "            dayofmonth(\"DateKey\").alias(\"Day\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "# [TASK] Create the Fact Sales Materialized View\n",
    "@dp.materialized_view(name=\"fact_sales\")\n",
    "def fact_sales():\n",
    "    od = spark.read.table(\"___\")\n",
    "    oh = spark.read.table(\"silver_orders\")\n",
    "    \n",
    "    # Perform the join\n",
    "    return (\n",
    "        od.join(oh, \"___\", \"inner\") # Join key?\n",
    "        .select(\n",
    "            od[\"SalesOrderID\"],\n",
    "            oh[\"OrderDate\"],\n",
    "            oh[\"CustomerID\"],\n",
    "            od[\"LineTotal\"]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db527e5b",
   "metadata": {},
   "source": [
    "## Orchestration & Deployment\n",
    "\n",
    "In a real-world scenario, you wouldn't run these cells interactively to process data. Instead:\n",
    "\n",
    "1.  You commit these files (`pipeline.py` or `.sql` files) to a Git repository.\n",
    "2.  You create a **Delta Live Tables (DLT)** pipeline in Databricks Workflows.\n",
    "3.  You point the DLT pipeline to your source code.\n",
    "4.  Databricks handles the orchestration, retries, and scaling.\n",
    "\n",
    "## Stuck?\n",
    "\n",
    "If you are stuck or want to see the full, working solution, check the files in the `lakeflow/lakeflow_workshop` folder:\n",
    "*   `lakeflow/lakeflow_workshop/sql/` for the SQL solution.\n",
    "*   `lakeflow/lakeflow_workshop/python/pipeline.py` for the Python solution."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
