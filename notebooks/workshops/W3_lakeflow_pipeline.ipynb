{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf439070",
   "metadata": {},
   "source": [
    "# Workshop 3: Lakeflow Pipelines (Delta Live Tables)\n",
    "\n",
    "## 3.1. The Story\n",
    "\n",
    "We are building a modern Data Warehouse using **Lakeflow Pipelines** (formerly Delta Live Tables).\n",
    "Lakeflow uses **Spark Declarative Pipelines (SDP)** to define data flows.\n",
    "\n",
    "The business requires a **Star Schema** to analyze sales, with specific requirements for handling data changes:\n",
    "\n",
    "1.  **Products (SCD Type 1)**: If a product name changes, simply update it. We only care about the current name.\n",
    "2.  **Customers (SCD Type 2)**: If a customer updates their profile, we need to keep a history of changes.\n",
    "3.  **Sales (Fact)**: Transactional data linked to dimensions.\n",
    "\n",
    "**Your Mission:**\n",
    "1.  **Ingest**: Create a **Bronze** layer using Auto Loader.\n",
    "2.  **Clean**: Create a **Silver** layer with data quality expectations.\n",
    "3.  **Model**: Use `APPLY CHANGES INTO` (Auto CDC) to implement **SCD Type 1** and **SCD Type 2** logic.\n",
    "4.  **Deploy**: Create a Lakeflow Pipeline job.\n",
    "5.  **Update**: Simulate data updates and observe SCD behavior.\n",
    "\n",
    "**Time:** 45 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27db8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This notebook is a Lakeflow Pipeline definition. \n",
    "# It is NOT meant to be run cell-by-cell interactively (except this setup block).\n",
    "# You will deploy this as a Pipeline in the \"Workflows\" tab.\n",
    "\n",
    "%run ../00_setup\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "# We will use a separate path for this workshop to simulate data arrival\n",
    "WORKSHOP_SOURCE_PATH = f\"{volume_path}/lakeflow_workshop_source\"\n",
    "CHECKPOINT_PATH = f\"{volume_path}/lakeflow_checkpoints\"\n",
    "\n",
    "# Clean up previous runs\n",
    "dbutils.fs.rm(WORKSHOP_SOURCE_PATH, True)\n",
    "dbutils.fs.rm(CHECKPOINT_PATH, True)\n",
    "\n",
    "# === DATA PREPARATION (SIMULATION) ===\n",
    "# We split source data into 2 batches to simulate incremental updates\n",
    "\n",
    "# 1. Load Source Data\n",
    "df_customers = spark.read.option(\"header\", \"true\").csv(f\"{DATASET_BASE_PATH}/workshop/main/Customers.csv\")\n",
    "df_products = spark.read.option(\"header\", \"true\").csv(f\"{DATASET_BASE_PATH}/workshop/main/Product.csv\")\n",
    "df_sales = spark.read.option(\"header\", \"true\").csv(f\"{DATASET_BASE_PATH}/workshop/main/SalesOrderDetail.csv\")\n",
    "\n",
    "# 2. Split into Batch 1 (Initial Load) and Batch 2 (Updates)\n",
    "# Customers: Split 50/50\n",
    "cust_b1, cust_b2 = df_customers.randomSplit([0.5, 0.5], seed=42)\n",
    "\n",
    "# Products: Split 50/50\n",
    "prod_b1, prod_b2 = df_products.randomSplit([0.5, 0.5], seed=42)\n",
    "\n",
    "# Sales: Split 50/50\n",
    "sales_b1, sales_b2 = df_sales.randomSplit([0.5, 0.5], seed=42)\n",
    "\n",
    "# 3. Write Batch 1 to Source Path\n",
    "print(f\"Preparing Batch 1 in {WORKSHOP_SOURCE_PATH}...\")\n",
    "cust_b1.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{WORKSHOP_SOURCE_PATH}/Customers\")\n",
    "prod_b1.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{WORKSHOP_SOURCE_PATH}/Product\")\n",
    "sales_b1.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{WORKSHOP_SOURCE_PATH}/SalesOrderDetail\")\n",
    "\n",
    "print(\"\u2705 Batch 1 ready. You can now create the pipeline.\")\n",
    "print(f\"\ud83d\udc49 Source Path for Pipeline: {WORKSHOP_SOURCE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934fbd77",
   "metadata": {},
   "source": [
    "## 3.2. Pipeline Definition\n",
    "\n",
    "In the cell below, you will define the entire pipeline using **Delta Live Tables (DLT)** syntax.\n",
    "\n",
    "### Requirements:\n",
    "\n",
    "1.  **Bronze Layer (Ingestion)**\n",
    "    *   Use `spark.readStream.format(\"cloudFiles\")` (Auto Loader).\n",
    "    *   Load `Customers`, `Product`, and `SalesOrderDetail` from the source path.\n",
    "    *   Format is `csv`.\n",
    "\n",
    "2.  **Silver Layer (Quality & Cleaning)**\n",
    "    *   **Silver Customers**: Expect `EmailAddress IS NOT NULL`.\n",
    "    *   **Silver Products**: Expect `Name IS NOT NULL`.\n",
    "    *   **Silver Sales**: Expect `OrderQty > 0`.\n",
    "\n",
    "3.  **Gold Layer (Dimensional Modeling)**\n",
    "    *   **Dim Product (SCD Type 1)**: Use `dlt.apply_changes`. Keys: `ProductID`. Sequence: `ModifiedDate`.\n",
    "    *   **Dim Customer (SCD Type 2)**: Use `dlt.apply_changes`. Keys: `CustomerID`. Sequence: `ModifiedDate`.\n",
    "    *   **Fact Sales**: Join `silver_sales` with `dim_product` (current) to enrich with product details.\n",
    "    *   *(Note: SalesOrderDetail does not contain CustomerID, so we will focus on Product analysis for this workshop)*\n",
    "\n",
    "### Hint: SCD Type 2 Logic\n",
    "```python\n",
    "dlt.create_streaming_table(\"dim_customer\")\n",
    "\n",
    "dlt.apply_changes(\n",
    "  target = \"dim_customer\",\n",
    "  source = \"silver_customers\",\n",
    "  keys = [\"CustomerID\"],\n",
    "  sequence_by = col(\"ModifiedDate\"),\n",
    "  stored_as_scd_type = 2\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a159caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Path to the workshop source data (defined in the setup cell above)\n",
    "source_path = f\"{volume_path}/lakeflow_workshop_source\"\n",
    "\n",
    "# ==========================================\n",
    "# BRONZE LAYER\n",
    "# ==========================================\n",
    "\n",
    "# TODO: Define bronze_customers\n",
    "# @dlt.table\n",
    "# def bronze_customers():\n",
    "#   return (\n",
    "#     spark.readStream.format(\"cloudFiles\")\n",
    "#       .option(\"cloudFiles.format\", \"csv\")\n",
    "#       .load(f\"{source_path}/Customers\")\n",
    "#   )\n",
    "\n",
    "# TODO: Define bronze_products\n",
    "\n",
    "# TODO: Define bronze_sales (SalesOrderDetail)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# SILVER LAYER\n",
    "# ==========================================\n",
    "\n",
    "# TODO: Define silver_customers (Expect EmailAddress IS NOT NULL)\n",
    "\n",
    "# TODO: Define silver_products (Expect Name IS NOT NULL)\n",
    "\n",
    "# TODO: Define silver_sales (Expect OrderQty > 0)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# GOLD LAYER\n",
    "# ==========================================\n",
    "\n",
    "# TODO: Create dim_product (SCD Type 1)\n",
    "# dlt.create_streaming_table(\"dim_product\")\n",
    "# dlt.apply_changes(...)\n",
    "\n",
    "# TODO: Create dim_customer (SCD Type 2)\n",
    "# dlt.create_streaming_table(\"dim_customer\")\n",
    "# dlt.apply_changes(..., stored_as_scd_type = 2)\n",
    "\n",
    "# TODO: Create fact_sales\n",
    "# @dlt.table\n",
    "# def fact_sales():\n",
    "#     # Read silver_sales\n",
    "#     # Join with dim_product (current records only)\n",
    "#     pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef2b85b",
   "metadata": {},
   "source": [
    "## 3.3. Deployment\n",
    "\n",
    "To run this code, you must create a Pipeline resource.\n",
    "\n",
    "1.  **Navigate**: Click **Workflows** in the sidebar, then **Delta Live Tables**.\n",
    "2.  **Create Pipeline**:\n",
    "    *   **Pipeline Name**: `Lakeflow_Workshop_YourName`\n",
    "    *   **Product Edition**: `Advanced` (Required for SCD Type 2).\n",
    "    *   **Source Code**: Select **this notebook**.\n",
    "    *   **Destination**: Unity Catalog (`catalog.schema`).\n",
    "    *   **Channel**: `Current`.\n",
    "3.  **Start**: Click **Start** to run the pipeline.\n",
    "\n",
    "### Observe:\n",
    "*   The **Graph** visualization showing dependencies.\n",
    "*   **Data Quality** metrics (how many records dropped in Silver).\n",
    "*   **SCD Handling**: Check `dim_customer` - it will have `__START_AT` and `__END_AT` columns automatically added!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d5f615",
   "metadata": {},
   "source": [
    "## 3.4. Lakeflow Jobs (Orchestration)\n",
    "\n",
    "Once your pipeline is ready, you can orchestrate it as part of a larger workflow.\n",
    "\n",
    "1.  Go to **Workflows** -> **Jobs**.\n",
    "2.  Create a new Job.\n",
    "3.  **Task 1**: Type = **Pipeline**, Select your `Lakeflow_Workshop` pipeline.\n",
    "4.  **Task 2**: Type = **Notebook**, Select a notebook that queries the Gold tables (e.g., for reporting).\n",
    "\n",
    "This allows you to chain the ingestion/ETL pipeline with downstream consumers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1760e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SIMULATE DATA UPDATE (BATCH 2) ===\n",
    "# Run this cell AFTER the pipeline has finished the first run.\n",
    "\n",
    "print(\"Simulating data updates (Batch 2)...\")\n",
    "\n",
    "# Write Batch 2 to Source Path\n",
    "cust_b2.write.mode(\"append\").option(\"header\", \"true\").csv(f\"{WORKSHOP_SOURCE_PATH}/Customers\")\n",
    "prod_b2.write.mode(\"append\").option(\"header\", \"true\").csv(f\"{WORKSHOP_SOURCE_PATH}/Product\")\n",
    "sales_b2.write.mode(\"append\").option(\"header\", \"true\").csv(f\"{WORKSHOP_SOURCE_PATH}/SalesOrderDetail\")\n",
    "\n",
    "print(\"\u2705 Batch 2 arrived.\")\n",
    "print(\"\ud83d\udc49 Now go back to your Pipeline and click 'Start' (Refresh) to process new data.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}