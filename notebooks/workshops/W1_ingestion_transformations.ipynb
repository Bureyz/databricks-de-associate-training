{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee790a99",
   "metadata": {},
   "source": [
    "# Workshop 1: Ingestion & Transformations\n",
    "\n",
    "## The Story\n",
    "\n",
    "You are a Data Engineer at a retail company. The marketing team has requested a clean list of customers to run a new email campaign.\n",
    "The data is currently sitting in a CSV file in the landing zone, but it's raw and needs processing.\n",
    "\n",
    "**Your Mission:**\n",
    "1. Ingest the raw customer data from CSV.\n",
    "2. Select only the relevant columns (Name, Email, Company).\n",
    "3. Create a `FullName` column by combining First and Last names.\n",
    "4. Add an audit timestamp to track when the data was processed.\n",
    "5. Save the clean data as a Delta table for the marketing team to use.\n",
    "\n",
    "**Time:** 30 minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5ae5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration variables\n",
    "%run ../00_setup\n",
    "\n",
    "# --- INDEPENDENT SETUP ---\n",
    "# Ensure source data exists for this workshop\n",
    "import os\n",
    "\n",
    "# Define path\n",
    "source_dir = f\"{volume_path}/workshop/main\"\n",
    "source_file = f\"{source_dir}/Customers.csv\"\n",
    "\n",
    "# Check if source file exists\n",
    "try:\n",
    "    dbutils.fs.ls(source_file)\n",
    "    print(f\"Source file found: {source_file}\")\n",
    "except:\n",
    "    print(f\"WARNING: Source file not found at {source_file}. Please ensure datasets are uploaded to the Volume.\")\n",
    "\n",
    "print(f\"Catalog: {catalog}\")\n",
    "print(f\"Schema:  {schema}\")\n",
    "print(f\"Silver Schema: {SILVER_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d748c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if variables were loaded\n",
    "print(f\"üìÅ Catalog: {catalog}\")\n",
    "print(f\"üìÅ Schema:  {schema}\")\n",
    "print(f\"üìÅ Volume:  {volume_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e031e7eb",
   "metadata": {},
   "source": [
    "## Step 1: Source Data Exploration\n",
    "\n",
    "Before loading data, let's see what we have available in the source directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637bff94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files in the workshop directory\n",
    "dbutils.fs.ls(f\"{volume_path}/workshop/main/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f565ff",
   "metadata": {},
   "source": [
    "## Step 2: Loading Customer Data\n",
    "\n",
    "### Task 2.1: Load `Customers.csv` file\n",
    "\n",
    "**Requirements:**\n",
    "- Use CSV format\n",
    "- File has headers\n",
    "- Let Spark automatically detect data types (`inferSchema`)\n",
    "\n",
    "**Hint:**\n",
    "```python\n",
    "spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .load(\"path\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cbc4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to file\n",
    "customers_path = f\"{volume_path}/workshop/main/Customers.csv\"\n",
    "\n",
    "# TODO: Load Customers.csv file into df_customers DataFrame\n",
    "df_customers = (\n",
    "    spark.read\n",
    "    # Complete the code here\n",
    "    # .format(...)\n",
    "    # .option(...)\n",
    "    # .load(...)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e61ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check result\n",
    "print(f\"Loaded {df_customers.count()} customers\")\n",
    "display(df_customers.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb1f2d9",
   "metadata": {},
   "source": [
    "## Step 3: Transformations\n",
    "\n",
    "### Task 3.1: Select required columns\n",
    "\n",
    "The marketing team needs only:\n",
    "- `CustomerID`\n",
    "- `FirstName`\n",
    "- `LastName`\n",
    "- `EmailAddress`\n",
    "- `CompanyName`\n",
    "- `Phone`\n",
    "\n",
    "**Hint:** Use `.select(\"column1\", \"column2\", ...)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9055ffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Select only required columns\n",
    "df_customers_clean = df_customers.select(\n",
    "    # Add columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01bb3eb",
   "metadata": {},
   "source": [
    "### Task 3.2: Create `FullName` column\n",
    "\n",
    "Combine `FirstName` and `LastName` into a single `FullName` column.\n",
    "\n",
    "**Hint:** Use the `concat_ws` function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132870aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws, col, upper, trim, current_timestamp\n",
    "\n",
    "# TODO: Add FullName column\n",
    "df_customers_enriched = df_customers_clean.withColumn(\n",
    "    \"FullName\",\n",
    "    # Complete the code here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e712c897",
   "metadata": {},
   "source": [
    "### Task 3.3: Filter invalid emails\n",
    "\n",
    "Filter out customers who do not have a valid email address (must contain '@')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3404368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Filter rows where EmailAddress contains '@'\n",
    "df_customers_filtered = df_customers_enriched.filter(\n",
    "    # Complete the code here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f356a19e",
   "metadata": {},
   "source": [
    "### Task 3.4: Analyze Company Distribution\n",
    "\n",
    "Check how many customers belong to each company. Sort the result by count in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7890852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Group by CompanyName and count\n",
    "# display(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f451bee1",
   "metadata": {},
   "source": [
    "## Step 4: Adding Audit Column\n",
    "\n",
    "### Task 4.1: Add audit column\n",
    "\n",
    "Add an `ingestion_timestamp` column with the current time - this is a good practice in ETL!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93e084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add ingestion_timestamp column\n",
    "df_final = df_customers_filtered.withColumn(\n",
    "    \"ingestion_timestamp\",\n",
    "    # Complete the code here - use current_timestamp()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05144402",
   "metadata": {},
   "source": [
    "## Step 5: Save to Delta Lake\n",
    "\n",
    "### Task 5.1: Save as Delta table\n",
    "\n",
    "Save the resulting DataFrame as a managed Delta Lake table named `customers_silver`.\n",
    "\n",
    "**Hint:**\n",
    "```python\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"catalog.schema.table_name\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb7225",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = f\"{catalog}.{SILVER_SCHEMA}.customers_silver\"\n",
    "\n",
    "# TODO: Save df_final as Delta table\n",
    "(\n",
    "    df_final.write\n",
    "    # Complete the code here\n",
    ")\n",
    "\n",
    "print(f\"Saved table: {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49908a3",
   "metadata": {},
   "source": [
    "## Step 6: Verification\n",
    "\n",
    "Let's check if the table was created correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3832cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the table\n",
    "display(spark.table(table_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e83b307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Delta metadata\n",
    "display(spark.sql(f\"DESCRIBE DETAIL {table_name}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e50b02",
   "metadata": {},
   "source": [
    "## Step 7: SQL Access (The Lakehouse Advantage)\n",
    "\n",
    "You just created a table using Python. Now, let's query it immediately using SQL!\n",
    "This demonstrates how Data Engineers and Data Analysts can work on the same data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efca7357",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Query the table using SQL\n",
    "SELECT * \n",
    "FROM catalog.silver.customers_silver \n",
    "WHERE CompanyName = 'A Bike Store'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d3da63",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ad727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: Uncomment only if you want to delete the table!\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95da78db",
   "metadata": {},
   "source": [
    "# Solution\n",
    "\n",
    "The complete code is below. Try to solve it yourself first!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bbed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FULL SOLUTION - Workshop 1: Ingestion & Transformations\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import concat_ws, col, current_timestamp, trim\n",
    "\n",
    "# --- Step 2: Loading data ---\n",
    "customers_path = f\"{volume_path}/workshop/main/Customers.csv\"\n",
    "\n",
    "df_customers = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .load(customers_path)\n",
    ")\n",
    "\n",
    "# --- Step 3: Transformations ---\n",
    "df_customers_clean = df_customers.select(\n",
    "    \"CustomerID\", \"FirstName\", \"LastName\", \n",
    "    \"EmailAddress\", \"CompanyName\", \"Phone\"\n",
    ")\n",
    "\n",
    "df_customers_enriched = df_customers_clean.withColumn(\n",
    "    \"FullName\",\n",
    "    concat_ws(\" \", col(\"FirstName\"), col(\"LastName\"))\n",
    ")\n",
    "\n",
    "# Task 3.3: Filter\n",
    "df_customers_filtered = df_customers_enriched.filter(col(\"EmailAddress\").contains(\"@\"))\n",
    "\n",
    "# Task 3.4: Analysis\n",
    "print(\"Company Distribution:\")\n",
    "# Using display() allows for built-in plotting!\n",
    "display(df_customers_filtered.groupBy(\"CompanyName\").count().orderBy(\"count\", ascending=False))\n",
    "\n",
    "# --- Step 4: Add audit column ---\n",
    "df_final = df_customers_filtered.withColumn(\n",
    "    \"ingestion_timestamp\",\n",
    "    current_timestamp()\n",
    ")\n",
    "\n",
    "# --- Step 5: Save to Delta ---\n",
    "table_name = f\"{catalog}.{SILVER_SCHEMA}.customers_silver\"\n",
    "\n",
    "(\n",
    "    df_final.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(table_name)\n",
    ")\n",
    "\n",
    "print(f\"Solution executed! Table: {table_name}\")\n",
    "print(f\"Row count: {spark.table(table_name).count()}\")\n",
    "display(spark.table(table_name).limit(5))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
