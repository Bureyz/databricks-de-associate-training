{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee790a99",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß Step 0: Environment Setup\n",
    "\n",
    "Run the cell below to load configuration variables (`catalog`, `schema`, `volume_path`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5ae5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration variables\n",
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d748c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if variables were loaded\n",
    "print(f\"üìÅ Catalog: {catalog}\")\n",
    "print(f\"üìÅ Schema:  {schema}\")\n",
    "print(f\"üìÅ Volume:  {volume_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e031e7eb",
   "metadata": {},
   "source": [
    "---\n",
    "## üìÇ Step 1: Source Data Exploration\n",
    "\n",
    "Before loading data, let's see what we have available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637bff94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files in the workshop directory\n",
    "dbutils.fs.ls(f\"{volume_path}/main/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f565ff",
   "metadata": {},
   "source": [
    "---\n",
    "## üì• Step 2: Loading Customer Data\n",
    "\n",
    "### Task 2.1: Load `Customers.csv` file\n",
    "\n",
    "**Requirements:**\n",
    "- Use CSV format\n",
    "- File has headers (header)\n",
    "- Let Spark automatically detect data types (`inferSchema`)\n",
    "\n",
    "**Hint:**\n",
    "```python\n",
    "spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .load(\"path\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cbc4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to file\n",
    "customers_path = f\"{volume_path}/main/Customers.csv\"\n",
    "\n",
    "# TODO: Load Customers.csv file into df_customers DataFrame\n",
    "df_customers = (\n",
    "    spark.read\n",
    "    # Complete the code here\n",
    "    # .format(...)\n",
    "    # .option(...)\n",
    "    # .load(...)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e61ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check result - should be 847 rows\n",
    "print(f\"‚úÖ Loaded {df_customers.count()} customers\")\n",
    "display(df_customers.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb1f2d9",
   "metadata": {},
   "source": [
    "---\n",
    "## üîÑ Step 3: Transformations\n",
    "\n",
    "### Task 3.1: Select required columns from customers\n",
    "\n",
    "The marketing team needs only:\n",
    "- `CustomerID`\n",
    "- `FirstName`\n",
    "- `LastName`\n",
    "- `EmailAddress`\n",
    "- `CompanyName`\n",
    "- `Phone`\n",
    "\n",
    "**Hint:** Use `.select(\"column1\", \"column2\", ...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9055ffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Select only required columns\n",
    "df_customers_clean = df_customers.select(\n",
    "    # Add columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01bb3eb",
   "metadata": {},
   "source": [
    "### Task 3.2: Create `FullName` column\n",
    "\n",
    "Combine `FirstName` and `LastName` into a single `FullName` column.\n",
    "\n",
    "**Hint:** Use the `concat_ws` or `concat` function:\n",
    "```python\n",
    "from pyspark.sql.functions import concat_ws, col\n",
    "df.withColumn(\"FullName\", concat_ws(\" \", col(\"FirstName\"), col(\"LastName\")))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132870aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws, col, upper, trim, current_timestamp\n",
    "\n",
    "# TODO: Add FullName column\n",
    "df_customers_enriched = df_customers_clean.withColumn(\n",
    "    \"FullName\",\n",
    "    # Complete the code here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f451bee1",
   "metadata": {},
   "source": [
    "---\n",
    "## üîó Step 4: Adding Audit Column\n",
    "\n",
    "### Task 4.1: Add audit column\n",
    "\n",
    "Add an `ingestion_timestamp` column with the current time - this is a good practice in ETL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93e084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add ingestion_timestamp column\n",
    "df_final = df_customers_enriched.withColumn(\n",
    "    \"ingestion_timestamp\",\n",
    "    # Complete the code here - use current_timestamp()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05144402",
   "metadata": {},
   "source": [
    "---\n",
    "## üíæ Step 5: Save to Delta Lake\n",
    "\n",
    "### Task 5.1: Save as Delta table\n",
    "\n",
    "Save the resulting DataFrame as a managed Delta Lake table.\n",
    "\n",
    "**Hint:**\n",
    "```python\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"catalog.schema.table_name\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb7225",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = f\"{catalog}.{schema}.customers_silver\"\n",
    "\n",
    "# TODO: Save df_final as Delta table\n",
    "(\n",
    "    df_final.write\n",
    "    # Complete the code here\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Saved table: {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49908a3",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Step 6: Verification\n",
    "\n",
    "Let's check if the table was created correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3832cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the table\n",
    "display(spark.table(table_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e83b307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Delta metadata\n",
    "display(spark.sql(f\"DESCRIBE DETAIL {table_name}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8fde38",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Bonus: Additional Tasks (if you have time)\n",
    "\n",
    "1. **Add validation:** Check if all emails have the correct format (`@` in the middle)\n",
    "2. **Aggregation:** Count how many customers are from each company\n",
    "3. **Filtering:** Find all customers from company \"A Bike Store\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e27cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus 1: Email validation\n",
    "# TODO: Count how many emails contain '@'\n",
    "# df_final.filter(col(\"EmailAddress\").contains(\"@\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b0f8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus 2: Customers per company\n",
    "# TODO: groupBy aggregation\n",
    "# df_final.groupBy(\"CompanyName\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d3da63",
   "metadata": {},
   "source": [
    "---\n",
    "## üßπ Cleanup (optional)\n",
    "\n",
    "If you want to remove created resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ad727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: Uncomment only if you want to delete the table!\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95da78db",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# üìã SOLUTION\n",
    "\n",
    "‚ö†Ô∏è **Don't look here until you've tried it yourself!** ‚ö†Ô∏è\n",
    "\n",
    "Below you'll find the complete code solving all workshop tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bbed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üìã FULL SOLUTION - Workshop 1: Ingestion & Transformations\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import concat_ws, col, current_timestamp, trim\n",
    "\n",
    "# --- Step 2: Loading data ---\n",
    "customers_path = f\"{volume_path}/main/Customers.csv\"\n",
    "\n",
    "df_customers = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .load(customers_path)\n",
    ")\n",
    "\n",
    "# --- Step 3: Transformations ---\n",
    "df_customers_clean = df_customers.select(\n",
    "    \"CustomerID\", \"FirstName\", \"LastName\", \n",
    "    \"EmailAddress\", \"CompanyName\", \"Phone\"\n",
    ")\n",
    "\n",
    "df_customers_enriched = df_customers_clean.withColumn(\n",
    "    \"FullName\",\n",
    "    concat_ws(\" \", col(\"FirstName\"), col(\"LastName\"))\n",
    ")\n",
    "\n",
    "# --- Step 4: Add audit column ---\n",
    "df_final = df_customers_enriched.withColumn(\n",
    "    \"ingestion_timestamp\",\n",
    "    current_timestamp()\n",
    ")\n",
    "\n",
    "# --- Step 5: Save to Delta ---\n",
    "table_name = f\"{catalog}.{schema}.customers_silver\"\n",
    "\n",
    "(\n",
    "    df_final.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(table_name)\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Solution executed! Table: {table_name}\")\n",
    "print(f\"üìä Row count: {spark.table(table_name).count()}\")\n",
    "display(spark.table(table_name).limit(5))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
