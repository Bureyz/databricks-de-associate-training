{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81af7e84-b553-4283-b3aa-cd346003002e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "source": [
    "# BONUS: Timestamps, UTC i Auto Loader z Archiwizacją\n",
    "\n",
    "Notatnik bonusowy dla Data Engineerów pracujących w środowisku Databricks.\n",
    "\n",
    "Obejmuje dwa kluczowe tematy:\n",
    "1. **Praca z datami i timestampami** w kontekście zespołów wielonarodowych — standardy UTC, konwersja stref czasowych, pułapki i najlepsze praktyki.\n",
    "2. **Auto Loader z mechanizmem archiwizacji** — automatyczne przenoszenie przetworzonych plików do folderu archiwum (`cloudFiles.cleanSource = MOVE`).\n",
    "\n",
    "| Poziom | Czas |\n",
    "|--------|------|\n",
    "| Intermediate / Advanced | ~45 min |\n",
    "\n",
    "> **Dokumentacja:** [Auto Loader options — docs.databricks.com](https://docs.databricks.com/aws/en/ingestion/cloud-object-storage/auto-loader/options)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9755424f-7e2f-4a56-8770-1a7861d731b0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 2"
    }
   },
   "source": [
    "## Part 1: Timestamps i UTC\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24dda7e2-d27f-4c0f-90fb-4e727f40a3f1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 3"
    }
   },
   "source": [
    "### Setup\n",
    "\n",
    "Konfiguracja środowiska — importy, zmienne środowiskowe, ścieżki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6664cbb6-7e28-4f59-87c0-32befba03cb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../setup/00_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e455cad-e57b-4396-b63b-3b8a97deb63c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    StringType, TimestampType, DateType, LongType\n",
    ")\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# ── KLUCZOWE USTAWIENIE: Spark zawsze operuje na UTC wewnętrznie.\n",
    "# Ustawienie session.timeZone na UTC eliminuje niejednoznaczności\n",
    "# przy wyświetlaniu i eksporcie danych między strefami czasowymi.\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "# ── Ścieżki do demo archiwizacji (Part 2)\n",
    "BONUS_BASE       = f\"{DATASET_PATH}/bonus_demo\"\n",
    "SOURCE_PATH      = f\"{BONUS_BASE}/source\"\n",
    "ARCHIVE_PATH     = f\"{BONUS_BASE}/archive\"\n",
    "CHECKPOINT_PATH  = f\"{BONUS_BASE}/checkpoint_archive\"\n",
    "SCHEMA_PATH      = f\"{BONUS_BASE}/schema_archive\"\n",
    "TARGET_TABLE     = f\"{CATALOG}.{BRONZE_SCHEMA}.bonus_orders_archive\"\n",
    "\n",
    "# Cleanup poprzedniego uruchomienia\n",
    "dbutils.fs.rm(BONUS_BASE, True)\n",
    "print(f\"  Środowisko przygotowane: {BONUS_BASE}\")\n",
    "print(f\"    spark.sql.session.timeZone = {spark.conf.get('spark.sql.session.timeZone')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "273eb2d2-ff4a-4abd-a55c-8f0cb8c2df2c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 6"
    }
   },
   "source": [
    "### 1.1 Date vs Timestamp — podstawowe różnice\n",
    "\n",
    "| Typ | Precyzja | Strefa czasowa | Użycie |\n",
    "|-----|----------|----------------|--------|\n",
    "| `DATE` | dzień | brak | daty kalendarzowe, partycjonowanie |\n",
    "| `TIMESTAMP` / `TIMESTAMP_LTZ` | mikrosekundy | tak (UTC wewnętrznie) | zdarzenia z dokładnym czasem |\n",
    "| `TIMESTAMP_NTZ` | mikrosekundy | brak | harmonogramy lokalne, etykiety czasowe |\n",
    "\n",
    "> **Reguła:** Jeśli Twoje dane reprezentują **moment w czasie** (log, transakcja, zdarzenie) → `TIMESTAMP_LTZ` (UTC).  \n",
    "> Jeśli reprezentują **datę lokalną** bez kontekstu strefy → `DATE` lub `TIMESTAMP_NTZ`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c791046-d424-41a3-82fa-47cd4e25ed02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dane wejściowe — zdarzenia z różnych systemów i stref czasowych\n",
    "raw_data = [\n",
    "    # (id, zdarzenie,      timestamp jako string,       strefa źródłowa)\n",
    "    (1, \"zamówienie PL\", \"2024-06-15 14:30:00\",        \"Europe/Warsaw\"),\n",
    "    (2, \"zamówienie US\", \"2024-06-15 08:30:00\",        \"America/New_York\"),\n",
    "    (3, \"zamówienie JP\", \"2024-06-15 21:30:00\",        \"Asia/Tokyo\"),\n",
    "    (4, \"zamówienie UK\", \"2024-06-15 13:30:00\",        \"Europe/London\"),\n",
    "    (5, \"zamówienie AU\", \"2024-06-15 23:30:00\",        \"Australia/Sydney\"),\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\",            LongType(),   False),\n",
    "    StructField(\"event\",         StringType(), True),\n",
    "    StructField(\"local_time_str\", StringType(), True),\n",
    "    StructField(\"source_tz\",     StringType(), True),\n",
    "])\n",
    "\n",
    "df_raw = spark.createDataFrame(raw_data, schema)\n",
    "\n",
    "# Parsujemy string do TIMESTAMP (Spark zakłada session.timeZone = UTC)\n",
    "df_raw = df_raw.withColumn(\n",
    "    \"local_timestamp\",\n",
    "    F.to_timestamp(\"local_time_str\", \"yyyy-MM-dd HH:mm:ss\")\n",
    ")\n",
    "\n",
    "display(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f76aa646-bd2b-4035-91cf-4e8b6615ee2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.2 Konwersja stref czasowych → UTC\n",
    "\n",
    "**Problem:** Każdy system dostarczył timestamp w swojej strefie lokalnej. Musimy je wszystkie sprowadzić do UTC.\n",
    "\n",
    "`from_utc_timestamp(ts, tz)` — konwertuje UTC → lokalna strefa  \n",
    "`to_utc_timestamp(ts, tz)` — konwertuje lokalna strefa → UTC  \n",
    "`convert_timezone(src_tz, tgt_tz, ts)` — nowszy wariant (SQL / Unity Catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb085053-26ae-4c63-913f-668047dea3b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── to_utc_timestamp(col, tz): traktuje kolumnę jako czas w podanej strefie\n",
    "# i przelicza go na UTC. To jest wzorzec dla danych wejściowych.\n",
    "df_utc = (\n",
    "    df_raw\n",
    "    # Dla każdego rekordu znamy strefę źródłową — przeliczamy do UTC\n",
    "    .withColumn(\n",
    "        \"event_utc\",\n",
    "        F.to_utc_timestamp(\"local_timestamp\", F.col(\"source_tz\"))\n",
    "    )\n",
    "    # Kolumna daty (tylko dzień) — przydatna do partycjonowania\n",
    "    .withColumn(\"event_date\", F.to_date(\"event_utc\"))\n",
    ")\n",
    "\n",
    "display(\n",
    "    df_utc.select(\n",
    "        \"id\", \"event\", \"source_tz\",\n",
    "        \"local_time_str\",\n",
    "        \"event_utc\",\n",
    "        \"event_date\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4189852a-28d4-44b8-aab9-a94a5b7103c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.3 Wyświetlanie czasu lokalnego — `from_utc_timestamp`\n",
    "\n",
    "Dane są przechowywane w UTC. Gdy użytkownik polskiego biura chce zobaczyć czas lokalny, konwertujemy **tylko przy wyświetlaniu** — nie zmieniamy wartości w tabeli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ef9c339-94d9-4ee6-818d-10f9c2ae71d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── from_utc_timestamp(col, tz): przelicza UTC → podaną strefę lokalną\n",
    "# Wzorzec: przechowuj UTC, konwertuj tylko dla prezentacji\n",
    "\n",
    "TARGET_TIMEZONES = {\n",
    "    \"PL (Warsaw)\":   \"Europe/Warsaw\",\n",
    "    \"US (New York)\": \"America/New_York\",\n",
    "    \"JP (Tokyo)\":    \"Asia/Tokyo\",\n",
    "}\n",
    "\n",
    "df_display = df_utc.select(\"id\", \"event\", \"event_utc\")\n",
    "\n",
    "for label, tz in TARGET_TIMEZONES.items():\n",
    "    col_name = f\"display_{label.split(' ')[0].lower()}\"\n",
    "    df_display = df_display.withColumn(\n",
    "        col_name,\n",
    "        F.date_format(\n",
    "            F.from_utc_timestamp(\"event_utc\", tz),\n",
    "            \"yyyy-MM-dd HH:mm:ss z\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "display(df_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f51cc942-8905-4866-8b48-db1b6580736e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.4 Pułapka: `session.timeZone` a parsowanie stringów\n",
    "\n",
    "Jeśli nie ustawimy `spark.sql.session.timeZone = UTC`, Spark interpretuje stringe bez informacji o strefie jako **czas lokalny serwera klastra** (np. UTC-5 w AWS us-east).  \n",
    "Godziny zostaną przesunięte przy odczycie/zapisie — bug trudny do wykrycia w produkcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a50e947d-bb08-41c0-933a-421e8f3763dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TS_STRING = \"2024-06-15 12:00:00\"  # brak informacji o strefie\n",
    "\n",
    "# ── Demonstracja wpływu session.timeZone na parsowanie\n",
    "results = []\n",
    "for tz_setting in [\"UTC\", \"America/New_York\", \"Asia/Tokyo\", \"Europe/Warsaw\"]:\n",
    "    spark.conf.set(\"spark.sql.session.timeZone\", tz_setting)\n",
    "    val = spark.sql(f\"SELECT CAST('{TS_STRING}' AS TIMESTAMP) AS ts\").collect()[0][\"ts\"]\n",
    "    results.append((tz_setting, str(val)))\n",
    "\n",
    "# Przywracamy UTC (standard)\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "df_tz_demo = spark.createDataFrame(results, [\"session_timeZone\", \"parsed_timestamp_utc_repr\"])\n",
    "display(df_tz_demo)\n",
    "\n",
    "print(\"\\n  Przywrócono spark.sql.session.timeZone = UTC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4da05643-23af-4958-a506-18207e631762",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.5 Best practices — tabela Delta z UTC\n",
    "\n",
    "Rekomendowany schemat dla tabel produkcyjnych przyjmujących zdarzenia z różnych stref czasowych:\n",
    "\n",
    "```\n",
    "event_id          BIGINT\n",
    "event_timestamp   TIMESTAMP    -- zawsze UTC (TIMESTAMP_LTZ)\n",
    "event_date        DATE         -- partycja po UTC date\n",
    "source_timezone   STRING       -- zachowujemy info o strefie źródłowej\n",
    "ingested_at       TIMESTAMP    -- czas ingestii (current_timestamp() = UTC)\n",
    "```\n",
    "\n",
    "Nigdy nie mieszaj stref w jednej kolumnie. Zachowaj `source_timezone` jako metadane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1c41fbd-6c31-4b6f-87d2-015bd0a533ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "# Tworzymy tabelę Delta ze wzorcowym schematem UTC\n",
    "spark.sql(\"DROP TABLE IF EXISTS bonus_events_utc\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS bonus_events_utc (\n",
    "        event_id        BIGINT,\n",
    "        event_timestamp TIMESTAMP,   -- zawsze UTC\n",
    "        event_date      DATE,        -- partycja po UTC date\n",
    "        source_timezone STRING,      -- strefa źródłowa jako metadane\n",
    "        event_name      STRING,\n",
    "        ingested_at     TIMESTAMP    -- czas ingestii (UTC)\n",
    "    )\n",
    "    USING DELTA\n",
    "    PARTITIONED BY (event_date)\n",
    "    TBLPROPERTIES (\n",
    "        'delta.enableChangeDataFeed' = 'false',\n",
    "        'comment' = 'Zdarzenia z wielu stref — timestamp zawsze UTC'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Wstawiamy dane z wcześniej przygotowanego DataFrame\n",
    "df_final = (\n",
    "    df_utc\n",
    "    .withColumnRenamed(\"id\", \"event_id\")\n",
    "    .withColumnRenamed(\"event\", \"event_name\")\n",
    "    .withColumnRenamed(\"event_utc\", \"event_timestamp\")\n",
    "    .withColumn(\"ingested_at\", F.current_timestamp())\n",
    "    .select(\"event_id\", \"event_timestamp\", \"event_date\",\n",
    "            \"source_tz\", \"event_name\", \"ingested_at\")\n",
    "    .withColumnRenamed(\"source_tz\", \"source_timezone\")\n",
    ")\n",
    "\n",
    "df_final.write.mode(\"append\").saveAsTable(\"bonus_events_utc\")\n",
    "\n",
    "display(spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        event_id,\n",
    "        event_name,\n",
    "        source_timezone,\n",
    "        event_timestamp                                          AS utc_time,\n",
    "        from_utc_timestamp(event_timestamp, 'Europe/Warsaw')    AS warsaw_time,\n",
    "        from_utc_timestamp(event_timestamp, 'America/New_York') AS new_york_time,\n",
    "        from_utc_timestamp(event_timestamp, 'Asia/Tokyo')       AS tokyo_time\n",
    "    FROM bonus_events_utc\n",
    "    ORDER BY event_id\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9970c82a-4987-4397-8e82-1b595a923c36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Part 2: Auto Loader z Archiwizacją Plików\n",
    "\n",
    "---\n",
    "\n",
    "### Scenariusz\n",
    "\n",
    "System zamówień co kilka minut zrzuca pliki JSON do folderu `source/`.  \n",
    "Po przetworzeniu pliki muszą zostać **przeniesione do `archive/`**, zamiast usuwane — wymaganie compliance.\n",
    "\n",
    "**Opcja dokumentacyjna:** [`cloudFiles.cleanSource = MOVE`](https://docs.databricks.com/aws/en/ingestion/cloud-object-storage/auto-loader/options#common-auto-loader-options)\n",
    "\n",
    "| Opcja | Wartość | Opis |\n",
    "|-------|---------|------|\n",
    "| `cloudFiles.cleanSource` | `MOVE` | Przenieś pliki po przetworzeniu |\n",
    "| `cloudFiles.cleanSource.moveDestination` | ścieżka | Gdzie przeносić pliki (musi być w tym samym bucket/container) |\n",
    "| `cloudFiles.cleanSource.retentionDuration` | `0 hours` | Jak długo czekać przed przeniesieniem (domyślnie 30 dni; min. 0 dla MOVE) |\n",
    "\n",
    "> **Uwaga:** Auto Loader musi mieć uprawnienia zapisu do `source/` i `archive/`. Folder `archive/` **nie może być podfolderem** `source/` — spowoduje ponowną ingestię."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "121ce000-2684-4f3d-917d-acf00ce52a5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.1 Przygotowanie danych symulacyjnych\n",
    "\n",
    "Tworzymy 3 mikro-batche plików JSON symulujące przybycie zamówień."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcabfbb3-9f0d-4481-bf0b-8bf09790dbb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Wczytujemy istniejące pliki zamówień ze streamingu i dzielimy na 3 batche\n",
    "SOURCE_STREAM_FILES = f\"{DATASET_PATH}/orders/stream/*.json\"\n",
    "df_orders = spark.read.json(SOURCE_STREAM_FILES)\n",
    "batches = df_orders.randomSplit([0.33, 0.33, 0.34], seed=42)\n",
    "\n",
    "for i, batch_df in enumerate(batches, start=1):\n",
    "    batch_df.coalesce(1).write.mode(\"overwrite\").json(f\"{SOURCE_PATH}/batch_{i:02d}\")\n",
    "    count = batch_df.count()\n",
    "    print(f\"  batch_{i:02d}: {count} rekordów → {SOURCE_PATH}/batch_{i:02d}\")\n",
    "\n",
    "# Sprawdzamy co jest w source\n",
    "print(\"\\nZawartość source/:\")\n",
    "for f in dbutils.fs.ls(SOURCE_PATH):\n",
    "    print(f\"  {f.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20e8df55-d9e1-4ef6-aefa-b1d2c7c863e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.2 Auto Loader z `cloudFiles.cleanSource = MOVE`\n",
    "\n",
    "Kluczowe opcje:\n",
    "\n",
    "- `cloudFiles.cleanSource` = `\"MOVE\"` — automatycznie przenosi pliki po przetworzeniu\n",
    "- `cloudFiles.cleanSource.moveDestination` — folder docelowy (archiwum)\n",
    "- `cloudFiles.cleanSource.retentionDuration` — czas przed przeniesieniem (min. `0 hours` dla MOVE)\n",
    "- `cloudFiles.schemaLocation` — wymagane przy inferencji schematu\n",
    "- `cloudFiles.inferColumnTypes` = `True` — inferuje typy (nie tylko String)\n",
    "\n",
    "> **Ważne:** `retentionDuration` = `\"0 hours\"` oznacza przeniesienie natychmiast po potwierdzeniu commit przez strumień. Dostępne od **Databricks Runtime 16.4+**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6300f151-cf08-4677-aa82-091fd98d803f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.streaming import StreamingQuery\n",
    "\n",
    "# ── Tworzymy strumień Auto Loader z archiwizacją\n",
    "df_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\",                    \"json\")\n",
    "    # ── Schemat i typy\n",
    "    .option(\"cloudFiles.schemaLocation\",            SCHEMA_PATH)\n",
    "    .option(\"cloudFiles.inferColumnTypes\",          \"true\")\n",
    "    # ── Archiwizacja: przenieś przetworzone pliki do ARCHIVE_PATH\n",
    "    .option(\"cloudFiles.cleanSource\",               \"MOVE\")\n",
    "    .option(\"cloudFiles.cleanSource.moveDestination\", ARCHIVE_PATH)\n",
    "    .option(\"cloudFiles.cleanSource.retentionDuration\", \"0 hours\")  # DBR 16.4+\n",
    "    # ── Kontrola przepustowości\n",
    "    .option(\"cloudFiles.maxFilesPerTrigger\",        \"1\")\n",
    "    # ── Dodaje kolumnę _metadata z nazwą pliku, rozmiarem, czasem modyfikacji\n",
    "    .load(SOURCE_PATH)\n",
    "    # Dodajemy timestamp ingestii (UTC)\n",
    "    .withColumn(\"ingested_at\", F.current_timestamp())\n",
    "    # Wyciągamy nazwę pliku źródłowego z metadanych\n",
    "    .withColumn(\"source_file\", F.col(\"_metadata.file_path\"))\n",
    ")\n",
    "\n",
    "# ── Uruchamiamy w trybie Triggered (AvailableNow) — przetwarza wszystkie\n",
    "# dostępne pliki i kończy się. Idealne do batch-like jobs w potoku.\n",
    "query: StreamingQuery = (\n",
    "    df_stream.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_PATH)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .trigger(availableNow=True)\n",
    "    .table(TARGET_TABLE)\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "print(f\" Stream zakończony. Przetworzone rekordy zapisane do: {TARGET_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d05cca97-ce8d-48a9-95dc-9dcee750dcc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.3 Weryfikacja — dane w tabeli i pliki w archiwum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b80e154-c485-4f2b-bb2b-a4b1dc29f248",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── 1. Ile rekordów w tabeli docelowej?\n",
    "count = spark.table(TARGET_TABLE).count()\n",
    "print(f\"Rekordów w tabeli '{TARGET_TABLE}': {count}\")\n",
    "\n",
    "# ── 2. Podgląd danych z kolumną source_file i ingested_at\n",
    "display(\n",
    "    spark.table(TARGET_TABLE)\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "# ── 3. Sprawdzamy czy source/ jest pusty (pliki przeniesione)\n",
    "print(\"\\nZawartość source/ po przetworzeniu:\")\n",
    "try:\n",
    "    remaining = dbutils.fs.ls(SOURCE_PATH)\n",
    "    for f in remaining:\n",
    "        print(f\"  {f.path}\")\n",
    "except Exception:\n",
    "    print(\"  (folder pusty lub brak plików JSON)\")\n",
    "\n",
    "# ── 4. Sprawdzamy archiwum\n",
    "print(\"\\nZawartość archive/:\")\n",
    "for f in dbutils.fs.ls(ARCHIVE_PATH):\n",
    "    print(f\"  {f.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64c031b1-ddc9-45fb-8d72-aaef83f57d4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.4 Stan strumienia — `cloud_files_state`\n",
    "\n",
    "Databricks udostępnia funkcję tabelaryczną `cloud_files_state(checkpoint_path)`,  \n",
    "która pokazuje stan każdego pliku: `queued`, `processed`, `archived`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "829d1d15-3220-4d9c-9f7e-bef1a064e8eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# cloud_files_state — TVF zwraca metadane o każdym pliku przetworzonym przez Auto Loader\n",
    "# Kolumny: path, timestamp, batchId, processedTimestamp, commit_time, size\n",
    "display(\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT\n",
    "            path,\n",
    "            size,\n",
    "            batchId,\n",
    "            timestamp       AS file_modification_time,\n",
    "            commit_time\n",
    "        FROM cloud_files_state('{CHECKPOINT_PATH}')\n",
    "        ORDER BY batchId, path\n",
    "    \"\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c33a4f98-31b9-4f30-8d29-c42d3d476814",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.5 Alternatywa: Ręczna archiwizacja przez `foreachBatch`\n",
    "\n",
    "Gdy potrzebujemy **zaawansowanej logiki archiwizacji** (np. foldery `archive/YYYY/MM/DD/`, różne reguły dla różnych plików, logowanie do tabeli audytowej), używamy `foreachBatch` zamiast `cloudFiles.cleanSource`.\n",
    "\n",
    "> Używaj `cloudFiles.cleanSource = MOVE` dla prostych przypadków.  \n",
    "> Używaj `foreachBatch` gdy potrzebujesz pełnej kontroli nad logiką przenoszenia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c2969b6-5b82-4dc0-80b8-240065f4c7d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f043794d-d262-4b4c-a9cf-8a5be4808495",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Czyszczenie zasobów po demo\n",
    "spark.sql(\"DROP TABLE IF EXISTS bonus_events_utc\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {TARGET_TABLE}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {TARGET_TABLE_V2}\")\n",
    "dbutils.fs.rm(BONUS_BASE, True)\n",
    "print(\"✅  Cleanup zakończony\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a620a1d8-020a-4551-ac4a-bae76ee82498",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Podsumowanie — Cheatsheet\n",
    "\n",
    "### Timestamps i UTC\n",
    "\n",
    "| Zadanie | Funkcja Spark / SQL |\n",
    "|---------|---------------------|\n",
    "| Ustaw UTC jako standard klastra | `spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")` |\n",
    "| Parsuj string do timestamp | `to_timestamp(col, format)` |\n",
    "| Lokalna → UTC | `to_utc_timestamp(col, \"Europe/Warsaw\")` |\n",
    "| UTC → Lokalna (wyświetlanie) | `from_utc_timestamp(col, \"Asia/Tokyo\")` |\n",
    "| Aktualny czas UTC | `current_timestamp()` |\n",
    "| Tylko data z timestamp | `to_date(col)` |\n",
    "\n",
    "### Auto Loader z archiwizacją\n",
    "\n",
    "| Opcja | Wartość | Kiedy używać |\n",
    "|-------|---------|--------------|\n",
    "| `cloudFiles.cleanSource` | `\"MOVE\"` | Prosta archiwizacja (DBR 16.4+) |\n",
    "| `cloudFiles.cleanSource.moveDestination` | ścieżka archiwum | Wymagane gdy `cleanSource=MOVE` |\n",
    "| `cloudFiles.cleanSource.retentionDuration` | `\"0 hours\"` | Natychmiastowe przenoszenie |\n",
    "| `foreachBatch` + `dbutils.fs.mv` | — | Zaawansowana logika, partycje datowe |\n",
    "\n",
    "### Reguły złotego standardu\n",
    "\n",
    "1. `spark.sql.session.timeZone = UTC` — **zawsze** w konfiguracji klastra/notatnika\n",
    "2. Kolumna `event_timestamp` → **zawsze TIMESTAMP (UTC)**\n",
    "3. Kolumna `source_timezone` → zachowaj jako metadane\n",
    "4. Konwertuj do czasu lokalnego **tylko na potrzeby wyświetlania**\n",
    "5. Partycjonuj po `event_date` (UTC) — nie po lokalnej dacie\n",
    "6. `archive/` **nigdy** nie może być podfolderem `source/`"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BONUS_timestamps_autoloader_archive",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
