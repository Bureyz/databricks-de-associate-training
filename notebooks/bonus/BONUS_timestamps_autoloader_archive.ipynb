{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aff5e873",
   "metadata": {},
   "source": [
    "# BONUS: Timestamps, UTC i Auto Loader z Archiwizacją\n",
    "\n",
    "Notatnik bonusowy dla Data Engineerów pracujących w środowisku Databricks.\n",
    "\n",
    "Obejmuje dwa kluczowe tematy:\n",
    "1. **Praca z datami i timestampami** w kontekście zespołów wielonarodowych — standardy UTC, konwersja stref czasowych, pułapki i najlepsze praktyki.\n",
    "2. **Auto Loader z mechanizmem archiwizacji** — automatyczne przenoszenie przetworzonych plików do folderu archiwum (`cloudFiles.cleanSource = MOVE`).\n",
    "\n",
    "| Poziom | Czas |\n",
    "|--------|------|\n",
    "| Intermediate / Advanced | ~45 min |\n",
    "\n",
    "> **Dokumentacja:** [Auto Loader options — docs.databricks.com](https://docs.databricks.com/aws/en/ingestion/cloud-object-storage/auto-loader/options)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd190e5c",
   "metadata": {},
   "source": [
    "## Part 1: Timestamps i UTC\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb93f96",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Konfiguracja środowiska — importy, zmienne środowiskowe, ścieżki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4204b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../setup/00_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f508e12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    StringType, TimestampType, DateType, LongType\n",
    ")\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# ── KLUCZOWE USTAWIENIE: Spark zawsze operuje na UTC wewnętrznie.\n",
    "# Ustawienie session.timeZone na UTC eliminuje niejednoznaczności\n",
    "# przy wyświetlaniu i eksporcie danych między strefami czasowymi.\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "# ── Ścieżki do demo archiwizacji (Part 2)\n",
    "BONUS_BASE       = f\"{DATASET_PATH}/bonus_demo\"\n",
    "SOURCE_PATH      = f\"{BONUS_BASE}/source\"\n",
    "ARCHIVE_PATH     = f\"{BONUS_BASE}/archive\"\n",
    "CHECKPOINT_PATH  = f\"{BONUS_BASE}/checkpoint_archive\"\n",
    "SCHEMA_PATH      = f\"{BONUS_BASE}/schema_archive\"\n",
    "TARGET_TABLE     = f\"{CATALOG}.{BRONZE_SCHEMA}.bonus_orders_archive\"\n",
    "\n",
    "# Cleanup poprzedniego uruchomienia\n",
    "dbutils.fs.rm(BONUS_BASE, True)\n",
    "print(f\"✅  Środowisko przygotowane: {BONUS_BASE}\")\n",
    "print(f\"    spark.sql.session.timeZone = {spark.conf.get('spark.sql.session.timeZone')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d06ed6",
   "metadata": {},
   "source": [
    "### 1.1 Date vs Timestamp — podstawowe różnice\n",
    "\n",
    "| Typ | Precyzja | Strefa czasowa | Użycie |\n",
    "|-----|----------|----------------|--------|\n",
    "| `DATE` | dzień | brak | daty kalendarzowe, partycjonowanie |\n",
    "| `TIMESTAMP` / `TIMESTAMP_LTZ` | mikrosekundy | tak (UTC wewnętrznie) | zdarzenia z dokładnym czasem |\n",
    "| `TIMESTAMP_NTZ` | mikrosekundy | brak | harmonogramy lokalne, etykiety czasowe |\n",
    "\n",
    "> **Reguła:** Jeśli Twoje dane reprezentują **moment w czasie** (log, transakcja, zdarzenie) → `TIMESTAMP_LTZ` (UTC).  \n",
    "> Jeśli reprezentują **datę lokalną** bez kontekstu strefy → `DATE` lub `TIMESTAMP_NTZ`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1230b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dane wejściowe — zdarzenia z różnych systemów i stref czasowych\n",
    "raw_data = [\n",
    "    # (id, zdarzenie,      timestamp jako string,       strefa źródłowa)\n",
    "    (1, \"zamówienie PL\", \"2024-06-15 14:30:00\",        \"Europe/Warsaw\"),\n",
    "    (2, \"zamówienie US\", \"2024-06-15 08:30:00\",        \"America/New_York\"),\n",
    "    (3, \"zamówienie JP\", \"2024-06-15 21:30:00\",        \"Asia/Tokyo\"),\n",
    "    (4, \"zamówienie UK\", \"2024-06-15 13:30:00\",        \"Europe/London\"),\n",
    "    (5, \"zamówienie AU\", \"2024-06-15 23:30:00\",        \"Australia/Sydney\"),\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\",            LongType(),   False),\n",
    "    StructField(\"event\",         StringType(), True),\n",
    "    StructField(\"local_time_str\", StringType(), True),\n",
    "    StructField(\"source_tz\",     StringType(), True),\n",
    "])\n",
    "\n",
    "df_raw = spark.createDataFrame(raw_data, schema)\n",
    "\n",
    "# Parsujemy string do TIMESTAMP (Spark zakłada session.timeZone = UTC)\n",
    "df_raw = df_raw.withColumn(\n",
    "    \"local_timestamp\",\n",
    "    F.to_timestamp(\"local_time_str\", \"yyyy-MM-dd HH:mm:ss\")\n",
    ")\n",
    "\n",
    "display(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c027bfc",
   "metadata": {},
   "source": [
    "### 1.2 Konwersja stref czasowych → UTC\n",
    "\n",
    "**Problem:** Każdy system dostarczył timestamp w swojej strefie lokalnej. Musimy je wszystkie sprowadzić do UTC.\n",
    "\n",
    "`from_utc_timestamp(ts, tz)` — konwertuje UTC → lokalna strefa  \n",
    "`to_utc_timestamp(ts, tz)` — konwertuje lokalna strefa → UTC  \n",
    "`convert_timezone(src_tz, tgt_tz, ts)` — nowszy wariant (SQL / Unity Catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c4f64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── to_utc_timestamp(col, tz): traktuje kolumnę jako czas w podanej strefie\n",
    "# i przelicza go na UTC. To jest wzorzec dla danych wejściowych.\n",
    "df_utc = (\n",
    "    df_raw\n",
    "    # Dla każdego rekordu znamy strefę źródłową — przeliczamy do UTC\n",
    "    .withColumn(\n",
    "        \"event_utc\",\n",
    "        F.to_utc_timestamp(\"local_timestamp\", F.col(\"source_tz\"))\n",
    "    )\n",
    "    # Kolumna daty (tylko dzień) — przydatna do partycjonowania\n",
    "    .withColumn(\"event_date\", F.to_date(\"event_utc\"))\n",
    ")\n",
    "\n",
    "display(\n",
    "    df_utc.select(\n",
    "        \"id\", \"event\", \"source_tz\",\n",
    "        \"local_time_str\",\n",
    "        \"event_utc\",\n",
    "        \"event_date\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957a8fe3",
   "metadata": {},
   "source": [
    "### 1.3 Wyświetlanie czasu lokalnego — `from_utc_timestamp`\n",
    "\n",
    "Dane są przechowywane w UTC. Gdy użytkownik polskiego biura chce zobaczyć czas lokalny, konwertujemy **tylko przy wyświetlaniu** — nie zmieniamy wartości w tabeli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2888bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── from_utc_timestamp(col, tz): przelicza UTC → podaną strefę lokalną\n",
    "# Wzorzec: przechowuj UTC, konwertuj tylko dla prezentacji\n",
    "\n",
    "TARGET_TIMEZONES = {\n",
    "    \"PL (Warsaw)\":   \"Europe/Warsaw\",\n",
    "    \"US (New York)\": \"America/New_York\",\n",
    "    \"JP (Tokyo)\":    \"Asia/Tokyo\",\n",
    "}\n",
    "\n",
    "df_display = df_utc.select(\"id\", \"event\", \"event_utc\")\n",
    "\n",
    "for label, tz in TARGET_TIMEZONES.items():\n",
    "    col_name = f\"display_{label.split(' ')[0].lower()}\"\n",
    "    df_display = df_display.withColumn(\n",
    "        col_name,\n",
    "        F.date_format(\n",
    "            F.from_utc_timestamp(\"event_utc\", tz),\n",
    "            \"yyyy-MM-dd HH:mm:ss z\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "display(df_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e6700c",
   "metadata": {},
   "source": [
    "### 1.4 Pułapka: `session.timeZone` a parsowanie stringów\n",
    "\n",
    "Jeśli nie ustawimy `spark.sql.session.timeZone = UTC`, Spark interpretuje stringe bez informacji o strefie jako **czas lokalny serwera klastra** (np. UTC-5 w AWS us-east).  \n",
    "Godziny zostaną przesunięte przy odczycie/zapisie — bug trudny do wykrycia w produkcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fb9118",
   "metadata": {},
   "outputs": [],
   "source": [
    "TS_STRING = \"2024-06-15 12:00:00\"  # brak informacji o strefie\n",
    "\n",
    "# ── Demonstracja wpływu session.timeZone na parsowanie\n",
    "results = []\n",
    "for tz_setting in [\"UTC\", \"America/New_York\", \"Asia/Tokyo\", \"Europe/Warsaw\"]:\n",
    "    spark.conf.set(\"spark.sql.session.timeZone\", tz_setting)\n",
    "    val = spark.sql(f\"SELECT CAST('{TS_STRING}' AS TIMESTAMP) AS ts\").collect()[0][\"ts\"]\n",
    "    results.append((tz_setting, str(val)))\n",
    "\n",
    "# Przywracamy UTC (standard)\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "df_tz_demo = spark.createDataFrame(results, [\"session_timeZone\", \"parsed_timestamp_utc_repr\"])\n",
    "display(df_tz_demo)\n",
    "\n",
    "print(\"\\n✅  Przywrócono spark.sql.session.timeZone = UTC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a8fda9",
   "metadata": {},
   "source": [
    "### 1.5 Best practices — tabela Delta z UTC\n",
    "\n",
    "Rekomendowany schemat dla tabel produkcyjnych przyjmujących zdarzenia z różnych stref czasowych:\n",
    "\n",
    "```\n",
    "event_id          BIGINT\n",
    "event_timestamp   TIMESTAMP    -- zawsze UTC (TIMESTAMP_LTZ)\n",
    "event_date        DATE         -- partycja po UTC date\n",
    "source_timezone   STRING       -- zachowujemy info o strefie źródłowej\n",
    "ingested_at       TIMESTAMP    -- czas ingestii (current_timestamp() = UTC)\n",
    "```\n",
    "\n",
    "Nigdy nie mieszaj stref w jednej kolumnie. Zachowaj `source_timezone` jako metadane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e6302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "# Tworzymy tabelę Delta ze wzorcowym schematem UTC\n",
    "spark.sql(\"DROP TABLE IF EXISTS bonus_events_utc\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS bonus_events_utc (\n",
    "        event_id        BIGINT,\n",
    "        event_timestamp TIMESTAMP,   -- zawsze UTC\n",
    "        event_date      DATE,        -- partycja po UTC date\n",
    "        source_timezone STRING,      -- strefa źródłowa jako metadane\n",
    "        event_name      STRING,\n",
    "        ingested_at     TIMESTAMP    -- czas ingestii (UTC)\n",
    "    )\n",
    "    USING DELTA\n",
    "    PARTITIONED BY (event_date)\n",
    "    TBLPROPERTIES (\n",
    "        'delta.enableChangeDataFeed' = 'false',\n",
    "        'comment' = 'Zdarzenia z wielu stref — timestamp zawsze UTC'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Wstawiamy dane z wcześniej przygotowanego DataFrame\n",
    "df_final = (\n",
    "    df_utc\n",
    "    .withColumnRenamed(\"id\", \"event_id\")\n",
    "    .withColumnRenamed(\"event\", \"event_name\")\n",
    "    .withColumnRenamed(\"event_utc\", \"event_timestamp\")\n",
    "    .withColumn(\"ingested_at\", F.current_timestamp())\n",
    "    .select(\"event_id\", \"event_timestamp\", \"event_date\",\n",
    "            \"source_tz\", \"event_name\", \"ingested_at\")\n",
    "    .withColumnRenamed(\"source_tz\", \"source_timezone\")\n",
    ")\n",
    "\n",
    "df_final.write.mode(\"append\").saveAsTable(\"bonus_events_utc\")\n",
    "\n",
    "display(spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        event_id,\n",
    "        event_name,\n",
    "        source_timezone,\n",
    "        event_timestamp                                          AS utc_time,\n",
    "        from_utc_timestamp(event_timestamp, 'Europe/Warsaw')    AS warsaw_time,\n",
    "        from_utc_timestamp(event_timestamp, 'America/New_York') AS new_york_time,\n",
    "        from_utc_timestamp(event_timestamp, 'Asia/Tokyo')       AS tokyo_time\n",
    "    FROM bonus_events_utc\n",
    "    ORDER BY event_id\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f582df",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Auto Loader z Archiwizacją Plików\n",
    "\n",
    "---\n",
    "\n",
    "### Scenariusz\n",
    "\n",
    "System zamówień co kilka minut zrzuca pliki JSON do folderu `source/`.  \n",
    "Po przetworzeniu pliki muszą zostać **przeniesione do `archive/`**, zamiast usuwane — wymaganie compliance.\n",
    "\n",
    "**Opcja dokumentacyjna:** [`cloudFiles.cleanSource = MOVE`](https://docs.databricks.com/aws/en/ingestion/cloud-object-storage/auto-loader/options#common-auto-loader-options)\n",
    "\n",
    "| Opcja | Wartość | Opis |\n",
    "|-------|---------|------|\n",
    "| `cloudFiles.cleanSource` | `MOVE` | Przenieś pliki po przetworzeniu |\n",
    "| `cloudFiles.cleanSource.moveDestination` | ścieżka | Gdzie przeносić pliki (musi być w tym samym bucket/container) |\n",
    "| `cloudFiles.cleanSource.retentionDuration` | `0 hours` | Jak długo czekać przed przeniesieniem (domyślnie 30 dni; min. 0 dla MOVE) |\n",
    "\n",
    "> **Uwaga:** Auto Loader musi mieć uprawnienia zapisu do `source/` i `archive/`. Folder `archive/` **nie może być podfolderem** `source/` — spowoduje ponowną ingestię."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c6e12a",
   "metadata": {},
   "source": [
    "### 2.1 Przygotowanie danych symulacyjnych\n",
    "\n",
    "Tworzymy 3 mikro-batche plików JSON symulujące przybycie zamówień."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb81afa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Wczytujemy istniejące pliki zamówień ze streamingu i dzielimy na 3 batche\n",
    "SOURCE_STREAM_FILES = f\"{DATASET_PATH}/orders/stream/*.json\"\n",
    "df_orders = spark.read.json(SOURCE_STREAM_FILES)\n",
    "batches = df_orders.randomSplit([0.33, 0.33, 0.34], seed=42)\n",
    "\n",
    "for i, batch_df in enumerate(batches, start=1):\n",
    "    batch_df.coalesce(1).write.mode(\"overwrite\").json(f\"{SOURCE_PATH}/batch_{i:02d}\")\n",
    "    count = batch_df.count()\n",
    "    print(f\"  batch_{i:02d}: {count} rekordów → {SOURCE_PATH}/batch_{i:02d}\")\n",
    "\n",
    "# Sprawdzamy co jest w source\n",
    "print(\"\\nZawartość source/:\")\n",
    "for f in dbutils.fs.ls(SOURCE_PATH):\n",
    "    print(f\"  {f.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce56486",
   "metadata": {},
   "source": [
    "### 2.2 Auto Loader z `cloudFiles.cleanSource = MOVE`\n",
    "\n",
    "Kluczowe opcje:\n",
    "\n",
    "- `cloudFiles.cleanSource` = `\"MOVE\"` — automatycznie przenosi pliki po przetworzeniu\n",
    "- `cloudFiles.cleanSource.moveDestination` — folder docelowy (archiwum)\n",
    "- `cloudFiles.cleanSource.retentionDuration` — czas przed przeniesieniem (min. `0 hours` dla MOVE)\n",
    "- `cloudFiles.schemaLocation` — wymagane przy inferencji schematu\n",
    "- `cloudFiles.inferColumnTypes` = `True` — inferuje typy (nie tylko String)\n",
    "\n",
    "> **Ważne:** `retentionDuration` = `\"0 hours\"` oznacza przeniesienie natychmiast po potwierdzeniu commit przez strumień. Dostępne od **Databricks Runtime 16.4+**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27ffd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.streaming import StreamingQuery\n",
    "\n",
    "# ── Tworzymy strumień Auto Loader z archiwizacją\n",
    "df_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\",                    \"json\")\n",
    "    # ── Schemat i typy\n",
    "    .option(\"cloudFiles.schemaLocation\",            SCHEMA_PATH)\n",
    "    .option(\"cloudFiles.inferColumnTypes\",          \"true\")\n",
    "    # ── Archiwizacja: przenieś przetworzone pliki do ARCHIVE_PATH\n",
    "    .option(\"cloudFiles.cleanSource\",               \"MOVE\")\n",
    "    .option(\"cloudFiles.cleanSource.moveDestination\", ARCHIVE_PATH)\n",
    "    .option(\"cloudFiles.cleanSource.retentionDuration\", \"0 hours\")  # DBR 16.4+\n",
    "    # ── Kontrola przepustowości\n",
    "    .option(\"cloudFiles.maxFilesPerTrigger\",        \"1\")\n",
    "    # ── Dodaje kolumnę _metadata z nazwą pliku, rozmiarem, czasem modyfikacji\n",
    "    .load(SOURCE_PATH)\n",
    "    # Dodajemy timestamp ingestii (UTC)\n",
    "    .withColumn(\"ingested_at\", F.current_timestamp())\n",
    "    # Wyciągamy nazwę pliku źródłowego z metadanych\n",
    "    .withColumn(\"source_file\", F.col(\"_metadata.file_path\"))\n",
    ")\n",
    "\n",
    "# ── Uruchamiamy w trybie Triggered (AvailableNow) — przetwarza wszystkie\n",
    "# dostępne pliki i kończy się. Idealne do batch-like jobs w potoku.\n",
    "query: StreamingQuery = (\n",
    "    df_stream.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_PATH)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .trigger(availableNow=True)\n",
    "    .table(TARGET_TABLE)\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "print(f\"✅  Stream zakończony. Przetworzone rekordy zapisane do: {TARGET_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e0c56d",
   "metadata": {},
   "source": [
    "### 2.3 Weryfikacja — dane w tabeli i pliki w archiwum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925f4267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 1. Ile rekordów w tabeli docelowej?\n",
    "count = spark.table(TARGET_TABLE).count()\n",
    "print(f\"Rekordów w tabeli '{TARGET_TABLE}': {count}\")\n",
    "\n",
    "# ── 2. Podgląd danych z kolumną source_file i ingested_at\n",
    "display(\n",
    "    spark.table(TARGET_TABLE)\n",
    "    .select(\"order_id\", \"customer_id\", \"order_status\",\n",
    "            \"source_file\", \"ingested_at\")\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "# ── 3. Sprawdzamy czy source/ jest pusty (pliki przeniesione)\n",
    "print(\"\\nZawartość source/ po przetworzeniu:\")\n",
    "try:\n",
    "    remaining = dbutils.fs.ls(SOURCE_PATH)\n",
    "    for f in remaining:\n",
    "        print(f\"  {f.path}\")\n",
    "except Exception:\n",
    "    print(\"  (folder pusty lub brak plików JSON)\")\n",
    "\n",
    "# ── 4. Sprawdzamy archiwum\n",
    "print(\"\\nZawartość archive/:\")\n",
    "for f in dbutils.fs.ls(ARCHIVE_PATH):\n",
    "    print(f\"  {f.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39975d2",
   "metadata": {},
   "source": [
    "### 2.4 Stan strumienia — `cloud_files_state`\n",
    "\n",
    "Databricks udostępnia funkcję tabelaryczną `cloud_files_state(checkpoint_path)`,  \n",
    "która pokazuje stan każdego pliku: `queued`, `processed`, `archived`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa6804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud_files_state — TVF zwraca metadane o każdym pliku przetworzonym przez Auto Loader\n",
    "# Kolumny: path, timestamp, batchId, processedTimestamp, commit_time, size\n",
    "display(\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT\n",
    "            path,\n",
    "            size,\n",
    "            batchId,\n",
    "            timestamp       AS file_modification_time,\n",
    "            commit_time\n",
    "        FROM cloud_files_state('{CHECKPOINT_PATH}')\n",
    "        ORDER BY batchId, path\n",
    "    \"\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73e0563",
   "metadata": {},
   "source": [
    "### 2.5 Alternatywa: Ręczna archiwizacja przez `foreachBatch`\n",
    "\n",
    "Gdy potrzebujemy **zaawansowanej logiki archiwizacji** (np. foldery `archive/YYYY/MM/DD/`, różne reguły dla różnych plików, logowanie do tabeli audytowej), używamy `foreachBatch` zamiast `cloudFiles.cleanSource`.\n",
    "\n",
    "> Używaj `cloudFiles.cleanSource = MOVE` dla prostych przypadków.  \n",
    "> Używaj `foreachBatch` gdy potrzebujesz pełnej kontroli nad logiką przenoszenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4aa8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone as dt_timezone\n",
    "\n",
    "# Ścieżki dla alternatywnego demo\n",
    "SOURCE_PATH_V2     = f\"{BONUS_BASE}/source_v2\"\n",
    "ARCHIVE_PATH_V2    = f\"{BONUS_BASE}/archive_v2\"\n",
    "CHECKPOINT_PATH_V2 = f\"{BONUS_BASE}/checkpoint_v2\"\n",
    "SCHEMA_PATH_V2     = f\"{BONUS_BASE}/schema_v2\"\n",
    "TARGET_TABLE_V2    = f\"{CATALOG}.{BRONZE_SCHEMA}.bonus_orders_manual_archive\"\n",
    "\n",
    "# Kopiujemy batch_01 z powrotem do nowego source\n",
    "batches[0].coalesce(1).write.mode(\"overwrite\").json(f\"{SOURCE_PATH_V2}/batch_01\")\n",
    "print(f\"✅  Dane źródłowe gotowe: {SOURCE_PATH_V2}\")\n",
    "\n",
    "\n",
    "def archive_batch(batch_df, batch_id: int):\n",
    "    \"\"\"\n",
    "    Funkcja wywoływana dla każdego micro-batcha.\n",
    "    1. Wyciąga unikalne ścieżki do plików z _metadata.\n",
    "    2. Zapisuje DataFrame do tabeli Delta.\n",
    "    3. Przenosi pliki do folderu archiwum z partycją po dacie UTC.\n",
    "    \"\"\"\n",
    "    if batch_df.isEmpty():\n",
    "        return\n",
    "\n",
    "    # Archiwum z partycją po dacie UTC — łatwe zarządzanie retencją\n",
    "    today_utc = datetime.now(dt_timezone.utc).strftime(\"%Y/%m/%d\")\n",
    "    archive_today = f\"{ARCHIVE_PATH_V2}/{today_utc}/batch_{batch_id:04d}\"\n",
    "\n",
    "    # Pobieramy unikalne ścieżki plików źródłowych\n",
    "    source_files = (\n",
    "        batch_df\n",
    "        .select(F.col(\"_metadata.file_path\").alias(\"path\"))\n",
    "        .distinct()\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    # Zapisujemy dane do tabeli Delta\n",
    "    (\n",
    "        batch_df\n",
    "        .withColumn(\"ingested_at\", F.current_timestamp())\n",
    "        .withColumn(\"batch_id\",    F.lit(batch_id))\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .saveAsTable(TARGET_TABLE_V2)\n",
    "    )\n",
    "\n",
    "    # Przenosimy każdy plik do archiwum\n",
    "    for row in source_files:\n",
    "        src = row[\"path\"]\n",
    "        # Zachowujemy tylko nazwę pliku\n",
    "        filename = src.split(\"/\")[-1]\n",
    "        dst = f\"{archive_today}/{filename}\"\n",
    "        try:\n",
    "            dbutils.fs.mv(src, dst)\n",
    "            print(f\"  [batch {batch_id}] przeniesiono: {src} → {dst}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [batch {batch_id}] BŁĄD: {src} → {e}\")\n",
    "\n",
    "\n",
    "# Uruchamiamy strumień z foreachBatch\n",
    "df_stream_v2 = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\",       \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", SCHEMA_PATH_V2)\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .load(SOURCE_PATH_V2)\n",
    ")\n",
    "\n",
    "query_v2 = (\n",
    "    df_stream_v2.writeStream\n",
    "    .foreachBatch(archive_batch)\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_PATH_V2)\n",
    "    .trigger(availableNow=True)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query_v2.awaitTermination()\n",
    "print(f\"\\n✅  Archiwizacja zakończona. Sprawdź: {ARCHIVE_PATH_V2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fbcec8",
   "metadata": {},
   "source": [
    "### 2.6 Weryfikacja archiwum v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaa2c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podgląd danych w tabeli\n",
    "display(\n",
    "    spark.table(TARGET_TABLE_V2)\n",
    "    .select(\"order_id\", \"customer_id\", \"order_status\", \"batch_id\", \"ingested_at\")\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "# Struktura archiwum\n",
    "print(\"Struktura archiwum (foreachBatch):\")\n",
    "def list_recursive(path, depth=0):\n",
    "    try:\n",
    "        for f in dbutils.fs.ls(path):\n",
    "            print(\"  \" * depth + f.name)\n",
    "            if f.name.endswith(\"/\"):\n",
    "                list_recursive(f.path, depth + 1)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "list_recursive(ARCHIVE_PATH_V2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa0c60e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3eca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Czyszczenie zasobów po demo\n",
    "spark.sql(\"DROP TABLE IF EXISTS bonus_events_utc\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {TARGET_TABLE}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {TARGET_TABLE_V2}\")\n",
    "dbutils.fs.rm(BONUS_BASE, True)\n",
    "print(\"✅  Cleanup zakończony\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f3d73d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Podsumowanie — Cheatsheet\n",
    "\n",
    "### Timestamps i UTC\n",
    "\n",
    "| Zadanie | Funkcja Spark / SQL |\n",
    "|---------|---------------------|\n",
    "| Ustaw UTC jako standard klastra | `spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")` |\n",
    "| Parsuj string do timestamp | `to_timestamp(col, format)` |\n",
    "| Lokalna → UTC | `to_utc_timestamp(col, \"Europe/Warsaw\")` |\n",
    "| UTC → Lokalna (wyświetlanie) | `from_utc_timestamp(col, \"Asia/Tokyo\")` |\n",
    "| Aktualny czas UTC | `current_timestamp()` |\n",
    "| Tylko data z timestamp | `to_date(col)` |\n",
    "\n",
    "### Auto Loader z archiwizacją\n",
    "\n",
    "| Opcja | Wartość | Kiedy używać |\n",
    "|-------|---------|--------------|\n",
    "| `cloudFiles.cleanSource` | `\"MOVE\"` | Prosta archiwizacja (DBR 16.4+) |\n",
    "| `cloudFiles.cleanSource.moveDestination` | ścieżka archiwum | Wymagane gdy `cleanSource=MOVE` |\n",
    "| `cloudFiles.cleanSource.retentionDuration` | `\"0 hours\"` | Natychmiastowe przenoszenie |\n",
    "| `foreachBatch` + `dbutils.fs.mv` | — | Zaawansowana logika, partycje datowe |\n",
    "\n",
    "### Reguły złotego standardu\n",
    "\n",
    "1. `spark.sql.session.timeZone = UTC` — **zawsze** w konfiguracji klastra/notatnika\n",
    "2. Kolumna `event_timestamp` → **zawsze TIMESTAMP (UTC)**\n",
    "3. Kolumna `source_timezone` → zachowaj jako metadane\n",
    "4. Konwertuj do czasu lokalnego **tylko na potrzeby wyświetlania**\n",
    "5. Partycjonuj po `event_date` (UTC) — nie po lokalnej dacie\n",
    "6. `archive/` **nigdy** nie może być podfolderem `source/`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
