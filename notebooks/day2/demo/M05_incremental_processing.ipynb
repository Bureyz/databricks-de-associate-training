{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d886e96e-caab-4179-bca5-7416b87f4435",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# M05: Incremental Processing\n",
    "\n",
    "| Exam Domain | Weight |\n",
    "|---|---|\n",
    "| Incremental Data Processing | 20% |\n",
    "| ELT with Spark SQL and Python | 29% |\n",
    "\n",
    "Topics: COPY INTO, Auto Loader (cloudFiles), Structured Streaming, Trigger Modes, Schema Evolution, Rescued Data, Stream-Static Joins.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9267c39b-ed38-43a1-a273-b356418b368d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "Initialize the environment, import libraries, and prepare simulated data sources for all ingestion demos in this module.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb7ab4af-0592-4a45-8677-9c783b689c8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../setup/00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5932f8b8-d869-4108-b696-a10fce1d3935",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Configuration\n",
    "\n",
    "Import libraries and define paths for source data, checkpoints, and schema locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6ab1a78-f2d9-4dc6-8b98-9e30ba0f0620",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e317eca-d483-430c-a87a-b2ddf7f51cad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set default catalog and schema\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "# === SOURCE DATA (REAL DATASET) ===\n",
    "SOURCE_CUSTOMERS = f\"{DATASET_PATH}/customers/customers.csv\"\n",
    "SOURCE_ORDERS = f\"{DATASET_PATH}/orders/orders_batch.json\"\n",
    "\n",
    "# === DEMO PATHS (SIMULATED ARRIVAL) ===\n",
    "DEMO_BASE_PATH = f\"{DATASET_PATH}/ingestion_demo\"\n",
    "BATCH_SOURCE_PATH = f\"{DEMO_BASE_PATH}/batch_source\"\n",
    "STREAM_SOURCE_PATH = f\"{DEMO_BASE_PATH}/stream_source\"\n",
    "\n",
    "# === TECHNICAL PATHS ===\n",
    "CHECKPOINT_BASE_PATH = f\"{DEMO_BASE_PATH}/checkpoints\"\n",
    "SCHEMA_BASE_PATH = f\"{DEMO_BASE_PATH}/schemas\"\n",
    "BAD_RECORDS_PATH = f\"{DEMO_BASE_PATH}/bad_records\"\n",
    "\n",
    "# Cleanup from previous runs\n",
    "dbutils.fs.rm(DEMO_BASE_PATH, True)\n",
    "print(f\"Demo environment prepared at: {DEMO_BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "895e7ba1-07a3-4e38-9be5-b75fab0b300a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"CATALOG\", CATALOG),\n",
    "        (\"BRONZE_SCHEMA\", BRONZE_SCHEMA),\n",
    "        (\"SILVER_SCHEMA\", SILVER_SCHEMA),\n",
    "        (\"USER\", raw_user),\n",
    "        (\"CUSTOMERS_CSV\", SOURCE_CUSTOMERS),\n",
    "        (\"STREAMING_SOURCE_PATH\", STREAM_SOURCE_PATH)\n",
    "    ], [\"Variable\", \"Value\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0cbfc77-8fd1-4710-81cb-a4541cbb4e72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === DATA PREPARATION (SIMULATION) ===\n",
    "\n",
    "# 1. Prepare Batch Data (Customers)\n",
    "# We split customers into 2 days for COPY INTO demo\n",
    "df_customers = spark.read.option(\"header\", \"true\").csv(SOURCE_CUSTOMERS)\n",
    "df_batch_day1, df_batch_day2,df_batch_day3,df_batch_day4 = df_customers.randomSplit([0.25]*4, seed=42)\n",
    "\n",
    "# Save Day 1 immediately\n",
    "df_batch_day1.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{BATCH_SOURCE_PATH}/day1\")\n",
    "print(f\"Batch Data: Day 1 ready at {BATCH_SOURCE_PATH}/day1\")\n",
    "\n",
    "# 2. Prepare Streaming Data (Orders)\n",
    "# We take existing stream files, merge them, and split into 10 micro-batches for simulation\n",
    "SOURCE_STREAM_FILES = f\"{DATASET_PATH}/orders/stream/*.json\"\n",
    "df_all_orders = spark.read.json(SOURCE_STREAM_FILES)\n",
    "\n",
    "# Split into 20 parts (5% each)\n",
    "stream_batches = df_all_orders.randomSplit([0.05] * 20, seed=42)\n",
    "\n",
    "# Save Batch 1 immediately to start the stream\n",
    "stream_batches[0].write.mode(\"overwrite\").json(f\"{STREAM_SOURCE_PATH}/batch_01\")\n",
    "print(f\"Stream Data: Batch 1 ready at {STREAM_SOURCE_PATH}/batch_01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cbd6bab-cdb3-4715-b258-edc2b7a56050",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "### Data Loading Methods Overview\n",
    "\n",
    "<img src=\"../../../assets/images/d271b8dfc29049aaab22d97536aca66d.avif\" width=\"800\">\n",
    "\n",
    "| Feature | CTAS | COPY INTO | Auto Loader |\n",
    "|---------|------|-----------|-------------|\n",
    "| **Incremental** | No | Yes | Yes |\n",
    "| **Idempotent** | No | Yes | Yes |\n",
    "| **Schema Evolution** | No | Limited | Advanced |\n",
    "| **File Tracking** | No | Metadata | Checkpoint |\n",
    "| **Scalability** | Low | Medium | High |\n",
    "| **Streaming** | No | No | Yes |\n",
    "| **Use Case** | One-time | Scheduled batch | Real-time/Streaming |\n",
    "\n",
    "> **Exam Tip:** Auto Loader (`cloudFiles`) is the **recommended** ingestion method for new projects. COPY INTO is suitable for scheduled batch loads with <100K files.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08f69694-2300-42b9-b550-62420e42ce90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Demo: CTAS (Create Table As Select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01cd3356-9001-40c1-b460-a4cb9a899d42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example: Load CSV from volume to customer_cts table using CTAS\n",
    "\n",
    "table_name = \"customer_cts\"\n",
    "\n",
    "display(spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {table_name} AS\n",
    "SELECT *\n",
    "FROM csv.`{SOURCE_CUSTOMERS}`\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b63d78bb-a625-47a8-b332-39d2fef620f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from customer_cts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abd5d815-3650-41ee-ae24-7cb4cc75f0da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## COPY INTO — Batch Loading\n",
    "\n",
    "COPY INTO provides idempotent, file-level batch ingestion from cloud storage into Delta tables, automatically tracking which files have already been loaded.\n",
    "\n",
    "> **Exam Tip:** `COPY INTO` is **idempotent** — it tracks processed files and won't load duplicates on re-run.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fec72cf5-ca12-4f93-be80-926e1ec05d60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Demo: COPY INTO from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d395a42e-48b0-4533-869a-eb646b96ee8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TABLE_CUSTOMERS = f\"{BRONZE_SCHEMA}.customers_batch\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fd0dbb4-7c5d-404c-bb44-e599dfb1ceac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Creating target table:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4c46f3d-dc6a-402f-9f0d-52492c9c48b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {TABLE_CUSTOMERS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df68fd13-424c-4aca-af28-40c265795467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {TABLE_CUSTOMERS} (\n",
    "  customer_id STRING,\n",
    "  first_name STRING,\n",
    "  last_name STRING,\n",
    "  email STRING,\n",
    "  phone STRING,\n",
    "  city STRING,\n",
    "  state STRING,\n",
    "  country STRING,\n",
    "  registration_date DATE,\n",
    "  customer_segment STRING,\n",
    "  _ingestion_timestamp TIMESTAMP\n",
    ") USING DELTA\n",
    "COMMENT 'Customers data - Bronze layer'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "492bce7c-49bc-4b56-8ea4-c8fe79989674",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Execute COPY INTO:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44e063b0-6d21-4587-9ea3-6f922721f169",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load Day 1 data\n",
    "result = spark.sql(f\"\"\"\n",
    "COPY INTO {TABLE_CUSTOMERS}\n",
    "FROM (\n",
    "  SELECT \n",
    "    customer_id,\n",
    "    first_name,\n",
    "    last_name,\n",
    "    email,\n",
    "    phone,\n",
    "    city,\n",
    "    state,\n",
    "    country,\n",
    "    TO_DATE(registration_date, 'yyyy-MM-dd') as registration_date,\n",
    "    customer_segment,\n",
    "    current_timestamp() as _ingestion_timestamp\n",
    "  FROM '{BATCH_SOURCE_PATH}/day1'\n",
    ")\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS ('header' = 'true', 'inferSchema' = 'false')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true')\n",
    "\"\"\")\n",
    "\n",
    "display(result)\n",
    "display(spark.table(TABLE_CUSTOMERS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fb116ae-62a4-4b81-abc3-90af3dd9d0a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Demo: Idempotency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e89be490-2f32-4a92-aefc-53611efb21f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "count_before = spark.table(TABLE_CUSTOMERS).count()\n",
    "\n",
    "# Re-run COPY INTO (same source path)\n",
    "spark.sql(f\"\"\"\n",
    "COPY INTO {TABLE_CUSTOMERS}\n",
    "FROM (\n",
    "  SELECT \n",
    "    customer_id, first_name, last_name, email, phone,\n",
    "    city, state, country,\n",
    "    TO_DATE(registration_date, 'yyyy-MM-dd') as registration_date,\n",
    "    customer_segment,\n",
    "    current_timestamp() as _ingestion_timestamp\n",
    "  FROM '{BATCH_SOURCE_PATH}/*'\n",
    ")\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS ('header' = 'true', 'inferSchema' = 'false')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c5b861d-fdbe-4299-94c1-4300027e44eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "count_after = spark.table(TABLE_CUSTOMERS).count()\n",
    "display(count_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bbb9217-d435-4acc-af8e-c3168d40a06f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"Before\", count_before),\n",
    "        (\"After\", count_after),\n",
    "        (\"Difference\", count_after - count_before)\n",
    "    ], [\"State\", \"Count\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7efe11e8-74c3-432c-b4cd-f85518fb9ae9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Demo: Adding More Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "119c2ffe-4ef2-4059-ab80-c4d59b1cc1a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load all days data\n",
    "result = spark.sql(f\"\"\"\n",
    "COPY INTO {TABLE_CUSTOMERS}\n",
    "FROM (\n",
    "  SELECT \n",
    "    customer_id,\n",
    "    first_name,\n",
    "    last_name,\n",
    "    email,\n",
    "    phone,\n",
    "    city,\n",
    "    state,\n",
    "    country,\n",
    "    TO_DATE(registration_date, 'yyyy-MM-dd') as registration_date,\n",
    "    customer_segment,\n",
    "    current_timestamp() as _ingestion_timestamp\n",
    "  FROM '{BATCH_SOURCE_PATH}/*'\n",
    ")\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS ('header' = 'true', 'inferSchema' = 'false')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true')\n",
    "\"\"\")\n",
    "\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc4ffa9c-6583-4c48-8766-272032f4c554",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_batch_day2.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{BATCH_SOURCE_PATH}/day6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efda3c19-1b8a-4ae2-851c-6ee85d549cd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_batch_day3.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{BATCH_SOURCE_PATH}/day3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07e30995-0274-4e0a-959f-6daabdac4552",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_batch_day4.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{BATCH_SOURCE_PATH}/day4\")\n",
    "display(spark.table(TABLE_CUSTOMERS).count())\n",
    "\n",
    "display(spark.table(TABLE_CUSTOMERS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a85699d-66dd-4b83-91cc-8267b0ef312c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Auto Loader — Streaming Ingestion\n",
    "\n",
    "Auto Loader (`cloudFiles`) provides scalable, checkpoint-based streaming ingestion from cloud storage with built-in schema evolution and exactly-once guarantees.\n",
    "\n",
    "> **Exam Tip:** Auto Loader uses `cloudFiles` format with **checkpoint-based** exactly-once processing. It's the **recommended** method for new ingestion pipelines.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91de6578-4381-4007-8cb9-5197c4254ce2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Trigger Modes & Output Modes\n",
    "\n",
    "| Trigger Mode | Behavior | Use Case |\n",
    "|------|----------|----------|\n",
    "| `availableNow=True` | Process all available → stop | Scheduled jobs |\n",
    "| `processingTime=\"10 seconds\"` | Every N seconds | Real-time |\n",
    "| `once=True` | Legacy (deprecated) | — |\n",
    "\n",
    "<img src=\"../../../assets/images/113d4c6273584dc6aa6882e2afe85d0b.png\" width=\"800\">\n",
    "\n",
    "| Output Mode | Description | Use Case |\n",
    "|------|-------------|----------|\n",
    "| **Append** | Only new rows | Raw data ingestion (stateless) |\n",
    "| **Update** | Only updated rows | Aggregations (stateful) |\n",
    "| **Complete** | Entire result rewritten | Small aggregations |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b55bec23-5424-416b-93bc-d92f0edbe2eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    dbutils.fs.rm(CHECKPOINT_BASE_PATH, True)\n",
    "    dbutils.fs.rm(SCHEMA_BASE_PATH, True)\n",
    "    dbutils.fs.rm(BAD_RECORDS_PATH, True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ecddc1e-8381-47b1-9bb8-4bb910967534",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TARGET_TABLE_AL = f\"{BRONZE_SCHEMA}.orders_autoloader\"\n",
    "CHECKPOINT_AL = f\"{CHECKPOINT_BASE_PATH}/autoloader\"\n",
    "SCHEMA_AL = f\"{SCHEMA_BASE_PATH}/autoloader\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74068144-ba19-48f7-b56f-19e3848255f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Auto Loader readStream configuration:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11639851-8bd4-45db-b9b9-d6774dae5c1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Auto Loader Configuration Options\n",
    "\n",
    "| Category | Key Options |\n",
    "|---|---|\n",
    "| **Common** | `cloudFiles.format`, `cloudFiles.schemaLocation`, `cloudFiles.includeExistingFiles` |\n",
    "| **Schema** | `cloudFiles.inferColumnTypes`, `cloudFiles.schemaEvolutionMode` (`addNewColumns`, `rescue`, `failOnNewColumns`) |\n",
    "| **File Discovery** | `cloudFiles.useIncrementalListing`, `cloudFiles.maxFilesPerTrigger` |\n",
    "| **Notification** | `cloudFiles.useNotifications` (cloud-specific: SQS, Event Grid) |\n",
    "\n",
    "[Full options reference](https://learn.microsoft.com/en-us/azure/databricks/ingestion/cloud-object-storage/auto-loader/options/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5480ace8-e9ca-40cc-8060-c2a293774fbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {TARGET_TABLE_AL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56253bc2-82c1-469c-9aa8-e0dd3372407c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_autoloader = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", SCHEMA_AL)\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")\n",
    "    .load(STREAM_SOURCE_PATH) # Reading from our simulated stream source\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88ff21ec-c900-482f-98b4-48f6b62adf10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Adding metadata columns:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b19880d3-5c2a-4f50-a42a-bcd8ef1e5e20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_enriched = (df_autoloader\n",
    "    .withColumn(\"_processing_time\", F.current_timestamp())\n",
    "    .withColumn(\"_source_file\", col(\"_metadata.file_path\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf28a8c5-299e-4e8c-b7b5-8eb2735ffea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Start streaming with `availableNow` trigger:**\n",
    "\n",
    "> `availableNow` - processes all available data and stops (batch-like streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf328d6a-3818-4c43-a390-a713d67e7eb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = (df_enriched.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_AL)\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(TARGET_TABLE_AL)\n",
    ")\n",
    "display(spark.table(f\"{TARGET_TABLE_AL}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b625f881-ac95-4166-99c2-1f5655ce9860",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Auto Loader results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66517539-62ea-4bf1-a818-7bca51b8fee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"Records Loaded\", str(spark.table(TARGET_TABLE_AL).count())),\n",
    "        (\"Source Files\", str(spark.table(TARGET_TABLE_AL).select(\"_source_file\").distinct().count()))\n",
    "    ], [\"Metric\", \"Value\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45836a58-c68e-40bf-aa85-4fc491f43818",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Demo: Incremental Processing — Add New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "639b741a-669b-4e73-8eaa-cb25889535a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save Batch 2 immediately to start the stream\n",
    "stream_batches[1].write.mode(\"overwrite\").json(f\"{STREAM_SOURCE_PATH}/batch_02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f553058-19d4-4a74-9d1d-b0fa804d4955",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Demo: Continuous Processing (processingTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9492c6c-cc7d-4986-b540-d7edc1059372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {TARGET_TABLE_AL}_continuous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "625aebf2-2adb-424f-9c7c-da8029669b32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Start stream in background\n",
    "query_continuous = (df_enriched.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_AL}_continuous\")\n",
    "    .trigger(processingTime=\"10 seconds\")\n",
    "    .toTable(f\"{TARGET_TABLE_AL}_continuous\")\n",
    ")\n",
    "\n",
    "print(\"Stream started... Waiting for initialization.\")\n",
    "display(spark.table(f\"{TARGET_TABLE_AL}_continuous\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e858ff5-fb5d-4cd1-82db-c0070772137a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simulate arrival of remaining batches (2 to 10)\n",
    "print(\"Starting data simulation...\")\n",
    "\n",
    "for i in range(2, 10): # Batches 2 to 10 (indices of remaining parts)\n",
    "    batch_num = i + 1\n",
    "    print(f\"Arriving: Batch {batch_num}...\", end=\" \")\n",
    "    \n",
    "    # Write next batch\n",
    "    stream_batches[i].write.mode(\"overwrite\").json(f\"{STREAM_SOURCE_PATH}/batch_{batch_num:02d}\")\n",
    "    \n",
    "    print(\"Done. Waiting for stream...\")\n",
    "    time.sleep(4) # Wait for trigger to pick it up\n",
    "\n",
    "print(\"All batches arrived.\")\n",
    "display(spark.table(f\"{TARGET_TABLE_AL}_continuous\").count())\n",
    "display(spark.table(f\"{TARGET_TABLE_AL}_continuous\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7eabdb50-0f4c-403b-babf-cc5a0bfe1936",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stop stream\n",
    "query_continuous.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98e5e13f-ad7b-4a2d-8e2d-1c2f2f132259",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Stream-Static Joins & Aggregations\n",
    "\n",
    "Stream-Static Join = enriching a stream (e.g., Orders) with a static table (e.g., Customers). Static table is re-read at each micro-batch start.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4754aff9-6d0f-4b61-acf0-40ef81fb4948",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load Static Table\n",
    "df_static_customers = spark.table(TABLE_CUSTOMERS)\n",
    "display(df_static_customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee030957-86fe-4684-887d-a2fae0646f7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Demo: Stream-Static Join (Append Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8cc4f36-e94f-4dca-bfd9-64cce0c225d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS jointed_orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cc78623-670f-4692-8e8e-d02d7a7be854",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_enriched.createOrReplaceTempView(\"enriched_orders_stream\")\n",
    "df_static_customers.createOrReplaceTempView(\"static_customers\")\n",
    "\n",
    "df_joined = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  o.order_id,\n",
    "  o.total_amount,\n",
    "  c.first_name,\n",
    "  c.last_name,\n",
    "  c.email,\n",
    "  o._processing_time\n",
    "FROM enriched_orders_stream o\n",
    "LEFT JOIN static_customers c\n",
    "  ON o.customer_id = c.customer_id\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b4b4e1b-ec1d-429c-929b-d0ad62611750",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write enriched stream\n",
    "query_join = (df_joined.writeStream\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"enriched_orders\")\n",
    "    .outputMode(\"append\") # Joins (Left Stream-Static) are append-only\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_BASE_PATH}/join_demo_3\")\n",
    "    .trigger(processingTime=\"5 seconds\")\n",
    "    .option(\"includeExistingFiles\", \"true\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d0daa85-eacd-42f4-a024-8455f59f7a46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simulate arrival of remaining batches (2 to 10)\n",
    "print(\"Starting data simulation...\")\n",
    "\n",
    "for i in range(11, 13): # Batches 2 to 10 (indices of remaining parts)\n",
    "    batch_num = i + 1\n",
    "    print(f\"Arriving: Batch {batch_num}...\", end=\" \")\n",
    "    \n",
    "    # Write next batch\n",
    "    stream_batches[i].write.mode(\"overwrite\").json(f\"{STREAM_SOURCE_PATH}/batch_{batch_num:02d}\")\n",
    "    \n",
    "    print(\"Done. Waiting for stream...\")\n",
    "    time.sleep(4) # Wait for trigger to pick it up\n",
    "\n",
    "print(\"All batches arrived.\")\n",
    "display(spark.table(\"enriched_orders\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f29721c3-3626-4ea7-a376-133ee30ba8fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query_join.stop()\n",
    "display(spark.sql(\"SELECT count(1) FROM enriched_orders \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fabf8e5d-1388-4910-af46-25aa7a116c7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Demo: Streaming Aggregation (Watermarking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7537cfcb-1342-49dd-86ed-3c02db5862e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Define Streaming Aggregation (Orders per 30 seconds)\n",
    "# We use the previously defined df_enriched (from Auto Loader)\n",
    "\n",
    "windowed_counts = (df_enriched\n",
    "    .withWatermark(\"_processing_time\", \"1 minutes\") # Allow 1 mins late data\n",
    "    .groupBy(\n",
    "        F.window(\"_processing_time\", \"30 seconds\"),\n",
    "        \"customer_id\"\n",
    "    )\n",
    "    .count()\n",
    ")\n",
    "\n",
    "# 2. Write Stream with UPDATE mode\n",
    "# Update mode is efficient for aggregations - it emits only changed windows\n",
    "query_agg = (windowed_counts.writeStream\n",
    "    .format(\"console\") \n",
    "    .queryName(\"orders_counts\")\n",
    "    .outputMode(\"update\")\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_BASE_PATH}/agg_demo\")\n",
    "    .trigger(processingTime=\"5 seconds\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "print(\"Aggregation stream started...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8da7c99-1342-405e-b9b1-d1ee8ccc0a4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simulate arrival of remaining batches (2 to 10)\n",
    "print(\"Starting data simulation...\")\n",
    "\n",
    "for i in range(14, 16): # Batches 2 to 10 (indices of remaining parts)\n",
    "    batch_num = i + 1\n",
    "    print(f\"Arriving: Batch {batch_num}...\", end=\" \")\n",
    "    \n",
    "    # Write next batch\n",
    "    stream_batches[i].write.mode(\"overwrite\").json(f\"{STREAM_SOURCE_PATH}/batch_{batch_num:02d}\")\n",
    "    \n",
    "    print(\"Done. Waiting for stream...\")\n",
    "    time.sleep(4) # Wait for trigger to pick it up\n",
    "\n",
    "print(\"All batches arrived.\")\n",
    "display(windowed_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74faaba0-5370-47ec-ba9e-bf985ba1ea2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stop for demo purposes\n",
    "query_agg.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc0aff7d-41af-4785-9845-619e41087a3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Error Handling\n",
    "\n",
    "Databricks provides multiple strategies for handling malformed, corrupted, or schema-mismatched records during ingestion.\n",
    "\n",
    "| Mode | Behavior |\n",
    "|------|----------|\n",
    "| `PERMISSIVE` | Parses what it can, errors → `_corrupt_record` |\n",
    "| `DROPMALFORMED` | Removes malformed records |\n",
    "| `FAILFAST` | Stops on first error |\n",
    "\n",
    "> **Best Practice:** Use `badRecordsPath` to save malformed records for later analysis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ec4262c-1c19-4828-85d4-0da5e73dbd28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Schema Evolution & Rescued Data\n",
    "\n",
    "| `schemaEvolutionMode` | Behavior |\n",
    "|---|---|\n",
    "| `addNewColumns` | Automatically adds new columns |\n",
    "| `rescue` | New/mismatched → `_rescued_data` JSON column |\n",
    "| `failOnNewColumns` | Fail if schema changes |\n",
    "| `none` | Ignores new columns |\n",
    "\n",
    "> **Exam Tip:** `_rescued_data` column captures new columns, type mismatches, and malformed records when using `rescue` mode.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49cd1f2c-f354-4709-9a86-18b0958d87fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TARGET_TABLE_RESCUE = f\"{BRONZE_SCHEMA}.orders_rescued\"\n",
    "CHECKPOINT_RESCUE = f\"{CHECKPOINT_BASE_PATH}/rescue\"\n",
    "SCHEMA_RESCUE = f\"{SCHEMA_BASE_PATH}/rescue\"\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {TARGET_TABLE_RESCUE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64130422-fd79-4be4-87c7-67431fdc0b0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Define explicit schema (partial):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b434ee06-b8f5-411a-807c-f65161e0598f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Deliberately define only some columns - rest will go to _rescued_data\n",
    "partial_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7699b09-f861-45a6-908f-226ac3fb36ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Auto Loader with rescue mode:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d03ef69-63fc-4dd2-bc41-7a0008fb7171",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create BAD data (Extra column + Type mismatch)\n",
    "bad_data = [{\"order_id\": 99999, \"total_amount\": \"INVALID_NUMBER\", \"new_col\": \"surprise\"}]\n",
    "spark.createDataFrame(bad_data).write.mode(\"overwrite\").json(f\"{STREAM_SOURCE_PATH}/bad_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eab793fd-bc8b-4e97-91d1-ee3ae0e9b775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_rescue = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", SCHEMA_RESCUE)\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")  # Rescue mode!\n",
    "    .schema(partial_schema)  # Partial schema\n",
    "    .load(STREAM_SOURCE_PATH)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c256190d-5dff-4d30-9400-90befefbc6f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Start stream:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21169df2-704e-4a1b-a08a-77dba6a3c709",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query_rescue = (df_rescue.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_RESCUE)\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(TARGET_TABLE_RESCUE)\n",
    ")\n",
    "display(spark.table(TARGET_TABLE_RESCUE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9866b65d-1af3-4b9f-b7d4-f772e7dc6cfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Schema with `_rescued_data` column:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b9facae-c8e1-4373-a46c-787d3ff30d2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.table(TARGET_TABLE_RESCUE).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba9dad62-adc2-4bbf-a625-d4a238095cd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Data with rescued columns:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43ad1a6e-69e5-4ff0-8804-02bca0e5c7f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.table(TARGET_TABLE_RESCUE)\n",
    "    .limit(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "172d65d9-53c1-4a75-a153-b7a0286c657d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Demo: badRecordsPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd46e30d-a729-4de9-b6e7-d517a0ed2ba0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TABLE_ERRORS = f\"{BRONZE_SCHEMA}.customers_with_validation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67871578-4110-4c64-90c4-5e7a849bcdfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Creating table with `_corrupt_record`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cfe2bd9-018c-48ae-a953-1c7a8e9f86f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {TABLE_ERRORS}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {TABLE_ERRORS} (\n",
    "  customer_id STRING,\n",
    "  first_name STRING,\n",
    "  last_name STRING,\n",
    "  email STRING,\n",
    "  phone STRING,\n",
    "  city STRING,\n",
    "  state STRING,\n",
    "  country STRING,\n",
    "  registration_date DATE,\n",
    "  customer_segment STRING,\n",
    "  _corrupt_record STRING,\n",
    "  _ingestion_timestamp TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbfbceac-fe6b-4d94-bb24-91f9ab851737",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Loading with error handling:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17c085d1-af69-4b48-8227-94264ca514b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a CSV with a bad row\n",
    "bad_csv_data = [\n",
    "    (999, \"Eve\", \"2023-01-03\"),\n",
    "    (888, \"Frank\", \"NOT_A_DATE\") # This will fail date parsing\n",
    "]\n",
    "spark.createDataFrame(bad_csv_data, [\"customer_id\", \"first_name\", \"registration_date\"]).write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{BATCH_SOURCE_PATH}/bad_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f78243b-e754-4759-ba96-f6eabf7450a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_errors = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
    "    .option(\"badRecordsPath\", BAD_RECORDS_PATH)\n",
    "    .schema(\"\"\"\n",
    "        customer_id STRING,\n",
    "        first_name STRING,\n",
    "        registration_date DATE,\n",
    "        _corrupt_record STRING\n",
    "    \"\"\")\n",
    "    .load(f\"{BATCH_SOURCE_PATH}/bad_csv\")\n",
    "    .withColumn(\"_ingestion_timestamp\", F.current_timestamp())\n",
    ")\n",
    "display(df_with_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "626f1086-464e-49f5-99ea-b0b5c80c3f82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Bad records statistics:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "812f16e8-3425-4c18-b449-029845a536b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "files = [f.path for f in dbutils.fs.ls(BAD_RECORDS_PATH)]\n",
    "files = [f + \"*\" for f in files]\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f4614c6-31dd-4307-8e69-08825d729d0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "bad_record_schema = StructType([\n",
    "    StructField(\"path\", StringType(), True),\n",
    "    StructField(\"record\", StringType(), True),\n",
    "    StructField(\"reason\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_bad_records = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .schema(bad_record_schema)\n",
    "    .load(files)\n",
    ")\n",
    "display(df_bad_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06ca148f-f9e7-422e-a290-87bb1b34e8b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Lakeflow Connect (Informational)\n",
    "\n",
    "Zero-code SaaS ingestion via UI: Salesforce, Workday, HubSpot, SAP, ServiceNow, etc.\n",
    "\n",
    "| Method | Use Case |\n",
    "|--------|----------|\n",
    "| **COPY INTO** | Files in cloud storage (batch) |\n",
    "| **Auto Loader** | Files in cloud storage (streaming) |\n",
    "| **Lakeflow Connect** | Data from SaaS systems |\n",
    "| **Lakeflow Declarative Pipelines** | Transformations Bronze → Silver → Gold |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b8901d4-859d-4920-bc1b-4a40199da6bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "| Topic | Key Concept | Exam Keywords |\n",
    "|---|---|---|\n",
    "| **COPY INTO** | Idempotent batch loading, file tracking | `COPY INTO`, `FILEFORMAT`, `mergeSchema` |\n",
    "| **Auto Loader** | `cloudFiles` streaming ingestion | `cloudFiles.format`, `schemaLocation`, `schemaEvolutionMode` |\n",
    "| **Trigger Modes** | `availableNow=True`, `processingTime` | Batch-like vs continuous |\n",
    "| **Schema Evolution** | `rescue`, `addNewColumns`, `failOnNewColumns` | `_rescued_data` column |\n",
    "| **Stream-Static Join** | Enrich stream with dimension table | Append mode, static reload |\n",
    "| **Watermarking** | State cleanup in aggregations | `withWatermark()`, late data handling |\n",
    "| **Lakeflow Connect** | Zero-code SaaS ingestion | Salesforce, Workday, SAP |\n",
    "\n",
    "---\n",
    "\n",
    "> **← M04: Delta Optimization | Day 2 | M06: Advanced Transforms →**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62712302-8e7c-4539-83c1-f10c0beae485",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of created tables\n",
    "created_tables = [\n",
    "    \"customers_batch\",\n",
    "    \"orders_autoloader\",\n",
    "    \"orders_rescued\",\n",
    "    \"customers_with_validation\",\n",
    "    \"orders_trigger_test\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df386671-3c5d-4cdd-8a06-3292819bbe12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "Remove demo tables, checkpoints, and temporary data created during this module.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40962ec5-3478-449d-865b-7ec5e756d9f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for table in created_tables:\n",
    "    full_table = f\"{CATALOG}.{BRONZE_SCHEMA}.{table}\"\n",
    "    try:\n",
    "        if spark.catalog.tableExists(full_table):\n",
    "            count = spark.table(full_table).count()\n",
    "            results.append((table, \"EXISTS\", str(count)))\n",
    "        else:\n",
    "            results.append((table, \"NOT FOUND\", \"-\"))\n",
    "    except Exception as e:\n",
    "        results.append((table, \"ERROR\", str(e)[:30]))\n",
    "\n",
    "display(spark.createDataFrame(results, [\"Table\", \"Status\", \"Records\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cec9d84a-4418-440c-8776-07a70a45762f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup flag\n",
    "CLEANUP_ENABLED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "765c9b92-0528-4384-b9f6-2651d59021f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Execute cleanup (if enabled):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e69e637-a538-42f6-b3c6-20ead9495071",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if CLEANUP_ENABLED:\n",
    "    results = []\n",
    "    for table in created_tables:\n",
    "        full_table = f\"{CATALOG}.{BRONZE_SCHEMA}.{table}\"\n",
    "        try:\n",
    "            spark.sql(f\"DROP TABLE IF EXISTS {full_table}\")\n",
    "            results.append((table, \"DROPPED\"))\n",
    "        except Exception as e:\n",
    "            results.append((table, f\"ERROR: {str(e)[:30]}\"))\n",
    "    \n",
    "    # Cleanup checkpoints\n",
    "    try:\n",
    "        dbutils.fs.rm(CHECKPOINT_BASE_PATH, True)\n",
    "        results.append((\"checkpoints\", \"REMOVED\"))\n",
    "    except:\n",
    "        results.append((\"checkpoints\", \"NOT FOUND\"))\n",
    "    \n",
    "    display(spark.createDataFrame(results, [\"Resource\", \"Status\"]))\n",
    "else:\n",
    "    display(spark.createDataFrame([\n",
    "        (\"CLEANUP_ENABLED\", \"False\"),\n",
    "        (\"Action\", \"Change to True to delete resources\")\n",
    "    ], [\"Setting\", \"Value\"]))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6142587020061808,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "M05_incremental_processing",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
