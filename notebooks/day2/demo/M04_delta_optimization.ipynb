{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b1999a3-07ad-4368-80f3-ed2f1ca9a468",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# M04: Delta Optimization & Performance\n",
    "\n",
    "| Exam Domain | Weight |\n",
    "|---|---|\n",
    "| ELT with Spark SQL and Python | 29% |\n",
    "| Incremental Data Processing | 20% |\n",
    "\n",
    "Topics: OPTIMIZE, Z-ORDER, Liquid Clustering, Partitioning, Change Data Feed, Deletion Vectors, Predictive Optimization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5286ec41-d1ce-4777-bd30-3625f7e04f7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2611ded3-c381-4a04-a85c-0a0628c67e3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../setup/00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1ac3ce7-8c67-43de-83f3-7da7187019e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45a8db5a-a1d1-44f3-8175-c71f3e42a257",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Display user context\n",
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (CATALOG, BRONZE_SCHEMA, SILVER_SCHEMA, GOLD_SCHEMA)\n",
    "    ], ['catalog', 'bronze_schema', 'silver_schema', 'gold_schema'])\n",
    ")\n",
    "\n",
    "# Set catalog and schema as default\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f0b53e1-b8ba-4236-9ead-c2a298a15d12",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 102"
    }
   },
   "source": [
    "## 4.1. Optimization\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "As data grows, query performance can degrade due to several factors:\n",
    "- **Small Files Problem**: Too many small files increase metadata overhead\n",
    "- **Data Layout**: Data not organized for common query patterns\n",
    "- **Predicate Pushdown Inefficiency**: Scanning more data than necessary\n",
    "\n",
    "Delta Lake provides several optimization techniques:\n",
    "\n",
    "| Technique | Description | When to Use |\n",
    "|-----------|-------------|-------------|\n",
    "| **OPTIMIZE** | Compacts small files into larger ones | After many small writes |\n",
    "| **Partitioning** | Physical data separation by column values | Low-cardinality filter columns |\n",
    "| **Z-ORDER** | Co-locates related data for better pruning | Frequently filtered columns |\n",
    "| **Liquid Clustering** | Modern alternative to partitioning + Z-ORDER | New tables (recommended) |\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../../../assets/images/6653c397bbc24993975c4fc11a356ab6.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1345ad36-163c-49af-915f-f674f595d19a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 103"
    }
   },
   "source": [
    "### 4.1.1. Example: The Small Files Problem\n",
    "\n",
    "**Objective:** Demonstrate how many small files impact performance and how OPTIMIZE solves it\n",
    "\n",
    "The \"small files problem\" occurs when:\n",
    "- Streaming jobs write many small files\n",
    "- Frequent small batch inserts\n",
    "- High-concurrency writes\n",
    "\n",
    "This leads to:\n",
    "- Increased metadata overhead\n",
    "- Slower query performance\n",
    "- Higher storage costs (metadata per file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84c88015-4079-4b8d-8140-a6d29981601b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{BRONZE_SCHEMA}.small_files_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6256554-625c-4264-ab8a-594cb08d15d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a table with many small files (simulating streaming ingestion)\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {CATALOG}.{BRONZE_SCHEMA}.small_files_demo (\n",
    "    id INT,\n",
    "    data STRING,\n",
    "    created_at TIMESTAMP\n",
    ") USING DELTA\n",
    "TBLPROPERTIES (\n",
    "    delta.autoOptimize.optimizeWrite = false,\n",
    "    delta.autoOptimize.autoCompact = false\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Insert data in many small batches (simulating streaming)\n",
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "import random\n",
    "import string\n",
    "\n",
    "print(\"Inserting 500 small batches to simulate streaming ingestion...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1fc0e49-c02d-485a-86ae-f44d1ad70236",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"location\":1269},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769001297155}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "\n",
    "DESCRIBE DETAIL small_files_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9850648-1202-4a18-aca1-1d548d6d8de9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, expr, current_timestamp\n",
    "import random\n",
    "\n",
    "# 1. Configuration\n",
    "total_files = 5000\n",
    "rows_per_file = 2  # Average 2 records per file\n",
    "total_rows = total_files * rows_per_file\n",
    "\n",
    "# 2. Generate data in memory (no Python loop!)\n",
    "df = (\n",
    "    spark.range(0, total_rows)\n",
    "    .withColumn(\"id\", lit(random.randint(1, 100)))\n",
    "    .withColumn(\"data\", expr(\"uuid()\"))\n",
    "    .withColumn(\"created_at\", current_timestamp())\n",
    ")\n",
    "\n",
    "# 3. Write with forced number of files\n",
    "df.repartition(total_files).write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.small_files_demo\")\n",
    "\n",
    "print(f\"Done! Created {total_files} small files in a single transaction.\")\n",
    "display(spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.small_files_demo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "587edefa-9b2c-4f74-85ed-1ada3b3a34ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check the number of files BEFORE optimization\n",
    "before_optimize = spark.sql(f\"DESCRIBE DETAIL {CATALOG}.{BRONZE_SCHEMA}.small_files_demo\")\n",
    "display(before_optimize.select(\"numFiles\", \"sizeInBytes\"))\n",
    "display(spark.sql(f\"DESCRIBE HISTORY {CATALOG}.{BRONZE_SCHEMA}.small_files_demo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eb16366-c750-4f05-a19b-9e1e32386f80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run OPTIMIZE to compact small files\n",
    "optimize_result = spark.sql(f\"\"\"\n",
    "    OPTIMIZE {CATALOG}.{BRONZE_SCHEMA}.small_files_demo\n",
    "\"\"\")\n",
    "\n",
    "display(optimize_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "068906d9-a615-46ee-bf0b-2fbd1ff5a034",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# VACUUM with 0 hours retention (DEMO ONLY - requires disabling safety check)\n",
    "# In production, NEVER use 0 hours - use default 7 days or more!\n",
    "spark.sql(\"SET spark.databricks.delta.retentionDurationCheck.enabled = false\")\n",
    "\n",
    "vacuum_result = spark.sql(f\"\"\"\n",
    "    VACUUM {CATALOG}.{BRONZE_SCHEMA}.small_files_demo RETAIN 0 HOURS\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dc2477f-5eb4-4dd4-8e3f-ab8ebd64f147",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check the number of files AFTER optimization\n",
    "after_optimize = spark.sql(f\"DESCRIBE DETAIL {CATALOG}.{BRONZE_SCHEMA}.small_files_demo\")\n",
    "display(after_optimize.select(\"numFiles\", \"sizeInBytes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3c1074b-5e35-4225-9c0d-3eaf081d9ec7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 114"
    }
   },
   "source": [
    "### 4.1.2. Example: Partitioning\n",
    "\n",
    "**Objective:** Demonstrate how partitioning improves query performance through partition pruning\n",
    "\n",
    "Partitioning physically separates data into directories based on column values. This enables:\n",
    "- **Partition Pruning**: Skip entire partitions that don't match query filters\n",
    "- **Parallel Processing**: Process partitions independently\n",
    "\n",
    "**Best Practices:**\n",
    "- Use low-cardinality columns (date, country, status)\n",
    "- Avoid over-partitioning (too many small partitions)\n",
    "- Aim for 1GB+ per partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a65ea7a-263c-45ae-9d99-d0029898952f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{BRONZE_SCHEMA}.orders_partitioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dabece7c-5bfd-4acc-ae02-dd6df5696a89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a partitioned table\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {CATALOG}.{BRONZE_SCHEMA}.orders_partitioned (\n",
    "    order_id STRING,\n",
    "    customer_id STRING,\n",
    "    product_id STRING,\n",
    "    order_date DATE,\n",
    "    amount DOUBLE,\n",
    "    status STRING\n",
    ") \n",
    "USING DELTA\n",
    "PARTITIONED BY (order_date,status)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8cc00cb-7a27-4b51-bf87-0d61480c5923",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Insert sample data across multiple dates\n",
    "from datetime import date, timedelta\n",
    "\n",
    "orders_data = []\n",
    "base_date = date(2024, 1, 1)\n",
    "\n",
    "for day_offset in range(30):  # 30 days of data\n",
    "    order_date = base_date + timedelta(days=day_offset)\n",
    "    for i in range(100):  # 100 orders per day\n",
    "        orders_data.append((\n",
    "            f\"ORD-{day_offset:02d}-{i:04d}\",\n",
    "            f\"CUST{i % 50:04d}\",\n",
    "            f\"PROD{i % 20:03d}\",\n",
    "            order_date,\n",
    "            50 + (i * 2.5),\n",
    "            \"completed\" if i % 3 != 0 else \"pending\"\n",
    "        ))\n",
    "\n",
    "orders_df = spark.createDataFrame(orders_data, \n",
    "    [\"order_id\", \"customer_id\", \"product_id\", \"order_date\", \"amount\", \"status\"])\n",
    "\n",
    "orders_df.write.format(\"delta\").mode(\"append\").partitionBy(\"order_date\",\"status\") \\\n",
    "    .saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.orders_partitioned\")\n",
    "\n",
    "print(f\"Inserted {len(orders_data)} orders across 30 days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c53cfda-d7e7-4b47-9f7c-09dc33e9a373",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check partitioning structure\n",
    "display(spark.sql(f\"DESCRIBE DETAIL {CATALOG}.{BRONZE_SCHEMA}.orders_partitioned\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a6061d1-68a1-4ae1-ba9d-0fb8ea7b2c6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Query with partition filter - only scans relevant partitions\n",
    "# Check the Spark UI to see partition pruning in action\n",
    "result = spark.sql(f\"\"\"\n",
    "    SELECT * FROM {CATALOG}.{BRONZE_SCHEMA}.orders_partitioned\n",
    "    WHERE order_date = '2024-01-15'\n",
    "\"\"\")\n",
    "\n",
    "print(\"Query for single date (should scan only 1 partition):\")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05513694-7990-402a-8be1-0c1f048b9011",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 120"
    }
   },
   "source": [
    "### 4.1.3. Example: Z-ORDER (Data Skipping)\n",
    "\n",
    "**Objective:** Demonstrate Z-ORDER for multi-dimensional clustering\n",
    "\n",
    "Z-ORDER is a multi-dimensional clustering technique that co-locates related data within files. This enables **data skipping** - reading only relevant files based on min/max statistics.\n",
    "\n",
    "**When to Use:**\n",
    "- Columns frequently in WHERE clauses\n",
    "- High-cardinality columns (customer_id, product_id)\n",
    "- Up to 4 columns (effectiveness decreases with more)\n",
    "\n",
    "**How it Works:**\n",
    "- Reorganizes data within files using Z-order curve\n",
    "- Maintains min/max statistics per file\n",
    "- Query engine skips files that don't match predicates\n",
    "\n",
    "\n",
    "<img src=\"../../../assets/images/8ddc3a9209e145cf8edec763d45344d7.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfd8d912-6954-4372-a566-afce479e0765",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{BRONZE_SCHEMA}.sales_zorder_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1725469c-75da-4371-9945-05a7b913e06e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a table for Z-ORDER demonstration with auto-optimization disabled\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {CATALOG}.{BRONZE_SCHEMA}.sales_zorder_demo (\n",
    "    sale_id STRING,\n",
    "    customer_id STRING,\n",
    "    product_id STRING,\n",
    "    store_id STRING,\n",
    "    sale_date DATE,\n",
    "    amount DOUBLE,\n",
    "    quantity INT\n",
    ") USING DELTA\n",
    "TBLPROPERTIES (\n",
    "    delta.autoOptimize.optimizeWrite = false,\n",
    "    delta.autoOptimize.autoCompact = false\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f5a7ef8-4e00-4e5b-89eb-35f22e130f7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Insert sample data\n",
    "from datetime import date\n",
    "import random\n",
    "\n",
    "sales_data = []\n",
    "for i in range(100000):  # 100K records\n",
    "    sales_data.append((\n",
    "        f\"SALE-{i:08d}\",\n",
    "        f\"CUST{random.randint(1, 1000):04d}\",\n",
    "        f\"PROD{random.randint(1, 500):03d}\",\n",
    "        f\"STORE{random.randint(1, 50):02d}\",\n",
    "        date(2024, random.randint(1, 12), random.randint(1, 28)),\n",
    "        random.uniform(10, 500),\n",
    "        random.randint(1, 10)\n",
    "    ))\n",
    "\n",
    "sales_df = spark.createDataFrame(\n",
    "    sales_data, \n",
    "    [\"sale_id\", \"customer_id\", \"product_id\", \"store_id\", \"sale_date\", \"amount\", \"quantity\"]\n",
    ")\n",
    "\n",
    "sales_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\n",
    "    f\"{CATALOG}.{BRONZE_SCHEMA}.sales_zorder_demo\"\n",
    ")\n",
    "display(sales_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34538db8-8523-4d31-8df0-05c55b550a9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check file statistics BEFORE Z-ORDER\n",
    "display(spark.sql(f\"DESCRIBE DETAIL {CATALOG}.{BRONZE_SCHEMA}.sales_zorder_demo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b08a9d5-0864-483d-b062-aaa717611bf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Queries filtering before Z-Order\n",
    "result = spark.sql(f\"\"\"\n",
    "    SELECT * FROM {CATALOG}.{BRONZE_SCHEMA}.sales_zorder_demo\n",
    "    WHERE customer_id = 'CUST0393' AND product_id = 'PROD259' --CUST0592\tPROD011\n",
    "\"\"\")\n",
    "\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c8bf255-a8b8-400a-8eda-3be7322576e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply Z-ORDER on frequently filtered columns\n",
    "# In this case: customer_id and product_id are common filter columns\n",
    "zorder_result = spark.sql(f\"\"\"\n",
    "    OPTIMIZE {CATALOG}.{BRONZE_SCHEMA}.sales_zorder_demo\n",
    "    ZORDER BY (customer_id, product_id)\n",
    "\"\"\")\n",
    "\n",
    "display(zorder_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e73adeb6-a92e-4144-8478-cb95b367cd70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example query that benefits from Z-ORDER\n",
    "result = spark.sql(f\"\"\"\n",
    "    SELECT * FROM {CATALOG}.{BRONZE_SCHEMA}.sales_zorder_demo\n",
    "     WHERE customer_id = 'CUST0393' AND product_id = 'PROD259' --CUST0592\tPROD011\n",
    "\"\"\")\n",
    "\n",
    "print(\"Query with Z-ORDER optimized columns (check Spark UI for data skipping):\")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f071fa3-0b05-47d8-a066-9474797353a8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 129"
    }
   },
   "source": [
    "### 4.1.4. Example: Liquid Clustering\n",
    "\n",
    "**Objective:** Demonstrate Liquid Clustering as a modern alternative to partitioning and Z-ORDER\n",
    "\n",
    "Liquid Clustering is Databricks' latest optimization technique that combines the benefits of partitioning and Z-ORDER while being easier to manage:\n",
    "\n",
    "**Key Benefits:**\n",
    "- **Automatic**: Databricks manages data layout automatically\n",
    "- **Adaptive**: Adjusts to changing query patterns over time\n",
    "- **Flexible**: Can change clustering columns without rewriting data\n",
    "- **Incremental**: Works incrementally with each OPTIMIZE\n",
    "- **Simpler**: No need to choose between partitioning and Z-ORDER\n",
    "\n",
    "**When to Use:**\n",
    "- New tables (recommended default)\n",
    "- Tables with evolving query patterns\n",
    "- When you're unsure about optimal partitioning strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d567b3a1-434c-43ec-bcf5-a3439e719281",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{BRONZE_SCHEMA}.sales_liquid_clustering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e47153b-1cdf-4d55-bea2-9d35c2b183c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a table with Liquid Clustering, auto-optimization disabled, and Predictive Optimization enabled\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {CATALOG}.{BRONZE_SCHEMA}.sales_liquid_clustering (\n",
    "    sale_id STRING,\n",
    "    customer_id STRING,\n",
    "    product_id STRING,\n",
    "    region STRING,\n",
    "    sale_date DATE,\n",
    "    amount DOUBLE,\n",
    "    quantity INT\n",
    ") \n",
    "USING DELTA\n",
    "TBLPROPERTIES (\n",
    "    delta.autoOptimize.optimizeWrite = false,\n",
    "    delta.autoOptimize.autoCompact = false\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c89bfac-9e15-4ac7-8c02-488978393f7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\" ALTER TABLE {CATALOG}.{BRONZE_SCHEMA}.sales_liquid_clustering SET TBLPROPERTIES ('spark.databricks.sql.predictiveOptimization.enabled'='true'); \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dae3322d-fb0b-45ba-abaf-cb54a5c24c6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, rand, lit, concat, lpad, element_at, array, date_add, to_date, round\n",
    "\n",
    "# 1. Configuration\n",
    "target_files = 5000       # We want 5000 files\n",
    "rows_per_file = 10        # 10 records per file\n",
    "total_rows = target_files * rows_per_file # Total 50,000 records\n",
    "\n",
    "# Array of regions for random selection\n",
    "regions_list = array([lit(x) for x in ['North', 'South', 'East', 'West', 'Central']])\n",
    "\n",
    "# 2. Data generation (no for loop!)\n",
    "df = spark.range(0, total_rows).withColumnRenamed(\"id\", \"idx\") \\\n",
    "    .withColumn(\"sale_id\", concat(lit(\"SALE-\"), lpad(col(\"idx\"), 8, \"0\"))) \\\n",
    "    .withColumn(\"customer_id\", concat(lit(\"CUST\"), lpad((rand() * 500 + 1).cast(\"int\"), 4, \"0\"))) \\\n",
    "    .withColumn(\"product_id\", concat(lit(\"PROD\"), lpad((rand() * 200 + 1).cast(\"int\"), 3, \"0\"))) \\\n",
    "    .withColumn(\"region\", element_at(regions_list, (rand() * 5 + 1).cast(\"int\"))) \\\n",
    "    .withColumn(\"sale_date\", date_add(to_date(lit(\"2024-01-01\")), (rand() * 364).cast(\"int\"))) \\\n",
    "    .withColumn(\"amount\", round(rand() * 490 + 10, 2)) \\\n",
    "    .withColumn(\"quantity\", (rand() * 10 + 1).cast(\"int\")) \\\n",
    "    .drop(\"idx\") # Remove helper column\n",
    "\n",
    "# 3. Write - repartition is key\n",
    "# Create table with Liquid Clustering enabled (if not exists)\n",
    "# or append to existing one.\n",
    "df.repartition(target_files).write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.sales_liquid_clustering\")\n",
    "\n",
    "print(f\"Done! Inserted {total_rows} records in {target_files} small files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c64603c-281f-41bc-9be1-befe33ae125a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769003215235}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(f\"DESCRIBE DETAIL {CATALOG}.{BRONZE_SCHEMA}.sales_liquid_clustering\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c5dd1b5-43c9-4890-b4e6-442916913d81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enable liquid clustering on an existing table by specifying clustering columns\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE {CATALOG}.{BRONZE_SCHEMA}.sales_liquid_clustering\n",
    "CLUSTER BY AUTO\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48cb56d5-0256-40f6-a8cd-084b407066e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# OPTIMIZE automatically applies Liquid Clustering\n",
    "# No need to specify ZORDER - it's built into the table definition!\n",
    "optimize_result = spark.sql(f\"\"\"\n",
    "    OPTIMIZE {CATALOG}.{BRONZE_SCHEMA}.sales_liquid_clustering\n",
    "\"\"\")\n",
    "\n",
    "display(optimize_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbbdca62-32f8-4602-a898-a970f6b45d54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check clustering information\n",
    "display(spark.sql(f\"DESCRIBE DETAIL {CATALOG}.{BRONZE_SCHEMA}.sales_liquid_clustering\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40e95234-86d1-4849-a440-8b24be141012",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Queries filtering by clustering columns are automatically optimized\n",
    "result = spark.sql(f\"\"\"\n",
    "    SELECT region, COUNT(*) as sales_count, SUM(amount) as total_amount\n",
    "    FROM {CATALOG}.{BRONZE_SCHEMA}.sales_liquid_clustering\n",
    "    WHERE customer_id LIKE 'CUST00%' AND region = 'North'\n",
    "    GROUP BY region\n",
    "\"\"\")\n",
    "\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "745a1e46-9041-41b1-a3a9-387d3dbdd10c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Comparison: Partitioning vs Z-ORDER vs Liquid Clustering**\n",
    "\n",
    "| Feature | Partitioning | Z-ORDER | Liquid Clustering |\n",
    "|---------|-------------|---------|-------------------|\n",
    "| When to choose | Low-cardinality columns | High-cardinality filter columns | General purpose (recommended) |\n",
    "| Data layout | Directory per partition | Co-located in files | Automatic clustering |\n",
    "| Schema change | Requires rewrite | Easy to change | Easy to change |\n",
    "| Maintenance | Manual | Manual OPTIMIZE | Automatic with OPTIMIZE |\n",
    "| Best for | Date/Region filters | Multi-column filters | Evolving workloads |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9cbd393-31cc-4a02-abeb-7127f8ef4bd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.1.5. Data Skew & Data Distribution Problems\n",
    "\n",
    "### What is Data Skew?\n",
    "\n",
    "**Data skew** occurs when data is unevenly distributed across partitions. Instead of each partition having roughly the same number of rows, one or more partitions end up with significantly more data (\"hot partitions\"). This is one of the most common and impactful performance problems in distributed data processing.\n",
    "\n",
    "<img src=\"../../../assets/images/eef58dd32657427eb8f4f2fdacff9b39.png\" width=\"800\">\n",
    "\n",
    "---\n",
    "\n",
    "### Common Causes of Data Skew\n",
    "\n",
    "| Cause | Example | Impact |\n",
    "|-------|---------|--------|\n",
    "| **Skewed join keys** | Joining on `country` where 80% of customers are from one country | One executor handles most of the join |\n",
    "| **Null values in join/group keys** | `customer_id IS NULL` for anonymous orders | All NULLs land on one partition |\n",
    "| **Hot keys in groupBy** | `groupBy(\"product_category\")` where \"Electronics\" has 90% of sales | One partition aggregates most data |\n",
    "| **Uneven source files** | One file is 10 GB, others are 100 MB | One task reads disproportionate data |\n",
    "| **Time-based partitioning** | `PARTITION BY (order_date)` when most orders are from last month | Recent partition is orders of magnitude larger |\n",
    "\n",
    "---\n",
    "\n",
    "### Detecting Data Skew\n",
    "\n",
    "**Symptoms:**\n",
    "- One stage takes much longer than others in Spark UI\n",
    "- One task in a stage uses much more memory/time than siblings\n",
    "- `SpillToMemory` or `SpillToDisk` metrics appear for one task\n",
    "- OOM (Out of Memory) errors on specific executors\n",
    "\n",
    "**Diagnostic queries:**\n",
    "\n",
    "```python\n",
    "# Check partition sizes (data distribution)\n",
    "df.groupBy(spark.sql.functions.spark_partition_id()).count().orderBy(\"count\", ascending=False).show()\n",
    "\n",
    "# Check key distribution for join/group columns\n",
    "df.groupBy(\"join_key_column\").count().orderBy(\"count\", ascending=False).show(20)\n",
    "\n",
    "# In Databricks SQL:\n",
    "# SELECT join_key, COUNT(*) as cnt FROM table GROUP BY join_key ORDER BY cnt DESC LIMIT 20\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Solutions for Data Skew\n",
    "\n",
    "#### 1. Adaptive Query Execution (AQE) — Automatic\n",
    "\n",
    "Databricks enables AQE by default. It detects skew at runtime and automatically:\n",
    "- Splits skewed partitions into smaller sub-partitions\n",
    "- Adjusts shuffle partition count based on data volume\n",
    "- Converts sort-merge joins to broadcast joins when beneficial\n",
    "\n",
    "```sql\n",
    "-- AQE is ON by default in Databricks. To verify:\n",
    "SET spark.sql.adaptive.enabled;  -- true\n",
    "\n",
    "-- Key AQE skew optimization settings:\n",
    "SET spark.sql.adaptive.skewJoin.enabled;  -- true (auto-splits skewed partitions)\n",
    "SET spark.sql.adaptive.skewJoin.skewedPartitionFactor;  -- 5 (partition 5x median = skewed)\n",
    "SET spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes;  -- 256MB\n",
    "```\n",
    "\n",
    "> **Exam Tip:** AQE is a Spark 3.x feature enabled by default in Databricks. It handles many skew scenarios automatically. On the exam, know that AQE can dynamically adjust partition counts and handle skewed joins.\n",
    "\n",
    "#### 2. Salting — Manual Technique\n",
    "\n",
    "When AQE is insufficient (extreme skew), **salting** splits hot keys artificially:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import concat, lit, floor, rand\n",
    "\n",
    "# Add random salt (0-9) to the skewed key\n",
    "num_salts = 10\n",
    "df_large_salted = df_large.withColumn(\"salted_key\", concat(\"join_key\", lit(\"_\"), floor(rand() * num_salts).cast(\"int\")))\n",
    "\n",
    "# Explode the small table to match all salt values\n",
    "from pyspark.sql.functions import explode, array\n",
    "df_small_salted = df_small.withColumn(\"salted_key\",\n",
    "    explode(array([concat(\"join_key\", lit(f\"_{i}\")) for i in range(num_salts)])))\n",
    "\n",
    "# Join on salted key — data distributed evenly\n",
    "result = df_large_salted.join(df_small_salted, \"salted_key\")\n",
    "```\n",
    "\n",
    "#### 3. Broadcast Join — Small Table Optimization\n",
    "\n",
    "If one side of the join is small enough (< 10 MB default, configurable), broadcast it to all executors to avoid shuffle entirely:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Force broadcast join — eliminates shuffle\n",
    "result = df_large.join(broadcast(df_small), \"join_key\")\n",
    "```\n",
    "\n",
    "```sql\n",
    "-- SQL equivalent:\n",
    "SELECT /*+ BROADCAST(small_table) */ *\n",
    "FROM large_table JOIN small_table ON large_table.key = small_table.key;\n",
    "```\n",
    "\n",
    "> **Exam Tip:** The default broadcast threshold is 10 MB (`spark.sql.autoBroadcastJoinThreshold`). AQE can also automatically convert sort-merge joins to broadcast joins at runtime.\n",
    "\n",
    "#### 4. Handling NULL Keys\n",
    "\n",
    "```python\n",
    "# Separate NULL keys from the join (they won't match anyway in inner join)\n",
    "df_nulls = df.filter(\"join_key IS NULL\")\n",
    "df_non_nulls = df.filter(\"join_key IS NOT NULL\")\n",
    "\n",
    "# Process non-null keys normally, handle nulls separately\n",
    "result = df_non_nulls.join(df_other, \"join_key\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Skew vs. Optimization Techniques — Summary\n",
    "\n",
    "| Problem | Solution | When to Use |\n",
    "|---------|----------|-------------|\n",
    "| Skewed join keys | AQE (automatic) | First attempt — usually sufficient |\n",
    "| Extreme skew in joins | Salting | AQE insufficient, 100:1+ key imbalance |\n",
    "| Small table join | Broadcast join | One table < 10 MB (adjustable) |\n",
    "| NULL keys | Filter + handle separately | Many NULLs in join/group column |\n",
    "| Skewed groupBy | `repartition()` + salting | One group dominates others |\n",
    "| Uneven file sizes | Auto Loader / OPTIMIZE | Ingestion or post-load compaction |\n",
    "| Time partition skew | Liquid Clustering | Replace static partitioning |\n",
    "\n",
    "> **Exam Tip:** Data skew detection and resolution is a practical skill tested indirectly. Questions about AQE, broadcast joins, and partition management often relate to skew scenarios. Remember: AQE handles most cases automatically in Databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5eb35fb6-d930-43e1-8c64-499144995ce0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 140"
    }
   },
   "source": [
    "## 4.2. Change Data Feed vs Change Data Capture\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Two terms are often confused in the data engineering world: **Change Data Feed (CDF)** and **Change Data Capture (CDC)**. Understanding the difference is crucial:\n",
    "\n",
    "### Change Data Capture (CDC)\n",
    "**What it is:** A *pattern/technique* for capturing changes from source systems (databases, APIs, etc.)\n",
    "\n",
    "**Characteristics:**\n",
    "- Source-side technology\n",
    "- Captures INSERT, UPDATE, DELETE from operational databases\n",
    "- Tools: Debezium, AWS DMS, Fivetran, Qlik Replicate\n",
    "- Produces a stream of change events\n",
    "\n",
    "**Example:** Capturing changes from PostgreSQL and streaming them to Kafka\n",
    "\n",
    "### Change Data Feed (CDF)\n",
    "**What it is:** A *Delta Lake feature* that records row-level changes within Delta tables\n",
    "\n",
    "**Characteristics:**\n",
    "- Delta Lake native feature\n",
    "- Tracks changes that happen WITHIN Delta tables\n",
    "- Provides `_change_type`, `_commit_version`, `_commit_timestamp` columns\n",
    "- Enables efficient incremental processing\n",
    "\n",
    "**Example:** Reading only the rows that changed since the last pipeline run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb39be04-b261-4ce5-b788-ac3248b1156e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 141"
    }
   },
   "source": [
    "### 4.2.1. Example: Enabling Change Data Feed\n",
    "\n",
    "**Objective:** Enable CDF on a Delta table to track all changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e46d409c-5047-4927-833e-6b30facf28fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a table with CDF enabled from the start\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {CATALOG}.{BRONZE_SCHEMA}.cdf_demo (\n",
    "    user_id STRING,\n",
    "    name STRING,\n",
    "    email STRING,\n",
    "    status STRING,\n",
    "    updated_at TIMESTAMP\n",
    ") \n",
    "USING DELTA\n",
    "TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "\"\"\")\n",
    "\n",
    "print(\"Table created with Change Data Feed enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df6f5ad3-7521-4adc-867a-d2bffea5583a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify CDF is enabled\n",
    "properties = spark.sql(f\"SHOW TBLPROPERTIES {CATALOG}.{BRONZE_SCHEMA}.cdf_demo\")\n",
    "display(properties.filter(F.col(\"key\").like(\"%change%\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e998bfb-bd2a-433e-8cf4-4cb4074f97fd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 144"
    }
   },
   "source": [
    "### 4.2.2. Example: Generating and Tracking Changes\n",
    "\n",
    "**Objective:** Perform various DML operations and observe how CDF tracks them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "152fb78b-0f62-469c-8e04-c0d1d5b5dd64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# INSERT some initial data\n",
    "spark.sql(f\"\"\"\n",
    "INSERT INTO {CATALOG}.{BRONZE_SCHEMA}.cdf_demo VALUES\n",
    "    ('U001', 'Alice', 'alice@example.com', 'active', current_timestamp()),\n",
    "    ('U002', 'Bob', 'bob@example.com', 'active', current_timestamp()),\n",
    "    ('U003', 'Charlie', 'charlie@example.com', 'active', current_timestamp())\n",
    "\"\"\")\n",
    "print(\"Version 1: Initial INSERT completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "134459ab-94c3-4cb2-af5a-c378dc2ebe62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UPDATE a record\n",
    "spark.sql(f\"\"\"\n",
    "UPDATE {CATALOG}.{BRONZE_SCHEMA}.cdf_demo\n",
    "SET status = 'premium', updated_at = current_timestamp()\n",
    "WHERE user_id = 'U001'\n",
    "\"\"\")\n",
    "print(\"Version 2: UPDATE completed - Alice upgraded to premium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd1e7aae-1034-474e-82b4-9e5b318d1ec2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DELETE a record\n",
    "spark.sql(f\"\"\"\n",
    "DELETE FROM {CATALOG}.{BRONZE_SCHEMA}.cdf_demo\n",
    "WHERE user_id = 'U002'\n",
    "\"\"\")\n",
    "print(\"Version 3: DELETE completed - Bob removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f55bf949-d3e6-47c8-b27c-23542e83bf12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# INSERT new record\n",
    "spark.sql(f\"\"\"\n",
    "INSERT INTO {CATALOG}.{BRONZE_SCHEMA}.cdf_demo VALUES\n",
    "    ('U004', 'Diana', 'diana@example.com', 'trial', current_timestamp())\n",
    "\"\"\")\n",
    "print(\"Version 4: INSERT completed - Diana added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef3f3dc6-8887-48a1-8d8d-e7df25cda2f7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 149"
    }
   },
   "source": [
    "### 4.2.3. Example: Reading Change Data Feed\n",
    "\n",
    "**Objective:** Read and analyze change data with CDF metadata columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8de8430c-0b56-47f0-b2e6-aa0a98b9b0fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "changes = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .table(f\"{CATALOG}.{BRONZE_SCHEMA}.cdf_demo\")\n",
    "\n",
    "display(changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75a16c02-d0cb-4639-be52-1ad7d2be0bf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read all changes from the beginning\n",
    "changes = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"readChangeFeed\", \"true\") \\\n",
    "    .option(\"startingVersion\", 0) \\\n",
    "    .table(f\"{CATALOG}.{BRONZE_SCHEMA}.cdf_demo\")\n",
    "\n",
    "# Show changes with CDF metadata columns\n",
    "display(\n",
    "    changes.select(\n",
    "        \"user_id\", \"name\", \"status\",\n",
    "        \"_change_type\",        # insert, update_preimage, update_postimage, delete\n",
    "        \"_commit_version\",     # Delta version number\n",
    "        \"_commit_timestamp\"    # When the change occurred\n",
    "    ).orderBy(\"_commit_version\", \"user_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "885e9b6e-eb22-46a3-98d6-0b148b705ac8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Understanding `_change_type` values:**\n",
    "\n",
    "| Change Type | Description |\n",
    "|-------------|-------------|\n",
    "| `insert` | New row inserted |\n",
    "| `update_preimage` | Row value BEFORE update |\n",
    "| `update_postimage` | Row value AFTER update |\n",
    "| `delete` | Row that was deleted |\n",
    "\n",
    "This enables powerful incremental processing patterns - you can process only what changed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9c70462-0945-4f22-8c1e-0ee252951874",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example: Get only new inserts since version 2\n",
    "new_inserts = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"readChangeFeed\", \"true\") \\\n",
    "    .option(\"startingVersion\", 2) \\\n",
    "    .table(f\"{CATALOG}.{BRONZE_SCHEMA}.cdf_demo\") \\\n",
    "    .filter(F.col(\"_change_type\") == \"insert\")\n",
    "\n",
    "print(\"New inserts since version 2:\")\n",
    "display(new_inserts.select(\"user_id\", \"name\", \"status\", \"_commit_version\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccd47c79-7f85-4a18-a9e3-f027f6edd111",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example: Get all deletions for audit purposes\n",
    "deletions = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"readChangeFeed\", \"true\") \\\n",
    "    .option(\"startingVersion\", 0) \\\n",
    "    .table(f\"{CATALOG}.{BRONZE_SCHEMA}.cdf_demo\") \\\n",
    "    .filter(F.col(\"_change_type\") == \"delete\")\n",
    "\n",
    "print(\"All deleted records (for audit):\")\n",
    "display(deletions.select(\"user_id\", \"name\", \"_commit_version\", \"_commit_timestamp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6ba49bf-2a86-426f-8b3a-60ff78d3a157",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 155"
    }
   },
   "source": [
    "### 4.2.4. Example: CDF for Incremental ETL\n",
    "\n",
    "**Objective:** Demonstrate how CDF enables efficient incremental processing in ETL pipelines\n",
    "\n",
    "Instead of reprocessing entire tables, use CDF to process only changed rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8df0cbfd-422c-4ed5-ab6d-c2a570349b74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simulate an incremental ETL pipeline\n",
    "# First run: Process all data (startingVersion = 0)\n",
    "# Subsequent runs: Process only changes since last processed version\n",
    "\n",
    "# Store the last processed version (in practice, save this to a checkpoint table)\n",
    "last_processed_version = 0\n",
    "\n",
    "# Read incremental changes\n",
    "incremental_changes = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"readChangeFeed\", \"true\") \\\n",
    "    .option(\"startingVersion\", last_processed_version) \\\n",
    "    .table(f\"{CATALOG}.{BRONZE_SCHEMA}.cdf_demo\")\n",
    "\n",
    "# Apply transformations only to changed records\n",
    "transformed = incremental_changes \\\n",
    "    .filter(F.col(\"_change_type\").isin([\"insert\", \"update_postimage\"])) \\\n",
    "    .withColumn(\"processed_at\", F.current_timestamp()) \\\n",
    "    .withColumn(\"email_domain\", F.split(F.col(\"email\"), \"@\")[1])\n",
    "\n",
    "print(\"Incremental processing - only changed records:\")\n",
    "display(transformed.select(\"user_id\", \"name\", \"email_domain\", \"status\", \"_change_type\", \"processed_at\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd42b10b-7bd8-45c7-9da4-a4fdb117bfcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.3. Deletion Vectors\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Deletion Vectors are a storage optimization feature in Delta Lake that improves DELETE, UPDATE, and MERGE performance.\n",
    "\n",
    "**How it works:**\n",
    "- Instead of rewriting entire data files on DELETE/UPDATE, Delta Lake marks rows as deleted in a separate **deletion vector file**\n",
    "- The actual data files remain unchanged until next OPTIMIZE or REORG\n",
    "- Reads automatically filter out deleted rows using the deletion vector\n",
    "\n",
    "\n",
    "<img src=\"../../../assets/images/3358ea8efef949f09710fd4423728a31.png\" width=\"800\">\n",
    "\n",
    "\n",
    "| Aspect | Without Deletion Vectors | With Deletion Vectors |\n",
    "|--------|------------------------|----------------------|\n",
    "| DELETE speed | Slow (rewrite files) | Fast (mark in vector) |\n",
    "| Storage during DELETE | Temporary 2x | Minimal overhead |\n",
    "| Read performance | Normal | Slight overhead (filter) |\n",
    "| Cleanup | Automatic | Needs REORG TABLE ... APPLY |\n",
    "\n",
    "```sql\n",
    "-- Enable deletion vectors on a table\n",
    "ALTER TABLE my_table SET TBLPROPERTIES ('delta.enableDeletionVectors' = true);\n",
    "\n",
    "-- Purge deletion vectors (rewrite files)\n",
    "REORG TABLE my_table APPLY (PURGE);\n",
    "```\n",
    "\n",
    "**Exam Note:** Deletion Vectors are enabled by default on new tables in Databricks. They make DML operations faster but may slightly increase read overhead until REORG is run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd3183fa-52ac-4234-8ede-bb9a5770d22f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.4. Predictive Optimization\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Predictive Optimization is an automatic maintenance feature in Databricks that eliminates the need for manual OPTIMIZE and VACUUM commands.\n",
    "\n",
    "**How it works:**\n",
    "- Databricks automatically monitors table health metrics\n",
    "- Runs OPTIMIZE (file compaction) when small files accumulate\n",
    "- Runs VACUUM when orphaned files need cleanup\n",
    "- Managed at catalog or schema level\n",
    "\n",
    "```sql\n",
    "-- Enable at schema level\n",
    "ALTER SCHEMA my_catalog.my_schema\n",
    "ENABLE PREDICTIVE OPTIMIZATION;\n",
    "\n",
    "-- Check optimization history\n",
    "SELECT * FROM system.storage.predictive_optimization_operations_history\n",
    "WHERE catalog_name = 'my_catalog'\n",
    "ORDER BY timestamp DESC;\n",
    "```\n",
    "\n",
    "| Feature | Manual | Predictive Optimization |\n",
    "|---------|--------|------------------------|\n",
    "| OPTIMIZE | Run manually / schedule | Automatic |\n",
    "| VACUUM | Run manually / schedule | Automatic |\n",
    "| Tuning | DBA responsibility | ML-based decisions |\n",
    "| Cost | Compute cost on schedule | Pay per optimization |\n",
    "\n",
    "**Exam Note:** Predictive Optimization requires Unity Catalog managed tables. It is the recommended approach for production workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc98f619-739b-4d55-b83c-771337ed9d7e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 158"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Topic | Key Takeaway |\n",
    "|---|---|\n",
    "| **OPTIMIZE** | Compacts small files. Run after streaming/frequent inserts |\n",
    "| **Partitioning** | Low-cardinality columns, partition > 1GB |\n",
    "| **Z-ORDER** | Co-locates data for data skipping on filter columns |\n",
    "| **Liquid Clustering** | Modern replacement — incremental, no manual OPTIMIZE |\n",
    "| **Data Skew** | AQE handles most cases. Salting, broadcast for extreme cases |\n",
    "| **CDF vs CDC** | CDC = source capture. CDF = Delta Lake row-level tracking |\n",
    "| **Deletion Vectors** | Fast DELETE/UPDATE via marking instead of rewriting |\n",
    "| **Predictive Optimization** | Automatic OPTIMIZE + VACUUM for UC managed tables |\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "| Operation | Command |\n",
    "|---|---|\n",
    "| Optimize | `OPTIMIZE table` |\n",
    "| Z-ORDER | `OPTIMIZE table ZORDER BY (col)` |\n",
    "| Vacuum | `VACUUM table RETAIN X HOURS` |\n",
    "| Enable CDF | `ALTER TABLE SET TBLPROPERTIES (delta.enableChangeDataFeed = true)` |\n",
    "| Read CDF | `.option(\"readChangeFeed\", \"true\").option(\"startingVersion\", 0)` |\n",
    "\n",
    "---\n",
    "\n",
    "> **← M03: Delta Fundamentals | Day 2 | M05: Incremental Processing →**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7345fa46-39e7-40b3-85cb-0f989d83baa2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 159"
    }
   },
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57591f66-475d-425f-bfc8-6b6f4d9e3094",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optional test resource cleanup\n",
    "# NOTE: Run only if you want to delete all created data\n",
    "\n",
    "# Tables to clean up:\n",
    "cleanup_tables = [\n",
    "    \"customers_delta\",\n",
    "    \"orders_modern\", \n",
    "    \"time_travel_demo\",\n",
    "    \"small_files_demo\",\n",
    "    \"orders_partitioned\",\n",
    "    \"sales_zorder_demo\",\n",
    "    \"sales_liquid_clustering\",\n",
    "    \"cdf_demo\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e99b47a-4e64-4887-b7ee-90841ba4465c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment below to execute cleanup:\n",
    "# for table in cleanup_tables:\n",
    "#     spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{BRONZE_SCHEMA}.{table}\")\n",
    "#     print(f\"Dropped: {table}\")\n",
    "\n",
    "# spark.sql(\"DROP VIEW IF EXISTS customer_updates\")\n",
    "# spark.catalog.clearCache()\n",
    "\n",
    "# print(\"All resources cleaned up!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6142587020061720,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "M04_delta_optimization",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
