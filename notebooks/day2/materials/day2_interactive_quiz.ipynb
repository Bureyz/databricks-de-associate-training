{
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Quiz -- Day 2\n",
    "\n",
    "**Modules:** M04 (Delta Optimization), M05 (Streaming), M06 (Advanced Transforms)  \n",
    "**Questions:** 20 (single correct answer A-D)  \n",
    "\n",
    "---\n",
    "\n",
    "### How it works\n",
    "1. **Run Cell 2** to create answer widgets (dropdowns at the top of the notebook)\n",
    "2. Read each question and select your answer from the dropdown\n",
    "3. **Run the last cell** to check your score\n",
    "\n",
    "> All widgets appear at the top of the notebook in Databricks. Scroll up to see them after running Cell 2."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run this cell FIRST to create answer widgets\n",
    "# Dropdowns will appear at the top of the notebook\n",
    "\n",
    "dbutils.widgets.dropdown(\"Q1\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q1\")\n",
    "dbutils.widgets.dropdown(\"Q2\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q2\")\n",
    "dbutils.widgets.dropdown(\"Q3\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q3\")\n",
    "dbutils.widgets.dropdown(\"Q4\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q4\")\n",
    "dbutils.widgets.dropdown(\"Q5\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q5\")\n",
    "dbutils.widgets.dropdown(\"Q6\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q6\")\n",
    "dbutils.widgets.dropdown(\"Q7\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q7\")\n",
    "dbutils.widgets.dropdown(\"Q8\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q8\")\n",
    "dbutils.widgets.dropdown(\"Q9\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q9\")\n",
    "dbutils.widgets.dropdown(\"Q10\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q10\")\n",
    "dbutils.widgets.dropdown(\"Q11\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q11\")\n",
    "dbutils.widgets.dropdown(\"Q12\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q12\")\n",
    "dbutils.widgets.dropdown(\"Q13\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q13\")\n",
    "dbutils.widgets.dropdown(\"Q14\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q14\")\n",
    "dbutils.widgets.dropdown(\"Q15\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q15\")\n",
    "dbutils.widgets.dropdown(\"Q16\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q16\")\n",
    "dbutils.widgets.dropdown(\"Q17\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q17\")\n",
    "dbutils.widgets.dropdown(\"Q18\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q18\")\n",
    "dbutils.widgets.dropdown(\"Q19\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q19\")\n",
    "dbutils.widgets.dropdown(\"Q20\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q20\")\n",
    "\n",
    "print(\"Created 20 widgets -- scroll up to see dropdowns\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "What does the `OPTIMIZE` command do on a Delta table?\n\n- A. Deletes old versions from the transaction log\n- B. Compacts small files into larger, optimally-sized files\n- C. Adds indexes to all columns\n- D. Converts the table from Parquet to Delta format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "What is the purpose of `ZORDER BY` when used with `OPTIMIZE`?\n\n- A. It sorts the table alphabetically by column name\n- B. It co-locates related data in the same files for faster filter queries\n- C. It compresses data using Z-algorithm\n- D. It creates a secondary index on the specified columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "What does `VACUUM` do on a Delta table?\n\n- A. Removes duplicate rows\n- B. Removes data files no longer referenced by the transaction log (older than retention threshold)\n- C. Compresses the transaction log\n- D. Recalculates table statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4\n",
    "\n",
    "Which Delta Lake feature replaces traditional partitioning and Z-Ordering with automatic data layout optimization?\n\n- A. Auto Optimize\n- B. Predictive Optimization\n- C. Liquid Clustering\n- D. Adaptive Query Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5\n",
    "\n",
    "What is the default retention period for `VACUUM` on a Delta table?\n\n- A. 24 hours\n- B. 7 days (168 hours)\n- C. 30 days\n- D. 90 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6\n",
    "\n",
    "A data engineer runs the following commands. What is the impact on time travel?\n\n```sql\nVACUUM my_table RETAIN 0 HOURS\n```\n\n- A. No impact -- time travel still works for all versions\n- B. All historical data files are removed; time travel to old versions will fail\n- C. Only the latest version is vacuumed\n- D. An error -- VACUUM requires at least 168 hours retention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7\n",
    "\n",
    "What is the role of a checkpoint directory in Structured Streaming?\n\n- A. It stores the output data\n- B. It tracks which data has already been processed to enable exactly-once semantics\n- C. It caches the stream schema\n- D. It logs user queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8\n",
    "\n",
    "Which Auto Loader option specifies the file format to ingest?\n\n- A. `cloudFiles.path`\n- B. `cloudFiles.format`\n- C. `cloudFiles.schemaLocation`\n- D. `cloudFiles.inferColumnTypes`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9\n",
    "\n",
    "What does `trigger(availableNow=True)` do in a streaming query?\n\n- A. Runs the stream continuously with micro-batches\n- B. Processes all available data incrementally and then stops\n- C. Waits for new data indefinitely\n- D. Runs exactly one micro-batch then pauses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10\n",
    "\n",
    "What is the difference between `COPY INTO` and Auto Loader for incremental file ingestion?\n\n- A. `COPY INTO` uses file notification; Auto Loader uses directory listing\n- B. `COPY INTO` stores state in a checkpoint; Auto Loader stores state in the Delta log\n- C. `COPY INTO` tracks processed files in the Delta log (idempotent SQL); Auto Loader uses checkpoints and scales to millions of files\n- D. There is no difference; they are aliases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q11\n",
    "\n",
    "Which of the following correctly shows an Auto Loader read stream?\n\n**A.**\n```python\nspark.readStream.format(\"cloudFiles\") \\\n    .option(\"cloudFiles.format\", \"json\") \\\n    .option(\"cloudFiles.schemaLocation\", checkpoint) \\\n    .load(\"/source/path\")\n```\n\n**B.**\n```python\nspark.read.format(\"autoLoader\") \\\n    .option(\"format\", \"json\") \\\n    .load(\"/source/path\")\n```\n\n**C.**\n```python\nspark.readStream.format(\"autoLoader\") \\\n    .option(\"fileFormat\", \"json\") \\\n    .load(\"/source/path\")\n```\n\n**D.**\n```python\nspark.readStream.format(\"delta\") \\\n    .option(\"autoLoader\", True) \\\n    .load(\"/source/path\")\n```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q12\n",
    "\n",
    "What is the purpose of Predictive Optimization in Databricks?\n\n- A. Predicts query execution time\n- B. Automatically runs OPTIMIZE, VACUUM, and ANALYZE TABLE based on table usage patterns\n- C. Optimizes cluster autoscaling\n- D. Predicts storage costs for the next month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q13\n",
    "\n",
    "What do Deletion Vectors improve in Delta Lake?\n\n- A. INSERT performance\n- B. DELETE, UPDATE, and MERGE performance by marking rows as deleted without rewriting files\n- C. SELECT performance for aggregations\n- D. VACUUM speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q14\n",
    "\n",
    "Which SQL construct creates a running total (cumulative sum) of `amount` ordered by `date` within each `category`?\n\n**A.**\n```sql\nSUM(amount) GROUP BY category\n```\n\n**B.**\n```sql\nSUM(amount) OVER (\n    PARTITION BY category\n    ORDER BY date\n    ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n)\n```\n\n**C.**\n```sql\nRUNNING_SUM(amount, category, date)\n```\n\n**D.**\n```sql\nCUMSUM(amount) WITHIN GROUP (ORDER BY date)\n```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q15\n",
    "\n",
    "What does the `explode()` function do?\n\n- A. Splits a string by delimiter into multiple rows\n- B. Converts each element of an array or map into a separate row\n- C. Flattens nested JSON into a flat schema\n- D. Decompresses compressed columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q16\n",
    "\n",
    "What is the correct syntax for a multi-step CTE (Common Table Expression) in SQL?\n\n**A.**\n```sql\nWITH step1 AS (SELECT ...),\n     step2 AS (SELECT ... FROM step1),\n     step3 AS (SELECT ... FROM step2)\nSELECT * FROM step3\n```\n\n**B.**\n```sql\nCTE step1 = (SELECT ...)\nCTE step2 = (SELECT ... FROM step1)\nSELECT * FROM step2\n```\n\n**C.**\n```sql\nWITH step1 AS (SELECT ...)\nWITH step2 AS (SELECT ... FROM step1)\nSELECT * FROM step2\n```\n\n**D.**\n```sql\nDECLARE step1 = SELECT ...\nSELECT * FROM step1\n```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q17\n",
    "\n",
    "Which window function assigns a unique sequential number to each row within a partition, with no gaps?\n\n- A. `RANK()`\n- B. `DENSE_RANK()`\n- C. `ROW_NUMBER()`\n- D. `NTILE()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q18\n",
    "\n",
    "What is the difference between `RANK()` and `DENSE_RANK()`?\n\n- A. `RANK()` skips numbers after ties; `DENSE_RANK()` does not skip\n- B. `RANK()` is for ascending only; `DENSE_RANK()` is for descending\n- C. They are identical\n- D. `DENSE_RANK()` works only with numeric columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q19\n",
    "\n",
    "Which higher-order function applies a transformation to each element of an array?\n\n**A.**\n```sql\nTRANSFORM(array_col, x -> x * 2)\n```\n\n**B.**\n```sql\nMAP(array_col, x -> x * 2)\n```\n\n**C.**\n```sql\nAPPLY(array_col, x -> x * 2)\n```\n\n**D.**\n```sql\nFOREACH(array_col, x -> x * 2)\n```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q20\n",
    "\n",
    "What does `DESCRIBE DETAIL my_table` return that `DESCRIBE TABLE my_table` does not?\n\n- A. Column names and data types\n- B. Table location, file count, size in bytes, partitioning info, and table properties\n- C. The SQL definition of the table\n- D. Access control permissions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run this cell to check your score\n",
    "\n",
    "answers = {\n",
    "    1: ('B', '`OPTIMIZE` compacts small files (small file problem) into larger files, improving read performance.'),\n",
    "    2: ('B', 'Z-Ordering co-locates related values in the same set of files, enabling data skipping and faster filtered reads.'),\n",
    "    3: ('B', '`VACUUM` removes stale data files no longer part of the current table version. Default retention is 7 days.'),\n",
    "    4: ('C', 'Liquid Clustering (`CLUSTER BY`) automatically optimizes data layout, replacing manual partitioning and ZORDER.'),\n",
    "    5: ('B', 'The default VACUUM retention is 7 days (168 hours). Files older than this and no longer referenced are removed.'),\n",
    "    6: ('B', '`VACUUM RETAIN 0 HOURS` (requires disabling safety check) removes all unreferenced files, breaking time travel for those versions.'),\n",
    "    7: ('B', 'The checkpoint directory stores offset info and state, ensuring exactly-once fault-tolerant processing.'),\n",
    "    8: ('B', '`cloudFiles.format` specifies the source file format (e.g., `json`, `csv`, `parquet`) for Auto Loader.'),\n",
    "    9: ('B', '`availableNow=True` processes all currently available data in incremental batches and then stops. Ideal for scheduled jobs.'),\n",
    "    10: ('C', '`COPY INTO` is SQL-based idempotent (Delta log). Auto Loader uses checkpoints + file notification, scaling to millions of files.'),\n",
    "    11: ('A', 'Auto Loader uses `format(\"cloudFiles\")` with `readStream`, and requires `cloudFiles.format` + `cloudFiles.schemaLocation` options.'),\n",
    "    12: ('B', 'Predictive Optimization automatically schedules and runs OPTIMIZE, VACUUM, and ANALYZE based on usage patterns.'),\n",
    "    13: ('B', 'Deletion Vectors mark rows as deleted in a side file without rewriting the entire Parquet file, speeding up DELETE/UPDATE/MERGE.'),\n",
    "    14: ('B', 'A window function with `ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW` creates a running total partitioned by category.'),\n",
    "    15: ('B', '`explode()` takes an array or map column and generates one row per element (or key-value pair).'),\n",
    "    16: ('A', 'Multi-step CTEs use a single `WITH` keyword, with each step separated by commas. The final `SELECT` references the last CTE.'),\n",
    "    17: ('C', '`ROW_NUMBER()` gives each row a unique sequential number (1, 2, 3...) with no gaps, regardless of ties.'),\n",
    "    18: ('A', '`RANK()` leaves gaps after ties (1, 1, 3). `DENSE_RANK()` does not skip (1, 1, 2).'),\n",
    "    19: ('A', '`TRANSFORM(array, x -> expr)` applies a lambda to each element. `FILTER` filters elements, `EXISTS` checks a condition.'),\n",
    "    20: ('B', '`DESCRIBE DETAIL` returns physical metadata: location, size, numFiles, partitioning, properties, createdAt, etc.'),\n",
    "}\n",
    "\n",
    "correct = 0\n",
    "total = len(answers)\n",
    "results = []\n",
    "\n",
    "for qnum in sorted(answers.keys()):\n",
    "    user_ans = dbutils.widgets.get(f\"Q{qnum}\")\n",
    "    correct_ans, explanation = answers[qnum]\n",
    "    if user_ans == correct_ans:\n",
    "        correct += 1\n",
    "        results.append(f\"  Q{qnum}: {user_ans} -- Correct!\")\n",
    "    elif user_ans == \"-\":\n",
    "        results.append(f\"  Q{qnum}: Not answered (correct: {correct_ans})\")\n",
    "    else:\n",
    "        results.append(f\"  Q{qnum}: {user_ans} -- Wrong. Correct: {correct_ans} | {explanation}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  QUIZ RESULTS -- Day 2\")\n",
    "print(\"=\" * 60)\n",
    "for r in results:\n",
    "    print(r)\n",
    "print(\"=\" * 60)\n",
    "pct = round(correct / total * 100)\n",
    "print(f\"\\n  Score: {correct}/{total} ({pct}%)\")\n",
    "if pct >= 90:\n",
    "    print(\"  Excellent! Exam-ready!\")\n",
    "elif pct >= 70:\n",
    "    print(\"  Good job! Review missed topics.\")\n",
    "elif pct >= 50:\n",
    "    print(\"  Keep studying -- revisit the cheatsheet.\")\n",
    "else:\n",
    "    print(\"  Re-read the demo notebooks and try again.\")\n",
    "print(\"=\" * 60)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# (Optional) Remove all widgets\n",
    "# dbutils.widgets.removeAll()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}