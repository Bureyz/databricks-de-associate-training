{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3592e1b8",
   "metadata": {},
   "source": [
    "# LAB 04: Delta Lake Optimization\n\n**Duration:** ~30 min | **Day:** 2 | **Difficulty:** Intermediate\n**After module:** M04: Delta Lake Optimization\n\n> *\"Optimize the orders table: compact files, apply Z-ORDER, clean up with VACUUM, try Liquid Clustering.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff33fd7",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c481e57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../setup/00_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1c1467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create orders table with many small files (simulating production fragmentation)\n",
    "import json\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{BRONZE_SCHEMA}\")\n",
    "\n",
    "orders_path = f\"{DATASET_PATH}/orders/orders_batch.json\"\n",
    "df_orders = spark.read.format(\"json\").load(orders_path)\n",
    "\n",
    "# Write in small batches to create many files\n",
    "table_name = f\"{CATALOG}.{BRONZE_SCHEMA}.orders_optimize_lab\"\n",
    "df_orders.repartition(20).write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "# Add a few more appends to create small files\n",
    "for i in range(5):\n",
    "    df_orders.limit(10).write.mode(\"append\").saveAsTable(table_name)\n",
    "\n",
    "print(f\"Table ready with fragmented files: {spark.table(table_name).count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0365597",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1: Inspect Table Metrics\n",
    "\n",
    "Use `DESCRIBE DETAIL` to check the number of files and total size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6bda76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Inspect table detail\n",
    "df_detail = spark.sql(f\"________ {table_name}\")\n",
    "display(df_detail.select(\"format\", \"numFiles\", \"sizeInBytes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca9e394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "detail = df_detail.first()\n",
    "num_files_before = detail[\"numFiles\"]\n",
    "assert detail[\"format\"] == \"delta\", \"Table should be Delta format\"\n",
    "print(f\"Task 1 OK: {num_files_before} files, {detail['sizeInBytes']:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c6dd82",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: OPTIMIZE\n",
    "\n",
    "Run `OPTIMIZE` to compact the small files into larger ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0c567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run OPTIMIZE\n",
    "spark.sql(f\"________ {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832bc4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check files after OPTIMIZE\n",
    "df_detail_after = spark.sql(f\"DESCRIBE DETAIL {table_name}\")\n",
    "num_files_after = df_detail_after.first()[\"numFiles\"]\n",
    "\n",
    "print(f\"Files BEFORE: {num_files_before}\")\n",
    "print(f\"Files AFTER:  {num_files_after}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f333a46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert num_files_after <= num_files_before, \"OPTIMIZE should reduce file count\"\n",
    "print(f\"Task 2 OK: Compacted from {num_files_before} to {num_files_after} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b709146b",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3: ZORDER BY\n",
    "\n",
    "Run `OPTIMIZE ... ZORDER BY (customer_id)` to co-locate data for customer queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc521596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: OPTIMIZE with ZORDER\n",
    "spark.sql(f\"\"\"\n",
    "    OPTIMIZE {table_name}\n",
    "    ________ (customer_id)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a7d19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "history = spark.sql(f\"DESCRIBE HISTORY {table_name}\").collect()\n",
    "ops = [r[\"operation\"] for r in history]\n",
    "assert \"OPTIMIZE\" in ops, \"Expected OPTIMIZE in history\"\n",
    "print(f\"Task 3 OK: ZORDER applied. History: {ops[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5c2919",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 4: VACUUM\n",
    "\n",
    "Run `VACUUM` to remove obsolete files. Use `DRY RUN` first to preview, then execute.\n",
    "\n",
    "> Note: We'll use 0 hours retention for the lab (requires disabling safety check)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605715c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable retention check (LAB ONLY - never do this in production!)\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "\n",
    "# TODO: Run VACUUM DRY RUN first\n",
    "display(spark.sql(f\"VACUUM {table_name} RETAIN 0 HOURS ________\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c2cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run actual VACUUM\n",
    "spark.sql(f\"________ {table_name} RETAIN 0 HOURS\")\n",
    "print(\"VACUUM complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53f991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "# After VACUUM, old versions should no longer be accessible\n",
    "current_count = spark.table(table_name).count()\n",
    "assert current_count > 0, \"Table should still have data\"\n",
    "print(f\"Task 4 OK: VACUUM done. Current table: {current_count} rows\")\n",
    "\n",
    "# Re-enable safety check\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a78675",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 5: Liquid Clustering\n",
    "\n",
    "Create a new table WITH Liquid Clustering enabled, then copy data into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f33411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create table with Liquid Clustering\n",
    "lc_table = f\"{CATALOG}.{BRONZE_SCHEMA}.orders_liquid_cluster\"\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {lc_table}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE {lc_table}\n",
    "    ________ (customer_id)\n",
    "    AS SELECT * FROM {table_name}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f31ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "lc_detail = spark.sql(f\"DESCRIBE DETAIL {lc_table}\").first()\n",
    "lc_count = spark.table(lc_table).count()\n",
    "assert lc_count > 0, \"Liquid Clustered table should have data\"\n",
    "print(f\"Task 5 OK: Liquid Clustered table created with {lc_count} rows\")\n",
    "print(f\"  Clustering columns: {lc_detail['clusteringColumns']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c6a7a3",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad2a054",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {lc_table}\")\n",
    "print(\"Lab tables cleaned up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c3e206",
   "metadata": {},
   "source": [
    "---\n",
    "## Lab Complete!\n",
    "\n",
    "You have:\n",
    "- Inspected table metrics with DESCRIBE DETAIL\n",
    "- Compacted small files with OPTIMIZE\n",
    "- Applied Z-ORDER for query optimization\n",
    "- Cleaned obsolete files with VACUUM\n",
    "- Created a Liquid Clustered table\n",
    "\n",
    "> **Exam Tip:** Liquid Clustering replaces both partitioning and Z-ORDER. Use `ALTER TABLE ... CLUSTER BY (new_cols)` to change clustering columns without rewriting data.\n",
    "\n",
    "> **Next:** LAB 05 - Streaming & Auto Loader"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}