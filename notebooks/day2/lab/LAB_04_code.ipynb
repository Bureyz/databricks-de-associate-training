{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3592e1b8",
   "metadata": {},
   "source": [
    "# LAB 04: Delta Lake Optimization\n",
    "\n",
    "**Duration:** ~30 min | **Day:** 2 | **Difficulty:** Intermediate\n",
    "**After module:** M04: Delta Lake Optimization\n",
    "\n",
    "> *\"Optimize the orders table: compact files, apply Z-ORDER, clean up with VACUUM, try Liquid Clustering.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff33fd7",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c481e57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../setup/00_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1c1467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create orders table with many small files (simulating production fragmentation)\n",
    "import json\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{BRONZE_SCHEMA}\")\n",
    "\n",
    "orders_path = f\"{DATASET_PATH}/orders/orders_batch.json\"\n",
    "df_orders = spark.read.format(\"json\").load(orders_path)\n",
    "\n",
    "# Write in small batches to create many files\n",
    "table_name = f\"{CATALOG}.{BRONZE_SCHEMA}.orders_optimize_lab\"\n",
    "df_orders.repartition(20).write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "# Add a few more appends to create small files\n",
    "for i in range(5):\n",
    "    df_orders.limit(10).write.mode(\"append\").saveAsTable(table_name)\n",
    "\n",
    "print(f\"Table ready with fragmented files: {spark.table(table_name).count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0365597",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1: Inspect Table Metrics\n",
    "\n",
    "Use `DESCRIBE DETAIL` to check the number of files and total size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6bda76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Inspect table detail\n",
    "df_detail = spark.sql(f\"________ {table_name}\")\n",
    "display(df_detail.select(\"format\", \"numFiles\", \"sizeInBytes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca9e394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "detail = df_detail.first()\n",
    "num_files_before = detail[\"numFiles\"]\n",
    "assert detail[\"format\"] == \"delta\", \"Table should be Delta format\"\n",
    "print(f\"Task 1 OK: {num_files_before} files, {detail['sizeInBytes']:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c6dd82",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: OPTIMIZE\n",
    "\n",
    "Run `OPTIMIZE` to compact the small files into larger ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0c567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run OPTIMIZE\n",
    "spark.sql(f\"________ {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832bc4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check files after OPTIMIZE\n",
    "df_detail_after = spark.sql(f\"DESCRIBE DETAIL {table_name}\")\n",
    "num_files_after = df_detail_after.first()[\"numFiles\"]\n",
    "\n",
    "print(f\"Files BEFORE: {num_files_before}\")\n",
    "print(f\"Files AFTER:  {num_files_after}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f333a46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert num_files_after <= num_files_before, \"OPTIMIZE should reduce file count\"\n",
    "print(f\"Task 2 OK: Compacted from {num_files_before} to {num_files_after} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b709146b",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3: ZORDER BY\n",
    "\n",
    "Run `OPTIMIZE ... ZORDER BY (customer_id)` to co-locate data for customer queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc521596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: OPTIMIZE with ZORDER\n",
    "spark.sql(f\"\"\"\n",
    "    OPTIMIZE {table_name}\n",
    "    ________ (customer_id)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a7d19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "history = spark.sql(f\"DESCRIBE HISTORY {table_name}\").collect()\n",
    "ops = [r[\"operation\"] for r in history]\n",
    "assert \"OPTIMIZE\" in ops, \"Expected OPTIMIZE in history\"\n",
    "print(f\"Task 3 OK: ZORDER applied. History: {ops[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5c2919",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 4: VACUUM\n",
    "\n",
    "Run `VACUUM` to remove obsolete files. Use `DRY RUN` first to preview, then execute.\n",
    "\n",
    "> Note: We'll use 0 hours retention for the lab (requires disabling safety check)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605715c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable retention check (LAB ONLY - never do this in production!)\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "\n",
    "# TODO: Run VACUUM DRY RUN first\n",
    "display(spark.sql(f\"VACUUM {table_name} RETAIN 0 HOURS ________\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c2cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run actual VACUUM\n",
    "spark.sql(f\"________ {table_name} RETAIN 0 HOURS\")\n",
    "print(\"VACUUM complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53f991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "# After VACUUM, old versions should no longer be accessible\n",
    "current_count = spark.table(table_name).count()\n",
    "assert current_count > 0, \"Table should still have data\"\n",
    "print(f\"Task 4 OK: VACUUM done. Current table: {current_count} rows\")\n",
    "\n",
    "# Re-enable safety check\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a78675",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 5: Liquid Clustering\n",
    "\n",
    "Create a new table WITH Liquid Clustering enabled, then copy data into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f33411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create table with Liquid Clustering\n",
    "lc_table = f\"{CATALOG}.{BRONZE_SCHEMA}.orders_liquid_cluster\"\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {lc_table}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE {lc_table}\n",
    "    ________ (customer_id)\n",
    "    AS SELECT * FROM {table_name}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f31ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "lc_detail = spark.sql(f\"DESCRIBE DETAIL {lc_table}\").first()\n",
    "lc_count = spark.table(lc_table).count()\n",
    "assert lc_count > 0, \"Liquid Clustered table should have data\"\n",
    "print(f\"Task 5 OK: Liquid Clustered table created with {lc_count} rows\")\n",
    "print(f\"  Clustering columns: {lc_detail['clusteringColumns']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13312dae",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 6: Detect and Handle Data Skew\n",
    "\n",
    "Create a skewed dataset, detect the skew, and fix it using a **broadcast join**.\n",
    "\n",
    "**Scenario:** A `sales` table has 90% of rows for `customer_id = 1` (hot key). Joining it with a small `customers` lookup table causes skew → one executor does most of the work.\n",
    "\n",
    "**Steps:**\n",
    "1. Create a skewed `sales` table\n",
    "2. Detect the skew by counting rows per key\n",
    "3. Use `broadcast()` hint to optimize the join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a447db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, lit, rand, round as spark_round\n",
    "\n",
    "# Step 1: Create a skewed sales table (90% hot key)\n",
    "skew_table = f\"{CATALOG}.{BRONZE_SCHEMA}.sales_skew_lab\"\n",
    "\n",
    "# 9000 rows for customer_id=1, 100 rows each for customers 2-11\n",
    "df_hot = spark.range(9000).withColumn(\"customer_id\", lit(1)).withColumn(\"amount\", spark_round(rand() * 100, 2))\n",
    "df_rest = spark.range(1000).withColumn(\"customer_id\", (col(\"id\") % 10 + 2).cast(\"int\")).withColumn(\"amount\", spark_round(rand() * 100, 2))\n",
    "df_skewed = df_hot.unionByName(df_rest)\n",
    "\n",
    "df_skewed.write.mode(\"overwrite\").saveAsTable(skew_table)\n",
    "print(f\"Skewed table ready: {spark.table(skew_table).count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1ea6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Detect skew — count rows per customer_id, ordered DESC\n",
    "# Fill in the GROUP BY and ORDER BY\n",
    "\n",
    "df_skew_check = spark.sql(f\"\"\"\n",
    "    SELECT customer_id, ________(________) as row_count\n",
    "    FROM {skew_table}\n",
    "    GROUP BY ________\n",
    "    ORDER BY row_count ________\n",
    "\"\"\")\n",
    "\n",
    "display(df_skew_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0791e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "top_row = df_skew_check.first()\n",
    "assert top_row[\"customer_id\"] == 1, \"Customer 1 should have the most rows (hot key)\"\n",
    "assert top_row[\"row_count\"] > 5000, f\"Customer 1 should have 9000 rows, got {top_row['row_count']}\"\n",
    "print(f\"Skew detected! Customer {top_row['customer_id']}: {top_row['row_count']} rows (hot key)\")\n",
    "print(f\"Other customers: ~100 rows each\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f721e880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast, sum as spark_sum\n",
    "\n",
    "# Create a small lookup table (customers)\n",
    "customers_lookup = spark.createDataFrame(\n",
    "    [(i, f\"Customer_{i}\") for i in range(1, 12)],\n",
    "    [\"customer_id\", \"customer_name\"]\n",
    ")\n",
    "\n",
    "# TODO: Use broadcast() to join skewed sales with small customers table\n",
    "# This avoids shuffle on the large skewed table\n",
    "\n",
    "df_joined = spark.table(skew_table).join(\n",
    "    ________(customers_lookup),    # hint: broadcast(customers_lookup)\n",
    "    on=\"customer_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Aggregate: total amount per customer\n",
    "df_result = (\n",
    "    df_joined\n",
    "    .groupBy(\"customer_id\", \"customer_name\")\n",
    "    .agg(spark_sum(\"amount\").alias(\"total_amount\"))\n",
    "    .orderBy(\"total_amount\", ascending=False)\n",
    ")\n",
    "\n",
    "display(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f456e47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert df_result.count() > 0, \"Join result should not be empty\"\n",
    "assert \"customer_name\" in df_result.columns, \"customer_name should be present (from broadcast join)\"\n",
    "top = df_result.first()\n",
    "assert top[\"customer_id\"] == 1, \"Customer 1 should have highest total (9000 rows)\"\n",
    "print(f\"Task 6 OK: Broadcast join completed. Top customer: {top['customer_name']} = ${top['total_amount']:.2f}\")\n",
    "print(\"\\nKey takeaway: broadcast() sends the small table to all executors,\")\n",
    "print(\"  avoiding shuffle of the large skewed table.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c6a7a3",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad2a054",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {lc_table}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {skew_table}\")\n",
    "print(\"Lab tables cleaned up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c3e206",
   "metadata": {},
   "source": [
    "---\n",
    "## Lab Complete!\n",
    "\n",
    "You have:\n",
    "- Inspected table metrics with DESCRIBE DETAIL\n",
    "- Compacted small files with OPTIMIZE\n",
    "- Applied Z-ORDER for query optimization\n",
    "- Cleaned obsolete files with VACUUM\n",
    "- Created a Liquid Clustered table\n",
    "- Detected data skew and resolved it with broadcast join\n",
    "\n",
    "> **Exam Tip:** Liquid Clustering replaces both partitioning and Z-ORDER. Use `ALTER TABLE ... CLUSTER BY (new_cols)` to change clustering columns without rewriting data. For data skew, `broadcast()` works when one side is small (< 10MB by default). AQE handles skew automatically in most cases.\n",
    "\n",
    "> **Next:** LAB 05 - Streaming & Auto Loader"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
