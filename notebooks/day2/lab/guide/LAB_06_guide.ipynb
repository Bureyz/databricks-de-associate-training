{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 06: Advanced Transforms -- PySpark & SQL\n",
    "\n",
    "**Duration:** ~40 min  \n",
    "**Day:** 2  \n",
    "**After module:** M06: Advanced Transforms  \n",
    "**Difficulty:** Intermediate-Advanced\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario\n",
    "\n",
    "> *\"The RetailHub business team needs analytical reports: top products by revenue, customer spending trends, and category performance over time. You'll build these using window functions, CTEs, explode, and CTAS -- both in PySpark and SQL.\"*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "- Use PySpark window functions (ROW_NUMBER, RANK, SUM OVER)\n",
    "- Write SQL CTEs (Common Table Expressions)  \n",
    "- Flatten nested JSON with `explode()`\n",
    "- Create tables using `CREATE TABLE AS SELECT` (CTAS)\n",
    "- Compare PySpark and SQL approaches side by side\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Window Functions (~15 min)\n",
    "\n",
    "### Task 1: Rank Products by Revenue (PySpark)\n",
    "\n",
    "Use `Window.partitionBy(\"category\").orderBy(desc(\"revenue\"))` with `row_number()` to rank products within each category.\n",
    "\n",
    "### Task 2: Running Totals (SQL)\n",
    "\n",
    "Write a SQL query with `SUM(total_amount) OVER (PARTITION BY customer_id ORDER BY order_datetime)` to compute a running total per customer.\n",
    "\n",
    "> **Exam Tip:** Window functions don't reduce rows (unlike GROUP BY). They add computed columns based on a \"window\" of related rows.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: CTE and Subqueries (~10 min)\n",
    "\n",
    "### Task 3: Multi-step CTE\n",
    "\n",
    "Write a SQL query with multiple CTEs:\n",
    "1. CTE `daily_sales` -- aggregate orders by date\n",
    "2. CTE `ranked_days` -- rank days by total revenue\n",
    "3. Final SELECT -- top 5 days by revenue\n",
    "\n",
    "### Task 4: Correlated Subquery\n",
    "\n",
    "Find customers whose total spending exceeds the average spending of all customers.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Explode & JSON (~10 min)\n",
    "\n",
    "### Task 5: Explode Array Column\n",
    "\n",
    "Given an orders table with an `items` array column, use `explode()` to create one row per item.\n",
    "\n",
    "### Task 6: Parse Nested JSON\n",
    "\n",
    "Use `from_json()` to parse a JSON string column, then access nested fields.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: CTAS -- Persist Results (~5 min)\n",
    "\n",
    "### Task 7: Create Gold Tables with CTAS\n",
    "\n",
    "Create Gold-layer summary tables using `CREATE TABLE AS SELECT`:\n",
    "- `gold.top_products` -- top 10 products by revenue\n",
    "- `gold.customer_segments` -- customers segmented by spending\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab you:\n",
    "- Applied window functions (ROW_NUMBER, RANK, running SUM) in PySpark and SQL\n",
    "- Wrote multi-step CTEs for complex analytics\n",
    "- Flattened arrays with explode()\n",
    "- Created Gold tables with CTAS\n",
    "\n",
    "> **What's next:** LAB 07 - Build a full Medallion pipeline in Lakeflow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
