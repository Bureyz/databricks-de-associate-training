{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 05: Streaming & Auto Loader\n",
    "\n",
    "**Duration:** ~35 min  \n",
    "**Day:** 2  \n",
    "**After module:** M05: Incremental Data Processing  \n",
    "**Difficulty:** Intermediate-Advanced\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario\n",
    "\n",
    "> *\"New order files arrive continuously in the landing zone. Set up Auto Loader for streaming JSON ingestion into the Bronze layer with exactly-once guarantees. Then explore Change Data Feed (CDF) for incremental ETL.\"*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "- Use `COPY INTO` for idempotent batch loading\n",
    "- Configure Auto Loader (`cloudFiles`) for streaming ingestion\n",
    "- Use `trigger(availableNow=True)` for incremental processing\n",
    "- Verify checkpoint-based exactly-once guarantees\n",
    "- Add metadata columns to streaming writes\n",
    "- Handle schema evolution with rescued data column\n",
    "- Perform a stream-static join\n",
    "- Use Change Data Feed (CDF) for incremental ETL\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Cluster running and attached to notebook\n",
    "- Stream files available in `dataset/orders/stream/`\n",
    "- Setup cell creates customers table if needed (for Task 8)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks Overview\n",
    "\n",
    "Open **`LAB_05_code.ipynb`** and complete the `# TODO` cells.\n",
    "\n",
    "| Task | What to do | Key concept |\n",
    "|------|-----------|-------------|\n",
    "| **Task 1** | COPY INTO (Batch) | `COPY INTO table FROM path FILEFORMAT = JSON` |\n",
    "| **Task 2** | Verify Idempotency | Re-run COPY INTO — 0 new rows |\n",
    "| **Task 3** | Configure Auto Loader Stream | `.format(\"cloudFiles\")`, `cloudFiles.format`, `schemaLocation` |\n",
    "| **Task 4** | Write Stream | `.trigger(availableNow=True)`, `.toTable()` |\n",
    "| **Task 5** | Incremental Processing | Re-run stream — checkpoint prevents reprocessing |\n",
    "| **Task 6** | Metadata Columns | `current_timestamp()`, `col(\"_metadata.file_path\")` |\n",
    "| **Task 7** | Schema Evolution — Rescued Data | `schemaEvolutionMode = \"rescue\"`, `_rescued_data` column |\n",
    "| **Task 8** | Stream-Static Join | Join streaming orders with static customers table |\n",
    "| **Task 9** | Change Data Feed (CDF) | Enable CDF, make changes, read with `table_changes()` |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Hints\n",
    "\n",
    "### Task 1: COPY INTO\n",
    "- `FILEFORMAT = JSON`\n",
    "- COPY INTO is idempotent — tracks which files have been loaded\n",
    "\n",
    "### Task 3: Auto Loader\n",
    "- Format: `\"cloudFiles\"`\n",
    "- `cloudFiles.format`: `\"json\"`\n",
    "- Schema location stores the inferred schema for evolution tracking\n",
    "\n",
    "### Task 4: Write Stream\n",
    "- `.trigger(availableNow=True)` — processes all available files and stops\n",
    "- Always specify `checkpointLocation`\n",
    "\n",
    "### Task 6: Metadata\n",
    "- `current_timestamp()` for processing time\n",
    "- `col(\"_metadata.file_path\")` for source file path\n",
    "\n",
    "### Task 7: Rescued Data\n",
    "- Set `cloudFiles.schemaEvolutionMode` to `\"rescue\"`\n",
    "- Extra columns not in the defined schema land in `_rescued_data`\n",
    "\n",
    "### Task 8: Stream-Static Join\n",
    "- Stream side: `spark.readStream.format(\"delta\").table(target_table)`\n",
    "- Static side: `spark.table(\"catalog.schema.customers\")`\n",
    "- Join on `\"customer_id\"` with `how=\"left\"`\n",
    "\n",
    "### Task 9: CDF\n",
    "- Enable: `TBLPROPERTIES ('delta.enableChangeDataFeed' = 'true')`\n",
    "- Read changes: `table_changes('table_name', start_version)`\n",
    "- Filter: `_change_type IN ('insert', 'update_postimage')`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab you:\n",
    "- Used COPY INTO for idempotent batch loading\n",
    "- Configured Auto Loader (cloudFiles) for streaming ingestion\n",
    "- Used trigger(availableNow=True) for incremental processing\n",
    "- Verified checkpoint-based exactly-once guarantees\n",
    "- Added metadata columns to streaming writes\n",
    "- Used rescued data column for schema evolution handling\n",
    "- Performed a stream-static join to enrich streaming data\n",
    "- Used Change Data Feed (CDF) for incremental ETL\n",
    "\n",
    "| Feature | COPY INTO | Auto Loader | CDF |\n",
    "|---------|-----------|-------------|-----|\n",
    "| Format | SQL command | readStream/writeStream | table_changes() |\n",
    "| Scalability | Thousands of files | Millions of files | Any Delta table |\n",
    "| Schema evolution | Manual | Automatic (rescue) | Follows source |\n",
    "| File tracking | SQL state | Checkpoint directory | Version-based |\n",
    "\n",
    "> **Exam Tip:** Auto Loader uses `cloudFiles` format. COPY INTO is simpler but Auto Loader scales better. CDF captures row-level changes (`insert`, `update_preimage`, `update_postimage`, `delete`).\n",
    "\n",
    "> **What's next:** In LAB 06 you will build analytical reports using window functions, CTEs, explode, and higher-order functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}