{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8e7f0d1",
   "metadata": {},
   "source": [
    "# LAB 05: Streaming & Auto Loader\n",
    "\n",
    "**Duration:** ~35 min | **Day:** 2 | **Difficulty:** Intermediate-Advanced\n",
    "**After module:** M05: Incremental Data Processing\n",
    "\n",
    "> *\"Set up Auto Loader for streaming JSON ingestion into the Bronze layer with exactly-once guarantees.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fbfc80",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d95d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../setup/00_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0046ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare landing zone and checkpoint paths\n",
    "landing_path = f\"{DATASET_PATH}/orders/stream\"\n",
    "checkpoint_path = f\"/tmp/{CATALOG}/lab05/checkpoint\"\n",
    "schema_path = f\"/tmp/{CATALOG}/lab05/schema\"\n",
    "target_table = f\"{CATALOG}.{BRONZE_SCHEMA}.orders_stream\"\n",
    "\n",
    "# Clean up from previous runs\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {target_table}\")\n",
    "dbutils.fs.rm(checkpoint_path, True)\n",
    "dbutils.fs.rm(schema_path, True)\n",
    "\n",
    "print(f\"Landing path: {landing_path}\")\n",
    "print(f\"Target table: {target_table}\")\n",
    "print(f\"Files available: {[f.name for f in dbutils.fs.ls(landing_path)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da740c00",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1: COPY INTO (Batch Ingestion)\n",
    "\n",
    "Use `COPY INTO` to load the first file from the landing zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd9cd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create the target table\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {target_table}\n",
    "    (order_id STRING, customer_id STRING, product_id STRING, \n",
    "     quantity INT, total_price DOUBLE, order_date STRING, \n",
    "     payment_method STRING, store_id STRING)\n",
    "\"\"\")\n",
    "\n",
    "# TODO: Use COPY INTO to load JSON files\n",
    "spark.sql(f\"\"\"\n",
    "    COPY INTO {target_table}\n",
    "    FROM '{landing_path}'\n",
    "    FILEFORMAT = ________\n",
    "    FORMAT_OPTIONS ('mergeSchema' = 'true')\n",
    "\"\"\")\n",
    "\n",
    "count_after_copy = spark.table(target_table).count()\n",
    "print(f\"Rows after COPY INTO: {count_after_copy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd6b49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert count_after_copy > 0, \"COPY INTO should have loaded data\"\n",
    "print(f\"Task 1 OK: {count_after_copy} rows loaded via COPY INTO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a96f94d",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Verify COPY INTO Idempotency\n",
    "\n",
    "Run COPY INTO again on the same files. How many new rows are loaded?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666136ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Re-run the same COPY INTO\n",
    "spark.sql(f\"\"\"\n",
    "    COPY INTO {target_table}\n",
    "    FROM '{landing_path}'\n",
    "    FILEFORMAT = JSON\n",
    "    FORMAT_OPTIONS ('mergeSchema' = 'true')\n",
    "\"\"\")\n",
    "\n",
    "count_after_rerun = spark.table(target_table).count()\n",
    "print(f\"Rows after re-run: {count_after_rerun} (was {count_after_copy})\")\n",
    "print(f\"New rows loaded: {count_after_rerun - count_after_copy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140e77ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert count_after_rerun == count_after_copy, \"COPY INTO should be idempotent - no new rows!\"\n",
    "print(\"Task 2 OK: COPY INTO is idempotent. 0 new rows on re-run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ed5a6b",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3: Auto Loader - Configure Stream\n",
    "\n",
    "Set up Auto Loader (`cloudFiles`) to read JSON files from the landing zone.\n",
    "\n",
    "Key options:\n",
    "- `cloudFiles.format` = json\n",
    "- `cloudFiles.schemaLocation` = path for inferred schema\n",
    "- `cloudFiles.inferColumnTypes` = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df6640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset target for Auto Loader test\n",
    "al_target = f\"{CATALOG}.{BRONZE_SCHEMA}.orders_autoloader\"\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {al_target}\")\n",
    "dbutils.fs.rm(checkpoint_path, True)\n",
    "dbutils.fs.rm(schema_path, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eff1728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Configure Auto Loader readStream\n",
    "df_stream = (\n",
    "    spark.readStream\n",
    "    .format(________)\n",
    "    .option(\"cloudFiles.format\", ________)\n",
    "    .option(\"cloudFiles.schemaLocation\", schema_path)\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .load(landing_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5fdf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert df_stream.isStreaming, \"Should be a streaming DataFrame\"\n",
    "print(f\"Task 3 OK: Streaming DataFrame configured with schema: {df_stream.schema.fieldNames()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c7522d",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 4: Write Stream with trigger(availableNow=True)\n",
    "\n",
    "Write the stream to a Delta table using `trigger(availableNow=True)`.\n",
    "\n",
    "This processes all available files and stops automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5397a84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write stream to Delta table\n",
    "query = (\n",
    "    df_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .trigger(________=________)\n",
    "    .toTable(al_target)\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "print(f\"Stream completed. Rows loaded: {spark.table(al_target).count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66c4e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "al_count = spark.table(al_target).count()\n",
    "assert al_count > 0, \"Auto Loader should have loaded data\"\n",
    "print(f\"Task 4 OK: {al_count} rows loaded via Auto Loader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fc942a",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 5: Incremental Processing\n",
    "\n",
    "Re-run the stream. Since no new files arrived, 0 new rows should be processed.\n",
    "\n",
    "This proves the checkpoint tracks processed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c1910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run the stream (same checkpoint = incremental)\n",
    "df_stream2 = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", schema_path)\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .load(landing_path)\n",
    ")\n",
    "\n",
    "query2 = (\n",
    "    df_stream2\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(al_target)\n",
    ")\n",
    "\n",
    "query2.awaitTermination()\n",
    "al_count2 = spark.table(al_target).count()\n",
    "print(f\"Rows after re-run: {al_count2} (was {al_count})\")\n",
    "print(f\"New rows: {al_count2 - al_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c67741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert al_count2 == al_count, f\"Should be 0 new rows, but got {al_count2 - al_count}\"\n",
    "print(\"Task 5 OK: Incremental processing verified. 0 new rows on re-run (checkpoint works!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Task 6: Metadata Columns\n",
    "\n",
    "Dodaj kolumny metadanych do danych strumieniowych: `_processing_time` (timestamp przetworzenia) i `_source_file` (ścieżka pliku źródłowego z `_metadata`).\n",
    "\n",
    "Te kolumny są kluczowe w produkcyjnych pipeline'ach do debugowania i audytu.\n",
    "\n",
    "**TODO:** Uzupełnij `current_timestamp()` i `_metadata.file_path`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import current_timestamp, col, input_file_name\n",
    "\n",
    "# Target table for metadata-enriched stream\n",
    "metadata_target = f\"{catalog}.{schema}.orders_with_metadata\"\n",
    "metadata_checkpoint = f\"{base_path}/checkpoints/orders_metadata\"\n",
    "\n",
    "# TODO: Configure Auto Loader with metadata columns\n",
    "df_with_metadata = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{base_path}/schema_metadata\")\n",
    "    .load(source_path)\n",
    "    .withColumn(\"_processing_time\", ________)       # hint: current_timestamp()\n",
    "    .withColumn(\"_source_file\", ________)            # hint: col(\"_metadata.file_path\")\n",
    ")\n",
    "\n",
    "# Write enriched stream\n",
    "(\n",
    "    df_with_metadata\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", metadata_checkpoint)\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(metadata_target)\n",
    "    .awaitTermination()\n",
    ")\n",
    "\n",
    "print(f\"Written to: {metadata_target}\")\n",
    "spark.table(metadata_target).display()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# -- Validation --\n",
    "meta_cols = spark.table(metadata_target).columns\n",
    "assert \"_processing_time\" in meta_cols, \"Missing '_processing_time' — did you use current_timestamp()?\"\n",
    "assert \"_source_file\" in meta_cols, \"Missing '_source_file' — did you use _metadata.file_path?\"\n",
    "print(f\"Task 6 OK: Metadata columns added — {meta_cols}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Task 7: Schema Evolution — Rescued Data\n",
    "\n",
    "Skonfiguruj Auto Loader z częściowym schematem (tylko `order_id` + `customer_id`). Ustaw `schemaEvolutionMode` na `\"rescue\"`, aby dodatkowe kolumny trafiły do `_rescued_data`.\n",
    "\n",
    "**TODO:** Uzupełnij `schemaEvolutionMode`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "rescue_target = f\"{catalog}.{schema}.orders_rescued\"\n",
    "rescue_checkpoint = f\"{base_path}/checkpoints/orders_rescued\"\n",
    "\n",
    "# Partial schema — only 2 columns out of many\n",
    "partial_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "])\n",
    "\n",
    "# TODO: Configure Auto Loader with rescue mode\n",
    "df_rescued = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{base_path}/schema_rescued\")\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"________\")  # hint: \"rescue\"\n",
    "    .schema(partial_schema)\n",
    "    .load(source_path)\n",
    ")\n",
    "\n",
    "# Write with rescued data\n",
    "(\n",
    "    df_rescued\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", rescue_checkpoint)\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(rescue_target)\n",
    "    .awaitTermination()\n",
    ")\n",
    "\n",
    "print(f\"Written to: {rescue_target}\")\n",
    "spark.table(rescue_target).display()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# -- Validation --\n",
    "rescue_cols = spark.table(rescue_target).columns\n",
    "assert \"_rescued_data\" in rescue_cols, \"Missing '_rescued_data' column — did you set schemaEvolutionMode to 'rescue'?\"\n",
    "rescue_count = spark.table(rescue_target).filter(\"_rescued_data IS NOT NULL\").count()\n",
    "assert rescue_count > 0, \"Expected rescued data for columns not in partial schema\"\n",
    "print(f\"Task 7 OK: {rescue_count} rows with rescued data (extra columns captured in _rescued_data)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Task 8: Stream-Static Join\n",
    "\n",
    "Połącz dane strumieniowe (zamówienia) z **tabelą statyczną** (klienci), aby wzbogacić strumień o informacje o kliencie.\n",
    "\n",
    "**Stream-Static Join** pozwala łączyć streaming DataFrame z batch DataFrame — strona statyczna jest odczytywana na nowo w każdym micro-batchu.\n",
    "\n",
    "**TODO:** Uzupełnij `readStream`, kolumny joina i `writeStream`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# -- Stream-Static Join: Enrich orders with customer info --\n",
    "\n",
    "join_target = f\"{catalog}.{schema}.orders_enriched\"\n",
    "join_checkpoint = f\"{base_path}/checkpoints/orders_enriched\"\n",
    "\n",
    "# Static side: read customers table\n",
    "customers_df = spark.table(f\"{catalog}.{schema}.customers\")\n",
    "\n",
    "# Stream side: read orders as a stream\n",
    "# TODO: Fill in the format and table name\n",
    "orders_stream = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"________\")        # hint: \"delta\"\n",
    "    .table(________)           # hint: target_table (the table from Task 1)\n",
    ")\n",
    "\n",
    "# TODO: Join stream with static DataFrame on customer_id\n",
    "enriched_stream = orders_stream.join(\n",
    "    ________,                  # hint: customers_df\n",
    "    on=\"________\",             # hint: \"customer_id\"\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# TODO: Write the joined stream to join_target\n",
    "(\n",
    "    enriched_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"________\")          # hint: \"append\"\n",
    "    .option(\"checkpointLocation\", join_checkpoint)\n",
    "    .trigger(availableNow=________)  # hint: True\n",
    "    .toTable(________)               # hint: join_target\n",
    "    .awaitTermination()\n",
    ")\n",
    "\n",
    "print(f\"Stream-static join written to: {join_target}\")\n",
    "spark.table(join_target).display()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# -- Validation --\n",
    "enriched_count = spark.table(join_target).count()\n",
    "enriched_cols = spark.table(join_target).columns\n",
    "assert enriched_count > 0, \"No rows in enriched table\"\n",
    "assert \"customer_id\" in enriched_cols, \"Missing customer_id\"\n",
    "# Check that join added customer columns\n",
    "assert any(c for c in enriched_cols if c not in spark.table(target_table).columns), \\\n",
    "    \"Join didn't add any new columns — check join expression\"\n",
    "print(f\"Task 8 OK: {enriched_count} enriched rows. Columns: {enriched_cols}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7e97ecaa",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 6: Add Metadata Columns\n",
    "\n",
    "Enrich the stream with processing metadata:\n",
    "- `_processing_time` — timestamp of when the record was processed\n",
    "- `_source_file` — path of the source file (from `_metadata.file_path`)\n",
    "\n",
    "> **Exam Tip:** `_metadata` is a hidden column available in Auto Loader streams. It contains `file_path`, `file_name`, `file_size`, `file_modification_time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5025ee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop any active streams\n",
    "for s in spark.streams.active:\n",
    "    s.stop()\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {target_table}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {al_target}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {catalog}.{schema}.orders_with_metadata\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {catalog}.{schema}.orders_rescued\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {catalog}.{schema}.orders_enriched\")\n",
    "dbutils.fs.rm(checkpoint_path, True)\n",
    "dbutils.fs.rm(schema_path, True)\n",
    "dbutils.fs.rm(f\"{base_path}/checkpoints/orders_metadata\", True)\n",
    "dbutils.fs.rm(f\"{base_path}/checkpoints/orders_rescued\", True)\n",
    "dbutils.fs.rm(f\"{base_path}/checkpoints/orders_enriched\", True)\n",
    "print(\"Lab cleanup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d91e8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "meta_cols = spark.table(meta_target).columns\n",
    "assert \"_processing_time\" in meta_cols, \"Missing '_processing_time' column\"\n",
    "assert \"_source_file\" in meta_cols, \"Missing '_source_file' column\"\n",
    "print(f\"Task 6 OK: {spark.table(meta_target).count()} rows with metadata columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e601971",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 7: Schema Evolution — Rescued Data\n",
    "\n",
    "Configure Auto Loader with `schemaEvolutionMode = \"rescue\"` and a **partial schema** (only `order_id` and `customer_id`). All other columns should land in the `_rescued_data` column.\n",
    "\n",
    "> **Exam Tip:** `_rescued_data` captures new columns, type mismatches, and unexpected fields. Key options: `addNewColumns`, `rescue`, `failOnNewColumns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e59835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Reset\n",
    "rescue_target = f\"{CATALOG}.{BRONZE_SCHEMA}.orders_rescued\"\n",
    "rescue_checkpoint = f\"/tmp/{CATALOG}/lab05/checkpoint_rescue\"\n",
    "rescue_schema = f\"/tmp/{CATALOG}/lab05/schema_rescue\"\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {rescue_target}\")\n",
    "dbutils.fs.rm(rescue_checkpoint, True)\n",
    "dbutils.fs.rm(rescue_schema, True)\n",
    "\n",
    "# Define partial schema (intentionally incomplete)\n",
    "partial_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "])\n",
    "\n",
    "# TODO: Configure Auto Loader with rescue mode\n",
    "df_rescue = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", rescue_schema)\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", ________)  # \"rescue\"\n",
    "    .schema(partial_schema)\n",
    "    .load(landing_path)\n",
    ")\n",
    "\n",
    "query_rescue = (\n",
    "    df_rescue.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", rescue_checkpoint)\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(rescue_target)\n",
    ")\n",
    "query_rescue.awaitTermination()\n",
    "\n",
    "# Check the rescued data column\n",
    "display(spark.table(rescue_target).limit(5))\n",
    "spark.table(rescue_target).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f91a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "rescue_cols = spark.table(rescue_target).columns\n",
    "assert \"_rescued_data\" in rescue_cols, \"Missing '_rescued_data' column — did you set schemaEvolutionMode to 'rescue'?\"\n",
    "rescue_count = spark.table(rescue_target).filter(\"_rescued_data IS NOT NULL\").count()\n",
    "assert rescue_count > 0, \"Expected rescued data for columns not in partial schema\"\n",
    "print(f\"Task 7 OK: {rescue_count} rows with rescued data (extra columns captured in _rescued_data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa323e28",
   "metadata": {},
   "source": [
    "### Task 8: Stream-Static Join\n",
    "\n",
    "Połącz dane strumieniowe (zamówienia) z **tabelą statyczną** (klienci), aby wzbogacić strumień o informacje o kliencie.\n",
    "\n",
    "**Stream-Static Join** pozwala łączyć streaming DataFrame z batch DataFrame — strona statyczna jest odczytywana na nowo w każdym micro-batchu.\n",
    "\n",
    "**TODO:** Uzupełnij `readStream`, kolumny joina i `writeStream`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f0a523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Stream-Static Join: Enrich orders with customer info --\n",
    "\n",
    "join_target = f\"{catalog}.{schema}.orders_enriched\"\n",
    "join_checkpoint = f\"{base_path}/checkpoints/orders_enriched\"\n",
    "\n",
    "# Static side: read customers table\n",
    "customers_df = spark.table(f\"{catalog}.{schema}.customers\")\n",
    "\n",
    "# Stream side: read orders as a stream\n",
    "# TODO: Fill in the format and table name\n",
    "orders_stream = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"________\")        # hint: \"delta\"\n",
    "    .table(________)           # hint: target_table (the table from Task 1)\n",
    ")\n",
    "\n",
    "# TODO: Join stream with static DataFrame on customer_id\n",
    "enriched_stream = orders_stream.join(\n",
    "    ________,                  # hint: customers_df\n",
    "    on=\"________\",             # hint: \"customer_id\"\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# TODO: Write the joined stream to join_target\n",
    "(\n",
    "    enriched_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"________\")          # hint: \"append\"\n",
    "    .option(\"checkpointLocation\", join_checkpoint)\n",
    "    .trigger(availableNow=________)  # hint: True\n",
    "    .toTable(________)               # hint: join_target\n",
    "    .awaitTermination()\n",
    ")\n",
    "\n",
    "print(f\"Stream-static join written to: {join_target}\")\n",
    "spark.table(join_target).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909e1bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "enriched_count = spark.table(join_target).count()\n",
    "enriched_cols = spark.table(join_target).columns\n",
    "assert enriched_count > 0, \"No rows in enriched table\"\n",
    "assert \"customer_id\" in enriched_cols, \"Missing customer_id\"\n",
    "# Check that join added customer columns\n",
    "assert any(c for c in enriched_cols if c not in spark.table(target_table).columns), \\\n",
    "    \"Join didn't add any new columns — check join expression\"\n",
    "print(f\"Task 8 OK: {enriched_count} enriched rows. Columns: {enriched_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68360d55",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c420707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop any active streams\n",
    "for s in spark.streams.active:\n",
    "    s.stop()\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {target_table}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {al_target}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {catalog}.{schema}.orders_with_metadata\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {catalog}.{schema}.orders_rescued\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {catalog}.{schema}.orders_enriched\")\n",
    "dbutils.fs.rm(checkpoint_path, True)\n",
    "dbutils.fs.rm(schema_path, True)\n",
    "dbutils.fs.rm(f\"{base_path}/checkpoints/orders_metadata\", True)\n",
    "dbutils.fs.rm(f\"{base_path}/checkpoints/orders_rescued\", True)\n",
    "dbutils.fs.rm(f\"{base_path}/checkpoints/orders_enriched\", True)\n",
    "print(\"Lab cleanup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094389d9",
   "metadata": {},
   "source": [
    "---\n",
    "## Lab Complete!\n",
    "\n",
    "You have:\n",
    "- Used COPY INTO for idempotent batch loading\n",
    "- Configured Auto Loader (cloudFiles) for streaming ingestion\n",
    "- Used trigger(availableNow=True) for incremental processing\n",
    "- Verified checkpoint-based exactly-once guarantees\n",
    "- Added metadata columns (`_processing_time`, `_source_file`) to streaming writes\n",
    "- Used rescued data column for schema evolution handling\n",
    "- Performed a stream-static join to enrich streaming data\n",
    "\n",
    "> **Exam Tip:** Auto Loader uses `cloudFiles` format. COPY INTO is simpler but Auto Loader scales better (file notification mode for millions of files). Both are idempotent.\n",
    "\n",
    "| Feature | COPY INTO | Auto Loader |\n",
    "|---------|-----------|-------------|\n",
    "| Format | SQL command | readStream/writeStream |\n",
    "| Scalability | Thousands of files | Millions of files |\n",
    "| Schema evolution | Manual | Automatic (rescue column) |\n",
    "| File tracking | SQL state | Checkpoint directory |\n",
    "\n",
    "> **Next:** LAB 06 - Advanced Transforms "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}