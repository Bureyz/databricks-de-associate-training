{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8e7f0d1",
   "metadata": {},
   "source": [
    "# LAB 05: Streaming & Auto Loader\n",
    "\n",
    "**Duration:** ~35 min | **Day:** 2 | **Difficulty:** Intermediate-Advanced\n",
    "**After module:** M05: Incremental Data Processing\n",
    "\n",
    "> *\"Set up Auto Loader for streaming JSON ingestion into the Bronze layer with exactly-once guarantees.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fbfc80",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d95d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../setup/00_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0046ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare landing zone and checkpoint paths\n",
    "landing_path = f\"{DATASET_PATH}/orders/stream\"\n",
    "checkpoint_path = f\"/tmp/{CATALOG}/lab05/checkpoint\"\n",
    "schema_path = f\"/tmp/{CATALOG}/lab05/schema\"\n",
    "target_table = f\"{CATALOG}.{BRONZE_SCHEMA}.orders_stream\"\n",
    "\n",
    "# Clean up from previous runs\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {target_table}\")\n",
    "dbutils.fs.rm(checkpoint_path, True)\n",
    "dbutils.fs.rm(schema_path, True)\n",
    "\n",
    "print(f\"Landing path: {landing_path}\")\n",
    "print(f\"Target table: {target_table}\")\n",
    "print(f\"Files available: {[f.name for f in dbutils.fs.ls(landing_path)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da740c00",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1: COPY INTO (Batch Ingestion)\n",
    "\n",
    "Use `COPY INTO` to load the first file from the landing zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd9cd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create the target table\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {target_table}\n",
    "    (order_id STRING, customer_id STRING, product_id STRING, \n",
    "     quantity INT, total_price DOUBLE, order_date STRING, \n",
    "     payment_method STRING, store_id STRING)\n",
    "\"\"\")\n",
    "\n",
    "# TODO: Use COPY INTO to load JSON files\n",
    "spark.sql(f\"\"\"\n",
    "    COPY INTO {target_table}\n",
    "    FROM '{landing_path}'\n",
    "    FILEFORMAT = ________\n",
    "    FORMAT_OPTIONS ('mergeSchema' = 'true')\n",
    "\"\"\")\n",
    "\n",
    "count_after_copy = spark.table(target_table).count()\n",
    "print(f\"Rows after COPY INTO: {count_after_copy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd6b49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert count_after_copy > 0, \"COPY INTO should have loaded data\"\n",
    "print(f\"Task 1 OK: {count_after_copy} rows loaded via COPY INTO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a96f94d",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Verify COPY INTO Idempotency\n",
    "\n",
    "Run COPY INTO again on the same files. How many new rows are loaded?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666136ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Re-run the same COPY INTO\n",
    "spark.sql(f\"\"\"\n",
    "    COPY INTO {target_table}\n",
    "    FROM '{landing_path}'\n",
    "    FILEFORMAT = JSON\n",
    "    FORMAT_OPTIONS ('mergeSchema' = 'true')\n",
    "\"\"\")\n",
    "\n",
    "count_after_rerun = spark.table(target_table).count()\n",
    "print(f\"Rows after re-run: {count_after_rerun} (was {count_after_copy})\")\n",
    "print(f\"New rows loaded: {count_after_rerun - count_after_copy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140e77ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert count_after_rerun == count_after_copy, \"COPY INTO should be idempotent - no new rows!\"\n",
    "print(\"Task 2 OK: COPY INTO is idempotent. 0 new rows on re-run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ed5a6b",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3: Auto Loader - Configure Stream\n",
    "\n",
    "Set up Auto Loader (`cloudFiles`) to read JSON files from the landing zone.\n",
    "\n",
    "Key options:\n",
    "- `cloudFiles.format` = json\n",
    "- `cloudFiles.schemaLocation` = path for inferred schema\n",
    "- `cloudFiles.inferColumnTypes` = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df6640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset target for Auto Loader test\n",
    "al_target = f\"{CATALOG}.{BRONZE_SCHEMA}.orders_autoloader\"\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {al_target}\")\n",
    "dbutils.fs.rm(checkpoint_path, True)\n",
    "dbutils.fs.rm(schema_path, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eff1728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Configure Auto Loader readStream\n",
    "df_stream = (\n",
    "    spark.readStream\n",
    "    .format(________)\n",
    "    .option(\"cloudFiles.format\", ________)\n",
    "    .option(\"cloudFiles.schemaLocation\", schema_path)\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .load(landing_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5fdf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert df_stream.isStreaming, \"Should be a streaming DataFrame\"\n",
    "print(f\"Task 3 OK: Streaming DataFrame configured with schema: {df_stream.schema.fieldNames()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c7522d",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 4: Write Stream with trigger(availableNow=True)\n",
    "\n",
    "Write the stream to a Delta table using `trigger(availableNow=True)`.\n",
    "\n",
    "This processes all available files and stops automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5397a84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write stream to Delta table\n",
    "query = (\n",
    "    df_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .trigger(________=________)\n",
    "    .toTable(al_target)\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "print(f\"Stream completed. Rows loaded: {spark.table(al_target).count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66c4e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "al_count = spark.table(al_target).count()\n",
    "assert al_count > 0, \"Auto Loader should have loaded data\"\n",
    "print(f\"Task 4 OK: {al_count} rows loaded via Auto Loader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fc942a",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 5: Incremental Processing\n",
    "\n",
    "Re-run the stream. Since no new files arrived, 0 new rows should be processed.\n",
    "\n",
    "This proves the checkpoint tracks processed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c1910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run the stream (same checkpoint = incremental)\n",
    "df_stream2 = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", schema_path)\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .load(landing_path)\n",
    ")\n",
    "\n",
    "query2 = (\n",
    "    df_stream2\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(al_target)\n",
    ")\n",
    "\n",
    "query2.awaitTermination()\n",
    "al_count2 = spark.table(al_target).count()\n",
    "print(f\"Rows after re-run: {al_count2} (was {al_count})\")\n",
    "print(f\"New rows: {al_count2 - al_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c67741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert al_count2 == al_count, f\"Should be 0 new rows, but got {al_count2 - al_count}\"\n",
    "print(\"Task 5 OK: Incremental processing verified. 0 new rows on re-run (checkpoint works!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf170d5",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 6: Metadata Columns\n",
    "\n",
    "Add metadata columns to streaming data: `_processing_time` (processing timestamp) and `_source_file` (source file path from `_metadata`).\n",
    "\n",
    "These columns are essential in production pipelines for debugging and auditing.\n",
    "\n",
    "**TODO:** Fill in `current_timestamp()` and `_metadata.file_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169b0201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, col\n",
    "\n",
    "# Target table for metadata-enriched stream\n",
    "metadata_target = f\"{CATALOG}.{BRONZE_SCHEMA}.orders_with_metadata\"\n",
    "metadata_checkpoint = f\"/tmp/{CATALOG}/lab05/checkpoint_metadata\"\n",
    "metadata_schema = f\"/tmp/{CATALOG}/lab05/schema_metadata\"\n",
    "\n",
    "# Reset\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {metadata_target}\")\n",
    "dbutils.fs.rm(metadata_checkpoint, True)\n",
    "dbutils.fs.rm(metadata_schema, True)\n",
    "\n",
    "# TODO: Configure Auto Loader with metadata columns\n",
    "df_with_metadata = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", metadata_schema)\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .load(landing_path)\n",
    "    .withColumn(\"_processing_time\", ________)       # hint: current_timestamp()\n",
    "    .withColumn(\"_source_file\", ________)            # hint: col(\"_metadata.file_path\")\n",
    ")\n",
    "\n",
    "# Write enriched stream\n",
    "(\n",
    "    df_with_metadata\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", metadata_checkpoint)\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(metadata_target)\n",
    "    .awaitTermination()\n",
    ")\n",
    "\n",
    "print(f\"Written to: {metadata_target}\")\n",
    "display(spark.table(metadata_target).limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2de8d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "meta_cols = spark.table(metadata_target).columns\n",
    "assert \"_processing_time\" in meta_cols, \"Missing '_processing_time' — did you use current_timestamp()?\"\n",
    "assert \"_source_file\" in meta_cols, \"Missing '_source_file' — did you use _metadata.file_path?\"\n",
    "print(f\"Task 6 OK: Metadata columns added — {meta_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2003525",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 7: Schema Evolution — Rescued Data\n",
    "\n",
    "Configure Auto Loader with a partial schema (only `order_id` + `customer_id`). Set `schemaEvolutionMode` to `\"rescue\"` so that extra columns land in `_rescued_data`.\n",
    "\n",
    "**TODO:** Fill in the `schemaEvolutionMode` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d072ebcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "rescue_target = f\"{CATALOG}.{BRONZE_SCHEMA}.orders_rescued\"\n",
    "rescue_checkpoint = f\"/tmp/{CATALOG}/lab05/checkpoint_rescue\"\n",
    "rescue_schema = f\"/tmp/{CATALOG}/lab05/schema_rescue\"\n",
    "\n",
    "# Reset\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {rescue_target}\")\n",
    "dbutils.fs.rm(rescue_checkpoint, True)\n",
    "dbutils.fs.rm(rescue_schema, True)\n",
    "\n",
    "# Partial schema — only 2 columns out of many\n",
    "partial_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "])\n",
    "\n",
    "# TODO: Configure Auto Loader with rescue mode\n",
    "df_rescued = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", rescue_schema)\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"________\")  # hint: \"rescue\"\n",
    "    .schema(partial_schema)\n",
    "    .load(landing_path)\n",
    ")\n",
    "\n",
    "# Write with rescued data\n",
    "(\n",
    "    df_rescued\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", rescue_checkpoint)\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(rescue_target)\n",
    "    .awaitTermination()\n",
    ")\n",
    "\n",
    "print(f\"Written to: {rescue_target}\")\n",
    "display(spark.table(rescue_target).limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9dd363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "rescue_cols = spark.table(rescue_target).columns\n",
    "assert \"_rescued_data\" in rescue_cols, \"Missing '_rescued_data' column — did you set schemaEvolutionMode to 'rescue'?\"\n",
    "rescue_count = spark.table(rescue_target).filter(\"_rescued_data IS NOT NULL\").count()\n",
    "assert rescue_count > 0, \"Expected rescued data for columns not in partial schema\"\n",
    "print(f\"Task 7 OK: {rescue_count} rows with rescued data (extra columns captured in _rescued_data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9e2929",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 8: Stream-Static Join\n",
    "\n",
    "Join streaming data (orders) with a **static table** (customers) to enrich the stream with customer information.\n",
    "\n",
    "A **Stream-Static Join** lets you combine a streaming DataFrame with a batch DataFrame — the static side is re-read on every micro-batch.\n",
    "\n",
    "**TODO:** Fill in `readStream`, join columns, and `writeStream`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d3d844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Stream-Static Join: Enrich orders with customer info --\n",
    "\n",
    "join_target = f\"{CATALOG}.{BRONZE_SCHEMA}.orders_enriched\"\n",
    "join_checkpoint = f\"/tmp/{CATALOG}/lab05/checkpoint_enriched\"\n",
    "\n",
    "# Reset\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {join_target}\")\n",
    "dbutils.fs.rm(join_checkpoint, True)\n",
    "\n",
    "# Static side: read customers table\n",
    "customers_df = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers\")\n",
    "\n",
    "# Stream side: read orders as a stream\n",
    "# TODO: Fill in the format and table name\n",
    "orders_stream = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"________\")        # hint: \"delta\"\n",
    "    .table(________)           # hint: target_table (the table from Task 1)\n",
    ")\n",
    "\n",
    "# TODO: Join stream with static DataFrame on customer_id\n",
    "enriched_stream = orders_stream.join(\n",
    "    ________,                  # hint: customers_df\n",
    "    on=\"________\",             # hint: \"customer_id\"\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# TODO: Write the joined stream to join_target\n",
    "(\n",
    "    enriched_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"________\")          # hint: \"append\"\n",
    "    .option(\"checkpointLocation\", join_checkpoint)\n",
    "    .trigger(availableNow=________)  # hint: True\n",
    "    .toTable(________)               # hint: join_target\n",
    "    .awaitTermination()\n",
    ")\n",
    "\n",
    "print(f\"Stream-static join written to: {join_target}\")\n",
    "display(spark.table(join_target).limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b3ce00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "enriched_count = spark.table(join_target).count()\n",
    "enriched_cols = spark.table(join_target).columns\n",
    "assert enriched_count > 0, \"No rows in enriched table\"\n",
    "assert \"customer_id\" in enriched_cols, \"Missing customer_id\"\n",
    "assert any(c for c in enriched_cols if c not in spark.table(target_table).columns), \\\n",
    "    \"Join didn't add any new columns — check join expression\"\n",
    "print(f\"Task 8 OK: {enriched_count} enriched rows. Columns: {enriched_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77307a6f",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 9: Change Data Feed (CDF) for Incremental ETL\n",
    "\n",
    "Use **Change Data Feed** to read only the changes made to a Delta table, instead of re-reading the entire table each time.\n",
    "\n",
    "**Scenario:** \n",
    "1. Create a CDF-enabled table with initial data\n",
    "2. Make some changes (INSERT + UPDATE)\n",
    "3. Read only the changes using `readChangeFeed`\n",
    "4. Build an incremental ETL pipeline from the change feed\n",
    "\n",
    "> **Exam Tip:** CDF captures `_change_type` (`insert`, `update_preimage`, `update_postimage`, `delete`), `_commit_version`, and `_commit_timestamp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ce39f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a CDF-enabled table with initial data\n",
    "cdf_source = f\"{CATALOG}.{BRONZE_SCHEMA}.cdf_orders_lab\"\n",
    "cdf_target = f\"{CATALOG}.{BRONZE_SCHEMA}.cdf_orders_incremental\"\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {cdf_source}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {cdf_target}\")\n",
    "\n",
    "# TODO: Create table with Change Data Feed enabled\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE {cdf_source} (\n",
    "        order_id INT, customer_id INT, amount DOUBLE, status STRING\n",
    "    )\n",
    "    TBLPROPERTIES (________ = ________)\n",
    "\"\"\")\n",
    "# hint: 'delta.enableChangeDataFeed' = 'true'\n",
    "\n",
    "# Insert initial data\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO {cdf_source} VALUES\n",
    "    (1, 101, 99.99, 'pending'),\n",
    "    (2, 102, 149.50, 'pending'),\n",
    "    (3, 103, 75.00, 'pending'),\n",
    "    (4, 101, 200.00, 'pending'),\n",
    "    (5, 104, 50.00, 'pending')\n",
    "\"\"\")\n",
    "\n",
    "initial_version = spark.sql(f\"DESCRIBE HISTORY {cdf_source}\").first()[\"version\"]\n",
    "print(f\"CDF table created at version {initial_version} with {spark.table(cdf_source).count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3e17ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Make some changes (simulate business operations)\n",
    "# Update: mark orders 1,2 as shipped\n",
    "spark.sql(f\"UPDATE {cdf_source} SET status = 'shipped' WHERE order_id IN (1, 2)\")\n",
    "\n",
    "# Insert: new order\n",
    "spark.sql(f\"INSERT INTO {cdf_source} VALUES (6, 105, 320.00, 'pending')\")\n",
    "\n",
    "print(f\"Changes applied: 2 updates + 1 insert\")\n",
    "print(f\"Current table has {spark.table(cdf_source).count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c53a8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Read the Change Data Feed starting from the version AFTER initial load\n",
    "# Use table_changes() SQL function\n",
    "\n",
    "df_changes = spark.sql(f\"\"\"\n",
    "    SELECT * FROM ________('{cdf_source}', {initial_version + 1})\n",
    "    ORDER BY _commit_version, order_id\n",
    "\"\"\")\n",
    "# hint: table_changes(...)\n",
    "\n",
    "display(df_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573e1591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build incremental ETL — read only inserts and update_postimage (new values)\n",
    "# Filter out update_preimage and delete to get \"current state of changes\"\n",
    "\n",
    "df_incremental = df_changes.filter(\n",
    "    col(\"_change_type\").isin(________, ________)\n",
    ")\n",
    "# hint: \"insert\", \"update_postimage\"\n",
    "\n",
    "# Write incremental changes to target\n",
    "df_incremental.drop(\"_change_type\", \"_commit_version\", \"_commit_timestamp\").write \\\n",
    "    .mode(\"append\").saveAsTable(cdf_target)\n",
    "\n",
    "print(f\"Incremental ETL: {df_incremental.count()} rows written to {cdf_target}\")\n",
    "display(spark.table(cdf_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0cafd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "cdf_count = spark.table(cdf_target).count()\n",
    "assert cdf_count == 3, f\"Expected 3 rows (2 updated + 1 inserted), got {cdf_count}\"\n",
    "statuses = [r[\"status\"] for r in spark.table(cdf_target).collect()]\n",
    "assert \"shipped\" in statuses, \"Should contain 'shipped' status from updates\"\n",
    "assert \"pending\" in statuses, \"Should contain 'pending' status from new insert\"\n",
    "print(f\"Task 9 OK: CDF incremental ETL — {cdf_count} change rows captured\")\n",
    "print(\"  2 update_postimage (shipped) + 1 insert (new order)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68360d55",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c420707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop any active streams\n",
    "for s in spark.streams.active:\n",
    "    s.stop()\n",
    "\n",
    "# Drop lab tables\n",
    "for t in [target_table, al_target, \n",
    "          f\"{CATALOG}.{BRONZE_SCHEMA}.orders_with_metadata\",\n",
    "          f\"{CATALOG}.{BRONZE_SCHEMA}.orders_rescued\",\n",
    "          f\"{CATALOG}.{BRONZE_SCHEMA}.orders_enriched\",\n",
    "          f\"{CATALOG}.{BRONZE_SCHEMA}.cdf_orders_lab\",\n",
    "          f\"{CATALOG}.{BRONZE_SCHEMA}.cdf_orders_incremental\"]:\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {t}\")\n",
    "\n",
    "# Clean temp paths\n",
    "for p in [checkpoint_path, schema_path,\n",
    "          f\"/tmp/{CATALOG}/lab05/checkpoint_metadata\",\n",
    "          f\"/tmp/{CATALOG}/lab05/schema_metadata\",\n",
    "          f\"/tmp/{CATALOG}/lab05/checkpoint_rescue\",\n",
    "          f\"/tmp/{CATALOG}/lab05/schema_rescue\",\n",
    "          f\"/tmp/{CATALOG}/lab05/checkpoint_enriched\"]:\n",
    "    dbutils.fs.rm(p, True)\n",
    "\n",
    "print(\"Lab cleanup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094389d9",
   "metadata": {},
   "source": [
    "---\n",
    "## Lab Complete!\n",
    "\n",
    "You have:\n",
    "- Used COPY INTO for idempotent batch loading\n",
    "- Configured Auto Loader (cloudFiles) for streaming ingestion\n",
    "- Used trigger(availableNow=True) for incremental processing\n",
    "- Verified checkpoint-based exactly-once guarantees\n",
    "- Added metadata columns (`_processing_time`, `_source_file`) to streaming writes\n",
    "- Used rescued data column for schema evolution handling\n",
    "- Performed a stream-static join to enrich streaming data\n",
    "- Used Change Data Feed (CDF) for incremental ETL\n",
    "\n",
    "| Feature | COPY INTO | Auto Loader | CDF |\n",
    "|---------|-----------|-------------|-----|\n",
    "| Format | SQL command | readStream/writeStream | readChangeFeed / table_changes() |\n",
    "| Scalability | Thousands of files | Millions of files | Any Delta table |\n",
    "| Schema evolution | Manual | Automatic (rescue column) | Follows source schema |\n",
    "| File tracking | SQL state | Checkpoint directory | Version-based |\n",
    "| Use case | Simple batch | File-based streaming | Change-based incremental |\n",
    "\n",
    "> **Exam Tip:** Auto Loader uses `cloudFiles` format. COPY INTO is simpler but Auto Loader scales better (file notification mode for millions of files). CDF captures row-level changes and is ideal for propagating updates through Medallion layers.\n",
    "\n",
    "> **Next:** LAB 06 - Advanced Transforms"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
