{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41147a89",
   "metadata": {},
   "source": [
    "# LAB 06: Advanced Transforms -- PySpark & SQL\n",
    "\n",
    "**Duration:** ~40 min | **Day:** 2 | **Difficulty:** Intermediate-Advanced\n",
    "**After module:** M06: Advanced Transforms\n",
    "\n",
    "> *\"Build analytical reports using window functions, CTEs, explode, and CTAS.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89c90b0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e696fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../setup/00_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933a7d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum, count, desc, row_number, rank, dense_rank, lag, lead, explode, from_json\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Load base data\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{GOLD_SCHEMA}\")\n",
    "\n",
    "df_orders = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.orders\")\n",
    "df_customers = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers\")\n",
    "df_products = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.products\")\n",
    "\n",
    "# Register as temp views for SQL tasks\n",
    "df_orders.createOrReplaceTempView(\"orders\")\n",
    "df_customers.createOrReplaceTempView(\"customers\")\n",
    "df_products.createOrReplaceTempView(\"products\")\n",
    "\n",
    "print(f\"Data loaded: {df_orders.count()} orders, {df_customers.count()} customers, {df_products.count()} products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b9f86d",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1: Window Function -- Rank Products by Revenue (PySpark)\n",
    "\n",
    "For each product, calculate total revenue. Then rank products using `row_number()`.\n",
    "\n",
    "Hint: Use `Window.orderBy(desc(\"total_revenue\"))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddedea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate total revenue per product and rank them\n",
    "df_product_revenue = (\n",
    "    df_orders\n",
    "    .groupBy(\"product_id\")\n",
    "    .agg(sum(\"total_price\").alias(\"total_revenue\"))\n",
    ")\n",
    "\n",
    "window_spec = Window.orderBy(________(\"total_revenue\"))\n",
    "\n",
    "df_ranked = (\n",
    "    df_product_revenue\n",
    "    .withColumn(\"rank\", ________(________))\n",
    ")\n",
    "\n",
    "display(df_ranked.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cef489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert \"rank\" in df_ranked.columns, \"Missing 'rank' column\"\n",
    "first = df_ranked.orderBy(\"rank\").first()\n",
    "assert first[\"rank\"] == 1, \"First row should have rank 1\"\n",
    "print(f\"Task 1 OK: Top product has revenue {first['total_revenue']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625942a1",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Running Total (SQL)\n",
    "\n",
    "Write a SQL query to compute a cumulative running total per customer ordered by order_date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae601329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the SQL window function\n",
    "df_running = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        order_date,\n",
    "        total_price,\n",
    "        SUM(total_price) OVER (\n",
    "            PARTITION BY ________\n",
    "            ORDER BY ________\n",
    "            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
    "        ) AS running_total\n",
    "    FROM orders\n",
    "    ORDER BY customer_id, order_date\n",
    "\"\"\")\n",
    "\n",
    "display(df_running.limit(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d048c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert \"running_total\" in df_running.columns, \"Missing 'running_total' column\"\n",
    "print(f\"Task 2 OK: Running totals computed for {df_running.select('customer_id').distinct().count()} customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b395b57",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3: Multi-step CTE\n",
    "\n",
    "Write a SQL query with two CTEs to find the top 5 days by total revenue:\n",
    "1. `daily_sales` -- total revenue per day\n",
    "2. `ranked_days` -- rank days by revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fa7e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the CTE query\n",
    "df_top_days = spark.sql(\"\"\"\n",
    "    WITH daily_sales AS (\n",
    "        SELECT \n",
    "            order_date,\n",
    "            ________(total_price) AS daily_revenue,\n",
    "            ________(*)          AS order_count\n",
    "        FROM orders\n",
    "        GROUP BY order_date\n",
    "    ),\n",
    "    ranked_days AS (\n",
    "        SELECT *,\n",
    "            ROW_NUMBER() OVER (ORDER BY daily_revenue ________) AS day_rank\n",
    "        FROM daily_sales\n",
    "    )\n",
    "    SELECT * FROM ranked_days\n",
    "    WHERE day_rank <= 5\n",
    "    ORDER BY day_rank\n",
    "\"\"\")\n",
    "\n",
    "display(df_top_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fcdb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert df_top_days.count() <= 5, \"Should return at most 5 rows\"\n",
    "assert df_top_days.first()[\"day_rank\"] == 1, \"First row should be rank 1\"\n",
    "print(f\"Task 3 OK: Top {df_top_days.count()} days by revenue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1cb829",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 4: Correlated Subquery\n",
    "\n",
    "Find customers whose total spending is above the overall average spending per customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c685d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write SQL with subquery\n",
    "df_high_spenders = spark.sql(\"\"\"\n",
    "    SELECT customer_id, SUM(total_price) AS total_spent\n",
    "    FROM orders\n",
    "    GROUP BY customer_id\n",
    "    HAVING SUM(total_price) > (\n",
    "        SELECT ________(total_spent) FROM (\n",
    "            SELECT customer_id, SUM(total_price) AS total_spent\n",
    "            FROM orders\n",
    "            GROUP BY customer_id\n",
    "        )\n",
    "    )\n",
    "    ORDER BY total_spent DESC\n",
    "\"\"\")\n",
    "\n",
    "display(df_high_spenders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f13e850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert df_high_spenders.count() > 0, \"Should find at least some high spenders\"\n",
    "print(f\"Task 4 OK: {df_high_spenders.count()} customers above average spending\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b582a90c",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 5: Explode Array Column\n",
    "\n",
    "Create a sample DataFrame with an array column and use `explode()` to flatten it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e002906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array, lit, explode\n",
    "\n",
    "# Sample data with array column\n",
    "df_with_array = spark.createDataFrame([\n",
    "    (1, [\"Electronics\", \"Books\", \"Clothing\"]),\n",
    "    (2, [\"Food\", \"Electronics\"]),\n",
    "    (3, [\"Books\"])\n",
    "], [\"customer_id\", \"categories\"])\n",
    "\n",
    "# TODO: Explode the categories array into individual rows\n",
    "df_exploded = df_with_array.select(\n",
    "    \"customer_id\",\n",
    "    ________(col(\"categories\")).alias(\"category\")\n",
    ")\n",
    "\n",
    "display(df_exploded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c314787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert df_exploded.count() == 6, f\"Expected 6 rows after explode, got {df_exploded.count()}\"\n",
    "assert \"category\" in df_exploded.columns, \"Missing 'category' column\"\n",
    "print(f\"Task 5 OK: Exploded {df_with_array.count()} rows into {df_exploded.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0a735e",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 6: CTAS -- Create Gold Tables\n",
    "\n",
    "Use `CREATE TABLE AS SELECT` to persist the top products analysis as a Gold table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18515769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create gold table using CTAS\n",
    "gold_table = f\"{CATALOG}.{GOLD_SCHEMA}.top_products\"\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {gold_table}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    ________ {gold_table} AS\n",
    "    SELECT \n",
    "        product_id,\n",
    "        SUM(total_price) AS total_revenue,\n",
    "        COUNT(*) AS order_count\n",
    "    FROM {CATALOG}.{BRONZE_SCHEMA}.orders\n",
    "    GROUP BY product_id\n",
    "    ORDER BY total_revenue DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "display(spark.table(gold_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597e7166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "gold_count = spark.table(gold_table).count()\n",
    "assert gold_count > 0 and gold_count <= 10, f\"Expected 1-10 rows, got {gold_count}\"\n",
    "detail = spark.sql(f\"DESCRIBE DETAIL {gold_table}\").first()\n",
    "assert detail[\"format\"] == \"delta\", \"CTAS should create a Delta table\"\n",
    "print(f\"Task 6 OK: Gold table '{gold_table}' created with {gold_count} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7ea30b",
   "metadata": {},
   "source": [
    "---\n",
    "### Task 7: JSON Processing -- `from_json` & `schema_of_json`\n",
    "\n",
    "In practice, data often arrives as a STRING column containing JSON. Use `from_json()` with an explicit schema to parse JSON into a struct and extract fields.\n",
    "\n",
    "**TODO:** Define the schema, parse JSON, and extract fields from the struct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2a5469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col, schema_of_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Sample data with JSON string column\n",
    "json_data = [\n",
    "    (1, '{\"product\": \"Laptop\", \"qty\": 2, \"price\": 999.99}'),\n",
    "    (2, '{\"product\": \"Mouse\", \"qty\": 5, \"price\": 29.99}'),\n",
    "    (3, '{\"product\": \"Monitor\", \"qty\": 1, \"price\": 449.00}'),\n",
    "]\n",
    "df_json = spark.createDataFrame(json_data, [\"order_id\", \"order_details\"])\n",
    "\n",
    "# TODO: Define the schema for the JSON string\n",
    "json_schema = StructType([\n",
    "    StructField(\"product\", ________(), True),       # hint: StringType\n",
    "    StructField(\"qty\", ________(), True),            # hint: IntegerType\n",
    "    StructField(\"price\", ________(), True),          # hint: DoubleType\n",
    "])\n",
    "\n",
    "# TODO: Parse JSON string into a struct column using from_json\n",
    "df_parsed = df_json.withColumn(\n",
    "    \"parsed\",\n",
    "    ________(col(\"order_details\"), json_schema)      # hint: from_json\n",
    ")\n",
    "\n",
    "# TODO: Extract individual fields from the struct\n",
    "df_extracted = df_parsed.select(\n",
    "    \"order_id\",\n",
    "    col(\"parsed.________\").alias(\"product_name\"),    # hint: product\n",
    "    col(\"parsed.________\").alias(\"quantity\"),         # hint: qty\n",
    "    col(\"parsed.________\").alias(\"unit_price\"),      # hint: price\n",
    "    (col(\"parsed.qty\") * col(\"parsed.price\")).alias(\"line_total\")\n",
    ")\n",
    "\n",
    "display(df_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddda37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert \"product_name\" in df_extracted.columns, \"Missing 'product_name' — check col('parsed.product')\"\n",
    "assert \"line_total\" in df_extracted.columns, \"Missing 'line_total' column\"\n",
    "assert df_extracted.count() == 3, f\"Expected 3 rows, got {df_extracted.count()}\"\n",
    "first = df_extracted.filter(\"order_id = 1\").first()\n",
    "assert first[\"product_name\"] == \"Laptop\", f\"Expected 'Laptop', got '{first['product_name']}'\"\n",
    "assert abs(first[\"line_total\"] - 1999.98) < 0.01, \"line_total for Laptop should be ~1999.98\"\n",
    "print(f\"Task 7 OK: JSON parsed and extracted — {df_extracted.count()} rows with {len(df_extracted.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14bdb4b",
   "metadata": {},
   "source": [
    "---\n",
    "### Task 8: CASE WHEN -- Customer Segmentation (SQL)\n",
    "\n",
    "Use `CASE WHEN` to segment customers based on their total spending:\n",
    "- **VIP**: > 500\n",
    "- **Regular**: 100–500\n",
    "- **Low**: < 100\n",
    "\n",
    "**TODO:** Fill in the CASE WHEN conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996f8d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the CASE WHEN to categorize customers\n",
    "df_segments = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        SUM(total_price) AS total_spent,\n",
    "        CASE\n",
    "            WHEN SUM(total_price) > ________ THEN 'VIP'\n",
    "            WHEN SUM(total_price) >= ________ THEN 'Regular'\n",
    "            ELSE '________'\n",
    "        END AS segment\n",
    "    FROM orders\n",
    "    GROUP BY customer_id\n",
    "    ORDER BY total_spent DESC\n",
    "\"\"\")\n",
    "\n",
    "display(df_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628265e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert \"segment\" in df_segments.columns, \"Missing 'segment' column\"\n",
    "segments = [r[\"segment\"] for r in df_segments.select(\"segment\").distinct().collect()]\n",
    "assert set(segments).issubset({\"VIP\", \"Regular\", \"Low\"}), f\"Unexpected segments: {segments}\"\n",
    "seg_counts = {r[\"segment\"]: r[\"count\"] for r in df_segments.groupBy(\"segment\").count().collect()}\n",
    "print(f\"Task 8 OK: Customer segments — {seg_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8753e684",
   "metadata": {},
   "source": [
    "---\n",
    "### Task 9: Higher-Order Functions -- `transform` & `filter`\n",
    "\n",
    "Higher-Order Functions let you operate on array elements without needing `explode()`.\n",
    "\n",
    "- `transform(arr, x -> expr)` — transforms each element\n",
    "- `filter(arr, x -> condition)` — filters elements matching the condition\n",
    "\n",
    "**TODO:** Use `transform` to convert prices and `filter` to find expensive products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b785d821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array, lit, expr\n",
    "\n",
    "# Sample data with price arrays (e.g. line items per order)\n",
    "df_prices = spark.createDataFrame([\n",
    "    (1, [29.99, 149.99, 9.99, 599.00]),\n",
    "    (2, [49.99, 19.99]),\n",
    "    (3, [999.99, 249.99, 79.99, 15.00, 450.00]),\n",
    "], [\"order_id\", \"item_prices\"])\n",
    "\n",
    "# TODO: Use transform to add 23% VAT to each price\n",
    "# hint: transform(item_prices, x -> x * 1.23)\n",
    "df_with_vat = df_prices.selectExpr(\n",
    "    \"order_id\",\n",
    "    \"item_prices\",\n",
    "    \"________( item_prices, x -> ROUND(x * 1.23, 2) ) AS prices_with_vat\"\n",
    "    # hint: transform\n",
    ")\n",
    "\n",
    "# TODO: Use filter to keep only items > 100\n",
    "# hint: filter(prices_with_vat, x -> x > 100)\n",
    "df_expensive = df_with_vat.selectExpr(\n",
    "    \"order_id\",\n",
    "    \"prices_with_vat\",\n",
    "    \"________( prices_with_vat, x -> x > 100 ) AS expensive_items\"\n",
    "    # hint: filter\n",
    ")\n",
    "\n",
    "display(df_expensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b2309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert \"prices_with_vat\" in df_expensive.columns, \"Missing 'prices_with_vat' column\"\n",
    "assert \"expensive_items\" in df_expensive.columns, \"Missing 'expensive_items' column\"\n",
    "# Verify VAT calculation: 29.99 * 1.23 = 36.89 (not > 100, so filtered out)\n",
    "row1 = df_expensive.filter(\"order_id = 1\").first()\n",
    "vat_prices = row1[\"prices_with_vat\"]\n",
    "expensive = row1[\"expensive_items\"]\n",
    "assert len(vat_prices) == 4, f\"Order 1 should have 4 VAT prices, got {len(vat_prices)}\"\n",
    "assert all(p > 100 for p in expensive), \"All expensive items should be > 100\"\n",
    "print(f\"Task 9 OK: transform applied VAT to {len(vat_prices)} items, filter kept {len(expensive)} expensive items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2e9c0d",
   "metadata": {},
   "source": [
    "---\n",
    "## Lab Complete!\n",
    "\n",
    "You have:\n",
    "- Applied window functions (row_number, running SUM) in PySpark and SQL\n",
    "- Written multi-step CTEs for complex analytics\n",
    "- Used subqueries to filter by aggregate conditions\n",
    "- Flattened arrays with explode()\n",
    "- Created Gold tables using CTAS\n",
    "- Parsed JSON strings with `from_json` and extracted struct fields\n",
    "- Used `CASE WHEN` for customer segmentation\n",
    "- Applied higher-order functions (`transform`, `filter`) on arrays\n",
    "\n",
    "> **Exam Tip:** Know the difference: `ROW_NUMBER()` always gives unique sequential numbers. `RANK()` gives the same number for ties (with gaps). `DENSE_RANK()` gives same number for ties (no gaps).\n",
    "\n",
    "> **Next:** LAB 07 - Build a Medallion Pipeline in Lakeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional cleanup\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{GOLD_SCHEMA}.top_products\")\n",
    "print(\"LAB 06 complete.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
