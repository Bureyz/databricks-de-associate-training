{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "657047e2-fd5c-4e60-9173-07d93b535bad",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "source": [
    "# M01: Platform & Workspace\n",
    "\n",
    "We explore the Databricks Lakehouse platform — architecture, compute types, Unity Catalog, and the notebook workspace. Understanding these concepts is foundational for the exam (24% of questions). After this module, you'll navigate the workspace confidently, create clusters, and manage Unity Catalog objects.\n",
    "\n",
    "| Exam Domain | Weight |\n",
    "|---|---|\n",
    "| Databricks Lakehouse Platform | **24%** |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56333524-0ce0-4ecc-9fd7-22ea9d849be1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 2"
    }
   },
   "source": [
    "## Lakehouse Architecture\n",
    "\n",
    "The Lakehouse architecture combines the benefits of Data Lakes and Data Warehouses. Understanding this evolution is key for the exam (24% of questions cover the platform).\n",
    "\n",
    "---\n",
    "\n",
    "### The Evolution of Data Architectures\n",
    "\n",
    "**Generation 1: Data Warehouse (1990s-2000s)**\n",
    "- Teradata, Oracle, SQL Server\n",
    "- Structured data only, expensive storage\n",
    "- Great for BI, terrible for ML/unstructured data\n",
    "\n",
    "**Generation 2: Data Lake (2010s)**\n",
    "- Hadoop, S3, ADLS\n",
    "- Cheap storage, any format\n",
    "- Problem: \"Data Swamp\" - no governance, no ACID, unreliable\n",
    "\n",
    "**Generation 3: Lakehouse (2020s)**\n",
    "- Delta Lake, Iceberg, Hudi\n",
    "- Best of both: cheap storage + ACID + governance\n",
    "- Single platform for BI, ML, streaming\n",
    "\n",
    "<img src=\"../../../assets/images/4c9090bd82f2475c810bafde13f978e0.png\" width=\"800\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Cost Comparison (Rough Estimates)\n",
    "\n",
    "| Component | Traditional (DW + Lake) | Lakehouse |\n",
    "|-----------|------------------------|-----------|\n",
    "| Storage | $$$$ (2x for Lake + DW) | $$ (single copy) |\n",
    "| ETL Compute | $$$ (sync jobs) | $$ (no sync needed) |\n",
    "| Governance Tools | $$$ (separate tools) | $ (built-in) |\n",
    "| Latency | Hours (ETL sync) | Minutes (direct access) |\n",
    "| **Total TCO** | **Higher** | **30-50% lower** |\n",
    "\n",
    "*Note: Actual costs depend on workload. Run POC with your data to validate.*\n",
    "\n",
    "### Alternatives to Databricks Lakehouse\n",
    "\n",
    "| Alternative | Pros | Cons | When to Choose |\n",
    "|-------------|------|------|----------------|\n",
    "| **Snowflake** | Mature, great SQL | Separate from ML, vendor lock-in | Pure SQL/BI workloads |\n",
    "| **BigQuery** | Serverless, cheap storage | GCP-only, less flexible | GCP shop, ad-hoc analytics |\n",
    "| **Spark + Iceberg on K8s** | Open source, no vendor | Complex ops, no unified governance | Strong DevOps team, cost-sensitive |\n",
    "| **Databricks** | Unified platform, strong ML/AI | Premium pricing | ML + Analytics + Streaming |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98079aa4-ce1d-47bc-86fc-f345e3e85392",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### How Apache Spark Works — Distributed Execution & Lazy Evaluation\n",
    "\n",
    "Understanding Spark's execution model is essential before working with Databricks. Every query you run — whether PySpark or SQL — follows the same principles.\n",
    "\n",
    "---\n",
    "\n",
    "#### Driver & Executors Architecture\n",
    "\n",
    "<img src=\"../../../assets/images/d82b6da777ca4f3a9eee333717287c15.png\" width=\"800\">\n",
    "\n",
    "- **Driver** — single process that coordinates the entire job. Runs on the master node. Holds the `SparkSession`, builds the query plan, schedules tasks.\n",
    "- **Executors** — worker JVMs running on cluster nodes. Each executor processes multiple **partitions** in parallel.\n",
    "- **Partition** — a chunk of data (default ~128 MB). Spark processes all partitions in parallel across executors.\n",
    "\n",
    "> **Exam Tip:** Spark does NOT move data to the computation. It moves small task code to where the data resides (data locality). This is a fundamental design principle.\n",
    "\n",
    "---\n",
    "\n",
    "#### Lazy Evaluation — Nothing Happens Until You Ask for Results\n",
    "\n",
    "Spark uses **lazy evaluation**: transformations (e.g., `filter`, `select`, `groupBy`) are NOT executed immediately. They are recorded as a logical plan. Execution only starts when an **action** is called.\n",
    "\n",
    "| Type | Examples | What Happens |\n",
    "|------|----------|-------------|\n",
    "| **Transformation** (lazy) | `filter()`, `select()`, `groupBy()`, `join()`, `withColumn()` | Added to the logical plan, nothing computed |\n",
    "| **Action** (triggers execution) | `count()`, `show()`, `collect()`, `write()`, `display()`, `save()` | Triggers the entire pipeline execution |\n",
    "\n",
    "**Why lazy evaluation?** It allows Spark to **optimize the entire pipeline** before executing it. The Catalyst Optimizer can:\n",
    "- Reorder operations for efficiency (e.g., push filters before joins)\n",
    "- Combine multiple transformations into a single pass over the data\n",
    "- Prune unnecessary columns early (column pruning)\n",
    "- Choose optimal join strategies (broadcast vs. shuffle)\n",
    "\n",
    "---\n",
    "\n",
    "#### Execution Flow: From Code to Results\n",
    "\n",
    "<img src=\"../../../assets/images/32aef39a4c8c47b0bd849732025ab43a.png\" width=\"800\">\n",
    "\n",
    "---\n",
    "\n",
    "#### Stages and Shuffles\n",
    "\n",
    "Spark divides a job into **stages**. A stage boundary occurs when data needs to be **shuffled** (redistributed across the cluster):\n",
    "\n",
    "| Operation | Shuffle? | Explanation |\n",
    "|-----------|----------|-------------|\n",
    "| `filter()`, `select()` | No (narrow) | Each partition processed independently |\n",
    "| `groupBy().agg()` | Yes (wide) | Data must be grouped by key across partitions |\n",
    "| `join()` | Yes (wide)* | Data with same key must be on same executor |\n",
    "| `repartition()` | Yes | Explicitly redistributes data |\n",
    "| `coalesce()` | No | Reduces partitions without full shuffle |\n",
    "\n",
    "*Exception: broadcast join avoids shuffle by sending the small table to all executors.\n",
    "\n",
    "> **Exam Tip:** Shuffles are the most expensive operation in Spark. Minimizing shuffles (e.g., using broadcast joins for small tables, pre-partitioning data) is key to performance. In Databricks, Adaptive Query Execution (AQE) automatically optimizes many shuffle scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Exam Concepts Recap\n",
    "\n",
    "| Concept | Definition |\n",
    "|---------|-----------|\n",
    "| **Lazy evaluation** | Transformations are recorded but not executed until an action is called |\n",
    "| **DAG (Directed Acyclic Graph)** | Execution plan showing stages and dependencies |\n",
    "| **Partition** | Unit of parallelism — one partition = one task on one core |\n",
    "| **Shuffle** | Data redistribution across cluster (expensive, causes stage boundary) |\n",
    "| **Catalyst Optimizer** | Rule-based + cost-based optimizer that rewrites logical plans |\n",
    "| **Adaptive Query Execution (AQE)** | Runtime optimization: adjusts shuffle partitions, converts joins, handles skew |\n",
    "| **Narrow transformation** | No data exchange between partitions (map, filter) |\n",
    "| **Wide transformation** | Requires shuffle (groupBy, join, distinct) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a45d94d-3139-49cf-bef1-760ae552b359",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 3"
    }
   },
   "source": [
    "## Databricks Platform Elements\n",
    "\n",
    "Kluczowe elementy platformy Databricks: Workspace, Catalog Explorer, Git Folders, Volumes. Zrozumienie struktury platformy to fundament pracy Data Engineera.\n",
    "\n",
    "---\n",
    "\n",
    "### Per-user Isolation\n",
    "\n",
    "Run the initialization script for per-user catalog and schema isolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f4d94df-6da8-468c-84ce-e4601a325e0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../setup/00_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f74fb0b1-9b23-4d51-b1fb-b12913824fc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import re\n",
    "\n",
    "# Display user context (variables from 00_setup)\n",
    "print(f\"Catalog: {CATALOG}\")\n",
    "print(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "print(f\"Schema Silver: {SILVER_SCHEMA}\")\n",
    "print(f\"Schema Gold: {GOLD_SCHEMA}\")\n",
    "print(f\"User: {raw_user}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad256fd2-ae90-4d9c-8681-be489429f78b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 10"
    }
   },
   "source": [
    "### Comparison of Traditional Architecture vs Lakehouse\n",
    "\n",
    "**Objective:** Visualize differences between traditional approach (Data Lake + Data Warehouse) and Lakehouse.\n",
    "\n",
    "**Traditional Architecture:**\n",
    "<img src=\"../../../assets/images/49f3830d3784442ea5582bc82e6fb89c.png\" width=\"800\">\n",
    "\n",
    "**Lakehouse Benefits:**\n",
    "- Single copy of data (single source of truth)\n",
    "- Lower storage costs\n",
    "- Elimination of synchronization latency\n",
    "- Common governance for all use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "604dbe6a-70c7-4147-8713-f922e3d88de4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 11"
    }
   },
   "source": [
    "### Databricks Platform Elements\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "The Databricks platform consists of several key components that together create a complete environment for working with data in the Lakehouse architecture.\n",
    "\n",
    "**Key Components:**\n",
    "- **Workspace**: Working environment containing notebooks, experiments, folders, and resources\n",
    "- **Catalog Explorer**: Interface for managing catalogs, schemas, tables, and views\n",
    "- **Git Folders (formerly Repos)**: Git integration for versioning notebooks and code\n",
    "- **Volumes**: Management of unstructured files (images, models, artifacts)\n",
    "- **DBFS (Databricks File System)**: Virtual file system over cloud storage\n",
    "\n",
    "**Practical Application:**\n",
    "- Workspace organizes projects and team collaboration\n",
    "- Catalog Explorer enables data exploration and governance\n",
    "- Git Folders integrates development workflow with Git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95bf774f-a63e-4432-ba50-c8da3d316631",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 12"
    }
   },
   "source": [
    "### Workspace Exploration\n",
    "\n",
    "#### Example: Workspace Exploration\n",
    "\n",
    "**Objective:** Familiarize with Databricks Workspace interface\n",
    "\n",
    "**Workspace Elements:**\n",
    "1. **Sidebar** (left side):\n",
    "   - Workspace: Folders and notebooks\n",
    "   - Git Folders: Git Integration\n",
    "   - Compute: Cluster management\n",
    "   - Workflows: Lakeflow Jobs\n",
    "   - Catalog: Unity Catalog explorer\n",
    "\n",
    "2. **Main Panel**: Notebook editor or details view\n",
    "\n",
    "3. **Top Bar**: Quick access to compute, account, help\n",
    "\n",
    "**Navigation Instructions:**\n",
    "- Use the left menu to switch between sections\n",
    "- In the Catalog section, you can browse catalogs, schemas, and tables\n",
    "- In the Compute section, you manage Spark clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e92a961-f13d-4068-9af6-1b9b3d5c0e0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Databricks Workspace UI\n",
    "\n",
    "<img src=\"../../../assets/images/848bc3658ab44bb09f586bd2b1f4231e.png\" width=\"800\">\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef0fa899-84b5-4b93-835c-f0afe15d4d57",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 13"
    }
   },
   "source": [
    "### Catalog Explorer - Unity Catalog Structure\n",
    "\n",
    "#### Example: Catalog Explorer\n",
    "\n",
    "**Objective:** Understand object hierarchy in Unity Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a84fd264-7cf0-4b84-9549-7c1f95633026",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Catalog Explorer Screenshot\n",
    "\n",
    "<img src=\"../../../assets/images/32356ba877a74bfe87feeb6d6ee93a46.png\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7d181c5-b49b-4c44-bad0-d18ae5988b31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display current catalog and schema\n",
    "current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "current_schema = spark.sql(\"SELECT current_schema()\").collect()[0][0]\n",
    "\n",
    "print(f\"Current catalog: {current_catalog}\")\n",
    "print(f\"Current schema: {current_schema}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e740d732-aa9e-464d-a6a8-430fb55f5cf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Unity Catalog Hierarchy:**\n",
    "\n",
    "<img src=\"../../../assets/images/cddc09f5ffc5482aa3063a13a7c4f927.png\" width=\"800\">\n",
    "\n",
    "> **3-level namespace:** `catalog.schema.object` — e.g., `prod.gold.sales_summary`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a81256c7-e5d6-4494-af87-36671b2da3ef",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 17"
    }
   },
   "source": [
    "### Browsing Catalogs and Schemas\n",
    "\n",
    "#### Example: Browsing Catalogs and Schemas\n",
    "\n",
    "**Objective:** Programmatic listing of objects in Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b4a711e-7765-4839-87bf-be0f1cc17263",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of all catalogs available to the user\n",
    "catalogs_df = spark.sql(\"SHOW CATALOGS\")\n",
    "display(catalogs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1acf7186-a6c5-4fe8-a5e2-3f8722729d73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of schemas in the current catalog\n",
    "schemas_df = spark.sql(f\"SHOW SCHEMAS IN {CATALOG}\")\n",
    "display(schemas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b04ffdba-2d7b-4363-8c15-9312d59f107a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 21"
    }
   },
   "source": [
    "### Git Folders and Git Integration\n",
    "\n",
    "In practice, working with code in Databricks should be based on **Git Folders** (formerly Repos), not single, orphaned notebooks in Workspace.\n",
    "\n",
    "Typical workflow:\n",
    "\n",
    "1. **Create Git Folder** in Databricks: `Workspace → Git Folders → Add Repo`.\n",
    "2. **Connect to Git** (GitHub / Azure DevOps / other).\n",
    "3. Work on **feature branches** (e.g., `feature/cleaning-module`).\n",
    "4. Regularly:\n",
    "   - commit and push changes from Databricks to remote repo,\n",
    "   - create PR and merge to main/dev.\n",
    "\n",
    "Best Practices:\n",
    "\n",
    "- One repo per project/domain (e.g., `databricks-dea-training`).\n",
    "- Do not work in **Workspace root** – always in **Git Folders**.\n",
    "- Training notebooks, test data, and README can be in one repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3b97f4c-f2d7-476b-876c-03e662fc8258",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 22"
    }
   },
   "source": [
    "### Volumes vs DBFS\n",
    "\n",
    "Where should you store files? In new Unity Catalog-based workspaces, **Volumes** are the preferred location.\n",
    "\n",
    "- `dbfs:/` is treated as a **legacy** layer or auxiliary area.\n",
    "- `/Volumes/catalog.schema.volume_name` is a fully managed, UC-controlled data area (permissions, audit, lineage).\n",
    "\n",
    "Volume Definition Example (SQL):\n",
    "\n",
    "```sql\n",
    "CREATE VOLUME IF NOT EXISTS ${catalog}.${schema}.training_volume\n",
    "COMMENT 'Workspace for training purposes';\n",
    "```\n",
    "\n",
    "Usage Example in PySpark:\n",
    "\n",
    "```python\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "\n",
    "volume_path = f\"/Volumes/{catalog}/{schema}/training_volume\"\n",
    "display(dbutils.fs.ls(volume_path))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59fe7bbe-41c2-4fc0-8247-61dbd858854f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 23"
    }
   },
   "source": [
    "### SQL Warehouse\n",
    "\n",
    "A SQL engine optimized for BI and ad-hoc analytics, an alternative to notebook clusters.\n",
    "\n",
    "When to use:\n",
    "- Reporting in Power BI / other BI tools.\n",
    "- Business analysts / power users working mainly in SQL.\n",
    "- Interactive dashboards and ad-hoc queries to **Gold** layer.\n",
    "\n",
    "Differences from all-purpose cluster:\n",
    "- Billing based on **DBU SQL** (different rates).\n",
    "- Automatic provisioning / scaling.\n",
    "- Isolation of BI workload from engineering clusters.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0269415-6dda-473d-a771-236303dbe4c8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 24"
    }
   },
   "source": [
    "## Compute Resources\n",
    "\n",
    "Types of compute resources in Databricks: All-Purpose Clusters, Job Clusters, SQL Warehouses. Choosing the right compute directly impacts costs.\n",
    "\n",
    "---\n",
    "\n",
    "### The Real Question: How Much Will This Cost?\n",
    "\n",
    "As a Data Engineer, you'll be asked: *\"Why is our Databricks bill so high?\"*\n",
    "\n",
    "Understanding compute options is essential for cost control.\n",
    "\n",
    "### Compute Options Comparison\n",
    "\n",
    "| Type | Startup Time | Cost Model | Best For |\n",
    "|------|--------------|------------|----------|\n",
    "| **All-Purpose Cluster** | 3-5 min | Per-minute (running) | Interactive development, exploration |\n",
    "| **Job Cluster** | 3-5 min | Per-minute (only during job) | Scheduled production jobs |\n",
    "| **Serverless** | <10 sec | Per-query DBUs | Ad-hoc queries, variable workloads |\n",
    "| **SQL Warehouse** | 0 (Serverless) or 3-5 min | Per-query DBUs | BI tools, SQL analysts |\n",
    "\n",
    "### Cost Optimization Strategies\n",
    "\n",
    "**1. Right-size clusters:**\n",
    "- Development: 2-4 workers, smallest instance type\n",
    "- Production: Autoscaling 2-10 workers based on workload\n",
    "\n",
    "**2. Use Spot/Preemptible instances:**\n",
    "- 60-80% cost savings for workers\n",
    "- Driver on on-demand (stability)\n",
    "- Trade-off: Job may be interrupted\n",
    "\n",
    "**3. Photon Engine:**\n",
    "- 2-3x faster for aggregations/joins\n",
    "- ~2x DBU cost, but finishes faster = often cheaper\n",
    "- Enable for: large scans, aggregations, joins\n",
    "- Skip for: simple transformations, ML training\n",
    "\n",
    "**4. Cluster policies:**\n",
    "- Enforce maximum worker count\n",
    "- Require autoscaling\n",
    "- Set auto-termination (e.g., 30 min idle)\n",
    "\n",
    "### Decision Tree: Which Compute to Use?\n",
    "\n",
    "<img src=\"../../../assets/images/f98e20f71ec541eb9b206877f1da98b5.png\" width=\"800\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "677ff10d-6ff8-4a96-b12f-5e84aba7f563",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 25"
    }
   },
   "source": [
    "### Cluster Info\n",
    "\n",
    "Check the runtime version and Photon status on the current cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91374905-ca6b-46c1-86da-c94a5549cede",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cluster runtime and Photon status\n",
    "dbr_version = spark.conf.get(\"spark.databricks.clusterUsageTags.sparkVersion\", \"unknown\")\n",
    "photon = spark.conf.get(\"spark.databricks.photon.enabled\", \"false\")\n",
    "print(f\"Runtime: {dbr_version}  |  Photon: {photon}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "566b74f2-66f4-4f1a-b5c9-15dd70cafa28",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 31"
    }
   },
   "source": [
    "## Magic Commands\n",
    "\n",
    "Magic commands allow you to switch between languages and perform system operations directly from notebook cells.\n",
    "\n",
    "---\n",
    "\n",
    "| Command | Purpose |\n",
    "|---------|---------|\n",
    "| `%sql` | SQL cell |\n",
    "| `%python` | Python cell (default) |\n",
    "| `%md` | Markdown documentation |\n",
    "| `%fs` | DBFS file operations |\n",
    "| `%sh` | Shell commands |\n",
    "| `%run` | Execute another notebook |\n",
    "| `%pip` | Install notebook-scoped libraries |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14084b35-dd2b-4bc3-abcb-b170d0e48117",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 32"
    }
   },
   "source": [
    "### Monitoring\n",
    "\n",
    "Where to look for problems: **Cluster → Event log** | **Spark UI** (Jobs, SQL tabs) | **Driver/Executor logs**\n",
    "\n",
    "> **Best Practice:** For production pipelines, log to Delta tables — not just cluster logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15a320a9-fd52-454d-9a08-5c2074746fef",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 33"
    }
   },
   "source": [
    "### Demo: %sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cdd5b6e-ce7d-4a23-9056-467163b1657a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SQL magic command allows writing pure SQL without Python wrapper\n",
    "\n",
    "SELECT \n",
    "  current_catalog() as catalog,\n",
    "  current_schema() as schema,\n",
    "  current_user() as user,\n",
    "  current_timestamp() as timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cda7429b-9d22-4a72-99f8-e546014c7529",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 39"
    }
   },
   "source": [
    "### Demo: Mixing Python + SQL\n",
    "\n",
    "Create data in Python → query with SQL via temp view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "039f22a9-d548-4c7b-9532-6ced5708a3aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Python: Raw data definition\n",
    "data = [\n",
    "    (1, \"Alice\", \"Engineering\", 95000),\n",
    "    (2, \"Bob\", \"Sales\", 75000),\n",
    "    (3, \"Charlie\", \"Engineering\", 105000),\n",
    "    (4, \"Diana\", \"Marketing\", 68000),\n",
    "    (5, \"Eve\", \"Engineering\", 98000)\n",
    "]\n",
    "\n",
    "# Schema definition\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"department\", StringType(), False),\n",
    "    StructField(\"salary\", IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "289d1ce0-479b-47f5-9e30-50c09b90c688",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e34557e1-9ac9-4f3a-88c9-3f40d3c8426f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register as temp view for SQL access\n",
    "df.createOrReplaceTempView(\"employees_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f59fce2a-0196-4145-8f6c-24d1f5d70b13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SQL: Aggregation on Python data\n",
    "\n",
    "SELECT \n",
    "  department,\n",
    "  COUNT(*) as employee_count,\n",
    "  AVG(salary) as avg_salary,\n",
    "  MAX(salary) as max_salary\n",
    "FROM employees_temp\n",
    "GROUP BY department\n",
    "ORDER BY avg_salary DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ef0c1cd-1d7c-4fad-910b-8087ab6fc581",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 46"
    }
   },
   "source": [
    "### Demo: %pip (notebook-scoped libraries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b998dba-508e-4ee7-a9a5-96f8419efe33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install emoji library\n",
    "%pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "731f59a0-93e7-4245-b94e-504b7b0c5bfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import emoji\n",
    "print(emoji.emojize('Databricks is :fire:'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6662cc7-46a9-4f65-915a-18079ed97f68",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 49"
    }
   },
   "source": [
    "### Databricks Assistant (AI)\n",
    "\n",
    "In 2025, coding work is assisted by AI. Databricks has a built-in assistant (**Databricks Assistant**) that is context-aware of your data (knows table schemas in Unity Catalog!).\n",
    "\n",
    "**How to use?**\n",
    "1. Shortcut **Cmd+I** (Mac) or **Ctrl+I** (Windows) inside a cell.\n",
    "2. \"Assistant\" side panel.\n",
    "\n",
    "**What is it for?**\n",
    "- **Code Generation**: \"Write a SQL query that calculates average sales by region from the sales table\".\n",
    "- **Code Explanation**: Select a complex snippet and ask \"Explain this code\".\n",
    "- **Fixing errors**: When a cell returns an error, click \"Diagnose Error\" – the assistant will explain the cause and propose a fix.\n",
    "- **Transformation**: \"Rewrite this code from PySpark to SQL\".\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cffcf7a7-4b58-4f36-a746-71603cb32a1a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 50"
    }
   },
   "source": [
    "## Unity Catalog\n",
    "\n",
    "A modern metadata management system replacing Hive Metastore. Provides centralized access control, lineage, and governance across the entire organization.\n",
    "\n",
    "---\n",
    "\n",
    "### Theoretical Introduction\n",
    "\n",
    "Databricks supports two metadata systems: legacy Hive Metastore and modern Unity Catalog. Unity Catalog is recommended for all new projects due to advanced governance and security features.\n",
    "\n",
    "**Key Differences:**\n",
    "\n",
    "| Aspect | Hive Metastore | Unity Catalog |\n",
    "|--------|----------------|---------------|\n",
    "| **Governance** | Limited | Full: RBAC, masking, audit |\n",
    "| **Namespace** | 2-level (db.table) | 3-level (catalog.schema.table) |\n",
    "| **Cross-workspace** | No | Yes (shared metastore) |\n",
    "| **Lineage** | None | End-to-end lineage |\n",
    "| **Data Sharing** | Limited | Delta Sharing protocol |\n",
    "| **Isolation** | Workspace-level | Catalog-level |\n",
    "\n",
    "**Why Unity Catalog?**\n",
    "- Central access management for all workspaces\n",
    "- Automatic lineage for audit and compliance\n",
    "- Fine-grained permissions (column-level, row-level)\n",
    "- Integration with external systems (Delta Sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f41c629-f1dd-4331-a607-b16b4c7a8314",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 51"
    }
   },
   "source": [
    "### Namespace - Hive vs Unity Catalog\n",
    "\n",
    "#### Example: Namespace Comparison\n",
    "\n",
    "**Objective:** Compare table access syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11188262-35c1-49e3-8b92-38362635bd2b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 54"
    }
   },
   "source": [
    "### Creating a Table in Unity Catalog\n",
    "\n",
    "#### Example: Creating a Table\n",
    "\n",
    "**Objective:** Demonstrate full syntax with 3-level namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19c1afdc-9dba-48ea-a8fd-4f31ff2d05bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create sample table in Unity Catalog\n",
    "table_name = f\"{CATALOG}.{BRONZE_SCHEMA}.lakehouse_demo\"\n",
    "\n",
    "# Demo data\n",
    "demo_data = [\n",
    "    (1, \"Unity Catalog\", \"Enabled\", \"2024-01-15\"),\n",
    "    (2, \"Delta Lake\", \"Enabled\", \"2024-01-15\"),\n",
    "    (3, \"Photon Engine\", \"Enabled\", \"2024-01-15\"),\n",
    "    (4, \"Hive Metastore\", \"Legacy\", \"2024-01-15\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e40fd65e-3d9b-4ed0-a807-3aa070e50685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save as Delta Table in Unity Catalog\n",
    "demo_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1506a541-232b-4b70-b15e-17fe667a9944",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification\n",
    "display(spark.table(table_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "490272be-2a84-46e2-a66d-0d6d1d573ff8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task: Check in UI\n",
    "\n",
    "1. Click **Catalog** in the left sidebar.\n",
    "2. Find your catalog (name in `CATALOG` variable, e.g., `retailhub_...`).\n",
    "3. Expand the `bronze` schema (or other defined in `BRONZE_SCHEMA`).\n",
    "4. Click on the `lakehouse_demo` table.\n",
    "5. See tabs: **Sample Data** (preview) and **Lineage** (data origin)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ec8af90-b3b3-43db-a965-d0c39e0c44f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "The table was created with a full 3-level namespace. In Unity Catalog, every table automatically:\n",
    "- Is managed by the governance system\n",
    "- Has tracked lineage\n",
    "- Has permissions assigned based on catalog and schema\n",
    "- Is available in Catalog Explorer for exploration\n",
    "\n",
    "**Managed vs External Tables:**\n",
    "The table above is a **Managed Table**. Databricks manages both metadata and data files (in default catalog/schema storage). Dropping the table (`DROP TABLE`) also deletes the data.\n",
    "\n",
    "**External Table** is created when we provide `LOCATION 'path'`. Then `DROP TABLE` removes only metadata, and files remain in storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dc408bc-fa10-46b1-aadf-e21efee43fd6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 63"
    }
   },
   "source": [
    "### Comparison PySpark vs SQL\n",
    "\n",
    "**DataFrame API (PySpark):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a1a2858-88a3-4425-b9ae-bbcb8dc3fd7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PySpark Approach - programmatic DataFrame API\n",
    "\n",
    "df_pyspark = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.lakehouse_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf9b31f8-ef80-4b2c-9d88-4a7106d1f7c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_pyspark = df_pyspark \\\n",
    "    .filter(F.col(\"status\") == \"Enabled\") \\\n",
    "    .select(\"feature\", \"status\", \"date\") \\\n",
    "    .orderBy(\"feature\")\n",
    "display(result_pyspark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26f26fb2-d01e-4ddd-9ba5-a4f125828150",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**SQL Equivalent:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71130cf7-560f-4571-9ea0-93c480f9d3bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(f\"select * from {CATALOG}.{BRONZE_SCHEMA}.lakehouse_demo\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "169a685d-24ac-4f56-b608-8a240f2cd18f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Parameterization with Databricks Widgets\n",
    "\n",
    "Below we use the **Widgets** mechanism, which allows creating interactive controls in the notebook. This allows passing parameters (e.g., table names, dates) to SQL and Python code, facilitating the building of universal reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3dff4b4-da92-40a8-9785-eb777db88424",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parameterization with Databricks Widgets\n",
    "# Set default values based on variables from 00_setup (if available)\n",
    "# This ensures SQL cells will use the same catalog as Python cells\n",
    "\n",
    "default_catalog = CATALOG if 'CATALOG' in locals() else \"retailhub_trainer\"\n",
    "default_schema = BRONZE_SCHEMA if 'BRONZE_SCHEMA' in locals() else \"bronze\"\n",
    "\n",
    "dbutils.widgets.text(\"CATALOG\", default_catalog)\n",
    "dbutils.widgets.text(\"BRONZE_SCHEMA\", default_schema)\n",
    "dbutils.widgets.text(\"BRONZE_SCHEMA_2\", default_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06523c1b-3a68-42d9-bd21-099e0ff46d49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT \n",
    "  feature,\n",
    "  status,\n",
    "  date\n",
    "FROM IDENTIFIER(:CATALOG || '.' || :BRONZE_SCHEMA || '.lakehouse_demo')\n",
    "WHERE status = 'Enabled'\n",
    "ORDER BY feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdd223e2-d3f6-42ca-ac53-acbbbea0ebbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Comparison:**\n",
    "- **Performance**: Identical - both approaches compile to the same Catalyst query plan\n",
    "- **When to use PySpark**: \n",
    "  - Complex business logic with UDFs\n",
    "  - Dynamic pipelines (parameterization, loops)\n",
    "  - Integration with Python libraries (pandas, scikit-learn)\n",
    "- **When to use SQL**: \n",
    "  - Simple transformations and aggregations\n",
    "  - Team with strong SQL skills\n",
    "  - Migration from traditional Data Warehouse\n",
    "  - Better support for business analysts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a16ec104-b474-4943-b590-30dd012a7aad",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 76"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What was achieved:\n",
    "- Learned Lakehouse concept as evolution of Data Lake + Data Warehouse\n",
    "- Explored Databricks platform elements: Workspace, Compute, Catalog\n",
    "- Understood Unity Catalog hierarchy: Metastore → Catalog → Schema → Objects\n",
    "- Practiced magic commands: %sql, %python, %fs, %pip\n",
    "- Compared Hive Metastore vs Unity Catalog\n",
    "- Created first Delta table in Unity Catalog with 3-level namespace\n",
    "\n",
    "### Key Takeaways:\n",
    "1. **Lakehouse eliminates data duplication**: Single copy serves BI, ML, and real-time analytics\n",
    "2. **Unity Catalog is governance foundation**: 3-level namespace, fine-grained permissions, automatic lineage\n",
    "3. **Clusters are flexible**: Autoscaling and spot instances reduce costs, Photon accelerates queries\n",
    "4. **Notebooks are powerful**: Mixing SQL/Python, magic commands, Git integration via Git Folders\n",
    "5. **Delta Lake is default format**: ACID transactions, time travel, schema evolution\n",
    "\n",
    "### Quick Reference - Key Commands:\n",
    "\n",
    "| Operation | PySpark | SQL |\n",
    "|-----------|---------|-----|\n",
    "| Set catalog | `spark.sql(f\"USE CATALOG {CATALOG}\")` | `USE CATALOG my_catalog` |\n",
    "| List catalogs | `spark.sql(\"SHOW CATALOGS\")` | `SHOW CATALOGS` |\n",
    "| List schemas | `spark.sql(\"SHOW SCHEMAS\")` | `SHOW SCHEMAS` |\n",
    "| Create table | `df.write.saveAsTable(\"cat.schema.table\")` | `CREATE TABLE cat.schema.table AS SELECT ...` |\n",
    "| Read table | `spark.table(\"cat.schema.table\")` | `SELECT * FROM cat.schema.table` |\n",
    "| Metadata | - | `SELECT * FROM system.information_schema.tables` |\n",
    "| Install lib | `%pip install package` | - |"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6142587020061451,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "M01_platform_workspace",
   "widgets": {
    "BRONZE_SCHEMA": {
     "currentValue": "bronze",
     "nuid": "3eb823de-3328-4753-a2bf-9d863378b4f1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "bronze",
      "label": null,
      "name": "BRONZE_SCHEMA",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "bronze",
      "label": null,
      "name": "BRONZE_SCHEMA",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "BRONZE_SCHEMA_2": {
     "currentValue": "bronze",
     "nuid": "55fcd747-0b42-452a-a57e-2d92169a6e4f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "bronze",
      "label": null,
      "name": "BRONZE_SCHEMA_2",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "bronze",
      "label": null,
      "name": "BRONZE_SCHEMA_2",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "CATALOG": {
     "currentValue": "retailhub_trainer",
     "nuid": "ab4cee0e-5693-4e7e-8e80-189b95567447",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "retailhub_trainer",
      "label": null,
      "name": "CATALOG",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "retailhub_trainer",
      "label": null,
      "name": "CATALOG",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
