{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6bc86c4-58e5-4f4f-af86-f3311937bb70",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Introduction"
    }
   },
   "source": [
    "# M02: ELT Ingestion & Transformations\n",
    "\n",
    "## 2.1. The Story Continues...\n",
    "\n",
    "Remember our e-commerce company? The data team has just gained access to multiple source systems:\n",
    "\n",
    "* **CRM System** exports customers as CSV (daily dump)\n",
    "* **Order Management** sends JSON via API \n",
    "* **Product Catalog** is in Parquet (from a legacy Hadoop system)\n",
    "* **Extended Customer Data** arrives in Excel from the marketing team\n",
    "\n",
    "**Your mission:** Import data from all sources, transform it, analyze it, and prepare it for downstream analytics.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.2. Why This Matters (Real-World Context)\n",
    "\n",
    "### The \"inferSchema\" Trap\n",
    "\n",
    "Most tutorials show: `spark.read.option(\"inferSchema\", \"true\")`\n",
    "\n",
    "**In production, this is often a bad idea:**\n",
    "\n",
    "| Scenario         | With inferSchema         | With explicit schema      |\n",
    "|------------------|-------------------------|--------------------------|\n",
    "| 10 GB file       | Scans entire file first | Direct read              |\n",
    "| Column `\"123\"`   | Might be INT or STRING? | You control it           |\n",
    "| New column added | Schema changes silently | Fails fast (good!)       |\n",
    "| Null values      | Might guess wrong type  | Explicit nullable        |\n",
    "\n",
    "**Rule of thumb:** Use `inferSchema=true` for exploration, explicit schema for production.\n",
    "\n",
    "### The Bronze Layer Philosophy\n",
    "\n",
    "In the Medallion architecture, the Bronze layer should:\n",
    "* Keep data as STRING (preserve original values)\n",
    "* Add metadata (ingestion time, source file)\n",
    "* NOT apply business logic\n",
    "* Be idempotent (safe to re-run)\n",
    "\n",
    "---\n",
    "\n",
    "### Data Process Patterns\n",
    "\n",
    "Understanding how data flows is critical for designing robust pipelines.\n",
    "\n",
    "| Pattern | Description | Trigger Type | Use Case |\n",
    "|---------|-------------|--------------|----------|\n",
    "| **Full Load** | Reloads the entire dataset from scratch every time. | Batch Read | Small tables, full history refreshes. |\n",
    "| **Incremental Batch** | Processes only new/changed data but runs as a scheduled job. | `Trigger.AvailableNow` (or `Once`) | Daily/Hourly ETL. efficient and cost-effective. |\n",
    "| **Continuous** | Continuously processes data as it arrives to minimize latency. | `Trigger.ProcessingTime` | Real-time monitoring, alerts. |\n",
    "\n",
    "![Data Ingestion Architecture](../../../assets/images/1ba507cd604849878e74e586f4df3559.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### Module Content Overview\n",
    "\n",
    "| Section | Topics Covered |\n",
    "|---------|----------------|\n",
    "| **Part 1: Data Ingestion** | • **DataFrame Reader API** – support for CSV, JSON, Parquet, Excel <br> • **Schema Definition** – explicit vs inferred approach <br> • **Format Options** – handling delimiters, quotes, headers |\n",
    "| **Part 2: DataFrame Operations** | • **Transformations** – `select`, `withColumn`, `cast`, `rename`, `drop` <br> • **Filtering** – complex conditions, null handling <br> • **Aggregations** – `groupBy`, `agg`, statistical functions |\n",
    "| **Part 3: Advanced Techniques** | • **SQL Integration** – Temp Views, mixed Python/SQL logic <br> • **Complex Types** – `explode`, `struct`, JSON parsing <br> • **Joins** – combining multiple datasets efficiently |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a0bd3d0-734d-4aa5-a893-d0e169909870",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.3. ELT vs ETL\n",
    "\n",
    "**Key Concept for Exam:**\n",
    "\n",
    "| Aspect | ETL (Extract-Transform-Load) | ELT (Extract-Load-Transform) |\n",
    "|--------|------------------------------|-------------------------------|\n",
    "| Transform Location | External engine (before load) | Inside data platform (after load) |\n",
    "| Data Storage | Only transformed data stored | Raw + transformed data stored |\n",
    "| Flexibility | Fixed schema upfront | Schema-on-read, evolve later |\n",
    "| Scalability | Limited by transform engine | Scales with cloud compute |\n",
    "| Use Case | Traditional DW | Modern Lakehouse |\n",
    "| Databricks Approach | **Not recommended** | **Default pattern** |\n",
    "\n",
    "**Databricks uses ELT pattern:**\n",
    "1. **Extract** raw data into Bronze layer (minimal transformation)\n",
    "2. **Load** as-is into Delta tables\n",
    "3. **Transform** using Spark SQL/PySpark in Silver and Gold layers\n",
    "\n",
    "**Why ELT in Lakehouse?**\n",
    "- Raw data preserved for reprocessing\n",
    "- Transformations can be updated without re-ingestion\n",
    "- Compute and storage scale independently\n",
    "- Schema evolution handled by Delta Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cf6c95b-2145-4ffe-a23d-8118983d4dd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.4. Environment Initialization\n",
    "\n",
    "Run the central configuration script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea9ed832-63c8-4788-8c7f-256fcf2ac6e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../setup/00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9981cb88-642b-4f84-8a87-ebf5d572e504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.4.1. Notebook Configuration\n",
    "\n",
    "Define variables specific to this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f11c5fa-8097-4ebe-89cf-93833a9e6f72",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Notebook Configuration"
    }
   },
   "outputs": [],
   "source": [
    "# Paths to data directories (subdirectories in DATASET_PATH from 00_setup)\n",
    "CUSTOMERS_PATH = f\"{DATASET_PATH}/customers\"\n",
    "ORDERS_PATH = f\"{DATASET_PATH}/orders\"\n",
    "PRODUCTS_PATH = f\"{DATASET_PATH}/products\"\n",
    "\n",
    "# Paths to specific files\n",
    "CUSTOMERS_CSV = f\"{CUSTOMERS_PATH}/customers.csv\"\n",
    "ORDERS_JSON = f\"{ORDERS_PATH}/orders_batch.json\"\n",
    "PRODUCTS_PARQUET = f\"{PRODUCTS_PATH}/products.parquet\"\n",
    "EXCEL_PATH = f\"{DATASET_PATH}/customers/customers_extented.xlsx\"  # Note: filename has typo 'extented'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82bbd0b5-8936-432c-bbab-21e9f46ec084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.4.2. Import Libraries\n",
    "\n",
    "Import all necessary libraries for data ingestion and transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e438f20-ee94-4e47-aa8f-85f56d553a45",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "# Import PySpark types for schema definition\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, \n",
    "    DoubleType, TimestampType, DateType, ArrayType\n",
    ")\n",
    "\n",
    "# Import PySpark functions for transformations and aggregations\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, concat, upper, lower, year, month, day,\n",
    "    sum, avg, min, max, count, stddev, desc, asc,\n",
    "    explode, explode_outer, struct, array,\n",
    "    get_json_object, from_json, to_json,\n",
    "    to_date, current_date, datediff\n",
    ")\n",
    "\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "889facc3-12b1-46b1-ac58-004aef6b0971",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.5. CSV Data Import (Customers)\n",
    "\n",
    "### 2.5.1. Loading CSV with Automatic Schema Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e44f411-bfe5-4692-8552-458c1dcfad50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load CSV data with automatic schema inference\n",
    "customers_auto_df = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")       # First line contains column names\n",
    "    .option(\"inferSchema\", \"true\")  # Automatic data type inference\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93a36f1a-c167-4a78-9da6-56a05d7edc64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify loaded data\n",
    "customers_auto_df.printSchema()\n",
    "display(customers_auto_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10f0f50b-3fa1-452a-85fd-f0df5773f08c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.5.2. Extended Reader Options – Typical Production Issues\n",
    "\n",
    "In a production environment, we often encounter CSV files with:\n",
    "\n",
    "- different separator (`;` instead of `,`),\n",
    "- quotes inside fields,\n",
    "- corrupted rows.\n",
    "\n",
    "Key options:\n",
    "\n",
    "- `delimiter` – custom column separator,\n",
    "- `quote` – character opening/closing text fields,\n",
    "- `escape` – way to \"escape\" special characters,\n",
    "- `mode` – handling of malformed records (`PERMISSIVE`, `DROPMALFORMED`, `FAILFAST`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5071f0b-5611-4d63-9e21-3f649dae5ee6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.5.3. Manual Schema Definition for CSV\n",
    "\n",
    "**Best Practice:** Defining schema manually ensures control and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1cdaa60-b1b1-46de-8604-7ae9aaa4c320",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Manual Schema Definition"
    }
   },
   "outputs": [],
   "source": [
    "# Schema definition for customers\n",
    "# Structure: customer_id (string), first_name (string), last_name (string), email (string), city (string), country (string), registration_date (timestamp)\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), nullable=False),\n",
    "    StructField(\"first_name\", StringType(), nullable=True),\n",
    "    StructField(\"last_name\", StringType(), nullable=True),\n",
    "    StructField(\"city\", StringType(), nullable=True),\n",
    "    StructField(\"email\", StringType(), nullable=True),\n",
    "    StructField(\"country\", StringType(), nullable=True),\n",
    "    StructField(\"registration_date\", TimestampType(), nullable=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bd147be-df3f-4274-8228-bf8cf6a626ba",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load CSV with Manual Schema"
    }
   },
   "outputs": [],
   "source": [
    "# Load CSV data with manually defined schema\n",
    "customers_df = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(customers_schema)\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08683c36-2a50-4b57-a169-c69ec0cac828",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify loaded data\n",
    "customers_df.printSchema()\n",
    "display(customers_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2599e27-eda1-4c27-885d-3e96326d9a6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.6. JSON Data Import (Orders)\n",
    "\n",
    "### 2.6.1. Loading JSON with Automatic Schema Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "574e1299-3b2b-4ee2-93bb-b2a4e136776e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load JSON with inferSchema"
    }
   },
   "outputs": [],
   "source": [
    "# Load JSON data with automatic schema inference\n",
    "orders_auto_df = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(ORDERS_JSON)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08afea1d-811a-4403-bb19-9ac586c8076e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify loaded data\n",
    "orders_auto_df.printSchema()\n",
    "display(orders_auto_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37d607c2-8aa5-4bbe-ad9c-1af6495d66a6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Manual Schema Header"
    }
   },
   "source": [
    "### 2.6.2. Manual Schema Definition for JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c826f976-b255-4e73-83fc-bcc6b3f5238b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Orders Schema Definition"
    }
   },
   "outputs": [],
   "source": [
    "# Schema definition for orders\n",
    "# Actual structure: order_id, customer_id, product_id, store_id, order_datetime, quantity, unit_price, discount_percent, total_amount, payment_method\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), nullable=True),  # Can be null in data\n",
    "    StructField(\"customer_id\", StringType(), nullable=False),\n",
    "    StructField(\"order_datetime\", StringType(), nullable=True),  # String, will convert to timestamp later\n",
    "    StructField(\"total_amount\", DoubleType(), nullable=False),\n",
    "    StructField(\"payment_method\", StringType(), nullable=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a928b826-9d57-43ae-826a-f50d97830be8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load JSON with Manual Schema"
    }
   },
   "outputs": [],
   "source": [
    "# Load JSON data with manually defined schema\n",
    "orders_df = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .schema(orders_schema)\n",
    "    .load(ORDERS_JSON)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e70b0b80-2f9e-4697-91f8-be7e859cf956",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify loaded data\n",
    "orders_df.printSchema()\n",
    "display(orders_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9f2041e-010b-42d1-b577-0a31eb841d95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.7. Parquet Data Import (Products)\n",
    "\n",
    "### 2.7.1. Loading Parquet (built-in schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "607e760f-8e10-45a7-9341-59f7da46a630",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Parquet"
    }
   },
   "outputs": [],
   "source": [
    "# Parquet already contains built-in schema - no need to define it\n",
    "products_df = (\n",
    "    spark.read\n",
    "    .format(\"parquet\")\n",
    "    .load(PRODUCTS_PARQUET)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "547d42fe-3196-411f-9336-7ee6876030d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify loaded data\n",
    "products_df.printSchema()\n",
    "display(products_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42527a0a-eae2-424d-9a35-7ffb8e88363e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Excel Import Header"
    }
   },
   "source": [
    "## 2.8. Excel Data Import (Extended Customer Data)\n",
    "\n",
    "### 2.8.1. Loading Excel with Spark-Excel Library\n",
    "\n",
    "For Excel files, Databricks Runtime provides built-in support for Excel format.\n",
    "\n",
    "**Benefits:**\n",
    "* Native Spark integration (distributed processing)\n",
    "* Supports large Excel files\n",
    "* Consistent API with other formats\n",
    "* Works with Unity Catalog Volumes\n",
    "\n",
    "**Note:** Requires spark-excel library to be installed on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6324e2f4-bcac-438a-9a53-992820bea242",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Excel"
    }
   },
   "outputs": [],
   "source": [
    "# Load Excel file using spark-excel library\n",
    "\n",
    "customers_extended_df = (\n",
    "    spark.read\n",
    "    .format(\"excel\")\n",
    "    .option(\"dataAddress\", \"Arkusz1!D6:M26\")\n",
    "    .option(\"headerRows\", 1)\n",
    "    .load(EXCEL_PATH)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78943e3d-bd75-47a7-90cc-56bd19a77978",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify loaded data\n",
    "customers_extended_df.printSchema()\n",
    "display(customers_extended_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bb09b67-4724-4fbc-8d4d-08cb6dad06c6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Performance Comparison Header"
    }
   },
   "source": [
    "## 2.9. Performance Comparison\n",
    "\n",
    "### 2.9.1. Loading CSV: inferSchema vs Manual Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21c1e752-420b-4f81-99ca-0943eca7cb6d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Benchmark - inferSchema"
    }
   },
   "outputs": [],
   "source": [
    "# Automatic schema inference\n",
    "start_auto = time.time()\n",
    "df_auto = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")\n",
    "count_auto = df_auto.count()  # Action - forces execution\n",
    "time_auto = time.time() - start_auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08647755-e8d0-4613-a186-11cba24c6844",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Benchmark - Manual Schema"
    }
   },
   "outputs": [],
   "source": [
    "# Manual schema definition\n",
    "start_manual = time.time()\n",
    "df_manual = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(customers_schema)\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")\n",
    "count_manual = df_manual.count()  # Action - forces execution\n",
    "time_manual = time.time() - start_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb4d139e-5c55-47d4-9cba-e1f111e4f072",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Benchmark - Comparison"
    }
   },
   "outputs": [],
   "source": [
    "# Comparison\n",
    "speedup = (time_auto - time_manual) / time_auto * 100\n",
    "print(f\"\\nConclusion: Manual schema is faster by {speedup:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "190bf43a-2bda-4128-8b41-9b220af80c45",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DataFrame Transformations Header"
    }
   },
   "source": [
    "## 2.10. DataFrame Transformations\n",
    "\n",
    "Now that we have loaded data from multiple sources (CSV, JSON, Parquet, Excel), let's explore common DataFrame transformation operations.\n",
    "\n",
    "### 2.10.1. Select - Choosing Columns\n",
    "\n",
    "The `select()` operation allows you to choose specific columns from a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e324d8e-a2c1-4430-9f8e-e61f82b03066",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select specific columns\n",
    "customers_selected = customers_df.select(\"customer_id\", \"first_name\", \"last_name\", \"email\")\n",
    "display(customers_selected.limit(5))\n",
    "\n",
    "# Select with column expressions\n",
    "customers_transformed = customers_df.select(\n",
    "    col(\"customer_id\"),\n",
    "    upper(col(\"first_name\")).alias(\"first_name_upper\"),\n",
    "    col(\"email\")\n",
    ")\n",
    "display(customers_transformed.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfc32c6d-27a6-4c78-986b-32e3dda3e13c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "WithColumn Header"
    }
   },
   "source": [
    "### 2.10.2. WithColumn - Adding/Modifying Columns\n",
    "\n",
    "The `withColumn()` operation adds a new column or replaces an existing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34d8a0e2-95ae-421d-a475-c22a8680b71c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add a new column\n",
    "customers_with_fullname = customers_df.withColumn(\n",
    "    \"full_name\", \n",
    "    concat(col(\"first_name\"), lit(\" \"), col(\"last_name\"))\n",
    ")\n",
    "display(customers_with_fullname.select(\"customer_id\", \"first_name\", \"last_name\", \"full_name\").limit(5))\n",
    "\n",
    "# Add multiple columns\n",
    "customers_enriched = customers_auto_df \\\n",
    "    .withColumn(\"email_lower\", lower(col(\"email\"))) \\\n",
    "    .withColumn(\"registration_year\", year(col(\"registration_date\")))\n",
    "    \n",
    "display(customers_enriched.select(\"customer_id\", \"email\", \"email_lower\", \"registration_date\", \"registration_year\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1edd3536-4285-49bc-a949-914b67931994",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cast Header"
    }
   },
   "source": [
    "### 2.10.3. Cast - Type Conversion\n",
    "\n",
    "The `cast()` operation converts column data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56d0e66e-8509-4853-8f7b-4f63ab4b9859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cast customer_id to string\n",
    "customers_casted = customers_df.withColumn(\"customer_id_str\", col(\"customer_id\").cast(StringType()))\n",
    "customers_df.select(\"customer_id\").printSchema()\n",
    "customers_casted.select(\"customer_id_str\").printSchema()\n",
    "\n",
    "# Cast timestamp to date\n",
    "customers_date = customers_df.withColumn(\"registration_date_only\", col(\"registration_date\").cast(DateType()))\n",
    "display(customers_date.select(\"customer_id\", \"registration_date\", \"registration_date_only\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b6eace4-a44e-4f1d-9935-c4fb71b141c8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Rename Header"
    }
   },
   "source": [
    "### 2.10.4. Rename - Changing Column Names\n",
    "\n",
    "The `withColumnRenamed()` operation renames columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40fcc840-29c6-413f-bbcd-116ec3545ec1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rename single column\n",
    "customers_renamed = customers_df.withColumnRenamed(\"customer_id\", \"id\")\n",
    "display(customers_renamed.limit(5))\n",
    "\n",
    "# Rename multiple columns using select with alias\n",
    "customers_multi_renamed = customers_df.select(\n",
    "    col(\"customer_id\").alias(\"id\"),\n",
    "    col(\"first_name\").alias(\"fname\"),\n",
    "    col(\"last_name\").alias(\"lname\"),\n",
    "    col(\"email\"),\n",
    "    col(\"city\")\n",
    ")\n",
    "display(customers_multi_renamed.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63fed83c-d1ce-4adb-8cf9-77c03a2b8342",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Drop Header"
    }
   },
   "source": [
    "### 2.10.5. Drop - Removing Columns\n",
    "\n",
    "The `drop()` operation removes columns from a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b523e0a-5fb2-442e-814c-8df55c51097b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop single column\n",
    "customers_dropped = customers_df.drop(\"email\")\n",
    "\n",
    "# Drop multiple columns\n",
    "customers_minimal = customers_df.drop(\"email\", \"city\", \"country\")\n",
    "display(customers_minimal.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d32f758d-5440-4a12-8bb1-85cc77bf11de",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Distinct Header"
    }
   },
   "source": [
    "### 2.10.6. Distinct - Unique Rows\n",
    "\n",
    "The `distinct()` operation returns unique rows, and `dropDuplicates()` allows column-specific deduplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99034ded-d4a2-46d0-94eb-948ee2cc3fc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get distinct countries\n",
    "distinct_countries = customers_df.select(\"country\").distinct()\n",
    "display(distinct_countries.orderBy(\"country\"))\n",
    "\n",
    "# Drop duplicates based on specific columns\n",
    "unique_locations = customers_df.select(\"city\", \"country\").dropDuplicates([\"city\", \"country\"])\n",
    "display(unique_locations.orderBy(\"country\", \"city\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "340a5de4-d9a8-4861-a1e2-02030eb07a32",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "OrderBy Header"
    }
   },
   "source": [
    "### 2.10.7. OrderBy - Sorting Data\n",
    "\n",
    "The `orderBy()` or `sort()` operations sort DataFrame rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1971b8a0-7d45-4a99-bc80-bf0ac0ba45a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sort by single column (ascending)\n",
    "customers_sorted_asc = customers_df.orderBy(\"registration_date\")\n",
    "display(customers_sorted_asc.select(\"customer_id\", \"first_name\", \"last_name\", \"registration_date\").limit(5))\n",
    "\n",
    "# Sort by multiple columns with different directions\n",
    "customers_sorted_multi = customers_df.orderBy(asc(\"country\"), desc(\"registration_date\"))\n",
    "display(customers_sorted_multi.select(\"customer_id\", \"first_name\", \"country\", \"registration_date\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bc7be71-ad70-445b-bace-b28ad05e0e91",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Filtering Section Header"
    }
   },
   "source": [
    "## 2.11. Filtering Data\n",
    "\n",
    "Filtering allows you to select rows that meet specific conditions.\n",
    "\n",
    "### 2.11.1. Simple Filter Conditions\n",
    "\n",
    "Use `filter()` or `where()` (they are equivalent) to apply conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67050782-7839-4ed8-983d-f575172e8b28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter by country\n",
    "usa_customers = customers_auto_df.filter(col(\"country\") == \"Texas\")\n",
    "display(usa_customers.select(\"customer_id\", \"first_name\", \"last_name\", \"country\").limit(5))\n",
    "\n",
    "# Filter using where (equivalent to filter)\n",
    "nyc_customers = customers_auto_df.where(col(\"city\") == \"New York\")\n",
    "display(nyc_customers.select(\"customer_id\", \"first_name\", \"city\", \"country\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e76ad1c8-597a-4de8-a38f-1a12b4412c44",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Multiple Conditions Header"
    }
   },
   "source": [
    "### 2.11.2. Multiple Conditions\n",
    "\n",
    "Combine conditions using `&` (AND) and `|` (OR). Remember to wrap each condition in parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1843d1d-8a85-43d6-8136-4b93127ad329",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# AND condition\n",
    "usa_2023 = customers_df.filter(\n",
    "    (col(\"country\") == \"USA\") & (year(col(\"registration_date\")) == 2023)\n",
    ")\n",
    "display(usa_2023.select(\"customer_id\", \"first_name\", \"country\", \"registration_date\").limit(5))\n",
    "\n",
    "# OR condition\n",
    "usa_or_uk = customers_df.filter(\n",
    "    (col(\"country\") == \"USA\") | (col(\"country\") == \"UK\")\n",
    ")\n",
    "display(usa_or_uk.select(\"customer_id\", \"first_name\", \"country\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6cbe8c5-d5b2-4a3b-baec-e0685570aaca",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "isin() Header"
    }
   },
   "source": [
    "### 2.11.3. isin() - Filtering by List of Values\n",
    "\n",
    "The `isin()` method checks if a column value is in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cab9015-fca4-463b-b9fa-9431f5e6d416",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "isin() Examples"
    }
   },
   "outputs": [],
   "source": [
    "# Filter by list of countries\n",
    "selected_countries = [\"USA\", \"UK\", \"Germany\", \"France\"]\n",
    "customers_selected_countries = customers_df.filter(col(\"country\").isin(selected_countries))\n",
    "\n",
    "# Show distribution by country\n",
    "display(customers_selected_countries.groupBy(\"country\").count().orderBy(desc(\"count\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "758bbc53-ffdd-40bc-b313-889f30ec896f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Null Handling Header"
    }
   },
   "source": [
    "### 2.11.4. Null Handling\n",
    "\n",
    "Use `isNull()` and `isNotNull()` to filter based on null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17532cae-1dd3-41e2-a100-290329bc4e65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter rows where email is NOT null\n",
    "customers_with_email = customers_df.filter(col(\"email\").isNotNull())\n",
    "\n",
    "# Filter rows where city IS null\n",
    "customers_no_city = customers_df.filter(col(\"city\").isNull())\n",
    "if customers_no_city.count() > 0:\n",
    "    display(customers_no_city.select(\"customer_id\", \"first_name\", \"city\", \"country\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c481ab17-70d3-468d-8584-6537fcd40630",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "String Operations Header"
    }
   },
   "source": [
    "### 2.11.5. String Operations\n",
    "\n",
    "Use string functions like `like()`, `contains()`, `startswith()`, and `endswith()` for pattern matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "705c602b-77ed-405a-92f6-94ba941597a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter using like (SQL-style pattern matching)\n",
    "gmail_customers = customers_df.filter(col(\"email\").like(\"%@gmail.com\"))\n",
    "display(gmail_customers.select(\"customer_id\", \"first_name\", \"email\").limit(5))\n",
    "\n",
    "# Filter using contains\n",
    "new_cities = customers_df.filter(col(\"city\").contains(\"New\"))\n",
    "display(new_cities.select(\"customer_id\", \"first_name\", \"city\").limit(5))\n",
    "\n",
    "# Filter using startswith\n",
    "j_names = customers_df.filter(col(\"first_name\").startswith(\"J\"))\n",
    "display(j_names.select(\"customer_id\", \"first_name\", \"last_name\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80b4c33f-a5e8-415c-9376-99f532a5087f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Aggregations Section Header"
    }
   },
   "source": [
    "## 2.12. Aggregations and Grouping\n",
    "\n",
    "Aggregations allow you to compute summary statistics on your data.\n",
    "\n",
    "### 2.12.1. Basic Aggregations\n",
    "\n",
    "Use `groupBy()` with aggregation functions like `count()`, `sum()`, `avg()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b747ce1-c590-46ea-ae32-1d7bb0b3641b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Aggregation - Count by Country"
    }
   },
   "outputs": [],
   "source": [
    "# Count by country\n",
    "customers_by_country = customers_df.groupBy(\"country\").count().orderBy(desc(\"count\"))\n",
    "display(customers_by_country.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a845f7c8-a15a-4f69-a0d3-da7d3b3c2100",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Aggregation - Revenue by Payment"
    }
   },
   "outputs": [],
   "source": [
    "# Sum and average on orders\n",
    "revenue_by_payment = orders_df.groupBy(\"payment_method\").agg(\n",
    "    sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "    avg(\"total_amount\").alias(\"avg_order_value\"),\n",
    "    count(\"*\").alias(\"order_count\")\n",
    ").orderBy(desc(\"total_revenue\"))\n",
    "\n",
    "display(revenue_by_payment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38e785f5-094d-42ee-9e18-2e9a89405d14",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Min/Max Header"
    }
   },
   "source": [
    "### 2.12.2. Min/Max Aggregations\n",
    "\n",
    "Find minimum and maximum values in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee0468e5-bcae-4bde-a746-e11d9beab731",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Aggregation - Min/Max Stats"
    }
   },
   "outputs": [],
   "source": [
    "# Min and max order amounts\n",
    "order_stats = orders_df.agg(\n",
    "    min(\"total_amount\").alias(\"min_amount\"),\n",
    "    max(\"total_amount\").alias(\"max_amount\"),\n",
    "    avg(\"total_amount\").alias(\"avg_amount\")\n",
    ")\n",
    "display(order_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eacbd31c-cf05-45fb-a647-8f7eda3a1713",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Multiple Aggregations Header"
    }
   },
   "source": [
    "### 2.12.3. Multiple Aggregations with agg()\n",
    "\n",
    "Use `agg()` to apply multiple aggregation functions at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "942abb37-7e74-46dc-81d1-d7b4902bff7a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Multiple Aggregations Examples"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Multiple aggregations on orders by customer\n",
    "customer_stats = orders_df.groupBy(\"customer_id\").agg(\n",
    "    count(\"*\").alias(\"total_orders\"),\n",
    "    sum(\"total_amount\").alias(\"total_spent\"),\n",
    "    avg(\"total_amount\").alias(\"avg_order_value\"),\n",
    "    min(\"total_amount\").alias(\"min_order\"),\n",
    "    max(\"total_amount\").alias(\"max_order\")\n",
    ").orderBy(desc(\"total_spent\"))\n",
    "\n",
    "display(customer_stats.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba5b7611-5a85-426f-8ed6-f01edea8d408",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Having Clause Header"
    }
   },
   "source": [
    "### 2.12.4. Having Clause (Filter After Aggregation)\n",
    "\n",
    "In Spark, use `filter()` after `groupBy()` to implement SQL's HAVING clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41bdf5d0-ef44-42d1-b18f-a31c6dbfbd2d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Having - High Frequency Customers"
    }
   },
   "outputs": [],
   "source": [
    "# Customers with more than 5 orders\n",
    "high_frequency = orders_df.groupBy(\"customer_id\").agg(\n",
    "    count(\"*\").alias(\"order_count\"),\n",
    "    sum(\"total_amount\").alias(\"total_spent\")\n",
    ").filter(col(\"order_count\") > 5).orderBy(desc(\"order_count\"))\n",
    "\n",
    "display(high_frequency.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9480eac2-231f-40c0-806a-c9bfcfe45cf4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Temp Views Section Header"
    }
   },
   "source": [
    "## 2.13. Temporary Views & SQL\n",
    "\n",
    "Bridge between DataFrame API and SQL by creating temporary views.\n",
    "\n",
    "### 2.13.1. Creating Temporary Views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25f0611e-6c71-417d-b7fd-4b55010d62ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Types of Views in Databricks (Exam Topic):**\n",
    "\n",
    "| View Type | Scope | Persistence | Syntax |\n",
    "|-----------|-------|-------------|--------|\n",
    "| Temp View | Current SparkSession | Session only | `CREATE TEMP VIEW` |\n",
    "| Global Temp View | All SparkSessions in cluster | Until cluster restart | `CREATE GLOBAL TEMP VIEW` (access via `global_temp.name`) |\n",
    "| Permanent View | Unity Catalog | Persistent | `CREATE VIEW catalog.schema.name` |\n",
    "\n",
    "**Key Exam Points:**\n",
    "- Temp views are **not visible** across notebooks\n",
    "- Global temp views use special schema `global_temp`\n",
    "- Permanent views are stored in Unity Catalog with full governance\n",
    "- Views do NOT store data - they store the query definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7358a04-9c35-4746-b379-0be9ff929f1d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Temp Views"
    }
   },
   "outputs": [],
   "source": [
    "# Create temporary views from our DataFrames\n",
    "\n",
    "customers_df.createOrReplaceTempView(\"customers\")\n",
    "orders_df.createOrReplaceTempView(\"orders\")\n",
    "products_df.createOrReplaceTempView(\"products\")\n",
    "\n",
    "print(\"\\These views are available for SQL queries in this session.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e07705b-218e-4522-ba14-308234e1f13c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SQL Query Examples Header"
    }
   },
   "source": [
    "### 2.13.2. Running SQL Queries\n",
    "\n",
    "Use `spark.sql()` to run SQL queries against temporary views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "830d9f7d-d7eb-4639-82d6-7fb021ae4bcc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SQL Query Examples"
    }
   },
   "outputs": [],
   "source": [
    "# Simple SELECT query\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        COUNT(*) as order_count,\n",
    "        SUM(total_amount) as total_spent\n",
    "    FROM orders\n",
    "    GROUP BY customer_id\n",
    "    ORDER BY order_count DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae368127-a06a-497d-a9de-e454fe930a07",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "API vs SQL Header"
    }
   },
   "source": [
    "### 2.13.3. DataFrame API vs SQL - Same Result, Different Syntax\n",
    "\n",
    "Compare how the same operation looks in DataFrame API and SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "707d61a2-19a2-404f-9cf5-596991de1405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Same query - two approaches\n",
    "\n",
    "# DataFrame API\n",
    "df_api_result = customers_df  \\\n",
    "    .groupBy(\"city\") \\\n",
    "    .count() \\\n",
    "    .orderBy(desc(\"count\")) \\\n",
    "    .limit(5)\n",
    "\n",
    "display(df_api_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52784051-d3d6-49f7-983e-d66b18827ff8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "API vs SQL Examples"
    }
   },
   "outputs": [],
   "source": [
    "# SQL\n",
    "sql_result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        city,\n",
    "        COUNT(*) as count\n",
    "    FROM customers\n",
    "    GROUP BY city\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "display(sql_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9651542e-5e1b-4a3b-b72e-9e157006f5e4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Global Temp View Header"
    }
   },
   "source": [
    "### 2.13.4. Global Temporary Views\n",
    "\n",
    "Global temp views are accessible across different notebooks on the same cluster (across SparkSessions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9346d9c0-3d2d-4160-91bf-6fdc3e1f3a73",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Global Temp View"
    }
   },
   "outputs": [],
   "source": [
    "# Create global temporary view\n",
    "customers_df.createOrReplaceGlobalTempView(\"global_customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ed1aca4-1221-4815-8af3-5e3726e25813",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Query Global Temp View"
    }
   },
   "outputs": [],
   "source": [
    "# Query global temp view\n",
    "global_result = spark.sql(\"\"\"\n",
    "    SELECT country, COUNT(*) as count\n",
    "    FROM global_temp.global_customers\n",
    "    GROUP BY country\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "display(global_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42f12c72-950f-427c-8c50-b65369d57a86",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Complex SQL Header"
    }
   },
   "source": [
    "### 2.13.5. Complex SQL Queries\n",
    "\n",
    "Combine multiple operations in SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e988b0ed-788a-4a83-b5c8-cb0412d1f3f7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Complex SQL Examples"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Complex query with JOIN and aggregation\n",
    "\n",
    "complex_query = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        c.customer_id,\n",
    "        c.first_name,\n",
    "        c.last_name,\n",
    "        c.country,\n",
    "        COUNT(o.order_id) as total_orders,\n",
    "        SUM(o.total_amount) as total_spent,\n",
    "        AVG(o.total_amount) as avg_order_value,\n",
    "        MAX(o.total_amount) as largest_order\n",
    "    FROM customers c\n",
    "    LEFT JOIN orders o ON c.customer_id = try_cast(o.customer_id AS STRING)\n",
    "    GROUP BY c.customer_id, c.first_name, c.last_name, c.country\n",
    "    HAVING COUNT(o.order_id) > 0\n",
    "    ORDER BY total_spent DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "display(complex_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e98e047-e1e4-4d9f-991e-f0509a698aee",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SQL Best Practices Note"
    }
   },
   "source": [
    "### 2.13.6. When to Use SQL vs DataFrame API?\n",
    "\n",
    "**Use SQL when:**\n",
    "* Team is more familiar with SQL\n",
    "* Complex queries with multiple JOINs\n",
    "* Ad-hoc analysis and exploration\n",
    "\n",
    "**Use DataFrame API when:**\n",
    "* Building reusable data pipelines\n",
    "* Need type safety and compile-time checks\n",
    "* Complex transformations with custom logic\n",
    "* Better IDE support and autocomplete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90caa8a9-a806-41e7-a292-d1527cdd543b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "JSON Operations Header"
    }
   },
   "source": [
    "## 2.14. JSON Operations\n",
    "\n",
    "Working with semi-structured JSON data requires special operations.\n",
    "\n",
    "### 2.14.1. Explode - Flattening Arrays\n",
    "\n",
    "The `explode()` function creates a new row for each element in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8416de6-f130-4465-a0e9-15428cdca3a9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Explode Example Setup"
    }
   },
   "outputs": [],
   "source": [
    "# Create sample data with arrays to demonstrate explode\n",
    "sample_data = [\n",
    "    (1, \"Customer A\", [\"product_1\", \"product_2\", \"product_3\"]),\n",
    "    (2, \"Customer B\", [\"product_1\"]),\n",
    "    (3, \"Customer C\", []),  # Empty array\n",
    "    (4, \"Customer D\", None)  # Null array\n",
    "]\n",
    "\n",
    "sample_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"customer_name\", StringType(), False),\n",
    "    StructField(\"purchased_products\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "sample_df = spark.createDataFrame(sample_data, schema=sample_schema)\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdadfc86-e99f-4a4a-857f-abd3c6aaad88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# explode() - skips null and empty arrays\n",
    "exploded_df = sample_df.select(\n",
    "    \"customer_id\",\n",
    "    \"customer_name\",\n",
    "    explode(\"purchased_products\").alias(\"product\")\n",
    ")\n",
    "display(exploded_df)\n",
    "\n",
    "# explode_outer() - keeps null and empty arrays\n",
    "exploded_outer_df = sample_df.select(\n",
    "    \"customer_id\",\n",
    "    \"customer_name\",\n",
    "    explode_outer(\"purchased_products\").alias(\"product\")\n",
    ")\n",
    "display(exploded_outer_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "030b26b0-7d94-418b-b410-bb88b211f7f8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Nested JSON Header"
    }
   },
   "source": [
    "### 2.14.2. Nested JSON Structures\n",
    "\n",
    "Access nested fields using dot notation or `getField()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f47c665c-097e-45df-b8af-5167cbdf3f06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create sample data with nested structures\n",
    "nested_data = customers_df.select(\n",
    "    col(\"customer_id\"),\n",
    "    struct(\n",
    "        col(\"first_name\"),\n",
    "        col(\"last_name\"),\n",
    "        col(\"email\")\n",
    "    ).alias(\"personal_info\"),\n",
    "    struct(\n",
    "        col(\"city\"),\n",
    "        col(\"country\")\n",
    "    ).alias(\"location\")\n",
    ")\n",
    "\n",
    "nested_data.printSchema()\n",
    "display(nested_data.limit(3))\n",
    "\n",
    "# Access nested fields\n",
    "flattened = nested_data.select(\n",
    "    \"customer_id\",\n",
    "    col(\"personal_info.first_name\").alias(\"first_name\"),\n",
    "    col(\"personal_info.email\").alias(\"email\"),\n",
    "    col(\"location.country\").alias(\"country\")\n",
    ")\n",
    "display(flattened.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7621b1e-76b8-4a44-a5eb-1775fd5c3370",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "JSON String Parsing Header"
    }
   },
   "source": [
    "### 2.14.3. Parsing JSON Strings\n",
    "\n",
    "Use `get_json_object()` or `from_json()` to parse JSON stored as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76fca5e0-6677-49cf-9d40-b83bfb1adc67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create sample data with JSON strings\n",
    "json_string_data = [\n",
    "    (1, '{\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}'),\n",
    "    (2, '{\"name\": \"Jane\", \"age\": 25, \"city\": \"London\"}'),\n",
    "    (3, '{\"name\": \"Bob\", \"age\": 35, \"city\": \"Paris\"}')\n",
    "]\n",
    "\n",
    "json_df = spark.createDataFrame(json_string_data, [\"id\", \"json_data\"])\n",
    "display(json_df)\n",
    "\n",
    "# Extract fields using get_json_object\n",
    "parsed_df = json_df.select(\n",
    "    \"id\",\n",
    "    get_json_object(\"json_data\", \"$.name\").alias(\"name\"),\n",
    "    get_json_object(\"json_data\", \"$.age\").alias(\"age\"),\n",
    "    get_json_object(\"json_data\", \"$.city\").alias(\"city\")\n",
    ")\n",
    "display(parsed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c863689-e99b-4d26-91fc-459d99e6784a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Joins Section Header"
    }
   },
   "source": [
    "## 2.15. Joins - Combining Datasets\n",
    "\n",
    "Joins allow you to combine data from multiple DataFrames based on common keys.\n",
    "\n",
    "### 2.15.1. Inner Join\n",
    "\n",
    "Inner join returns only matching rows from both DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e157b941-ba44-4f4c-a980-65f8a6c0ce56",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Inner Join Example"
    }
   },
   "outputs": [],
   "source": [
    "# Inner join - customers with their orders\n",
    "\n",
    "customers_with_orders = customers_df.join(\n",
    "    orders_df,\n",
    "    customers_df.customer_id == orders_df.customer_id,\n",
    "    \"inner\"\n",
    ").select(\n",
    "    customers_df.customer_id,\n",
    "    customers_df.first_name,\n",
    "    customers_df.last_name,\n",
    "    orders_df.order_id,\n",
    "    orders_df.total_amount,\n",
    "    orders_df.payment_method\n",
    ")\n",
    "\n",
    "display(customers_with_orders.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c991c155-59fa-471b-858f-fd615092d685",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Left Join Header"
    }
   },
   "source": [
    "### 2.15.2. Left Join\n",
    "\n",
    "Left join returns all rows from the left DataFrame and matching rows from the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e221573-16ac-4469-9d72-ef990233ae76",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Left Join Example"
    }
   },
   "outputs": [],
   "source": [
    "# Left join - all orders with customer details (if available)\n",
    "\n",
    "orders_with_customers = orders_df.join(\n",
    "    customers_df,\n",
    "    orders_df['customer_id'] == customers_df['customer_id'],\n",
    "    \"left\"\n",
    ").select(\n",
    "    orders_df.order_id,\n",
    "    orders_df.customer_id,\n",
    "    customers_df.first_name,\n",
    "    customers_df.last_name,\n",
    "    orders_df.total_amount\n",
    ")\n",
    "\n",
    "display(orders_with_customers.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e24acfa7-998d-4e6f-b31a-986256fbb385",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Right Join Header"
    }
   },
   "source": [
    "### 2.15.3. Right Join\n",
    "\n",
    "Right join returns all rows from the right DataFrame and matching rows from the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72f7d9b0-c21d-46cc-a5dd-371e70540eca",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Right Join Example"
    }
   },
   "outputs": [],
   "source": [
    "# Right join - all customers with their orders (if any)\n",
    "\n",
    "customers_with_orders = orders_df.join(\n",
    "    customers_df,\n",
    "    orders_df['customer_id'] == customers_df['customer_id'],\n",
    "    \"right\"\n",
    ").select(\n",
    "    customers_df.customer_id,\n",
    "    customers_df.first_name,\n",
    "    customers_df.last_name,\n",
    "    orders_df.order_id,\n",
    "    orders_df.total_amount\n",
    ")\n",
    "\n",
    "display(customers_with_orders.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b345bd9f-c785-448e-bc17-baa38b10e239",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Full Outer Join Header"
    }
   },
   "source": [
    "### 2.15.4. Full Outer Join\n",
    "\n",
    "Full outer join returns all rows from both DataFrames, with nulls where there's no match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f282a05f-aed1-484f-85d0-d130609d6d90",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Full Outer Join Example"
    }
   },
   "outputs": [],
   "source": [
    "# Full outer join - all customers and all orders\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "full_join = customers_df.join(\n",
    "    orders_df,\n",
    "    customers_df.customer_id == orders_df.customer_id,\n",
    "    \"outer\"\n",
    ").select(\n",
    "    customers_df.customer_id.alias(\"cust_id\"),\n",
    "    orders_df.customer_id.alias(\"order_cust_id\"),\n",
    "    customers_df.first_name,\n",
    "    col(\"order_id\").alias(\"order_id\"),\n",
    "    orders_df.total_amount\n",
    ")\n",
    "\n",
    "print(\"\\nSample with potential nulls:\")\n",
    "display(full_join.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ee81e10-ddc8-4ba9-a1a2-4ece939f04c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.16. read_files() - Unity Catalog Native Reader\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "`read_files()` is a Databricks SQL function that reads files from Volumes or cloud storage.\n",
    "\n",
    "| Feature | `read_files()` | `spark.read` |\n",
    "|---------|---------------|-------------|\n",
    "| Language | SQL | Python |\n",
    "| Schema inference | Automatic | Manual or `inferSchema` |\n",
    "| UC integration | Native | Via path |\n",
    "| Use case | SQL-first workflows | Programmatic workflows |\n",
    "\n",
    "```sql\n",
    "-- Read CSV from a Volume\n",
    "SELECT * FROM read_files(\n",
    "  '/Volumes/catalog/schema/volume/customers.csv',\n",
    "  format => 'csv',\n",
    "  header => true\n",
    ");\n",
    "\n",
    "-- Create table from files\n",
    "CREATE TABLE bronze.customers AS\n",
    "SELECT * FROM read_files('/Volumes/catalog/schema/volume/customers.csv');\n",
    "```\n",
    "\n",
    "**Exam Note:** `read_files()` is the recommended way to read files in SQL workflows with Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbcde0e4-5fff-4b55-b9fb-1e60a8764ced",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--Read CSV from a real Unity Catalog Volume using Spark DataFrame API\n",
    "\n",
    "-- Read CSV from a Volume\n",
    "SELECT * FROM read_files(\n",
    "  CUSTOMERS_CSV,\n",
    "  format => 'csv',\n",
    "  header => true\n",
    ");\n",
    "\n",
    "-- Create table from files\n",
    "CREATE TABLE bronze.customers AS\n",
    "SELECT * FROM read_files(CUSTOMERS_CSV);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31a35b3a-acb1-456a-b4b8-142472c9cec8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37afcf22-f2ca-4357-9191-f612b098b4fb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Final Summary"
    }
   },
   "source": [
    "## 2.17. Summary\n",
    "\n",
    "In this comprehensive notebook you learned:\n",
    "\n",
    "**Data Ingestion (Sections 2.4-2.9)**\n",
    "* Loading CSV with inferSchema vs manual schema\n",
    "* Loading JSON with automatic schema detection\n",
    "* Loading Parquet (built-in schema)\n",
    "* Loading Excel using spark-excel library\n",
    "* Performance comparison and best practices\n",
    "\n",
    "**DataFrame Transformations (Section 2.10)**\n",
    "* `select()` - choosing columns\n",
    "* `withColumn()` - adding/modifying columns\n",
    "* `cast()` - type conversion\n",
    "* `withColumnRenamed()` - renaming columns\n",
    "* `drop()` - removing columns\n",
    "* `distinct()` and `dropDuplicates()` - unique rows\n",
    "* `orderBy()` - sorting data\n",
    "\n",
    "**Filtering Data (Section 2.11)**\n",
    "* Simple filter conditions with `filter()` and `where()`\n",
    "* Multiple conditions with `&` (AND) and `|` (OR)\n",
    "* `isin()` - filtering by list of values\n",
    "* `isNull()` and `isNotNull()` - null handling\n",
    "* String operations: `like()`, `contains()`, `startswith()`\n",
    "\n",
    "**Aggregations (Section 2.12)**\n",
    "* `groupBy()` with `count()`, `sum()`, `avg()`\n",
    "* `min()` and `max()` aggregations\n",
    "* Multiple aggregations with `agg()`\n",
    "* HAVING clause equivalent with `filter()` after groupBy\n",
    "\n",
    "**Temporary Views & SQL (Section 2.13)**\n",
    "* Creating temp views with `createOrReplaceTempView()`\n",
    "* Running SQL queries with `spark.sql()`\n",
    "* DataFrame API vs SQL comparison\n",
    "* Global temporary views\n",
    "* Complex SQL queries with JOINs\n",
    "\n",
    "**JSON Operations (Section 2.14)**\n",
    "* `explode()` and `explode_outer()` for arrays\n",
    "* Accessing nested structures with dot notation\n",
    "* `struct()` for creating nested structures\n",
    "* `get_json_object()` for parsing JSON strings\n",
    "\n",
    "**Joins (Section 2.15)**\n",
    "* Inner join - matching rows only\n",
    "* Left join - all left + matching right\n",
    "* Right join - all right + matching left\n",
    "* Full outer join - all rows from both\n",
    "* Joining with extended data sources\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Schema Definition**: Use explicit schemas in production for performance and data quality\n",
    "2. **Format Selection**: CSV for compatibility, Parquet for performance, JSON for flexibility\n",
    "3. **Transformations**: Chain operations for readable, maintainable code\n",
    "4. **SQL Bridge**: Use temp views to leverage SQL when appropriate\n",
    "5. **Joins**: Choose the right join type based on your data requirements\n",
    "\n",
    "**Next Steps**: Apply these techniques to build ingestion pipelines! "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6142587020061574,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "M02_elt_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
