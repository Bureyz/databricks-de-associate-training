{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6bc86c4-58e5-4f4f-af86-f3311937bb70",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Introduction"
    }
   },
   "source": [
    "# M02: ELT Ingestion & Transformations\n",
    "\n",
    "| Exam Domain | Weight |\n",
    "|---|---|\n",
    "| ELT with Spark SQL and Python | 29% |\n",
    "| Incremental Data Processing | 20% |\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "> **Best Practice:** Use `inferSchema=true` only for exploration. In production, define schemas explicitly.\n",
    "\n",
    "| Scenario | With inferSchema | With explicit schema |\n",
    "|---|---|---|\n",
    "| Large file | Scans entire file first | Direct read |\n",
    "| Column `\"123\"` | INT or STRING? | You control it |\n",
    "| New column added | Schema changes silently | Fails fast (good!) |\n",
    "\n",
    "### Bronze Layer Philosophy\n",
    "\n",
    "In Medallion architecture, Bronze layer should:\n",
    "- Keep data as STRING (preserve original values)\n",
    "- Add metadata (ingestion time, source file)\n",
    "- NOT apply business logic\n",
    "- Be idempotent (safe to re-run)\n",
    "\n",
    "### Data Process Patterns\n",
    "\n",
    "| Pattern | Description | Trigger | Use Case |\n",
    "|---|---|---|---|\n",
    "| **Full Load** | Entire dataset reloaded | Batch Read | Small tables |\n",
    "| **Incremental Batch** | Only new/changed data | `Trigger.AvailableNow` | Daily/Hourly ETL |\n",
    "| **Continuous** | Real-time processing | `Trigger.ProcessingTime` | Monitoring, alerts |\n",
    "\n",
    "<img src=\"../../../assets/images/1ba507cd604849878e74e586f4df3559.png\" width=\"800\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a0bd3d0-734d-4aa5-a893-d0e169909870",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ELT vs ETL\n",
    "\n",
    "| Aspect | ETL | ELT |\n",
    "|---|---|---|\n",
    "| Transform location | External engine (before load) | Inside platform (after load) |\n",
    "| Data stored | Only transformed | Raw + transformed |\n",
    "| Flexibility | Fixed schema upfront | Schema-on-read, evolve later |\n",
    "| Databricks | **Not recommended** | **Default pattern** |\n",
    "\n",
    "> **Exam Tip:** Databricks uses ELT: Extract raw data → Load as-is into Bronze (Delta) → Transform in Silver/Gold layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cf6c95b-2145-4ffe-a23d-8118983d4dd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "Running the setup notebook and configuring paths for all data sources used in this module.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea9ed832-63c8-4788-8c7f-256fcf2ac6e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../setup/00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9981cb88-642b-4f84-8a87-ebf5d572e504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Configuration\n",
    "\n",
    "Defining file paths for customers, orders, and products datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f11c5fa-8097-4ebe-89cf-93833a9e6f72",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Notebook Configuration"
    }
   },
   "outputs": [],
   "source": [
    "# Paths to data directories (subdirectories in DATASET_PATH from 00_setup)\n",
    "CUSTOMERS_PATH = f\"{DATASET_PATH}/customers\"\n",
    "ORDERS_PATH = f\"{DATASET_PATH}/orders\"\n",
    "PRODUCTS_PATH = f\"{DATASET_PATH}/products\"\n",
    "\n",
    "# Paths to specific files\n",
    "CUSTOMERS_CSV = f\"{CUSTOMERS_PATH}/customers.csv\"\n",
    "ORDERS_JSON = f\"{ORDERS_PATH}/orders_batch.json\"\n",
    "PRODUCTS_PARQUET = f\"{PRODUCTS_PATH}/products.parquet\"\n",
    "EXCEL_PATH = f\"{DATASET_PATH}/customers/customers_extented.xlsx\"  # Note: filename has typo 'extented'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82bbd0b5-8936-432c-bbab-21e9f46ec084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Imports\n",
    "\n",
    "PySpark types for schema definition and functions for transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e438f20-ee94-4e47-aa8f-85f56d553a45",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "# Import PySpark types for schema definition\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, \n",
    "    DoubleType, TimestampType, DateType, ArrayType\n",
    ")\n",
    "\n",
    "# Import PySpark functions for transformations and aggregations\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, concat, upper, lower, year, month, day,\n",
    "    sum, avg, min, max, count, stddev, desc, asc,\n",
    "    explode, explode_outer, struct, array,\n",
    "    get_json_object, from_json, to_json,\n",
    "    to_date, current_date, datediff\n",
    ")\n",
    "\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "889facc3-12b1-46b1-ac58-004aef6b0971",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## CSV Import (Customers)\n",
    "\n",
    "Loading CSV files into DataFrames using `spark.read` with schema inference and manual schema definition. We'll work with customer data throughout this section.\n",
    "\n",
    "---\n",
    "\n",
    "### Auto Schema Inference\n",
    "\n",
    "Using `inferSchema=true` to let Spark detect column types automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e44f411-bfe5-4692-8552-458c1dcfad50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load CSV data with automatic schema inference\n",
    "customers_auto_df = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")       # First line contains column names\n",
    "    .option(\"inferSchema\", \"true\")  # Automatic data type inference\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93a36f1a-c167-4a78-9da6-56a05d7edc64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify loaded data\n",
    "customers_auto_df.printSchema()\n",
    "display(customers_auto_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10f0f50b-3fa1-452a-85fd-f0df5773f08c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Extended Reader Options\n",
    "\n",
    "Key CSV options for production:\n",
    "\n",
    "| Option | Purpose |\n",
    "|---|---|\n",
    "| `delimiter` | Custom column separator (e.g. `;`) |\n",
    "| `quote` | Character for text fields |\n",
    "| `escape` | Escape special characters |\n",
    "| `mode` | `PERMISSIVE` / `DROPMALFORMED` / `FAILFAST` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5071f0b-5611-4d63-9e21-3f649dae5ee6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Manual Schema Definition\n",
    "\n",
    "Defining an explicit `StructType` schema for type safety and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1cdaa60-b1b1-46de-8604-7ae9aaa4c320",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Manual Schema Definition"
    }
   },
   "outputs": [],
   "source": [
    "# Schema definition for customers\n",
    "# Structure: customer_id (string), first_name (string), last_name (string), email (string), city (string), country (string), registration_date (timestamp)\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), nullable=False),\n",
    "    StructField(\"first_name\", StringType(), nullable=True),\n",
    "    StructField(\"last_name\", StringType(), nullable=True),\n",
    "    StructField(\"city\", StringType(), nullable=True),\n",
    "    StructField(\"email\", StringType(), nullable=True),\n",
    "    StructField(\"country\", StringType(), nullable=True),\n",
    "    StructField(\"registration_date\", TimestampType(), nullable=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bd147be-df3f-4274-8228-bf8cf6a626ba",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load CSV with Manual Schema"
    }
   },
   "outputs": [],
   "source": [
    "# Load CSV data with manually defined schema\n",
    "customers_df = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(customers_schema)\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08683c36-2a50-4b57-a169-c69ec0cac828",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify loaded data\n",
    "customers_df.printSchema()\n",
    "display(customers_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2599e27-eda1-4c27-885d-3e96326d9a6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## JSON Import (Orders)\n",
    "\n",
    "Reading JSON files into DataFrames with automatic and manual schema approaches. JSON supports nested structures natively.\n",
    "\n",
    "---\n",
    "\n",
    "### Auto Schema Inference\n",
    "\n",
    "Letting Spark infer the JSON schema automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "574e1299-3b2b-4ee2-93bb-b2a4e136776e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load JSON with inferSchema"
    }
   },
   "outputs": [],
   "source": [
    "# Load JSON data with automatic schema inference\n",
    "orders_auto_df = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(ORDERS_JSON)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08afea1d-811a-4403-bb19-9ac586c8076e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify loaded data\n",
    "orders_auto_df.printSchema()\n",
    "display(orders_auto_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37d607c2-8aa5-4bbe-ad9c-1af6495d66a6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Manual Schema Header"
    }
   },
   "source": [
    "### Manual Schema Definition\n",
    "\n",
    "Providing an explicit schema to select and type only the columns we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c826f976-b255-4e73-83fc-bcc6b3f5238b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Orders Schema Definition"
    }
   },
   "outputs": [],
   "source": [
    "# Schema definition for orders\n",
    "# Actual structure: order_id, customer_id, product_id, store_id, order_datetime, quantity, unit_price, discount_percent, total_amount, payment_method\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), nullable=True),  # Can be null in data\n",
    "    StructField(\"customer_id\", StringType(), nullable=False),\n",
    "    StructField(\"order_datetime\", StringType(), nullable=True),  # String, will convert to timestamp later\n",
    "    StructField(\"total_amount\", DoubleType(), nullable=False),\n",
    "    StructField(\"payment_method\", StringType(), nullable=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a928b826-9d57-43ae-826a-f50d97830be8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load JSON with Manual Schema"
    }
   },
   "outputs": [],
   "source": [
    "# Load JSON data with manually defined schema\n",
    "orders_df = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .schema(orders_schema)\n",
    "    .load(ORDERS_JSON)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e70b0b80-2f9e-4697-91f8-be7e859cf956",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify loaded data\n",
    "orders_df.printSchema()\n",
    "display(orders_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9f2041e-010b-42d1-b577-0a31eb841d95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Parquet Import (Products)\n",
    "\n",
    "Reading Parquet files — the preferred columnar format in Databricks. Parquet has built-in schema, so no manual definition is needed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "607e760f-8e10-45a7-9341-59f7da46a630",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Parquet"
    }
   },
   "outputs": [],
   "source": [
    "# Parquet already contains built-in schema - no need to define it\n",
    "products_df = (\n",
    "    spark.read\n",
    "    .format(\"parquet\")\n",
    "    .load(PRODUCTS_PARQUET)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "547d42fe-3196-411f-9336-7ee6876030d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify loaded data\n",
    "products_df.printSchema()\n",
    "display(products_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42527a0a-eae2-424d-9a35-7ffb8e88363e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Excel Import Header"
    }
   },
   "source": [
    "## Excel Import (Extended Customers)\n",
    "\n",
    "Databricks Runtime has built-in support for Excel via `spark-excel` library.\n",
    "\n",
    "> **Note:** Requires spark-excel library installed on cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6324e2f4-bcac-438a-9a53-992820bea242",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Excel"
    }
   },
   "outputs": [],
   "source": [
    "# Load Excel file using spark-excel library\n",
    "\n",
    "customers_extended_df = (\n",
    "    spark.read\n",
    "    .format(\"excel\")\n",
    "    .option(\"dataAddress\", \"Arkusz1!D6:M26\")\n",
    "    .option(\"headerRows\", 1)\n",
    "    .load(EXCEL_PATH)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78943e3d-bd75-47a7-90cc-56bd19a77978",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify loaded data\n",
    "customers_extended_df.printSchema()\n",
    "display(customers_extended_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bb09b67-4724-4fbc-8d4d-08cb6dad06c6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Performance Comparison Header"
    }
   },
   "source": [
    "## Performance: inferSchema vs Manual Schema\n",
    "\n",
    "Comparing read performance between automatic schema inference and explicit schema definition to demonstrate why manual schemas are recommended in production.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21c1e752-420b-4f81-99ca-0943eca7cb6d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Benchmark - inferSchema"
    }
   },
   "outputs": [],
   "source": [
    "# Automatic schema inference\n",
    "start_auto = time.time()\n",
    "df_auto = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")\n",
    "count_auto = df_auto.count()  # Action - forces execution\n",
    "time_auto = time.time() - start_auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08647755-e8d0-4613-a186-11cba24c6844",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Benchmark - Manual Schema"
    }
   },
   "outputs": [],
   "source": [
    "# Manual schema definition\n",
    "start_manual = time.time()\n",
    "df_manual = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(customers_schema)\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")\n",
    "count_manual = df_manual.count()  # Action - forces execution\n",
    "time_manual = time.time() - start_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb4d139e-5c55-47d4-9cba-e1f111e4f072",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Benchmark - Comparison"
    }
   },
   "outputs": [],
   "source": [
    "# Comparison\n",
    "speedup = (time_auto - time_manual) / time_auto * 100\n",
    "print(f\"\\nConclusion: Manual schema is faster by {speedup:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "190bf43a-2bda-4128-8b41-9b220af80c45",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DataFrame Transformations Header"
    }
   },
   "source": [
    "## DataFrame Transformations\n",
    "\n",
    "Core PySpark transformations for data engineering: selecting columns, adding/renaming columns, type casting, deduplication, and sorting.\n",
    "\n",
    "---\n",
    "\n",
    "### select — Choosing Columns\n",
    "\n",
    "Projecting specific columns and applying expressions in the `select()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e324d8e-a2c1-4430-9f8e-e61f82b03066",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select specific columns\n",
    "customers_selected = customers_df.select(\"customer_id\", \"first_name\", \"last_name\", \"email\")\n",
    "display(customers_selected.limit(5))\n",
    "\n",
    "# Select with column expressions\n",
    "customers_transformed = customers_df.select(\n",
    "    col(\"customer_id\"),\n",
    "    upper(col(\"first_name\")).alias(\"first_name_upper\"),\n",
    "    col(\"email\")\n",
    ")\n",
    "display(customers_transformed.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfc32c6d-27a6-4c78-986b-32e3dda3e13c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "WithColumn Header"
    }
   },
   "source": [
    "### withColumn — Adding/Modifying Columns\n",
    "\n",
    "Creating new columns or modifying existing ones using expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34d8a0e2-95ae-421d-a475-c22a8680b71c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add a new column\n",
    "customers_with_fullname = customers_df.withColumn(\n",
    "    \"full_name\", \n",
    "    concat(col(\"first_name\"), lit(\" \"), col(\"last_name\"))\n",
    ")\n",
    "display(customers_with_fullname.select(\"customer_id\", \"first_name\", \"last_name\", \"full_name\").limit(5))\n",
    "\n",
    "# Add multiple columns\n",
    "customers_enriched = customers_auto_df \\\n",
    "    .withColumn(\"email_lower\", lower(col(\"email\"))) \\\n",
    "    .withColumn(\"registration_year\", year(col(\"registration_date\")))\n",
    "    \n",
    "display(customers_enriched.select(\"customer_id\", \"email\", \"email_lower\", \"registration_date\", \"registration_year\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1edd3536-4285-49bc-a949-914b67931994",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cast Header"
    }
   },
   "source": [
    "### cast — Type Conversion\n",
    "\n",
    "Converting column data types using `cast()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56d0e66e-8509-4853-8f7b-4f63ab4b9859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cast customer_id to string\n",
    "customers_casted = customers_df.withColumn(\"customer_id_str\", col(\"customer_id\").cast(StringType()))\n",
    "customers_df.select(\"customer_id\").printSchema()\n",
    "customers_casted.select(\"customer_id_str\").printSchema()\n",
    "\n",
    "# Cast timestamp to date\n",
    "customers_date = customers_df.withColumn(\"registration_date_only\", col(\"registration_date\").cast(DateType()))\n",
    "display(customers_date.select(\"customer_id\", \"registration_date\", \"registration_date_only\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b6eace4-a44e-4f1d-9935-c4fb71b141c8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Rename Header"
    }
   },
   "source": [
    "### withColumnRenamed — Renaming\n",
    "\n",
    "Renaming columns using `withColumnRenamed()` or `select()` with `alias()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40fcc840-29c6-413f-bbcd-116ec3545ec1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rename single column\n",
    "customers_renamed = customers_df.withColumnRenamed(\"customer_id\", \"id\")\n",
    "display(customers_renamed.limit(5))\n",
    "\n",
    "# Rename multiple columns using select with alias\n",
    "customers_multi_renamed = customers_df.select(\n",
    "    col(\"customer_id\").alias(\"id\"),\n",
    "    col(\"first_name\").alias(\"fname\"),\n",
    "    col(\"last_name\").alias(\"lname\"),\n",
    "    col(\"email\"),\n",
    "    col(\"city\")\n",
    ")\n",
    "display(customers_multi_renamed.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63fed83c-d1ce-4adb-8cf9-77c03a2b8342",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Drop Header"
    }
   },
   "source": [
    "### drop — Removing Columns\n",
    "\n",
    "Removing unwanted columns from a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b523e0a-5fb2-442e-814c-8df55c51097b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop single column\n",
    "customers_dropped = customers_df.drop(\"email\")\n",
    "\n",
    "# Drop multiple columns\n",
    "customers_minimal = customers_df.drop(\"email\", \"city\", \"country\")\n",
    "display(customers_minimal.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d32f758d-5440-4a12-8bb1-85cc77bf11de",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Distinct Header"
    }
   },
   "source": [
    "### distinct / dropDuplicates — Unique Rows\n",
    "\n",
    "Removing duplicate rows entirely or based on specific columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99034ded-d4a2-46d0-94eb-948ee2cc3fc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get distinct countries\n",
    "distinct_countries = customers_df.select(\"country\").distinct()\n",
    "display(distinct_countries.orderBy(\"country\"))\n",
    "\n",
    "# Drop duplicates based on specific columns\n",
    "unique_locations = customers_df.select(\"city\", \"country\").dropDuplicates([\"city\", \"country\"])\n",
    "display(unique_locations.orderBy(\"country\", \"city\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "340a5de4-d9a8-4861-a1e2-02030eb07a32",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "OrderBy Header"
    }
   },
   "source": [
    "### orderBy — Sorting\n",
    "\n",
    "Sorting rows by one or more columns in ascending or descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1971b8a0-7d45-4a99-bc80-bf0ac0ba45a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sort by single column (ascending)\n",
    "customers_sorted_asc = customers_df.orderBy(\"registration_date\")\n",
    "display(customers_sorted_asc.select(\"customer_id\", \"first_name\", \"last_name\", \"registration_date\").limit(5))\n",
    "\n",
    "# Sort by multiple columns with different directions\n",
    "customers_sorted_multi = customers_df.orderBy(asc(\"country\"), desc(\"registration_date\"))\n",
    "display(customers_sorted_multi.select(\"customer_id\", \"first_name\", \"country\", \"registration_date\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bc7be71-ad70-445b-bace-b28ad05e0e91",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Filtering Section Header"
    }
   },
   "source": [
    "## Filtering Data\n",
    "\n",
    "Filtering rows using conditions, multiple predicates, `isin()`, null handling, and string pattern matching.\n",
    "\n",
    "---\n",
    "\n",
    "### Simple Conditions\n",
    "\n",
    "`filter()` and `where()` are equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67050782-7839-4ed8-983d-f575172e8b28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter by country\n",
    "usa_customers = customers_auto_df.filter(col(\"country\") == \"Texas\")\n",
    "display(usa_customers.select(\"customer_id\", \"first_name\", \"last_name\", \"country\").limit(5))\n",
    "\n",
    "# Filter using where (equivalent to filter)\n",
    "nyc_customers = customers_auto_df.where(col(\"city\") == \"New York\")\n",
    "display(nyc_customers.select(\"customer_id\", \"first_name\", \"city\", \"country\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e76ad1c8-597a-4de8-a38f-1a12b4412c44",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Multiple Conditions Header"
    }
   },
   "source": [
    "### Multiple Conditions (`&` = AND, `|` = OR)\n",
    "\n",
    "Combining conditions with logical operators — remember to wrap each condition in parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1843d1d-8a85-43d6-8136-4b93127ad329",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# AND condition\n",
    "usa_2023 = customers_df.filter(\n",
    "    (col(\"country\") == \"USA\") & (year(col(\"registration_date\")) == 2023)\n",
    ")\n",
    "display(usa_2023.select(\"customer_id\", \"first_name\", \"country\", \"registration_date\").limit(5))\n",
    "\n",
    "# OR condition\n",
    "usa_or_uk = customers_df.filter(\n",
    "    (col(\"country\") == \"USA\") | (col(\"country\") == \"UK\")\n",
    ")\n",
    "display(usa_or_uk.select(\"customer_id\", \"first_name\", \"country\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6cbe8c5-d5b2-4a3b-baec-e0685570aaca",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "isin() Header"
    }
   },
   "source": [
    "### isin — Filter by List\n",
    "\n",
    "Filtering rows where a column value matches any item in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cab9015-fca4-463b-b9fa-9431f5e6d416",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "isin() Examples"
    }
   },
   "outputs": [],
   "source": [
    "# Filter by list of countries\n",
    "selected_countries = [\"USA\", \"UK\", \"Germany\", \"France\"]\n",
    "customers_selected_countries = customers_df.filter(col(\"country\").isin(selected_countries))\n",
    "\n",
    "# Show distribution by country\n",
    "display(customers_selected_countries.groupBy(\"country\").count().orderBy(desc(\"count\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "758bbc53-ffdd-40bc-b313-889f30ec896f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Null Handling Header"
    }
   },
   "source": [
    "### Null Handling (`isNull`, `isNotNull`)\n",
    "\n",
    "Filtering rows based on null or non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17532cae-1dd3-41e2-a100-290329bc4e65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter rows where email is NOT null\n",
    "customers_with_email = customers_df.filter(col(\"email\").isNotNull())\n",
    "\n",
    "# Filter rows where city IS null\n",
    "customers_no_city = customers_df.filter(col(\"city\").isNull())\n",
    "if customers_no_city.count() > 0:\n",
    "    display(customers_no_city.select(\"customer_id\", \"first_name\", \"city\", \"country\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c481ab17-70d3-468d-8584-6537fcd40630",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "String Operations Header"
    }
   },
   "source": [
    "### String Operations (`like`, `contains`, `startswith`)\n",
    "\n",
    "Pattern matching and string-based filtering on column values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "705c602b-77ed-405a-92f6-94ba941597a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter using like (SQL-style pattern matching)\n",
    "gmail_customers = customers_df.filter(col(\"email\").like(\"%@gmail.com\"))\n",
    "display(gmail_customers.select(\"customer_id\", \"first_name\", \"email\").limit(5))\n",
    "\n",
    "# Filter using contains\n",
    "new_cities = customers_df.filter(col(\"city\").contains(\"New\"))\n",
    "display(new_cities.select(\"customer_id\", \"first_name\", \"city\").limit(5))\n",
    "\n",
    "# Filter using startswith\n",
    "j_names = customers_df.filter(col(\"first_name\").startswith(\"J\"))\n",
    "display(j_names.select(\"customer_id\", \"first_name\", \"last_name\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80b4c33f-a5e8-415c-9376-99f532a5087f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Aggregations Section Header"
    }
   },
   "source": [
    "## Aggregations\n",
    "\n",
    "Computing summary statistics with `groupBy`, `agg()`, and built-in aggregate functions like `count`, `sum`, `avg`, `min`, and `max`.\n",
    "\n",
    "---\n",
    "\n",
    "### groupBy with count, sum, avg\n",
    "\n",
    "Basic grouping and aggregation operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b747ce1-c590-46ea-ae32-1d7bb0b3641b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Aggregation - Count by Country"
    }
   },
   "outputs": [],
   "source": [
    "# Count by country\n",
    "customers_by_country = customers_df.groupBy(\"country\").count().orderBy(desc(\"count\"))\n",
    "display(customers_by_country.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a845f7c8-a15a-4f69-a0d3-da7d3b3c2100",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Aggregation - Revenue by Payment"
    }
   },
   "outputs": [],
   "source": [
    "# Sum and average on orders\n",
    "revenue_by_payment = orders_df.groupBy(\"payment_method\").agg(\n",
    "    sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "    avg(\"total_amount\").alias(\"avg_order_value\"),\n",
    "    count(\"*\").alias(\"order_count\")\n",
    ").orderBy(desc(\"total_revenue\"))\n",
    "\n",
    "display(revenue_by_payment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38e785f5-094d-42ee-9e18-2e9a89405d14",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Min/Max Header"
    }
   },
   "source": [
    "### min / max\n",
    "\n",
    "Finding minimum and maximum values across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee0468e5-bcae-4bde-a746-e11d9beab731",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Aggregation - Min/Max Stats"
    }
   },
   "outputs": [],
   "source": [
    "# Min and max order amounts\n",
    "order_stats = orders_df.agg(\n",
    "    min(\"total_amount\").alias(\"min_amount\"),\n",
    "    max(\"total_amount\").alias(\"max_amount\"),\n",
    "    avg(\"total_amount\").alias(\"avg_amount\")\n",
    ")\n",
    "display(order_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eacbd31c-cf05-45fb-a647-8f7eda3a1713",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Multiple Aggregations Header"
    }
   },
   "source": [
    "### Multiple Aggregations with agg()\n",
    "\n",
    "Combining several aggregate functions in a single `agg()` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "942abb37-7e74-46dc-81d1-d7b4902bff7a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Multiple Aggregations Examples"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Multiple aggregations on orders by customer\n",
    "customer_stats = orders_df.groupBy(\"customer_id\").agg(\n",
    "    count(\"*\").alias(\"total_orders\"),\n",
    "    sum(\"total_amount\").alias(\"total_spent\"),\n",
    "    avg(\"total_amount\").alias(\"avg_order_value\"),\n",
    "    min(\"total_amount\").alias(\"min_order\"),\n",
    "    max(\"total_amount\").alias(\"max_order\")\n",
    ").orderBy(desc(\"total_spent\"))\n",
    "\n",
    "display(customer_stats.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba5b7611-5a85-426f-8ed6-f01edea8d408",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Having Clause Header"
    }
   },
   "source": [
    "### HAVING Equivalent (filter after groupBy)\n",
    "\n",
    "Applying `filter()` after `groupBy().agg()` to replicate SQL's `HAVING` clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41bdf5d0-ef44-42d1-b18f-a31c6dbfbd2d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Having - High Frequency Customers"
    }
   },
   "outputs": [],
   "source": [
    "# Customers with more than 5 orders\n",
    "high_frequency = orders_df.groupBy(\"customer_id\").agg(\n",
    "    count(\"*\").alias(\"order_count\"),\n",
    "    sum(\"total_amount\").alias(\"total_spent\")\n",
    ").filter(col(\"order_count\") > 5).orderBy(desc(\"order_count\"))\n",
    "\n",
    "display(high_frequency.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9480eac2-231f-40c0-806a-c9bfcfe45cf4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Temp Views Section Header"
    }
   },
   "source": [
    "## Temporary Views & SQL\n",
    "\n",
    "Registering DataFrames as temporary views and querying them with SQL. Covers temp views, global temp views, and the DataFrame API vs SQL equivalence.\n",
    "\n",
    "---\n",
    "\n",
    "### Creating Temporary Views\n",
    "\n",
    "Registering DataFrames as views for SQL access within the current SparkSession."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25f0611e-6c71-417d-b7fd-4b55010d62ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| View Type | Scope | Persistence | Syntax |\n",
    "|---|---|---|---|\n",
    "| **Temp View** | Current SparkSession | Session only | `CREATE TEMP VIEW` |\n",
    "| **Global Temp View** | All sessions on cluster | Until cluster restart | `CREATE GLOBAL TEMP VIEW` → `global_temp.name` |\n",
    "| **Permanent View** | Unity Catalog | Persistent | `CREATE VIEW catalog.schema.name` |\n",
    "\n",
    "> **Exam Tip:** Temp views are NOT visible across notebooks. Views store the query definition, not data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7358a04-9c35-4746-b379-0be9ff929f1d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Temp Views"
    }
   },
   "outputs": [],
   "source": [
    "# Create temporary views from our DataFrames\n",
    "\n",
    "customers_df.createOrReplaceTempView(\"customers\")\n",
    "orders_df.createOrReplaceTempView(\"orders\")\n",
    "products_df.createOrReplaceTempView(\"products\")\n",
    "\n",
    "print(\"\\These views are available for SQL queries in this session.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e07705b-218e-4522-ba14-308234e1f13c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SQL Query Examples Header"
    }
   },
   "source": [
    "### SQL Queries via spark.sql()\n",
    "\n",
    "Executing SQL statements against registered temporary views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "830d9f7d-d7eb-4639-82d6-7fb021ae4bcc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SQL Query Examples"
    }
   },
   "outputs": [],
   "source": [
    "# Simple SELECT query\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        COUNT(*) as order_count,\n",
    "        SUM(total_amount) as total_spent\n",
    "    FROM orders\n",
    "    GROUP BY customer_id\n",
    "    ORDER BY order_count DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae368127-a06a-497d-a9de-e454fe930a07",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "API vs SQL Header"
    }
   },
   "source": [
    "### DataFrame API vs SQL — Same Result\n",
    "\n",
    "Demonstrating that both approaches produce identical results under the same Catalyst optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "707d61a2-19a2-404f-9cf5-596991de1405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Same query - two approaches\n",
    "\n",
    "# DataFrame API\n",
    "df_api_result = customers_df  \\\n",
    "    .groupBy(\"city\") \\\n",
    "    .count() \\\n",
    "    .orderBy(desc(\"count\")) \\\n",
    "    .limit(5)\n",
    "\n",
    "display(df_api_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52784051-d3d6-49f7-983e-d66b18827ff8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "API vs SQL Examples"
    }
   },
   "outputs": [],
   "source": [
    "# SQL\n",
    "sql_result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        city,\n",
    "        COUNT(*) as count\n",
    "    FROM customers\n",
    "    GROUP BY city\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "display(sql_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9651542e-5e1b-4a3b-b72e-9e157006f5e4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Global Temp View Header"
    }
   },
   "source": [
    "### Global Temporary Views\n",
    "\n",
    "Creating views accessible across all SparkSessions on the cluster via `global_temp` schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9346d9c0-3d2d-4160-91bf-6fdc3e1f3a73",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Global Temp View"
    }
   },
   "outputs": [],
   "source": [
    "# Create global temporary view\n",
    "customers_df.createOrReplaceGlobalTempView(\"global_customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ed1aca4-1221-4815-8af3-5e3726e25813",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Query Global Temp View"
    }
   },
   "outputs": [],
   "source": [
    "# Query global temp view\n",
    "global_result = spark.sql(\"\"\"\n",
    "    SELECT country, COUNT(*) as count\n",
    "    FROM global_temp.global_customers\n",
    "    GROUP BY country\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "display(global_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42f12c72-950f-427c-8c50-b65369d57a86",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Complex SQL Header"
    }
   },
   "source": [
    "### Complex SQL (JOINs + Aggregations)\n",
    "\n",
    "Combining JOINs with GROUP BY and HAVING in a single SQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e988b0ed-788a-4a83-b5c8-cb0412d1f3f7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Complex SQL Examples"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Complex query with JOIN and aggregation\n",
    "\n",
    "complex_query = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        c.customer_id,\n",
    "        c.first_name,\n",
    "        c.last_name,\n",
    "        c.country,\n",
    "        COUNT(o.order_id) as total_orders,\n",
    "        SUM(o.total_amount) as total_spent,\n",
    "        AVG(o.total_amount) as avg_order_value,\n",
    "        MAX(o.total_amount) as largest_order\n",
    "    FROM customers c\n",
    "    LEFT JOIN orders o ON c.customer_id = try_cast(o.customer_id AS STRING)\n",
    "    GROUP BY c.customer_id, c.first_name, c.last_name, c.country\n",
    "    HAVING COUNT(o.order_id) > 0\n",
    "    ORDER BY total_spent DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "display(complex_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e98e047-e1e4-4d9f-991e-f0509a698aee",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SQL Best Practices Note"
    }
   },
   "source": [
    "| Criteria | SQL | DataFrame API |\n",
    "|---|---|---|\n",
    "| **Best for** | JOINs, ad-hoc analysis | Pipelines, custom logic |\n",
    "| **Team** | SQL-first, analysts | Engineers, ML |\n",
    "| **IDE support** | Limited | Autocomplete, type safety |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90caa8a9-a806-41e7-a292-d1527cdd543b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "JSON Operations Header"
    }
   },
   "source": [
    "## JSON Operations\n",
    "\n",
    "Working with complex JSON data: flattening arrays with `explode`, accessing nested fields with dot notation, and parsing JSON strings with `get_json_object` and `from_json`.\n",
    "\n",
    "---\n",
    "\n",
    "### explode — Flattening Arrays\n",
    "\n",
    "Converting array elements into individual rows using `explode()` and `explode_outer()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8416de6-f130-4465-a0e9-15428cdca3a9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Explode Example Setup"
    }
   },
   "outputs": [],
   "source": [
    "# Create sample data with arrays to demonstrate explode\n",
    "sample_data = [\n",
    "    (1, \"Customer A\", [\"product_1\", \"product_2\", \"product_3\"]),\n",
    "    (2, \"Customer B\", [\"product_1\"]),\n",
    "    (3, \"Customer C\", []),  # Empty array\n",
    "    (4, \"Customer D\", None)  # Null array\n",
    "]\n",
    "\n",
    "sample_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"customer_name\", StringType(), False),\n",
    "    StructField(\"purchased_products\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "sample_df = spark.createDataFrame(sample_data, schema=sample_schema)\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdadfc86-e99f-4a4a-857f-abd3c6aaad88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# explode() - skips null and empty arrays\n",
    "exploded_df = sample_df.select(\n",
    "    \"customer_id\",\n",
    "    \"customer_name\",\n",
    "    explode(\"purchased_products\").alias(\"product\")\n",
    ")\n",
    "display(exploded_df)\n",
    "\n",
    "# explode_outer() - keeps null and empty arrays\n",
    "exploded_outer_df = sample_df.select(\n",
    "    \"customer_id\",\n",
    "    \"customer_name\",\n",
    "    explode_outer(\"purchased_products\").alias(\"product\")\n",
    ")\n",
    "display(exploded_outer_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "030b26b0-7d94-418b-b410-bb88b211f7f8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Nested JSON Header"
    }
   },
   "source": [
    "### Nested JSON — dot notation\n",
    "\n",
    "Accessing fields within nested structs using `col(\"parent.child\")` syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f47c665c-097e-45df-b8af-5167cbdf3f06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create sample data with nested structures\n",
    "nested_data = customers_df.select(\n",
    "    col(\"customer_id\"),\n",
    "    struct(\n",
    "        col(\"first_name\"),\n",
    "        col(\"last_name\"),\n",
    "        col(\"email\")\n",
    "    ).alias(\"personal_info\"),\n",
    "    struct(\n",
    "        col(\"city\"),\n",
    "        col(\"country\")\n",
    "    ).alias(\"location\")\n",
    ")\n",
    "\n",
    "nested_data.printSchema()\n",
    "display(nested_data.limit(3))\n",
    "\n",
    "# Access nested fields\n",
    "flattened = nested_data.select(\n",
    "    \"customer_id\",\n",
    "    col(\"personal_info.first_name\").alias(\"first_name\"),\n",
    "    col(\"personal_info.email\").alias(\"email\"),\n",
    "    col(\"location.country\").alias(\"country\")\n",
    ")\n",
    "display(flattened.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7621b1e-76b8-4a44-a5eb-1775fd5c3370",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "JSON String Parsing Header"
    }
   },
   "source": [
    "### Parsing JSON Strings (`get_json_object`, `from_json`)\n",
    "\n",
    "Extracting values from JSON-encoded string columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76fca5e0-6677-49cf-9d40-b83bfb1adc67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create sample data with JSON strings\n",
    "json_string_data = [\n",
    "    (1, '{\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}'),\n",
    "    (2, '{\"name\": \"Jane\", \"age\": 25, \"city\": \"London\"}'),\n",
    "    (3, '{\"name\": \"Bob\", \"age\": 35, \"city\": \"Paris\"}')\n",
    "]\n",
    "\n",
    "json_df = spark.createDataFrame(json_string_data, [\"id\", \"json_data\"])\n",
    "display(json_df)\n",
    "\n",
    "# Extract fields using get_json_object\n",
    "parsed_df = json_df.select(\n",
    "    \"id\",\n",
    "    get_json_object(\"json_data\", \"$.name\").alias(\"name\"),\n",
    "    get_json_object(\"json_data\", \"$.age\").alias(\"age\"),\n",
    "    get_json_object(\"json_data\", \"$.city\").alias(\"city\")\n",
    ")\n",
    "display(parsed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c863689-e99b-4d26-91fc-459d99e6784a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Joins Section Header"
    }
   },
   "source": [
    "## Joins\n",
    "\n",
    "Combining DataFrames using different join types: inner, left, right, and full outer. All join types use the same Catalyst optimizer under the hood.\n",
    "\n",
    "---\n",
    "\n",
    "### Inner Join\n",
    "\n",
    "Returning only rows with matching keys in both DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e157b941-ba44-4f4c-a980-65f8a6c0ce56",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Inner Join Example"
    }
   },
   "outputs": [],
   "source": [
    "# Inner join - customers with their orders\n",
    "\n",
    "customers_with_orders = customers_df.join(\n",
    "    orders_df,\n",
    "    customers_df.customer_id == orders_df.customer_id,\n",
    "    \"inner\"\n",
    ").select(\n",
    "    customers_df.customer_id,\n",
    "    customers_df.first_name,\n",
    "    customers_df.last_name,\n",
    "    orders_df.order_id,\n",
    "    orders_df.total_amount,\n",
    "    orders_df.payment_method\n",
    ")\n",
    "\n",
    "display(customers_with_orders.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c991c155-59fa-471b-858f-fd615092d685",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Left Join Header"
    }
   },
   "source": [
    "### Left Join\n",
    "\n",
    "Keeping all rows from the left DataFrame, with nulls where no match exists on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e221573-16ac-4469-9d72-ef990233ae76",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Left Join Example"
    }
   },
   "outputs": [],
   "source": [
    "# Left join - all orders with customer details (if available)\n",
    "\n",
    "orders_with_customers = orders_df.join(\n",
    "    customers_df,\n",
    "    orders_df['customer_id'] == customers_df['customer_id'],\n",
    "    \"left\"\n",
    ").select(\n",
    "    orders_df.order_id,\n",
    "    orders_df.customer_id,\n",
    "    customers_df.first_name,\n",
    "    customers_df.last_name,\n",
    "    orders_df.total_amount\n",
    ")\n",
    "\n",
    "display(orders_with_customers.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e24acfa7-998d-4e6f-b31a-986256fbb385",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Right Join Header"
    }
   },
   "source": [
    "### Right Join\n",
    "\n",
    "Keeping all rows from the right DataFrame, with nulls where no match exists on the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72f7d9b0-c21d-46cc-a5dd-371e70540eca",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Right Join Example"
    }
   },
   "outputs": [],
   "source": [
    "# Right join - all customers with their orders (if any)\n",
    "\n",
    "customers_with_orders = orders_df.join(\n",
    "    customers_df,\n",
    "    orders_df['customer_id'] == customers_df['customer_id'],\n",
    "    \"right\"\n",
    ").select(\n",
    "    customers_df.customer_id,\n",
    "    customers_df.first_name,\n",
    "    customers_df.last_name,\n",
    "    orders_df.order_id,\n",
    "    orders_df.total_amount\n",
    ")\n",
    "\n",
    "display(customers_with_orders.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b345bd9f-c785-448e-bc17-baa38b10e239",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Full Outer Join Header"
    }
   },
   "source": [
    "### Full Outer Join\n",
    "\n",
    "Keeping all rows from both DataFrames, with nulls on either side where no match exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f282a05f-aed1-484f-85d0-d130609d6d90",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Full Outer Join Example"
    }
   },
   "outputs": [],
   "source": [
    "# Full outer join - all customers and all orders\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "full_join = customers_df.join(\n",
    "    orders_df,\n",
    "    customers_df.customer_id == orders_df.customer_id,\n",
    "    \"outer\"\n",
    ").select(\n",
    "    customers_df.customer_id.alias(\"cust_id\"),\n",
    "    orders_df.customer_id.alias(\"order_cust_id\"),\n",
    "    customers_df.first_name,\n",
    "    col(\"order_id\").alias(\"order_id\"),\n",
    "    orders_df.total_amount\n",
    ")\n",
    "\n",
    "print(\"\\nSample with potential nulls:\")\n",
    "display(full_join.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ee81e10-ddc8-4ba9-a1a2-4ece939f04c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## read_files() — Unity Catalog Native Reader\n",
    "\n",
    "| Feature | `read_files()` | `spark.read` |\n",
    "|---|---|---|\n",
    "| Language | SQL | Python |\n",
    "| Schema inference | Automatic | Manual or `inferSchema` |\n",
    "| UC integration | Native | Via path |\n",
    "| Use case | SQL-first workflows | Programmatic pipelines |\n",
    "\n",
    "```sql\n",
    "-- Read CSV from a Volume\n",
    "SELECT * FROM read_files('/Volumes/catalog/schema/volume/file.csv', format => 'csv', header => true);\n",
    "\n",
    "-- Create table from files\n",
    "CREATE TABLE bronze.customers AS SELECT * FROM read_files('/Volumes/.../file.csv');\n",
    "```\n",
    "\n",
    "> **Exam Tip:** `read_files()` is the recommended way to read files in SQL workflows with Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbcde0e4-5fff-4b55-b9fb-1e60a8764ced",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--Read CSV from a real Unity Catalog Volume using Spark DataFrame API\n",
    "\n",
    "-- Read CSV from a Volume\n",
    "SELECT * FROM read_files(\n",
    "  CUSTOMERS_CSV,\n",
    "  format => 'csv',\n",
    "  header => true\n",
    ");\n",
    "\n",
    "-- Create table from files\n",
    "CREATE TABLE bronze.customers AS\n",
    "SELECT * FROM read_files(CUSTOMERS_CSV);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37afcf22-f2ca-4357-9191-f612b098b4fb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Final Summary"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Topic | Key Takeaway |\n",
    "|---|---|\n",
    "| **Schema** | Use explicit schemas in production for performance and data quality |\n",
    "| **Formats** | CSV (compatibility), Parquet (performance), JSON (flexibility) |\n",
    "| **ELT** | Load raw → Bronze, transform in Silver/Gold |\n",
    "| **Transformations** | `select`, `withColumn`, `cast`, `drop`, `distinct`, `orderBy` |\n",
    "| **Filtering** | `filter`/`where`, `isin`, `isNull`, string ops |\n",
    "| **Aggregations** | `groupBy` + `agg()`, HAVING = filter after groupBy |\n",
    "| **Views** | Temp (session), Global Temp (`global_temp.`), Permanent (UC) |\n",
    "| **JSON** | `explode` for arrays, dot notation for nested, `get_json_object` for strings |\n",
    "| **Joins** | inner, left, right, outer — same Catalyst optimizer |\n",
    "| **read_files()** | SQL-native file reader, recommended for UC |\n",
    "\n",
    "---\n",
    "\n",
    "> **← M01: Platform & Workspace | Day 1 | M03: Delta Lake Fundamentals →**"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6142587020061574,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "M02_elt_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
