{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2be4ec58-6240-4699-9bb1-41ba30eb39f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# M03: Delta Lake Fundamentals\n",
    "\n",
    "| Exam Domain | Weight |\n",
    "|---|---|\n",
    "| ELT with Spark SQL and Python | 29% |\n",
    "| Data Governance | 14% |\n",
    "\n",
    "Delta Lake to open-source'owa warstwa storage zapewniająca transakcje ACID, schema enforcement i time travel na plikach Parquet. Jest domyślnym formatem w Databricks. W tym module poznasz operacje CRUD, MERGE INTO, zarządzanie schematem i wersjonowanie danych — kluczowe tematy egzaminacyjne.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b6114e2-3963-40c0-a5e7-6fcc20490568",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b5112c6-2e0a-46fc-9012-b8fd6121053b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../setup/00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eca18c58-7cce-4533-a15c-9efabb4d3d2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45a8db5a-a1d1-44f3-8175-c71f3e42a257",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Display user context\n",
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (CATALOG, BRONZE_SCHEMA, SILVER_SCHEMA, GOLD_SCHEMA)\n",
    "    ], ['catalog', 'bronze_schema', 'silver_schema', 'gold_schema'])\n",
    ")\n",
    "\n",
    "# Set catalog and schema as default\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "126d0f0e-5879-49bd-85c9-b17d1f1406d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Delta Lake Core Features\n",
    "\n",
    "Core capabilities of Delta Lake: ACID transactions, schema enforcement, schema evolution, identity columns, and data quality constraints. These features make Delta the default table format in Databricks.\n",
    "\n",
    "---\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Delta Lake is a transactional layer over Parquet that provides ACID properties (Atomicity, Consistency, Isolation, Durability). Every operation on a Delta table is recorded in the Delta Log - a JSON file containing metadata about changes.\n",
    "\n",
    "<img src=\"../../../assets/images/8850570fe2b147eb86cb690d51bc798c.png\" width=\"800\">\n",
    "\n",
    "\n",
    "**Key Concepts:**\n",
    "- **ACID Transactions**: All operations are atomic and consistent\n",
    "- **Delta Log**: `_delta_log/` folder with JSON files describing each transaction\n",
    "- **Schema Enforcement**: Automatic schema validation - prevents bad data from entering\n",
    "- **Schema Evolution**: Controlled addition of new columns without breaking existing pipelines\n",
    "- **Constraints**: Data quality rules enforced at the table level\n",
    "\n",
    "**Practical Application:**\n",
    "- Transactional updates in Data Lake\n",
    "- Ensuring data quality through schema validation and constraints\n",
    "- Unified data access for batch and streaming workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91b34266-7cb2-4cd7-b0db-1093a0674700",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example: Creating the First Delta Table\n",
    "\n",
    "**Objective:** Demonstration of creating a Delta table and basic properties\n",
    "\n",
    "**Approach:**\n",
    "1. Load data from Unity Catalog Volume\n",
    "2. Create a managed table in Delta format\n",
    "3. Explore Delta Log and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22d24262-f755-4091-81da-fa762eeab226",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load customer data from Unity Catalog Volume\n",
    "customers_df = (spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(f\"{DATASET_PATH}/customers/customers.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dfebe49-72aa-4f32-80d1-4c01c423d776",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{BRONZE_SCHEMA}.customers_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ffd69e6-5ab3-4d32-aa1e-3df1d544c992",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create managed Delta table\n",
    "customers_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5171f8f4-a1d6-40c1-8972-13ed4524a435",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\").limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb7d3d17-008a-4df3-8b2e-e450f88e6b2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write customers_df as an external Delta table to a specified path\n",
    "external_path = f\"{DATASET_PATH}/external/customers_delta\"\n",
    "customers_df.write.format(\"delta\").mode(\"overwrite\").save(external_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "000397e4-a09c-43b7-aad9-c27e4bdcd80c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example: Schema Enforcement in Action\n",
    "\n",
    "**Objective:** Demonstration of automatic schema validation during data insertion\n",
    "\n",
    "\n",
    "\n",
    "Schema Enforcement is a critical feature that prevents \"garbage in, garbage out\" scenarios. Delta Lake compares incoming data schema with the target table schema and **rejects incompatible writes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d731358c-e848-440d-bd5b-ebed7f681687",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check current table schema\n",
    "spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6442520-8462-44a2-9fc4-70656b960f5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Attempt to insert data with mismatched schema (extra/missing columns)\n",
    "invalid_data = spark.createDataFrame([\n",
    "    (\"CUST999999\", \"Test\", \"Customer\", \"invalid_email\", \"+48 123 456 789\",'2025-11-28')  #registration is string instead of date\n",
    "], [\"customer_id\", \"first_name\", \"last_name\", \"email\", \"phone\",\"registration_date\"])\n",
    "\n",
    "print(f\"Schema enforcement in action\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd67faf6-64d4-42a4-963d-008777d68107",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    invalid_data.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "except Exception:\n",
    "    print(\"Schema enforcement prevented the write due to a data type mismatch in the registration_date column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8792332-ad53-4748-844e-56c65110a9ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "invalid_data = invalid_data.withColumn(\"registration_date\", col(\"registration_date\").cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "548dde14-a65f-4078-b589-0f2ffda00298",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "invalid_data.printSchema()\n",
    "display(invalid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d2d5905-bd33-444b-96f7-23a2b7ef0384",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "invalid_data.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73a8fa9f-6c1f-4f34-a943-db63c55c55b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example: Identity and Generated Columns\n",
    "\n",
    "**Objective:** Demonstrate advanced column features - auto-generated surrogate keys and computed columns\n",
    "\n",
    "Delta Lake supports:\n",
    "- **IDENTITY columns**: Auto-incrementing surrogate keys (unique, increasing, but not contiguous)\n",
    "- **GENERATED columns**: Computed columns derived from other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a9c31eb-3356-481e-85c8-5092d7355c59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create table with Identity Column and Generated Column\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {CATALOG}.{BRONZE_SCHEMA}.orders_modern (\n",
    "    order_sk BIGINT GENERATED ALWAYS AS IDENTITY,  -- Surrogate Key\n",
    "    order_id STRING,\n",
    "    total_amount DOUBLE,\n",
    "    order_timestamp TIMESTAMP,\n",
    "    order_date DATE GENERATED ALWAYS AS (CAST(order_timestamp AS DATE)) -- Auto-calculated\n",
    ") USING DELTA\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "532e38c1-e6c0-4823-96eb-fa3facfde917",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> **Note:** In a distributed environment like Databricks (Spark/Delta Lake), `GENERATED ALWAYS AS IDENTITY` has specific behaviors. It guarantees **uniqueness** and an **increasing trend**, but does NOT guarantee contiguous numbering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa420261-9935-4d42-8e14-58588286aede",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now we will insert data. Note that in the `INSERT` query we omit `order_sk` and `order_date` columns:\n",
    "- `order_sk`: will be generated automatically (unique number)\n",
    "- `order_date`: will be calculated based on `order_timestamp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "838cd65d-9eb5-4340-b3de-20a54d52c601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Insert data without specifying generated columns\n",
    "spark.sql(f\"\"\"\n",
    "INSERT INTO {CATALOG}.{BRONZE_SCHEMA}.orders_modern (order_id, total_amount, order_timestamp)\n",
    "VALUES \n",
    "    ('ORD-001', 150.50, current_timestamp()),\n",
    "    ('ORD-002', 200.00, current_timestamp())\n",
    "\"\"\")\n",
    "display(spark.table(\"orders_modern\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dff54bc3-8723-4c71-b8be-a293e2639782",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example: Schema Evolution\n",
    "\n",
    "**Objective:** Demonstration of automatic schema evolution when adding new columns\n",
    "\n",
    "\n",
    "Schema Evolution allows for controlled addition of new columns to existing Delta tables without interrupting application operations. Delta Lake supports additive schema changes automatically when enabled with `mergeSchema` option.\n",
    "\n",
    "\n",
    "<img src=\"../../../assets/images/3c274f5e758047b2b8033f29fd179c85.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73b58e42-e016-40fe-a43e-a5aa696df822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data with additional column (customer_tier)\n",
    "extended_customers = spark.createDataFrame([\n",
    "    (\"CUST010001\", \"New\", \"Customer\", \"new@example.com\", \"+48 111 222 333\", \"Warsaw\", \"MZ\", \"Poland\", \"2023-12-01\", \"Basic\", \"Premium\"),\n",
    "    (\"CUST010002\", \"Another\", \"Customer\", \"another@example.com\", \"+48 444 555 666\", \"Krakow\", \"MP\", \"Poland\", \"2023-12-02\", \"Premium\", \"Standard\")\n",
    "], [\"customer_id\", \"first_name\", \"last_name\", \"email\", \"phone\", \"city\", \"state\", \"country\", \"registration_date\", \"customer_segment\", \"customer_tier\"])\n",
    "display(extended_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0f6e3d8-dd8b-4c8b-8b51-c626f8562ca2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"customers_delta\")\n",
    "display(df.orderBy(\"customer_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33d31325-b1b8-42ca-8308-fe6792e33d37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "# Cast registration_date to proper type\n",
    "extended_customers = extended_customers.withColumn(\"registration_date\", col(\"registration_date\").cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b5ff7aa-721f-43d2-84be-29677a4f12c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enable automatic schema evolution with mergeSchema option\n",
    "extended_customers.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "459b68d1-3c27-4e89-a8e4-1d972fdddc51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check new schema - notice the new customer_tier column\n",
    "spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8ea4e36-c7e2-451c-8868-e2cbdd8b8675",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify data - new column has NULL for old records\n",
    "display(\n",
    "    spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "    .select(\"customer_id\", \"first_name\", \"last_name\", \"customer_tier\")\n",
    "    .filter(col(\"customer_id\") == \"CUST010001\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b791844-f49d-45d6-97b3-76cf3dccdf34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example: Data Quality with Constraints\n",
    "\n",
    "**Objective:** Enforce data quality rules at the table level using CHECK constraints\n",
    "\n",
    "Delta Lake allows defining **Constraints** that guarantee data quality at the table level. This works similarly to traditional SQL databases.\n",
    "\n",
    "**Constraint Types:**\n",
    "- `NOT NULL`: Enforces the presence of a value\n",
    "- `CHECK`: Enforces any logical condition (e.g., `age > 0`, `customer_id LIKE 'CUST%'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a29765c-7b56-4757-acef-ae92626049cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned_df = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\").na.drop(subset=[\"customer_id\"])\n",
    "display(cleaned_df.orderBy(\"customer_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e938f119-c248-4bbd-9e6f-c91e126ca345",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "delete from customers_delta\n",
    "where customer_id is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "325b9cc5-f1f0-4a80-b961-00281dde151b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add CHECK constraint: customer_id must start with CUST\n",
    "try:\n",
    "    spark.sql(f\"\"\"\n",
    "        ALTER TABLE {CATALOG}.{BRONZE_SCHEMA}.customers_delta\n",
    "        ADD CONSTRAINT valid_customer_id CHECK (customer_id LIKE 'CUST%')\n",
    "    \"\"\")\n",
    "    print(\"Constraint 'valid_customer_id' added successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Info: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67131226-ad11-4e76-93eb-d2e0e595a30a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Attempt to insert invalid data (customer_id does not start with CUST)\n",
    "try:\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO {CATALOG}.{BRONZE_SCHEMA}.customers_delta (customer_id, first_name, last_name, email, phone, city, state, country, registration_date, customer_segment)\n",
    "        VALUES ('INVALID123', 'Bad', 'Customer', 'bad@example.com', '+48 000 000 000', 'Test', 'TS', 'Poland', '2023-01-01', 'Basic')\n",
    "    \"\"\")\n",
    "except Exception as e:\n",
    "    print(f\"Expected Data Quality error:\\n{str(e)[:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "698e8779-6c53-4d74-91d5-c79c012c26ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "  INSERT INTO {CATALOG}.{BRONZE_SCHEMA}.customers_delta (customer_id, first_name, last_name, email, phone, city, state, country, registration_date, customer_segment)\n",
    "  VALUES ('CUST123', 'Bad', 'Customer', 'bad@example.com', '+48 000 000 000', 'Test', 'TS', 'Poland', '2023-01-01', 'Basic')\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "230d162f-1993-41ea-9767-e5dab661dc93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## CRUD Operations & MERGE\n",
    "\n",
    "INSERT, UPDATE, DELETE and MERGE INTO operations on Delta tables. MERGE is the most important for the exam — it appears on almost every test.\n",
    "\n",
    "---\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Delta Lake supports the full range of CRUD operations (Create, Read, Update, Delete), making it ideal for transactional workloads in Data Lake. All operations are:\n",
    "- **Atomic**: Either fully complete or fully rolled back\n",
    "- **ACID-compliant**: Ensuring data consistency\n",
    "- **Recorded in Delta Log**: Full audit trail of all changes\n",
    "\n",
    "Additionally, Delta Lake provides the powerful **MERGE INTO** operation (also known as \"upsert\") that combines INSERT and UPDATE in a single atomic transaction - essential for CDC (Change Data Capture) scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaa7b298-38c0-4086-81c7-4eead242bb59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example: INSERT Operation\n",
    "\n",
    "**Objective:** Adding new records to an existing table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "698a32b5-d996-4640-82db-bd66efe3f57b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# INSERT new customers\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO {CATALOG}.{BRONZE_SCHEMA}.customers_delta\n",
    "    (customer_id, first_name, last_name, email, phone, city, state, country, registration_date, customer_segment, customer_tier)\n",
    "    VALUES \n",
    "        ('CUST020001', 'Insert', 'Customer1', 'insert1@example.com', '+48 111 111 111', 'Warsaw', 'MZ', 'Poland', '2023-12-10', 'Premium', 'Gold'),\n",
    "        ('CUST020002', 'Insert', 'Customer2', 'insert2@example.com', '+48 222 222 222', 'Gdansk', 'PM', 'Poland', '2023-12-11', 'Basic', 'Silver')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baa0ece9-fd4a-4b78-87b5-c7c1f126c9f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify insertion\n",
    "display(\n",
    "    spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "    .filter(F.col(\"customer_id\").like(\"CUST02%\"))\n",
    "    .orderBy(\"customer_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41b83d64-eb2b-4136-895a-b566f968d520",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example: UPDATE Operation\n",
    "\n",
    "**Objective:** Updating existing records in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf3045cc-46b5-4f49-b35d-e2e4f783c655",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UPDATE customer tier for specific customers\n",
    "spark.sql(f\"\"\"\n",
    "    UPDATE {CATALOG}.{BRONZE_SCHEMA}.customers_delta\n",
    "    SET customer_tier = 'Platinum'\n",
    "    WHERE customer_id IN ('CUST010001', 'CUST020001')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a588d6d-9565-4181-bcf4-9c3b83090437",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify update\n",
    "display(\n",
    "    spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "    .filter(F.col(\"customer_tier\") == \"Platinum\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f4be41f-6a69-4283-81f4-e380018cf42f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example: DELETE Operation\n",
    "\n",
    "**Objective:** Deleting records from a Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73d44cab-7318-4f43-a82d-101979f275b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DELETE specific customer\n",
    "spark.sql(f\"\"\"\n",
    "    DELETE FROM {CATALOG}.{BRONZE_SCHEMA}.customers_delta\n",
    "    WHERE customer_id = 'CUST020002'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5a7f1c7-263f-4723-b5c0-8155b4dcb581",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify deletion\n",
    "deleted_check = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\") \\\n",
    "    .filter(F.col(\"customer_id\") == \"CUST020002\") \\\n",
    "    .count()\n",
    "\n",
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"Records with customer_id CUST020002\", deleted_check)\n",
    "    ], [\"description\", \"count\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b3f55c2-8f34-4681-821b-47bb8f4d2f8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example: MERGE INTO (Upsert)\n",
    "\n",
    "**Objective:** Demonstration of upsert operation - update existing and insert new records in a single atomic transaction\n",
    "\n",
    "MERGE INTO is especially useful when processing changes from transactional systems (CDC patterns). It allows you to:\n",
    "- **Update** existing records when a match is found\n",
    "- **Insert** new records when no match exists\n",
    "- **Delete** records based on conditions (optional)\n",
    "\n",
    "\n",
    "<img src=\"../../../assets/images/ff01677d3d4a45d6a6a7530d8911b785.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77d13868-16a7-40ba-98d8-bc9955045f87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare data for merge (mix of updates and new records)\n",
    "merge_data = spark.createDataFrame([\n",
    "    (\"CUST010001\", \"Updated\", \"Name\", \"updated@example.com\", \"+48 999 999 999\", \"Poznan\", \"WP\", \"Poland\", \"2023-12-01\", \"VIP\", \"Diamond\"),  # Update existing\n",
    "    (\"CUST030001\", \"Brand\", \"New\", \"brand.new@example.com\", \"+48 777 777 777\", \"Wroclaw\", \"DS\", \"Poland\", \"2023-12-15\", \"Basic\", \"Bronze\"),   # Insert new\n",
    "    (\"CUST030002\", \"Another\", \"New\", \"another.new@example.com\", \"+48 888 888 888\", \"Lodz\", \"LD\", \"Poland\", \"2023-12-16\", \"Premium\", \"Silver\") # Insert new\n",
    "], [\"customer_id\", \"first_name\", \"last_name\", \"email\", \"phone\", \"city\", \"state\", \"country\", \"registration_date\", \"customer_segment\", \"customer_tier\"])\n",
    "\n",
    "# Create temporary view for merge operation\n",
    "merge_data.createOrReplaceTempView(\"customer_updates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bb5b458-eb7b-4d2a-a06c-545082f4cad3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MERGE INTO operation (Upsert)\n",
    "spark.sql(f\"\"\"\n",
    "    MERGE INTO {CATALOG}.{BRONZE_SCHEMA}.customers_delta AS target\n",
    "    USING customer_updates AS source\n",
    "    ON target.customer_id = source.customer_id\n",
    "    \n",
    "    WHEN MATCHED THEN\n",
    "        UPDATE SET\n",
    "            first_name = source.first_name,\n",
    "            last_name = source.last_name,\n",
    "            email = source.email,\n",
    "            phone = source.phone,\n",
    "            city = source.city,\n",
    "            state = source.state,\n",
    "            country = source.country,\n",
    "            customer_segment = source.customer_segment,\n",
    "            customer_tier = source.customer_tier\n",
    "    \n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT (customer_id, first_name, last_name, email, phone, city, state, country, registration_date, customer_segment, customer_tier)\n",
    "        VALUES (source.customer_id, source.first_name, source.last_name, source.email, source.phone, source.city, source.state, source.country, source.registration_date, source.customer_segment, source.customer_tier)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dce75c63-3f60-4869-a739-6227380d0174",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify MERGE results\n",
    "display(\n",
    "    spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "    .filter(F.col(\"customer_id\").isin([\"CUST010001\", \"CUST030001\", \"CUST030002\"]))\n",
    "    .orderBy(\"customer_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67d4810a-0024-4405-9872-26f6ea275f89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Metadata and Analytics\n",
    "\n",
    "DESCRIBE DETAIL, DESCRIBE HISTORY and Delta Log internals. Understanding metadata commands is essential for auditing, debugging, and compliance on the exam.\n",
    "\n",
    "---\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Delta Lake offers rich metadata about tables and operations that enables:\n",
    "- **Auditing**: Who changed what and when\n",
    "- **Debugging**: Understanding operation performance and metrics\n",
    "- **Compliance**: Meeting regulatory requirements for data lineage\n",
    "\n",
    "**Key Commands:**\n",
    "- `DESCRIBE DETAIL`: File structure, partitioning, table properties\n",
    "- `DESCRIBE HISTORY`: Complete audit trail of all operations\n",
    "- `SHOW TBLPROPERTIES`: Table configuration and settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c12b40ea-6458-4d39-b818-c8e23cfe5e5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example: DESCRIBE DETAIL\n",
    "\n",
    "**Objective:** Analysis of Delta table metadata and physical storage details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e585151-8035-4c64-95b9-0306f9565797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Detailed table information\n",
    "display(\n",
    "    spark.sql(f\"DESCRIBE DETAIL {CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39e71be0-51a3-4675-8bfc-eafcc4ce3e51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example: Operation History Analysis\n",
    "\n",
    "**Objective:** Deeper analysis of history and operation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef449b2b-1112-454d-a2e2-fbdcf6c3c97e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"operation\":482},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768999346434}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# History with additional metrics\n",
    "history_df = spark.sql(f\"DESCRIBE HISTORY {CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "\n",
    "display(\n",
    "    history_df.select(\n",
    "        \"version\", \n",
    "        \"timestamp\", \n",
    "        \"operation\", \n",
    "        \"operationMetrics.numTargetRowsInserted\",\n",
    "        \"operationMetrics.numTargetRowsUpdated\",\n",
    "        \"operationMetrics.numTargetRowsDeleted\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da5a3cd1-dde3-4c3a-9ddc-b3082037e338",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example: Delta Log Internals (Deep Dive)\n",
    "\n",
    "**Objective:** Understanding how Delta Lake ensures ACID by looking \"under the hood\" at JSON files in `_delta_log`\n",
    "\n",
    "The Delta Log is a transaction log stored in the `_delta_log/` folder. Each transaction creates a new JSON file containing:\n",
    "- `add`: Adding a new Parquet file with data\n",
    "- `remove`: Logical deletion of a file (e.g., during DELETE or OPTIMIZE)\n",
    "- `commitInfo`: Metadata about the transaction (who, when, what operation)\n",
    "\n",
    "<img src=\"../../../assets/images/f0361af0b2ef43c0a1f61ee1202e2df3.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76ecafe5-9b5e-4e33-bb57-81f5d6d06476",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get table path\n",
    "table_details = spark.sql(f\"DESCRIBE DETAIL {CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "display(table_details.select(\"location\", \"numFiles\", \"sizeInBytes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82bd65c2-dd3c-487b-afa2-17fba50cdcc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# View complete Delta table history\n",
    "display(spark.sql(f\"DESCRIBE HISTORY {CATALOG}.{BRONZE_SCHEMA}.customers_delta\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b906a04a-79c0-4188-a6e1-d61bc09a4b2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Time Travel and Disaster Recovery\n",
    "\n",
    "Access previous versions of data with VERSION AS OF and TIMESTAMP AS OF. RESTORE for disaster recovery and VACUUM for storage cleanup — all heavily tested on the exam.\n",
    "\n",
    "---\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Time Travel is a fundamental Delta Lake feature enabling access to previous versions of data. It is based on the **immutable transaction log** (`_delta_log`) - every change creates a new version of files, while old versions remain available until cleaned up by VACUUM.\n",
    "\n",
    "**Key Capabilities:**\n",
    "- **VERSION AS OF**: Query data at a specific version number\n",
    "- **TIMESTAMP AS OF**: Query data at a specific point in time\n",
    "- **RESTORE**: Rollback table to a previous state\n",
    "- **Audit**: Compare data between versions\n",
    "\n",
    "**Important Consideration:** VACUUM removes old files and directly impacts Time Travel capabilities. Understanding this relationship is crucial for data retention strategies.\n",
    "\n",
    "\n",
    "<img src=\"../../../assets/images/7e9c029eeeb14e44b5ce68c5af90f350.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47b9d2e2-038c-4cd9-b52b-abb3ce69b7ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Demo: Time Travel Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed49fd74-da66-4d4f-ab4e-5286660f96f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"DROP TABLE IF EXISTS {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e42b2c2-2211-431a-a831-a325c3eb5a67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a new table specifically for Time Travel demonstration\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo (\n",
    "    id INT,\n",
    "    name STRING,\n",
    "    status STRING,\n",
    "    updated_at TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# Version 0: Insert initial data\n",
    "spark.sql(f\"\"\"\n",
    "INSERT INTO {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo VALUES\n",
    "    (1, 'Alice', 'active', current_timestamp()),\n",
    "    (2, 'Bob', 'active', current_timestamp()),\n",
    "    (3, 'Charlie', 'active', current_timestamp())\n",
    "\"\"\")\n",
    "\n",
    "print(\"Version 0: Initial data inserted\")\n",
    "display(spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.time_travel_demo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad99d7ce-d1e1-4ace-a31a-9ef796eebae2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Version 1: Update some records\n",
    "spark.sql(f\"\"\"\n",
    "UPDATE {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo\n",
    "SET status = 'premium', updated_at = current_timestamp()\n",
    "WHERE name = 'Alice'\n",
    "\"\"\")\n",
    "\n",
    "print(\"Version 1: Alice upgraded to premium\")\n",
    "display(spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.time_travel_demo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39c48854-9f7f-4e1b-9f25-ba2a7157cc3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Version 2: Insert new record\n",
    "spark.sql(f\"\"\"\n",
    "INSERT INTO {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo VALUES\n",
    "    (4, 'Diana', 'new', current_timestamp())\n",
    "\"\"\")\n",
    "\n",
    "print(\"Version 2: Diana added\")\n",
    "display(spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.time_travel_demo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5357759-c955-4336-9979-efe9cfc3e93d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Version 3: Delete a record\n",
    "spark.sql(f\"\"\"\n",
    "DELETE FROM {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo\n",
    "WHERE name = 'Charlie'\n",
    "\"\"\")\n",
    "\n",
    "print(\"Version 3: Charlie deleted\")\n",
    "display(spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.time_travel_demo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9ee8525-73fb-4b7f-bae4-cb9660cef135",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example: Table History Exploration\n",
    "\n",
    "**Objective:** Use DESCRIBE HISTORY to analyze all operations on the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0689c3d-4919-449d-b662-3ac1ad55aa93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show complete history of all operations\n",
    "display(\n",
    "    spark.sql(f\"DESCRIBE HISTORY {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b37e332-036d-48b5-94d5-d782173cddb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example: Time Travel Queries\n",
    "\n",
    "**Objective:** Access previous versions of data using VERSION AS OF and TIMESTAMP AS OF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21ace1a5-e064-4328-82f3-6ecada2b5d05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Access data from version 0 (initial state)\n",
    "print(\"Version 0 - Initial data (before any changes):\")\n",
    "display(\n",
    "    spark.sql(f\"SELECT * FROM {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo VERSION AS OF 0\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57511d06-62c3-4cbb-a42e-b38f3371c160",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Access data from version 1 (after Alice upgrade)\n",
    "print(\"Version 1 - After Alice upgrade:\")\n",
    "display(\n",
    "    spark.sql(f\"SELECT * FROM {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo VERSION AS OF 1\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ad24c6e-71e0-4a67-b331-94c6752f7810",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compare record counts between versions\n",
    "version_counts = []\n",
    "for v in range(6):\n",
    "    count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo VERSION AS OF {v}\").first()[0]\n",
    "    version_counts.append((f\"Version {v}\", count))\n",
    "\n",
    "display(spark.createDataFrame(version_counts, [\"version\", \"record_count\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b7b28e9-a038-4c3f-9fa9-d10ac7ea3fb0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 82"
    }
   },
   "source": [
    "### Example: Disaster Recovery - Accidental Deletion\n",
    "\n",
    "**Objective:** Simulate accidental data deletion and recover using RESTORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5321e4bb-41d2-4146-b792-44c806b47fbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DISASTER! Accidental deletion of ALL data\n",
    "spark.sql(f\"DELETE FROM {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo\")\n",
    "\n",
    "print(\"Oh no! All data deleted!\")\n",
    "print(\"Record count after deletion:\", spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.time_travel_demo\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c84f0573-8a69-45c3-81c9-ce293f3722e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check history to find the last good version\n",
    "history = spark.sql(f\"DESCRIBE HISTORY {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo\")\n",
    "display(history.select(\"version\", \"timestamp\", \"operation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "869aeef4-8bda-492a-aba5-4cd88464aa91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESTORE to version before the accidental deletion\n",
    "# The last good version is the one before DELETE (version 3 in our case)\n",
    "last_good_version = spark.sql(f\"\"\"\n",
    "    SELECT version FROM (\n",
    "        SELECT version, operation, \n",
    "               ROW_NUMBER() OVER (ORDER BY version DESC) as rn\n",
    "        FROM (DESCRIBE HISTORY {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo)\n",
    "        WHERE operation != 'DELETE'\n",
    "    ) WHERE rn = 1\n",
    "\"\"\").first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "333b0cda-d05f-4d59-bc68-4e93e5103bbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Restoring to version: {last_good_version}\")\n",
    "spark.sql(f\"RESTORE TABLE {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo TO VERSION AS OF {last_good_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9f1a000-c9b8-4a0a-a660-a17704992eb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify restoration\n",
    "print(\"Data restored successfully!\")\n",
    "print(\"Record count after RESTORE:\", spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.time_travel_demo\").count())\n",
    "display(spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.time_travel_demo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71490cf7-1c35-4d74-9134-5939872f4b06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "DESCRIBE HISTORY time_travel_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c78005f-2973-41e4-9a56-1991acf521d0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 91"
    }
   },
   "source": [
    "### Example: VACUUM and Its Impact on Time Travel\n",
    "\n",
    "**Objective:** Understand how VACUUM affects Time Travel capabilities\n",
    "\n",
    "**Critical Concept:** VACUUM removes old data files that are no longer referenced by the current version of the table. Once vacuumed, **Time Travel to those versions becomes impossible**.\n",
    "\n",
    "**Default Retention:** 7 days (168 hours)\n",
    "- This means you can Time Travel to any version within the last 7 days\n",
    "- After VACUUM, only versions within the retention period are accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efca5fe4-b5f5-43d3-ac2a-f22c1b8d43e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "INSERT INTO {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo VALUES\n",
    "    (4, 'Diana', 'new', current_timestamp()),\n",
    "    (5, 'Eve', 'active', current_timestamp()),\n",
    "    (6, 'Frank', 'active', current_timestamp())\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2247270-f893-44ef-9d40-0cc6d6841261",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check current table size and files BEFORE VACUUM\n",
    "before_vacuum = spark.sql(f\"DESCRIBE DETAIL {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo\")\n",
    "display(before_vacuum.select(\"numFiles\", \"sizeInBytes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acdd4e4d-c226-4add-977b-6812966142bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's see what versions are available BEFORE vacuum\n",
    "print(\"Available versions before VACUUM:\")\n",
    "display(spark.sql(f\"DESCRIBE HISTORY {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo\").select(\"version\", \"timestamp\", \"operation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "484ce21a-8f4a-4f3e-971b-61ccd9c138bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# VACUUM with 0 hours retention (DEMO ONLY - requires disabling safety check)\n",
    "# In production, NEVER use 0 hours - use default 7 days or more!\n",
    "spark.sql(\"SET spark.databricks.delta.retentionDurationCheck.enabled = false\")\n",
    "\n",
    "vacuum_result = spark.sql(f\"\"\"\n",
    "    VACUUM {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo RETAIN 0 HOURS\n",
    "\"\"\")\n",
    "\n",
    "display(vacuum_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "802934bb-3cd7-49c1-842e-a83b0001e790",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check table size AFTER VACUUM\n",
    "after_vacuum = spark.sql(f\"DESCRIBE DETAIL {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo\")\n",
    "display(after_vacuum.select(\"numFiles\", \"sizeInBytes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e1a5ddb-1996-484d-abff-f7fd866c8bdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now try to access an old version - this will fail!\n",
    "\n",
    "spark.sql(f\"SELECT * FROM {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo VERSION AS OF 6\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "218d214a-d3f7-4594-947b-da238fb813de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered the **fundamentals of Delta Lake**:\n",
    "\n",
    "| Feature | Purpose | Key Command |\n",
    "|---------|---------|-------------|\n",
    "| ACID Transactions | Data reliability | Automatic via Delta format |\n",
    "| Schema Enforcement | Prevent bad data | Automatic on write |\n",
    "| Schema Evolution | Controlled changes | `mergeSchema`, `overwriteSchema` |\n",
    "| DML Operations | Insert/Update/Delete/Merge | `MERGE INTO`, `UPDATE`, `DELETE` |\n",
    "| Time Travel | Historical access | `VERSION AS OF`, `TIMESTAMP AS OF` |\n",
    "| RESTORE | Disaster recovery | `RESTORE TABLE ... TO VERSION AS OF` |\n",
    "| VACUUM | Storage cleanup | `VACUUM table RETAIN x HOURS` |\n",
    "\n",
    "**Next:** M04 covers Delta optimization techniques (OPTIMIZE, Z-ORDER, Liquid Clustering, CDF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a11ae65a-064a-4578-93e4-612f32dc5e68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Resource Cleanup\n",
    "\n",
    "Clean up resources created during the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59a1bf9d-1567-4abe-bed2-a2ee7bfe545a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optional test resource cleanup\n",
    "# NOTE: Run only if you want to delete all created data\n",
    "\n",
    "cleanup_tables = [\n",
    "    \"customers_delta\",\n",
    "    \"orders_modern\",\n",
    "    \"time_travel_demo\"\n",
    "]\n",
    "\n",
    "# Uncomment below to execute cleanup:\n",
    "# for table in cleanup_tables:\n",
    "#     spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{BRONZE_SCHEMA}.{table}\")\n",
    "#     print(f\"Dropped: {table}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6142587020061685,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "M03_delta_fundamentals",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
