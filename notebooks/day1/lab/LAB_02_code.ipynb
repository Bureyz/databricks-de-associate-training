{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8cf734c",
   "metadata": {},
   "source": [
    "# LAB 02: ELT Ingestion & Transformations\n",
    "\n",
    "**Duration:** ~40 min | **Day:** 1 | **Difficulty:** Beginner-Intermediate\n",
    "**After module:** M02: ELT Data Ingestion\n",
    "\n",
    "> *\"Load raw data from CSV/JSON files, transform it, and save as Delta tables in the Bronze layer.\"*\n",
    "\n",
    "Complete the `# TODO` cells below. Each task has a validation cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502724b5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5abd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../setup/00_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1a94c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import col, concat, lit, upper, trim, count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77947047",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1: Read Customers CSV with Explicit Schema\n",
    "\n",
    "Define a `StructType` schema and read the customers CSV file.\n",
    "\n",
    "**Columns:** customer_id (string), first_name (string), last_name (string), email (string), city (string), state (string), country (string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f9d76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the schema\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", ________, True),\n",
    "    StructField(\"first_name\", ________, True),\n",
    "    StructField(\"last_name\", ________, True),\n",
    "    StructField(\"email\", ________, True),\n",
    "    StructField(\"city\", ________, True),\n",
    "    StructField(\"state\", ________, True),\n",
    "    StructField(\"country\", ________, True),\n",
    "])\n",
    "\n",
    "# TODO: Read the CSV with your schema\n",
    "customers_path = f\"{DATASET_PATH}/customers/customers.csv\"\n",
    "\n",
    "df_customers = (\n",
    "    spark.read\n",
    "    .format(________)\n",
    "    .schema(________)\n",
    "    .option(\"header\", True)\n",
    "    .load(customers_path)\n",
    ")\n",
    "\n",
    "df_customers.printSchema()\n",
    "display(df_customers.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e92141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert df_customers.count() > 0, \"DataFrame is empty!\"\n",
    "assert df_customers.schema[\"customer_id\"].dataType == StringType(), \"customer_id should be StringType\"\n",
    "assert df_customers.schema[\"first_name\"].dataType == StringType(), \"first_name should be StringType\"\n",
    "print(f\"Task 1 OK: {df_customers.count()} customers loaded with correct schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c090d552",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Read Orders JSON\n",
    "\n",
    "Read the orders batch JSON file. JSON files have self-describing schema - no need for explicit definition.\n",
    "\n",
    "**File:** `{DATASET_PATH}/orders/orders_batch.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d723d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Read orders JSON\n",
    "orders_path = f\"{DATASET_PATH}/orders/orders_batch.json\"\n",
    "\n",
    "df_orders = (\n",
    "    spark.read\n",
    "    .format(________)\n",
    "    .load(orders_path)\n",
    ")\n",
    "\n",
    "df_orders.printSchema()\n",
    "display(df_orders.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3816facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert df_orders.count() > 0, \"Orders DataFrame is empty!\"\n",
    "print(f\"Task 2 OK: {df_orders.count()} orders loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca808e5",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3: Read Products CSV\n",
    "\n",
    "Read the products CSV file. Use `inferSchema` this time (for comparison with Task 1).\n",
    "\n",
    "**File:** `{DATASET_PATH}/products/products.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe448c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Read products CSV with inferSchema\n",
    "products_path = f\"{DATASET_PATH}/products/products.csv\"\n",
    "\n",
    "df_products = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(________, ________)\n",
    "    .load(products_path)\n",
    ")\n",
    "\n",
    "df_products.printSchema()\n",
    "display(df_products.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1772b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert df_products.count() > 0, \"Products DataFrame is empty!\"\n",
    "print(f\"Task 3 OK: {df_products.count()} products loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40af5ad6",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 4: Transform Customer Data\n",
    "\n",
    "Apply the following transformations to create `df_customers_clean`:\n",
    "\n",
    "1. **Select** columns: customer_id, first_name, last_name, email, city, country\n",
    "2. **Add column** `full_name` = first_name + \" \" + last_name\n",
    "3. **Transform** email to lowercase using `lower()`\n",
    "4. **Filter** to keep only non-null emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dc44d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower\n",
    "\n",
    "# TODO: Apply transformations\n",
    "df_customers_clean = (\n",
    "    df_customers\n",
    "    .select(\"customer_id\", \"first_name\", \"last_name\", \"email\", \"city\", \"country\")\n",
    "    .withColumn(\"full_name\", concat(col(\"first_name\"), lit(\" \"), col(________)))\n",
    "    .withColumn(\"email\", lower(col(________)))\n",
    "    .filter(col(\"email\").________())\n",
    ")\n",
    "\n",
    "display(df_customers_clean.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d017011a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert \"full_name\" in df_customers_clean.columns, \"Missing 'full_name' column\"\n",
    "sample = df_customers_clean.first()\n",
    "assert sample[\"email\"] == sample[\"email\"].lower(), \"Email should be lowercase\"\n",
    "assert df_customers_clean.filter(col(\"email\").isNull()).count() == 0, \"Should have no null emails\"\n",
    "print(f\"Task 4 OK: {df_customers_clean.count()} clean customer records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196e9c8e",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 5: Temporary View + SQL Query\n",
    "\n",
    "1. Register `df_customers_clean` as temp view `v_customers`\n",
    "2. Write a SQL query to count customers per **country**, ordered by count DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f6b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create temporary view\n",
    "df_customers_clean.createOrReplaceTempView(________)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006a0524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: SQL query - customers per country\n",
    "df_by_country = spark.sql(\"\"\"\n",
    "    SELECT ________, COUNT(*) as customer_count\n",
    "    FROM v_customers\n",
    "    GROUP BY ________\n",
    "    ORDER BY customer_count ________\n",
    "\"\"\")\n",
    "\n",
    "display(df_by_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420636c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert df_by_country.count() > 0, \"Query returned no results\"\n",
    "assert \"customer_count\" in df_by_country.columns, \"Missing 'customer_count' column\"\n",
    "first_row = df_by_country.first()\n",
    "second_row = df_by_country.collect()[1] if df_by_country.count() > 1 else first_row\n",
    "assert first_row[\"customer_count\"] >= second_row[\"customer_count\"], \"Should be sorted DESC\"\n",
    "print(f\"Task 5 OK: Found {df_by_country.count()} countries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad4f649",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 6: Save as Delta Tables (Bronze Layer)\n",
    "\n",
    "Save all three DataFrames as managed Delta tables in the Bronze schema.\n",
    "\n",
    "Use `mode(\"overwrite\")` so the lab can be re-run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bf5eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Save customers to Bronze\n",
    "(\n",
    "    df_customers_clean\n",
    "    .write\n",
    "    .mode(________)\n",
    "    .saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.customers\")\n",
    ")\n",
    "print(\"customers saved\")\n",
    "\n",
    "# TODO: Save orders to Bronze\n",
    "(\n",
    "    df_orders\n",
    "    .write\n",
    "    .mode(________)\n",
    "    .saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.orders\")\n",
    ")\n",
    "print(\"orders saved\")\n",
    "\n",
    "# TODO: Save products to Bronze\n",
    "(\n",
    "    df_products\n",
    "    .write\n",
    "    .mode(________)\n",
    "    .saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.products\")\n",
    ")\n",
    "print(\"products saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250446e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "tables = [f\"{CATALOG}.{BRONZE_SCHEMA}.customers\",\n",
    "          f\"{CATALOG}.{BRONZE_SCHEMA}.orders\",\n",
    "          f\"{CATALOG}.{BRONZE_SCHEMA}.products\"]\n",
    "\n",
    "for t in tables:\n",
    "    c = spark.table(t).count()\n",
    "    assert c > 0, f\"Table {t} is empty!\"\n",
    "    print(f\"  {t}: {c} rows\")\n",
    "\n",
    "print(\"\\nTask 6 OK: All Bronze tables created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebf5674",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 7: Verify with SQL\n",
    "\n",
    "Run a SQL query to show all tables in your Bronze schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540ae299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Show all tables in bronze schema\n",
    "display(spark.sql(f\"SHOW TABLES IN {CATALOG}.{BRONZE_SCHEMA}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705c748f",
   "metadata": {},
   "source": [
    "---\n",
    "## Lab Complete!\n",
    "\n",
    "You have:\n",
    "- Read CSV (explicit schema) and JSON (inferred schema) files\n",
    "- Applied transformations: select, withColumn, filter, concat, lower\n",
    "- Created a temp view and ran SQL aggregation queries\n",
    "- Saved 3 Delta tables in the Bronze layer\n",
    "\n",
    "> **Exam Tip:** `inferSchema` reads the file twice (once for schema, once for data). Always prefer explicit schema in production. JSON and Parquet have embedded schemas.\n",
    "\n",
    "> **Next:** LAB 03 - Delta DML & Time Travel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional cleanup\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{BRONZE_SCHEMA}.customers\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{BRONZE_SCHEMA}.orders\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{BRONZE_SCHEMA}.products\")\n",
    "print(\"LAB 02 complete. Bronze tables preserved for LAB 03.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
