{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6feb96b",
   "metadata": {},
   "source": [
    "# LAB 03: Delta DML & Time Travel\n",
    "\n",
    "**Duration:** ~40 min | **Day:** 1 | **Difficulty:** Intermediate\n",
    "**After module:** M03: Delta Lake Fundamentals\n",
    "\n",
    "> *\"Merge new customer data, handle accidental deletes, recover using Time Travel, and see how VACUUM affects it.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab4507a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8864ea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../setup/00_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de74a457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure bronze.customers table exists (idempotent)\n",
    "customers_path = f\"{DATASET_PATH}/customers/customers.csv\"\n",
    "df_base = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).load(customers_path)\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{BRONZE_SCHEMA}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{BRONZE_SCHEMA}.customers\")\n",
    "df_base.write.mode(\"overwrite\").saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.customers\")\n",
    "print(f\"Base table ready: {spark.table(f'{CATALOG}.{BRONZE_SCHEMA}.customers').count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a97f89e",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1: Examine the Update File\n",
    "\n",
    "Load `customers_new.csv` and compare counts. How many customers overlap with base table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Read the update file\n",
    "update_path = f\"{DATASET_PATH}/customers/customers_new.csv\"\n",
    "\n",
    "df_updates = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .load(________)\n",
    ")\n",
    "\n",
    "print(f\"Existing customers: {spark.table(f'{CATALOG}.{BRONZE_SCHEMA}.customers').count()}\")\n",
    "print(f\"Updates file: {df_updates.count()} rows\")\n",
    "\n",
    "display(df_updates.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe2ceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert df_updates.count() > 0, \"Updates file is empty\"\n",
    "print(f\"Task 1 OK: {df_updates.count()} update records loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0840d720",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: MERGE INTO (Upsert)\n",
    "\n",
    "Register `df_updates` as temp view `v_updates`, then use SQL MERGE to:\n",
    "- **UPDATE** existing customers (match on `customer_id`)\n",
    "- **INSERT** new customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165177d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register updates as temp view\n",
    "df_updates.createOrReplaceTempView(\"v_updates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be798732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the MERGE statement\n",
    "spark.sql(f\"\"\"\n",
    "    MERGE INTO {CATALOG}.{BRONZE_SCHEMA}.customers AS target\n",
    "    USING v_updates AS source\n",
    "    ON target.customer_id = source.________\n",
    "    WHEN MATCHED THEN\n",
    "        UPDATE SET *\n",
    "    WHEN NOT MATCHED THEN\n",
    "        ________ \n",
    "\"\"\")\n",
    "\n",
    "new_count = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers\").count()\n",
    "print(f\"Customers after MERGE: {new_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dc6229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "base_count = df_base.count()\n",
    "assert new_count >= base_count, f\"Expected at least {base_count} rows after MERGE, got {new_count}\"\n",
    "print(f\"Task 2 OK: MERGE completed. {new_count} total customers (was {base_count})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dc47df",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3: UPDATE Records\n",
    "\n",
    "Update the `state` column for all customers where `city = 'Austin'`. Set state to `'TX'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142797b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: UPDATE statement\n",
    "spark.sql(f\"\"\"\n",
    "    UPDATE {CATALOG}.{BRONZE_SCHEMA}.customers\n",
    "    SET state = ________\n",
    "    WHERE city = ________\n",
    "\"\"\")\n",
    "\n",
    "# Verify\n",
    "display(spark.sql(f\"SELECT * FROM {CATALOG}.{BRONZE_SCHEMA}.customers WHERE city = 'Austin'\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa64922",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 4: Accidental DELETE\n",
    "\n",
    "Simulate an accident -- delete all customers where country is not null.\n",
    "\n",
    "**WARNING:** This is intentional! We will recover the data using Time Travel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923a7cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record row count BEFORE the accident\n",
    "count_before = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers\").count()\n",
    "print(f\"Rows BEFORE delete: {count_before}\")\n",
    "\n",
    "# \"Accident\" - delete a large chunk\n",
    "spark.sql(f\"\"\"\n",
    "    DELETE FROM {CATALOG}.{BRONZE_SCHEMA}.customers\n",
    "    WHERE country IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "count_after = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers\").count()\n",
    "print(f\"Rows AFTER delete: {count_after} (lost {count_before - count_after} rows!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eb6c70",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 5: DESCRIBE HISTORY\n",
    "\n",
    "Check the table history to see all operations performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6ed6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Show table history\n",
    "display(spark.sql(f\"________ {CATALOG}.{BRONZE_SCHEMA}.customers\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94feba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "history = spark.sql(f\"DESCRIBE HISTORY {CATALOG}.{BRONZE_SCHEMA}.customers\").collect()\n",
    "operations = [row[\"operation\"] for row in history]\n",
    "assert \"DELETE\" in operations, \"Expected DELETE in history\"\n",
    "assert \"MERGE\" in operations, \"Expected MERGE in history\"\n",
    "print(f\"Task 5 OK: {len(history)} versions found. Operations: {operations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e67ea1",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 6: Time Travel - Query Previous Version\n",
    "\n",
    "Read the table as it was BEFORE the accidental delete (the version with the most rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a676e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find the version number before DELETE from the history above\n",
    "# Then read that version\n",
    "\n",
    "version_before_delete = ________  # Replace with the correct version number\n",
    "\n",
    "df_recovered = spark.sql(f\"\"\"\n",
    "    SELECT * FROM {CATALOG}.{BRONZE_SCHEMA}.customers\n",
    "    VERSION AS OF {version_before_delete}\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Recovered version has {df_recovered.count()} rows (current has {count_after})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c9a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert df_recovered.count() > count_after, \"Recovered version should have more rows than current\"\n",
    "print(f\"Task 6 OK: Time Travel successful! Recovered {df_recovered.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9049b206",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 7: RESTORE the Table\n",
    "\n",
    "Use `RESTORE TABLE` to bring the table back to the version before the accidental delete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43576e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Restore the table\n",
    "spark.sql(f\"\"\"\n",
    "    RESTORE TABLE {CATALOG}.{BRONZE_SCHEMA}.customers\n",
    "    TO VERSION AS OF ________\n",
    "\"\"\")\n",
    "\n",
    "restored_count = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers\").count()\n",
    "print(f\"Rows after RESTORE: {restored_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52d4544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert restored_count == count_before, f\"Expected {count_before} rows after restore, got {restored_count}\"\n",
    "print(f\"Task 7 OK: Table restored! {restored_count} rows (matches pre-delete count)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5262a2",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 8: VACUUM and Its Impact on Time Travel\n",
    "\n",
    "Run `VACUUM` with 0 hours retention, then try to query an old version. You'll see that Time Travel **no longer works** for vacuumed versions — the data files have been physically deleted.\n",
    "\n",
    "> **Warning:** We use `RETAIN 0 HOURS` for demo purposes only. In production, the default retention is **7 days** — never lower it without understanding the consequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c86617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Check how many versions exist before VACUUM\n",
    "history_before = spark.sql(f\"DESCRIBE HISTORY {CATALOG}.{BRONZE_SCHEMA}.customers\").collect()\n",
    "print(f\"Versions available: {len(history_before)}\")\n",
    "print(f\"Version numbers: {[r['version'] for r in history_before]}\")\n",
    "\n",
    "# Step 2: Disable safety check (LAB ONLY!)\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "\n",
    "# TODO: Run VACUUM with 0 hours retention\n",
    "spark.sql(f\"________ {CATALOG}.{BRONZE_SCHEMA}.customers RETAIN 0 HOURS\")\n",
    "\n",
    "# Re-enable safety\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"true\")\n",
    "print(\"VACUUM complete — old data files removed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ce472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try to query version 0 (the original table before any changes)\n",
    "# This should FAIL with FileNotFoundException — VACUUM deleted the old data files\n",
    "\n",
    "try:\n",
    "    df_old = spark.sql(f\"\"\"\n",
    "        SELECT * FROM {CATALOG}.{BRONZE_SCHEMA}.customers\n",
    "        VERSION AS OF 0\n",
    "    \"\"\")\n",
    "    df_old.count()  # Force evaluation\n",
    "    print(\"Unexpected: Time Travel still works (files not yet cleaned)\")\n",
    "except Exception as e:\n",
    "    print(f\"Expected error! Time Travel FAILED after VACUUM:\")\n",
    "    print(f\"  {type(e).__name__}: {str(e)[:200]}\")\n",
    "    print(\"\\n→ VACUUM removed the old data files. History metadata exists, but data is gone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ac3e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "# After VACUUM, the latest version should still be accessible\n",
    "current = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers\").count()\n",
    "assert current > 0, \"Current version should still work after VACUUM\"\n",
    "print(f\"Task 8 OK: Current table has {current} rows (latest version OK)\")\n",
    "print(\"Key takeaway: VACUUM removes old files → Time Travel breaks for vacuumed versions\")\n",
    "print(f\"  Default retention: 7 days | Production best practice: never set to 0 hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2b8db7",
   "metadata": {},
   "source": [
    "---\n",
    "## Lab Complete!\n",
    "\n",
    "You have:\n",
    "- Used MERGE INTO for upsert (insert + update)\n",
    "- Performed UPDATE and DELETE on Delta tables\n",
    "- Inspected history with DESCRIBE HISTORY\n",
    "- Queried previous versions with Time Travel\n",
    "- Restored a table with RESTORE TABLE\n",
    "- Ran VACUUM and observed its impact on Time Travel\n",
    "\n",
    "> **Exam Tip:** Time Travel uses the Delta transaction log. Data files for old versions are only removed by `VACUUM`. Default retention is **7 days**. After VACUUM, `DESCRIBE HISTORY` still shows metadata, but querying old versions fails because the underlying Parquet files are gone.\n",
    "\n",
    "> **Next:** LAB 04 - Delta Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional cleanup\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{BRONZE_SCHEMA}.customers\")\n",
    "print(\"LAB 03 complete.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
