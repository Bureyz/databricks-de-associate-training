{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 02: ELT Ingestion & Transformations\n",
    "\n",
    "**Duration:** ~40 min  \n",
    "**Day:** 1  \n",
    "**After module:** M02: ELT Data Ingestion  \n",
    "**Difficulty:** Beginner-Intermediate\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario\n",
    "\n",
    "> *\"The RetailHub data team received the first data export from the store system -- CSV files with customer and product data, plus a JSON file with order history. Your task: load these files into the Lakehouse, apply basic transformations, and save them as Delta tables in the Bronze layer.\"*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "- Read CSV, JSON, and Parquet files using `spark.read`\n",
    "- Compare `inferSchema` vs explicit schema definition\n",
    "- Apply DataFrame transformations: select, withColumn, filter, cast\n",
    "- Create temporary views and run SQL queries\n",
    "- Write DataFrames as managed Delta tables using `saveAsTable()`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Completed LAB 01 (or just run `%run ../setup/00_setup`)\n",
    "- Dataset files available in Volume or `DATASET_PATH`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Ingestion (~15 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4635373",
   "metadata": {},
   "source": [
    "### Task 1: Read Customers CSV\n",
    "\n",
    "Read the customers CSV file with explicit schema (don't use `inferSchema`).\n",
    "\n",
    "**Requirements:**\n",
    "- Define a `StructType` schema with appropriate types\n",
    "- Use `.option(\"header\", True)`\n",
    "- Verify column types with `printSchema()`\n",
    "\n",
    "> **Exam Tip:** `inferSchema` triggers an extra pass over the data. In production, always define schema explicitly for CSV files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90585e67",
   "metadata": {},
   "source": [
    "### Task 2: Read Orders JSON\n",
    "\n",
    "Read the orders batch JSON file. JSON infers schema automatically, but verify nested structures.\n",
    "\n",
    "**Requirements:**\n",
    "- Use `spark.read.format(\"json\")`\n",
    "- Check if any columns contain nested types (struct, array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a653d2a",
   "metadata": {},
   "source": [
    "### Task 3: Read Products CSV\n",
    "\n",
    "Read the products file with `inferSchema` and then compare the schema with a manually defined one.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebd59ff",
   "metadata": {},
   "source": [
    "## Part 2: Transformations (~15 min)\n",
    "\n",
    "### Task 4: Clean Customer Data\n",
    "\n",
    "Apply the following transformations to `df_customers`:\n",
    "1. `select` only: customer_id, first_name, last_name, email, city, country\n",
    "2. `withColumn` -- create `full_name` by concatenating first_name + \" \" + last_name\n",
    "3. `withColumn` -- convert email to lowercase using `lower()`\n",
    "4. `filter` -- keep only rows with non-null email\n",
    "\n",
    "### Task 5: Create Temp View + SQL Query\n",
    "\n",
    "1. Register `df_customers` as a temporary view `v_customers`\n",
    "2. Write a SQL query to count customers per country\n",
    "3. Display the result sorted by count descending\n",
    "\n",
    "> **Exam Tip:** `createOrReplaceTempView()` is session-scoped. `createOrReplaceGlobalTempView()` is accessible from any notebook on the same cluster via `global_temp.view_name`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3711f2c",
   "metadata": {},
   "source": [
    "## Part 3: Save as Delta Tables (~10 min)\n",
    "\n",
    "### Task 6: Write to Bronze Layer\n",
    "\n",
    "Save all three DataFrames as Delta tables in the Bronze schema:\n",
    "- `bronze.customers`\n",
    "- `bronze.orders`\n",
    "- `bronze.products`\n",
    "\n",
    "**Requirements:**\n",
    "- Use `.write.mode(\"overwrite\").saveAsTable()`\n",
    "- Use fully qualified names: `{CATALOG}.{BRONZE_SCHEMA}.table_name`\n",
    "\n",
    "### Task 7: Verify Tables\n",
    "\n",
    "Query the tables using SQL to verify they were saved correctly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab you:\n",
    "- Loaded CSV and JSON files into Spark DataFrames\n",
    "- Applied transformations: select, withColumn, filter, cast\n",
    "- Created temporary views and ran SQL queries\n",
    "- Saved cleaned data as Delta tables in the Bronze layer\n",
    "\n",
    "> **What's next:** In LAB 03 you will use MERGE to handle incremental data updates and learn Time Travel for disaster recovery."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
