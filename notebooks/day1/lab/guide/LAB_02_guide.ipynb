{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c25aad2",
   "metadata": {},
   "source": [
    "# LAB 02: ELT Ingestion & Transformations\n",
    "\n",
    "**Duration:** ~40 min  \n",
    "**Day:** 1  \n",
    "**After module:** M02: ELT Data Ingestion  \n",
    "**Difficulty:** Beginner-Intermediate\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c887c1",
   "metadata": {},
   "source": [
    "## Scenario\n",
    "\n",
    "> *\"Your workspace is ready. Now it's time to build the first data pipeline — load raw CSV and JSON files, apply transformations, and save them as Delta tables in the Bronze layer.\"*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa86a58",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "- Read CSV files with explicit `StructType` schema\n",
    "- Read JSON files with inferred schema\n",
    "- Read CSV files with `inferSchema` option\n",
    "- Apply PySpark transformations: `select`, `withColumn`, `filter`, `concat`, `lower`\n",
    "- Create temporary views and run SQL aggregation queries\n",
    "- Save DataFrames as managed Delta tables\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bce66bc",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- LAB 01 completed (cluster running, files uploaded to Volume)\n",
    "- Dataset files accessible via `DATASET_PATH` from `00_setup`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2920e17e",
   "metadata": {},
   "source": [
    "## Tasks Overview\n",
    "\n",
    "Open **`LAB_02_code.ipynb`** and complete the `# TODO` cells.\n",
    "\n",
    "| Task | What to do | Key concept |\n",
    "|------|-----------|-------------|\n",
    "| **Task 1** | Read Customers CSV with Explicit Schema | Define `StructType` with `StructField`, use `.schema()` |\n",
    "| **Task 2** | Read Orders JSON | `.format(\"json\")` — JSON has self-describing schema |\n",
    "| **Task 3** | Read Products CSV | `.option(\"inferSchema\", True)` — compare with explicit schema |\n",
    "| **Task 4** | Transform Customer Data | `.select()`, `.withColumn()`, `concat()`, `lower()`, `.filter()` |\n",
    "| **Task 5** | Temporary View + SQL Query | `.createOrReplaceTempView()`, `GROUP BY`, `ORDER BY DESC` |\n",
    "| **Task 6** | Save as Delta Tables (Bronze) | `.write.mode(\"overwrite\").saveAsTable()` |\n",
    "| **Task 7** | Verify with SQL | `SHOW TABLES IN catalog.schema` |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e89191",
   "metadata": {},
   "source": [
    "## Detailed Hints\n",
    "\n",
    "### Task 1: Explicit Schema\n",
    "- Each `StructField` takes: name (string), type (e.g. `StringType()`), nullable (bool)\n",
    "- Use `.format(\"csv\")` and `.schema(your_schema)` instead of `.option(\"inferSchema\")`\n",
    "\n",
    "### Task 2: JSON\n",
    "- JSON files are self-describing — just use `.format(\"json\")` and `.load(path)`\n",
    "- No need for schema definition or header option\n",
    "\n",
    "### Task 3: inferSchema\n",
    "- The option name is `\"inferSchema\"` and the value is `True` (or `\"true\"`)\n",
    "- Compare the resulting schema types with Task 1 (explicit = all StringType vs inferred types)\n",
    "\n",
    "### Task 4: Transformations\n",
    "- `concat(col(\"first_name\"), lit(\" \"), col(\"last_name\"))` builds full name\n",
    "- `lower(col(\"email\"))` converts to lowercase\n",
    "- `.filter(col(\"email\").isNotNull())` removes null emails\n",
    "\n",
    "### Task 5: TempView + SQL\n",
    "- Register view: `.createOrReplaceTempView(\"v_customers\")`\n",
    "- SQL: `SELECT country, COUNT(*) ... GROUP BY country ORDER BY customer_count DESC`\n",
    "\n",
    "### Task 6: Save as Delta\n",
    "- Use `.mode(\"overwrite\")` so the lab can be re-run\n",
    "- Table path format: `{CATALOG}.{BRONZE_SCHEMA}.table_name`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ae73ef",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab you:\n",
    "- Read CSV (explicit schema) and JSON (inferred schema) files\n",
    "- Applied transformations: select, withColumn, filter, concat, lower\n",
    "- Created a temp view and ran SQL aggregation queries\n",
    "- Saved 3 Delta tables in the Bronze layer\n",
    "\n",
    "> **Exam Tip:** `inferSchema` reads the file twice (once for schema, once for data). Always prefer explicit schema in production. JSON and Parquet have embedded schemas.\n",
    "\n",
    "> **What's next:** In LAB 03 you will use MERGE, UPDATE, DELETE on Delta tables and explore Time Travel for disaster recovery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 02: ELT Ingestion & Transformations\n",
    "\n",
    "**Duration:** ~40 min  \n",
    "**Day:** 1  \n",
    "**After module:** M02: ELT Data Ingestion  \n",
    "**Difficulty:** Beginner-Intermediate\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario\n",
    "\n",
    "> *\"The RetailHub data team received the first data export from the store system -- CSV files with customer and product data, plus a JSON file with order history. Your task: load these files into the Lakehouse, apply basic transformations, and save them as Delta tables in the Bronze layer.\"*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "- Read CSV, JSON, and Parquet files using `spark.read`\n",
    "- Compare `inferSchema` vs explicit schema definition\n",
    "- Apply DataFrame transformations: select, withColumn, filter, cast\n",
    "- Create temporary views and run SQL queries\n",
    "- Write DataFrames as managed Delta tables using `saveAsTable()`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Completed LAB 01 (or just run `%run ../setup/00_setup`)\n",
    "- Dataset files available in Volume or `DATASET_PATH`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Ingestion (~15 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4635373",
   "metadata": {},
   "source": [
    "### Task 1: Read Customers CSV\n",
    "\n",
    "Read the customers CSV file with explicit schema (don't use `inferSchema`).\n",
    "\n",
    "**Requirements:**\n",
    "- Define a `StructType` schema with appropriate types\n",
    "- Use `.option(\"header\", True)`\n",
    "- Verify column types with `printSchema()`\n",
    "\n",
    "> **Exam Tip:** `inferSchema` triggers an extra pass over the data. In production, always define schema explicitly for CSV files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90585e67",
   "metadata": {},
   "source": [
    "### Task 2: Read Orders JSON\n",
    "\n",
    "Read the orders batch JSON file. JSON infers schema automatically, but verify nested structures.\n",
    "\n",
    "**Requirements:**\n",
    "- Use `spark.read.format(\"json\")`\n",
    "- Check if any columns contain nested types (struct, array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a653d2a",
   "metadata": {},
   "source": [
    "### Task 3: Read Products CSV\n",
    "\n",
    "Read the products file with `inferSchema` and then compare the schema with a manually defined one.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebd59ff",
   "metadata": {},
   "source": [
    "## Part 2: Transformations (~15 min)\n",
    "\n",
    "### Task 4: Clean Customer Data\n",
    "\n",
    "Apply the following transformations to `df_customers`:\n",
    "1. `select` only: customer_id, first_name, last_name, email, city, country\n",
    "2. `withColumn` -- create `full_name` by concatenating first_name + \" \" + last_name\n",
    "3. `withColumn` -- convert email to lowercase using `lower()`\n",
    "4. `filter` -- keep only rows with non-null email\n",
    "\n",
    "### Task 5: Create Temp View + SQL Query\n",
    "\n",
    "1. Register `df_customers` as a temporary view `v_customers`\n",
    "2. Write a SQL query to count customers per country\n",
    "3. Display the result sorted by count descending\n",
    "\n",
    "> **Exam Tip:** `createOrReplaceTempView()` is session-scoped. `createOrReplaceGlobalTempView()` is accessible from any notebook on the same cluster via `global_temp.view_name`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3711f2c",
   "metadata": {},
   "source": [
    "## Part 3: Save as Delta Tables (~10 min)\n",
    "\n",
    "### Task 6: Write to Bronze Layer\n",
    "\n",
    "Save all three DataFrames as Delta tables in the Bronze schema:\n",
    "- `bronze.customers`\n",
    "- `bronze.orders`\n",
    "- `bronze.products`\n",
    "\n",
    "**Requirements:**\n",
    "- Use `.write.mode(\"overwrite\").saveAsTable()`\n",
    "- Use fully qualified names: `{CATALOG}.{BRONZE_SCHEMA}.table_name`\n",
    "\n",
    "### Task 7: Verify Tables\n",
    "\n",
    "Query the tables using SQL to verify they were saved correctly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab you:\n",
    "- Loaded CSV and JSON files into Spark DataFrames\n",
    "- Applied transformations: select, withColumn, filter, cast\n",
    "- Created temporary views and ran SQL queries\n",
    "- Saved cleaned data as Delta tables in the Bronze layer\n",
    "\n",
    "> **What's next:** In LAB 03 you will use MERGE to handle incremental data updates and learn Time Travel for disaster recovery."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
