{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 01: Platform & Workspace Setup\n\n**Duration:** ~40 min  \n**Day:** 1  \n**After module:** M01: Platform & Workspace  \n**Difficulty:** Beginner\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario\n\n> *\"It's your first day on the RetailHub data team. Before you can start building the analytics platform, you need to set up your workspace: create a compute cluster, explore Unity Catalog, and upload the raw data files that will be used throughout the training.\"*\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n\nAfter completing this lab you will be able to:\n- Create and configure a Databricks compute cluster\n- Navigate Unity Catalog hierarchy (Catalog > Schema > Table/Volume)\n- Create and use Unity Catalog Volumes for raw file storage  \n- Upload dataset files to a Volume\n- Run basic `dbutils` commands\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n\n- Access to Databricks workspace (URL + credentials provided by trainer)\n- Dataset files available in the Git repository (`dataset/` folder)\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Create Compute Cluster (~10 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Navigate to Compute\n\n1. In the left sidebar, click **Compute**\n2. Click **Create compute**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Configure Cluster\n\nUse the following settings:\n\n| Setting | Value |\n|---------|-------|\n| **Cluster name** | `training_{your_name}` |\n| **Policy** | Personal Compute (or as provided by trainer) |\n| **Access mode** | Single User |\n| **Databricks Runtime** | Latest LTS (e.g., 15.4 LTS) |\n| **Node type** | As provided by trainer |\n| **Terminate after** | 60 minutes of inactivity |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Start the Cluster\n\n1. Click **Create compute**\n2. Wait for the cluster status to change to **Running** (green circle)\n3. This may take 2-5 minutes\n\n> **Exam Tip:** Know the difference between **All-Purpose clusters** (interactive, development) and **Job clusters** (automated, ephemeral). Serverless compute is the default for SQL Warehouses and is becoming the default for notebooks.\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Explore Unity Catalog (~10 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Open Catalog Explorer\n\n1. In the left sidebar, click **Catalog**\n2. You should see your catalog: `retailhub_{your_name}`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Explore the Catalog Hierarchy\n\nNavigate through the hierarchy:\n\n```\nretailhub_{your_name}          -- CATALOG (top-level namespace)\n  ├── bronze                   -- SCHEMA (database)\n  │     ├── (tables)           -- TABLES\n  │     └── (volumes)          -- VOLUMES\n  ├── silver                   -- SCHEMA\n  ├── gold                     -- SCHEMA\n  └── default                  -- SCHEMA (auto-created)\n```\n\n1. Click on your catalog name to expand it\n2. Click on `bronze` schema\n3. Note the tabs: **Tables**, **Volumes**, **Functions**\n\n> **Exam Tip:** Unity Catalog uses a **3-level namespace**: `catalog.schema.object`. Always use fully qualified names in production code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Create a Volume\n\n1. Click on `default` schema\n2. Click **Create** > **Volume**\n3. Configure:\n   - **Name:** `datasets`\n   - **Volume type:** Managed\n4. Click **Create**\n\n> **Exam Tip:** **Managed volumes** store data in the metastore-managed location. **External volumes** point to existing cloud storage paths. Both are governed by Unity Catalog permissions.\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Explore External Connections (~5 min)\n\n### Step 7: Navigate to External Data\n\n1. In the left sidebar, click **Catalog**\n2. In the top navigation, click **External Data** (or look in the left panel under your catalog)\n3. Click **Connections**\n \"Connections\" view open>\n\n### Step 8: Understand External Connections\n\nExternal Connections define how Databricks connects to external data sources (cloud storage, databases, APIs).\n\nReview the Connections page and note:\n- A connection stores **credentials** and **endpoint** information\n- Connections are used by **External Locations** (cloud storage) and **Foreign Connections** (databases like PostgreSQL, MySQL, SQL Server)\n- To create a connection: **Create connection** > choose type > provide credentials\n\n| Connection Type | Use Case | Example |\n|---|---|---|\n| **Storage** | Access files on cloud storage | S3 bucket, ADLS container, GCS bucket |\n| **Database** | Query external databases via Lakehouse Federation | PostgreSQL, MySQL, SQL Server |\n\n> **Note:** Creating actual connections requires cloud credentials. In this training, we explore the UI to understand the concept. Your trainer may demonstrate a live connection.\n\n> **Exam Tip:** External Connections are part of Unity Catalog governance. They require `CREATE CONNECTION` privilege and are managed at the **metastore level**. Know the difference between **External Locations** (storage paths) and **Foreign Connections** (database endpoints for Lakehouse Federation).\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Upload Dataset Files (~10 min)\n\n### Step 9: Upload Files to the Volume\n\n1. Navigate to **Catalog** > `retailhub_{your_name}` > `default` > **Volumes** > `datasets`\n2. Click **Upload to this volume**\n3. Upload the following files from the `dataset/` folder:\n   - `customers/customers.csv`\n   - `products/products.csv`\n   - `orders/orders_batch.json`\n\n### Step 10: Verify Files with dbutils\n\nOpen the lab notebook (`LAB_01_code.ipynb`) and complete the tasks there.\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Notebook Tasks\n\nOpen **`LAB_01_code.ipynb`** and complete all `# TODO` cells.\n\nTasks in the notebook:\n1. Run the setup cell (`%run ../setup/00_setup`)\n2. List files in your Volume using `dbutils.fs.ls()`\n3. Read the customers CSV using `spark.read`\n4. Display the first 5 rows\n5. Check the catalog/schema context with `SELECT current_catalog(), current_schema()`\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n\nIn this lab you:\n- Created a compute cluster configured for the training\n- Explored the Unity Catalog 3-level namespace\n- Created a managed Volume for raw file storage\n- Uploaded dataset files and verified access via `dbutils`\n\n> **What's next:** In LAB 02 you will use these files to build your first ELT ingestion pipeline — reading CSV/JSON, transforming data, and writing Delta tables."
   ]
  }
 ]
}