{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 01: Platform & Workspace Setup\n",
    "\n",
    "**Duration:** ~40 min  \n",
    "**Day:** 1  \n",
    "**After module:** M01: Platform & Workspace  \n",
    "**Difficulty:** Beginner\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario\n",
    "\n",
    "> *\"It's your first day on the RetailHub data team. Before you can start building the analytics platform, you need to set up your workspace: create a compute cluster, explore Unity Catalog, and upload the raw data files that will be used throughout the training.\"*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "- Create and configure a Databricks compute cluster\n",
    "- Navigate Unity Catalog hierarchy (Catalog > Schema > Table/Volume)\n",
    "- Create and use Unity Catalog Volumes for raw file storage  \n",
    "- Upload dataset files to a Volume\n",
    "- Run basic `dbutils` commands\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Access to Databricks workspace (URL + credentials provided by trainer)\n",
    "- Dataset files available in the Git repository (`dataset/` folder)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Create Compute Cluster (~10 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be71f281",
   "metadata": {},
   "source": [
    "### Step 1: Navigate to Compute\n",
    "\n",
    "1. In the left sidebar, click **Compute**\n",
    "2. Click **Create compute**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5405f1",
   "metadata": {},
   "source": [
    "### Step 2: Configure Cluster\n",
    "\n",
    "Use the following settings:\n",
    "\n",
    "| Setting | Value |\n",
    "|---------|-------|\n",
    "| **Cluster name** | `training_{your_name}` |\n",
    "| **Policy** | Personal Compute (or as provided by trainer) |\n",
    "| **Access mode** | Single User |\n",
    "| **Databricks Runtime** | Latest LTS (e.g., 15.4 LTS) |\n",
    "| **Node type** | As provided by trainer |\n",
    "| **Terminate after** | 60 minutes of inactivity |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c0fe87",
   "metadata": {},
   "source": [
    "### Step 3: Start the Cluster\n",
    "\n",
    "1. Click **Create compute**\n",
    "2. Wait for the cluster status to change to **Running** (green circle)\n",
    "3. This may take 2-5 minutes\n",
    "\n",
    "> **Exam Tip:** Know the difference between **All-Purpose clusters** (interactive, development) and **Job clusters** (automated, ephemeral). Serverless compute is the default for SQL Warehouses and is becoming the default for notebooks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e8c90c",
   "metadata": {},
   "source": [
    "## Part 2: Explore Unity Catalog (~10 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a1e1d7",
   "metadata": {},
   "source": [
    "### Step 4: Open Catalog Explorer\n",
    "\n",
    "1. In the left sidebar, click **Catalog**\n",
    "2. You should see your catalog: `retailhub_{your_name}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26ef5ff",
   "metadata": {},
   "source": [
    "### Step 5: Explore the Catalog Hierarchy\n",
    "\n",
    "Navigate through the hierarchy:\n",
    "\n",
    "```\n",
    "retailhub_{your_name}          -- CATALOG (top-level namespace)\n",
    "  ├── bronze                   -- SCHEMA (database)\n",
    "  │     ├── (tables)           -- TABLES\n",
    "  │     └── (volumes)          -- VOLUMES\n",
    "  ├── silver                   -- SCHEMA\n",
    "  ├── gold                     -- SCHEMA\n",
    "  └── default                  -- SCHEMA (auto-created)\n",
    "```\n",
    "\n",
    "1. Click on your catalog name to expand it\n",
    "2. Click on `bronze` schema\n",
    "3. Note the tabs: **Tables**, **Volumes**, **Functions**\n",
    "\n",
    "",
    "\n",
    "> **Exam Tip:** Unity Catalog uses a **3-level namespace**: `catalog.schema.object`. Always use fully qualified names in production code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a46b89e",
   "metadata": {},
   "source": [
    "### Step 6: Create a Volume\n",
    "\n",
    "1. Click on `default` schema\n",
    "2. Click **Create** > **Volume**\n",
    "3. Configure:\n",
    "   - **Name:** `datasets`\n",
    "   - **Volume type:** Managed\n",
    "4. Click **Create**\n",
    "\n",
    "",
    "\n",
    "> **Exam Tip:** **Managed volumes** store data in the metastore-managed location. **External volumes** point to existing cloud storage paths. Both are governed by Unity Catalog permissions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258c9a90",
   "metadata": {},
   "source": [
    "## Part 3: Explore External Connections (~5 min)\n",
    "\n",
    "### Step 7: Navigate to External Data\n",
    "\n",
    "1. In the left sidebar, click **Catalog**\n",
    "2. In the top navigation, click **External Data** (or look in the left panel under your catalog)\n",
    "3. Click **Connections**\n",
    "\n",
    " \"Connections\" view open>\n",
    "\n",
    "### Step 8: Understand External Connections\n",
    "\n",
    "External Connections define how Databricks connects to external data sources (cloud storage, databases, APIs).\n",
    "\n",
    "Review the Connections page and note:\n",
    "- A connection stores **credentials** and **endpoint** information\n",
    "- Connections are used by **External Locations** (cloud storage) and **Foreign Connections** (databases like PostgreSQL, MySQL, SQL Server)\n",
    "- To create a connection: **Create connection** > choose type > provide credentials\n",
    "\n",
    "| Connection Type | Use Case | Example |\n",
    "|---|---|---|\n",
    "| **Storage** | Access files on cloud storage | S3 bucket, ADLS container, GCS bucket |\n",
    "| **Database** | Query external databases via Lakehouse Federation | PostgreSQL, MySQL, SQL Server |\n",
    "\n",
    "> **Note:** Creating actual connections requires cloud credentials. In this training, we explore the UI to understand the concept. Your trainer may demonstrate a live connection.\n",
    "\n",
    "> **Exam Tip:** External Connections are part of Unity Catalog governance. They require `CREATE CONNECTION` privilege and are managed at the **metastore level**. Know the difference between **External Locations** (storage paths) and **Foreign Connections** (database endpoints for Lakehouse Federation).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e07c4c0",
   "metadata": {},
   "source": [
    "## Part 4: Upload Dataset Files (~10 min)\n",
    "\n",
    "### Step 9: Upload Files to the Volume\n",
    "\n",
    "1. Navigate to **Catalog** > `retailhub_{your_name}` > `default` > **Volumes** > `datasets`\n",
    "2. Click **Upload to this volume**\n",
    "3. Upload the following files from the `dataset/` folder:\n",
    "   - `customers/customers.csv`\n",
    "   - `products/products.csv`\n",
    "   - `orders/orders_batch.json`\n",
    "\n",
    "",
    "\n",
    "### Step 10: Verify Files with dbutils\n",
    "\n",
    "Open the lab notebook (`LAB_01_code.ipynb`) and complete the tasks there.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Notebook Tasks\n",
    "\n",
    "Open **`LAB_01_code.ipynb`** and complete all `# TODO` cells.\n",
    "\n",
    "Tasks in the notebook:\n",
    "1. Run the setup cell (`%run ../setup/00_setup`)\n",
    "2. List files in your Volume using `dbutils.fs.ls()`\n",
    "3. Read the customers CSV using `spark.read`\n",
    "4. Display the first 5 rows\n",
    "5. Check the catalog/schema context with `SELECT current_catalog(), current_schema()`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab you:\n",
    "- Created a compute cluster configured for the training\n",
    "- Explored the Unity Catalog 3-level namespace\n",
    "- Created a managed Volume for raw file storage\n",
    "- Uploaded dataset files and verified access via `dbutils`\n",
    "\n",
    "> **What's next:** In LAB 02 you will use these files to build your first ELT ingestion pipeline — reading CSV/JSON, transforming data, and writing Delta tables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}