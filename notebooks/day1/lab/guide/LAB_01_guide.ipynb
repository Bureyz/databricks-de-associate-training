{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf1b8a26",
   "metadata": {},
   "source": [
    "# LAB 01: Platform & Workspace Setup\n",
    "\n",
    "**Duration:** ~35 min  \n",
    "**Day:** 1  \n",
    "**After module:** M01: Platform & Workspace  \n",
    "**Difficulty:** Beginner\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fbc014",
   "metadata": {},
   "source": [
    "## Scenario\n",
    "\n",
    "> *\"It's your first day on the RetailHub data team. Before you can start building the analytics platform, you need to set up your workspace: create a compute cluster, explore Unity Catalog, and upload the raw data files that will be used throughout the training.\"*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8a305c",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "- Create and configure a Databricks compute cluster\n",
    "- Navigate Unity Catalog hierarchy (Catalog > Schema > Table/Volume)\n",
    "- Create and use Unity Catalog Volumes for raw file storage\n",
    "- Upload dataset files to a Volume\n",
    "- Run basic `dbutils` commands\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a46bfef",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Access to Databricks workspace (URL + credentials provided by trainer)\n",
    "- Dataset files available in the Git repository (`dataset/` folder)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133c1c37",
   "metadata": {},
   "source": [
    "## Part 1: Create Compute Cluster (~10 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a50d7d6",
   "metadata": {},
   "source": [
    "### Step 1: Navigate to Compute\n",
    "\n",
    "1. In the left sidebar, click **Compute**\n",
    "2. Click **Create compute**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6097561",
   "metadata": {},
   "source": [
    "### Step 2: Configure Cluster\n",
    "\n",
    "Use the following settings:\n",
    "\n",
    "| Setting | Value |\n",
    "|---------|-------|\n",
    "| **Cluster name** | `training_{your_name}` |\n",
    "| **Policy** | Personal Compute (or as provided by trainer) |\n",
    "| **Access mode** | Single User |\n",
    "| **Databricks Runtime** | Latest LTS (e.g., 15.4 LTS) |\n",
    "| **Node type** | As provided by trainer |\n",
    "| **Terminate after** | 60 minutes of inactivity |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72a91e0",
   "metadata": {},
   "source": [
    "### Step 3: Start the Cluster\n",
    "\n",
    "1. Click **Create compute**\n",
    "2. Wait for the cluster status to change to **Running** (green circle)\n",
    "3. This may take 2-5 minutes\n",
    "\n",
    "> **Exam Tip:** Know the difference between **All-Purpose clusters** (interactive, development) and **Job clusters** (automated, ephemeral). Serverless compute is the default for SQL Warehouses and is becoming the default for notebooks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8a8bc0",
   "metadata": {},
   "source": [
    "## Part 2: Explore Unity Catalog (~10 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d21d49",
   "metadata": {},
   "source": [
    "### Step 4: Open Catalog Explorer\n",
    "\n",
    "1. In the left sidebar, click **Catalog**\n",
    "2. You should see your catalog: `retailhub_{your_name}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc96b64",
   "metadata": {},
   "source": [
    "### Step 5: Explore the Catalog Hierarchy\n",
    "\n",
    "Navigate through the hierarchy:\n",
    "\n",
    "```\n",
    "retailhub_{your_name}          -- CATALOG (top-level namespace)\n",
    "  ├── bronze                   -- SCHEMA (database)\n",
    "  │     ├── (tables)           -- TABLES\n",
    "  │     └── (volumes)          -- VOLUMES\n",
    "  ├── silver                   -- SCHEMA\n",
    "  ├── gold                     -- SCHEMA\n",
    "  └── default                  -- SCHEMA (auto-created)\n",
    "```\n",
    "\n",
    "1. Click on your catalog name to expand it\n",
    "2. Click on `bronze` schema\n",
    "3. Note the tabs: **Tables**, **Volumes**, **Functions**\n",
    "\n",
    "> **Exam Tip:** Unity Catalog uses a **3-level namespace**: `catalog.schema.object`. Always use fully qualified names in production code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8292df",
   "metadata": {},
   "source": [
    "### Step 6: Create a Volume\n",
    "\n",
    "1. Click on `default` schema\n",
    "2. Click **Create** > **Volume**\n",
    "3. Configure:\n",
    "   - **Name:** `datasets`\n",
    "   - **Volume type:** Managed\n",
    "4. Click **Create**\n",
    "\n",
    "> **Exam Tip:** **Managed volumes** store data in the metastore-managed location. **External volumes** point to existing cloud storage paths. Both are governed by Unity Catalog permissions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad04423",
   "metadata": {},
   "source": [
    "## Part 3: Explore External Connections (~5 min)\n",
    "\n",
    "### Step 7: Navigate to External Data\n",
    "\n",
    "1. In the left sidebar, click **Catalog**\n",
    "2. In the top navigation, click **External Data**\n",
    "3. Click **Connections**\n",
    "\n",
    "### Step 8: Understand External Connections\n",
    "\n",
    "External Connections define how Databricks connects to external data sources (cloud storage, databases, APIs).\n",
    "\n",
    "Review the Connections page and note:\n",
    "- A connection stores **credentials** and **endpoint** information\n",
    "- Connections are used by **External Locations** (cloud storage) and **Foreign Connections** (databases like PostgreSQL, MySQL, SQL Server)\n",
    "\n",
    "| Connection Type | Use Case | Example |\n",
    "|---|---|---|\n",
    "| **Storage** | Access files on cloud storage | S3 bucket, ADLS container, GCS bucket |\n",
    "| **Database** | Query external databases via Lakehouse Federation | PostgreSQL, MySQL, SQL Server |\n",
    "\n",
    "> **Exam Tip:** External Connections are part of Unity Catalog governance. They require `CREATE CONNECTION` privilege and are managed at the **metastore level**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1366031f",
   "metadata": {},
   "source": [
    "## Part 4: Upload Dataset Files (~10 min)\n",
    "\n",
    "### Step 9: Upload Files to the Volume\n",
    "\n",
    "1. Navigate to **Catalog** > `retailhub_{your_name}` > `default` > **Volumes** > `datasets`\n",
    "2. Click **Upload to this volume**\n",
    "3. Upload the following files from the `dataset/` folder:\n",
    "   - `customers/customers.csv`\n",
    "   - `products/products.csv`\n",
    "   - `orders/orders_batch.json`\n",
    "\n",
    "### Step 10: Verify Files with dbutils\n",
    "\n",
    "Open the lab notebook (`LAB_01_code.ipynb`) and complete the tasks there.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c3d6a0",
   "metadata": {},
   "source": [
    "## Part 5: Notebook Tasks\n",
    "\n",
    "Open **`LAB_01_code.ipynb`** and complete all `# TODO` cells.\n",
    "\n",
    "| Task | What to do | Key concept |\n",
    "|------|-----------|-------------|\n",
    "| **Task 1** | Verify Catalog Context | `SELECT current_catalog(), current_schema()` |\n",
    "| **Task 2** | List Files in Your Volume | `dbutils.fs.ls()` with Volume path |\n",
    "| **Task 3** | Read CSV File | `spark.read.format(\"csv\")` with header and inferSchema |\n",
    "| **Task 4** | Inspect Schema | `.printSchema()` method |\n",
    "| **Task 5** | Explore dbutils | `dbutils.fs.head()` to read raw bytes |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c50727",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab you:\n",
    "- Created a compute cluster configured for the training\n",
    "- Explored the Unity Catalog 3-level namespace\n",
    "- Created a managed Volume for raw file storage\n",
    "- Explored External Connections in the UI\n",
    "- Uploaded dataset files and verified access via `dbutils`\n",
    "\n",
    "> **What's next:** In LAB 02 you will use these files to build your first ELT ingestion pipeline — reading CSV/JSON, transforming data, and writing Delta tables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
