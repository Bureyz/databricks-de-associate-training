{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 01: Platform & Workspace Orientation\n",
    "\n",
    "**Duration:** ~35 min  \n",
    "**Day:** 1  \n",
    "**After module:** M01: Platform & Workspace  \n",
    "**Difficulty:** Beginner\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario\n",
    "\n",
    "> *\"Your first day at RetailHub! Before diving into data engineering, familiarize yourself with the Databricks workspace, Unity Catalog structure, and the tools you'll use throughout the training.\"*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "- Create and configure a Databricks cluster\n",
    "- Navigate the Unity Catalog hierarchy (Catalog → Schema → Table)\n",
    "- Explore external connections and volumes\n",
    "- Upload dataset files to a Databricks Volume\n",
    "- Use `dbutils` for file exploration\n",
    "- Read CSV files into DataFrames\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Access to the Databricks workspace\n",
    "- Trainer has run `00_pre_config.ipynb` to provision your catalog\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Create Your Cluster\n",
    "\n",
    "1. Go to **Compute → Create Cluster**\n",
    "2. Set the cluster name: `<your_name>_cluster`\n",
    "3. Select **Single Node** mode\n",
    "4. Choose **Runtime 15.4 LTS** or newer\n",
    "5. Click **Create** and wait for the cluster to start\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Explore Unity Catalog\n",
    "\n",
    "1. Open **Catalog** in the left sidebar\n",
    "2. Navigate: `retailhub_<your_name>` → `bronze` → Tables\n",
    "3. Observe the three-level namespace: `catalog.schema.table`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: External Connections\n",
    "\n",
    "1. Explore the **External Data** section in Catalog\n",
    "2. Understand how external connections link to cloud storage\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Upload Files\n",
    "\n",
    "1. Navigate to your catalog's **default** schema → **Volumes**\n",
    "2. Upload the dataset files from `dataset/` folder\n",
    "3. Verify files are accessible\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Notebook Tasks\n",
    "\n",
    "Open **`LAB_01_code.ipynb`** and complete the `# TODO` cells.\n",
    "\n",
    "| Task | What to do | Key concept |\n",
    "|------|-----------|-------------|\n",
    "| **Task 1** | Verify Catalog Context | `SELECT current_catalog(), current_schema()` |\n",
    "| **Task 2** | List Files in Volume | `dbutils.fs.ls()` on dataset path |\n",
    "| **Task 3** | Read CSV into DataFrame | `spark.read.csv(path, header=True)` |\n",
    "| **Task 4** | Inspect Schema | `.printSchema()`, `.dtypes` |\n",
    "| **Task 5** | Explore dbutils | `dbutils.fs.head()`, `dbutils.help()` |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab you:\n",
    "- Created your first Databricks cluster\n",
    "- Explored the Unity Catalog namespace\n",
    "- Uploaded datasets to a Volume\n",
    "- Read CSV files with Spark and inspected schemas\n",
    "- Used `dbutils` for file system operations\n",
    "\n",
    "> **Exam Tip:** Unity Catalog uses a 3-level namespace: `catalog.schema.table`. `dbutils.fs.ls()` lists files in cloud storage. `spark.read.csv(path, header=True)` reads CSV with headers.\n",
    "\n",
    "> **What's next:** In LAB 02 you will load data using explicit schemas, transform DataFrames, and save Delta tables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}