{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a762d0d9-0808-4a47-bc4a-a347f59e2be9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad86de30-cb3b-48d3-9de9-f7512b441afe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cleanup Existing Catalogs (Optional)\n",
    "\n",
    "Run this section **before** creating new environments if you need to remove existing ecommerce catalogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fd713d5-c893-42ea-bc81-6da9b062d81c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "# =============================================================================\n",
    "# CLEANUP EXISTING ECOMMERCE CATALOGS\n",
    "# =============================================================================\n",
    "# This will find and DROP all catalogs starting with 'ecommerce_platform_'\n",
    "# WARNING: This is a destructive operation!\n",
    "\n",
    "CATALOG_PREFIX_TO_CLEANUP = \"ecommerce_platform\"\n",
    "\n",
    "# Find all catalogs matching the prefix\n",
    "catalogs_df = spark.sql(\"SHOW CATALOGS\")\n",
    "ecommerce_catalogs = [row.catalog for row in catalogs_df.collect() \n",
    "                      if row.catalog.startswith(CATALOG_PREFIX_TO_CLEANUP)]\n",
    "\n",
    "print(f\"Found {len(ecommerce_catalogs)} catalogs to remove:\")\n",
    "for cat in ecommerce_catalogs:\n",
    "    print(f\"  - {cat}\")\n",
    "\n",
    "print(\"Run the next cell to DROP these catalogs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19e4b18d-647d-434a-a7cb-d54617411808",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "# =============================================================================\n",
    "# DROP ALL FOUND ECOMMERCE CATALOGS\n",
    "# =============================================================================\n",
    "# Uncomment the lines below to execute the cleanup\n",
    "\n",
    "dropped = []\n",
    "failed = []\n",
    "\n",
    "for catalog_name in ecommerce_catalogs:\n",
    "    try:\n",
    "        spark.sql(f\"DROP CATALOG IF EXISTS {catalog_name} CASCADE\")\n",
    "        dropped.append(catalog_name)\n",
    "        print(f\"Dropped: {catalog_name}\")\n",
    "    except Exception as e:\n",
    "        failed.append((catalog_name, str(e)))\n",
    "        print(f\"Failed to drop {catalog_name}: {e}\")\n",
    "\n",
    "\n",
    "print(f\"Cleanup complete: {len(dropped)} dropped, {len(failed)} failed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5574187-de50-45c2-979d-07688dcbcdd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP CATALOG IF EXISTS ecommerce_platform_trainer CASCADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f28e1455-eba8-451b-a2a0-25066715d49a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Adjust these values\n",
    "# =============================================================================\n",
    "\n",
    "TRAINING_GROUP = \"admins\"  # Databricks group with training participants\n",
    "CATALOG_PREFIX = \"ecommerce_platform\"  # Catalog naming: ecommerce_platform_{username}\n",
    "STORAGE_LOCATION = \"abfss://unity-catalog-storage@dbstoragelpmsv3hzon5ee.dfs.core.windows.net/7405610202063360\"  # ->> ad you ADLS url for storage account\n",
    "# GitHub raw content base URL\n",
    "GITHUB_RAW_BASE = \"https://raw.githubusercontent.com/Bureyz/DatabricksDataEngineerOne/main/dataset\" \n",
    "\n",
    "# Files to download (remote_path, local_path in Volume)\n",
    "# Structure: demo/main (exploration), demo/ingestion (streaming), workshop (AWLite)\n",
    "DATASET_FILES = [\n",
    "    # === DEMO - MAIN (exploration, basic demo) ===\n",
    "    (\"demo/main/customers/customers.csv\", \"customers/customers.csv\"),\n",
    "    (\"demo/main/customers/customers_extended.xlsx\", \"customers/customers_extended.xlsx\"),\n",
    "    (\"demo/main/orders/orders_batch.json\", \"orders/orders_batch.json\"),\n",
    "    (\"demo/main/orders/stream/orders_stream_001.json\", \"orders/stream/orders_stream_001.json\"),\n",
    "    (\"demo/main/orders/stream/orders_stream_002.json\", \"orders/stream/orders_stream_002.json\"),\n",
    "    (\"demo/main/orders/stream/orders_stream_003.json\", \"orders/stream/orders_stream_003.json\"),\n",
    "    (\"demo/main/products/products.csv\", \"products/products.csv\"),\n",
    "    \n",
    "    # === DEMO - INGESTION (incremental load, streaming demo) ===\n",
    "    (\"demo/ingestion/customers/customers_extended.csv\", \"demo/ingestion/customers/customers_extended.csv\"),\n",
    "    (\"demo/ingestion/customers/customers_new.csv\", \"demo/ingestion/customers/customers_new.csv\"),\n",
    "    (\"demo/ingestion/orders/stream/orders_stream_004.json\", \"demo/ingestion/orders/stream/orders_stream_004.json\"),\n",
    "    (\"demo/ingestion/orders/stream/orders_stream_005.json\", \"demo/ingestion/orders/stream/orders_stream_005.json\"),\n",
    "    (\"demo/ingestion/orders/stream/orders_stream_006.json\", \"demo/ingestion/orders/stream/orders_stream_006.json\"),\n",
    "    \n",
    "    # === WORKSHOP - AdventureWorks Lite (star schema exercises) ===\n",
    "    (\"workshop/main/Customers.csv\", \"workshop/Customers.csv\"),\n",
    "    (\"workshop/main/Product.csv\", \"workshop/Product.csv\"),\n",
    "    (\"workshop/main/ProductCategory.csv\", \"workshop/ProductCategory.csv\"),\n",
    "    (\"workshop/main/SalesOrderDetail.csv\", \"workshop/SalesOrderDetail.csv\"),\n",
    "    (\"workshop/main/SalesOrderHeader.csv\", \"workshop/SalesOrderHeader.csv\"),\n",
    "    (\"workshop/main/Customers.csv\", \"workshop/Lakeflow/Customers/Customers.csv\"),\n",
    "    (\"workshop/main/Product.csv\", \"workshop/Lakeflow/Product/Product.csv\"),\n",
    "    (\"workshop/main/ProductCategory.csv\", \"workshop/Lakeflow/ProductCategory/ProductCategory.csv\"),\n",
    "    (\"workshop/main/SalesOrderDetail.csv\", \"workshop/Lakeflow/SalesOrderDetail/SalesOrderDetail.csv\"),\n",
    "    (\"workshop/main/SalesOrderHeader.csv\", \"workshop/Lakeflow/SalesOrderHeader/SalesOrderHeader.csv\")\n",
    "]\n",
    "\n",
    "# Schema names (Medallion architecture)\n",
    "BRONZE_SCHEMA = \"bronze\"\n",
    "SILVER_SCHEMA = \"silver\"\n",
    "GOLD_SCHEMA = \"gold\"\n",
    "DEFAULT_SCHEMA = \"default\"\n",
    "\n",
    "# Volume name for datasets\n",
    "VOLUME_NAME = \"datasets\"\n",
    "\n",
    "print(f\"Training group: {TRAINING_GROUP}\")\n",
    "print(f\"Catalog prefix: {CATALOG_PREFIX}\")\n",
    "print(f\"GitHub base: {GITHUB_RAW_BASE}\")\n",
    "print(f\"Files to download: {len(DATASET_FILES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a3ea19a-bf9c-41ba-a575-229e89197931",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Get Users from Training Group\n",
    "\n",
    "We use Databricks REST API to get group members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a7e1831-ca5b-411c-9889-cab9294cc300",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "def get_group_members(group_name):\n",
    "    \"\"\"\n",
    "    Get all members of a Databricks group using REST API.\n",
    "    Returns list of usernames (email addresses).\n",
    "    \"\"\"\n",
    "    context = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "    host = context.apiUrl().get()\n",
    "    token = context.apiToken().get()\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # Get group ID first\n",
    "    groups_url = f\"{host}/api/2.0/preview/scim/v2/Groups\"\n",
    "    params = {\"filter\": f'displayName eq \"{group_name}\"'}\n",
    "    \n",
    "    response = requests.get(groups_url, headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    groups = response.json().get(\"Resources\", [])\n",
    "    if not groups:\n",
    "        raise ValueError(f\"Group '{group_name}' not found\")\n",
    "    \n",
    "    group_id = groups[0][\"id\"]\n",
    "    \n",
    "    # Get group members\n",
    "    group_url = f\"{host}/api/2.0/preview/scim/v2/Groups/{group_id}\"\n",
    "    response = requests.get(group_url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    members = response.json().get(\"members\", [])\n",
    "    \n",
    "    # Get user emails\n",
    "    user_emails = []\n",
    "    for member in members:\n",
    "        if member.get(\"$ref\", \"\").startswith(\"Users/\"):\n",
    "            user_id = member[\"value\"]\n",
    "            user_url = f\"{host}/api/2.0/preview/scim/v2/Users/{user_id}\"\n",
    "            user_response = requests.get(user_url, headers=headers)\n",
    "            user_response.raise_for_status()\n",
    "            user_data = user_response.json()\n",
    "            emails = user_data.get(\"emails\", [])\n",
    "            if emails:\n",
    "                user_emails.append(emails[0].get(\"value\", \"\"))\n",
    "    \n",
    "    return user_emails\n",
    "\n",
    "def sanitize_username(email):\n",
    "    \"\"\"\n",
    "    Convert email to safe catalog name suffix.\n",
    "    Example: jan.kowalski@company.com -> jan_kowalski\n",
    "    Example: krzysztof_burejza_3 -> krzysztof_burejza\n",
    "    \"\"\"\n",
    "    username = email.split('@')[0]\n",
    "    safe_name = re.sub(r'[^a-zA-Z0-9]', '_', username).lower()\n",
    "    safe_name = re.sub(r'_+', '_', safe_name)\n",
    "    \n",
    "    # Remove numbers and underscores from beginning and end\n",
    "    safe_name = re.sub(r'^[0-9_]+', '', safe_name)\n",
    "    safe_name = re.sub(r'[0-9_]+$', '', safe_name)\n",
    "    \n",
    "    if not safe_name:\n",
    "        safe_name = \"user\"\n",
    "        \n",
    "    return safe_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db7fb6d0-8de7-4d69-a245-b71f382273ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get users from training group\n",
    "try:\n",
    "    training_users = get_group_members(TRAINING_GROUP)\n",
    "    print(f\"Found {len(training_users)} users in group '{TRAINING_GROUP}':\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    user_catalog_map = {}\n",
    "    for email in training_users:\n",
    "        safe_name = sanitize_username(email)\n",
    "        \n",
    "        if safe_name in [\"trainer\", \"krzysztof_burejza\"]:\n",
    "            catalog_name = f\"{CATALOG_PREFIX}_trainer\"\n",
    "        else:\n",
    "            catalog_name = f\"{CATALOG_PREFIX}_{safe_name}\"\n",
    "        user_catalog_map[email] = catalog_name\n",
    "        print(f\"  {email} -> {catalog_name}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total: {len(user_catalog_map)} catalogs to create\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not get group members: {e}\")\n",
    "    print(f\"Possible issues:\")\n",
    "    print()\n",
    "    print(f\" 1. Group '{TRAINING_GROUP}' does not exist\")\n",
    "    print(\"  2. You don't have permission to read group members\")\n",
    "    print(\"  3. Group has no members\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58a40b7a-23ce-4a59-a7d2-db63580405bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Create Catalogs and Schemas\n",
    "\n",
    "For each user, we create:\n",
    "- Catalog: `ecommerce_platform_{username}`\n",
    "- Schemas: `bronze`, `silver`, `gold`, `default`\n",
    "- Volume: `datasets` in `default` schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe16c58d-89c4-4b7b-9625-c6cc3e155a35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_user_environment(email, catalog_name, storage_location):\n",
    "    \"\"\"\n",
    "    Create catalog, schemas, and volume for a user.\n",
    "    \"\"\"\n",
    "    results = {\"catalog\": False, \"schemas\": [], \"volume\": False, \"owner\": False}\n",
    "    \n",
    "    try:\n",
    "        # Get trainer (current user)\n",
    "        trainer_email = spark.sql(\"SELECT current_user()\").first()[0]\n",
    "\n",
    "        # Create catalog\n",
    "        spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog_name} MANAGED LOCATION '{storage_location}/{catalog_name}' \")\n",
    "        results[\"catalog\"] = True\n",
    "        \n",
    "        # Create schemas\n",
    "        for schema in [DEFAULT_SCHEMA, BRONZE_SCHEMA, SILVER_SCHEMA, GOLD_SCHEMA]:\n",
    "            spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema}\")\n",
    "            results[\"schemas\"].append(schema)\n",
    "        \n",
    "        # Create volume for datasets\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE VOLUME IF NOT EXISTS {catalog_name}.{DEFAULT_SCHEMA}.{VOLUME_NAME}\n",
    "            COMMENT 'Training datasets volume'\n",
    "        \"\"\")\n",
    "        results[\"volume\"] = True\n",
    "        \n",
    "        # Grant permissions to User and Trainer\n",
    "        spark.sql(f\"GRANT ALL PRIVILEGES ON CATALOG {catalog_name} TO `{email}`\")\n",
    "        if trainer_email != email:\n",
    "            spark.sql(f\"GRANT ALL PRIVILEGES ON CATALOG {catalog_name} TO `{trainer_email}`\")\n",
    "            \n",
    "        results[\"owner\"] = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        results[\"error\"] = str(e)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30bf9ba2-5fda-41d8-8d33-11e5c079751d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create environments for all users\n",
    "creation_results = {}\n",
    "\n",
    "for email, catalog_name in user_catalog_map.items():\n",
    "    print(f\"Processing: {email}\")\n",
    "    result = create_user_environment(email, catalog_name,STORAGE_LOCATION)\n",
    "    creation_results[email] = result\n",
    "    \n",
    "    if \"error\" in result:\n",
    "        print(f\"  ERROR: {result['error']}\")\n",
    "    else:\n",
    "        print(f\"  Catalog: {catalog_name}\")\n",
    "        print(f\"  Schemas: {', '.join(result['schemas'])}\")\n",
    "        print(f\"  Volume: {result['volume']}\")\n",
    "        print(f\"  PRIVILEGES set: {result['owner']}\")\n",
    "\n",
    "successful = sum(1 for r in creation_results.values() if \"error\" not in r)\n",
    "print(f\"Successfully created: {successful}/{len(creation_results)} environments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9f19726-7a5b-4b02-9e26-4095202cad76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Copy Dataset from Repo\n",
    "Copy files directly from the repository folder (`dataset/`) to each user's Volume.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e24d62e6-74f9-4643-89f0-ed764092f9d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "def copy_dataset_to_volume(catalog_name):\n",
    "    \"\"\"\n",
    "    Copy dataset files directly from Repo to user's Volume.\n",
    "    \"\"\"\n",
    "    results = {\"success\": [], \"failed\": []}\n",
    "    \n",
    "    # Paths\n",
    "    # Repo root is one level up from notebooks folder\n",
    "    repo_root = os.path.abspath(\"..\")\n",
    "    source_dataset_path = os.path.join(repo_root, \"dataset\")\n",
    "    \n",
    "    # Target Volume Path\n",
    "    volume_path = f\"/Volumes/{catalog_name}/{DEFAULT_SCHEMA}/{VOLUME_NAME}\"\n",
    "    \n",
    "    print(f\"Copying from: {source_dataset_path}\")\n",
    "    print(f\"To: {volume_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Check if source exists\n",
    "        if not os.path.exists(source_dataset_path):\n",
    "            raise Exception(f\"Source dataset not found at {source_dataset_path}\")\n",
    "            \n",
    "        # Copy entire directory structure\n",
    "        # copytree requires destination to NOT exist if dirs_exist_ok=False (default in older python)\n",
    "        # But in 3.8+ (Databricks) dirs_exist_ok=True works.\n",
    "        shutil.copytree(source_dataset_path, volume_path, dirs_exist_ok=True)\n",
    "        results[\"success\"].append(\"All files copied successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        results[\"failed\"].append((\"dataset folder\", str(e)))\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c54705c6-19d7-432d-8e02-787a6a49a8ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Copy dataset to each user's Volume\n",
    "print(\"Copying dataset to user Volumes...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "copy_results = {}\n",
    "\n",
    "for email, catalog_name in user_catalog_map.items():\n",
    "    print(f\"Copying to: {catalog_name}\")\n",
    "    result = copy_dataset_to_volume(catalog_name)\n",
    "    copy_results[email] = result\n",
    "    \n",
    "    if result[\"failed\"]:\n",
    "        print(f\"Failed: {len(result['failed'])} errors\")\n",
    "        for file, error in result[\"failed\"]:\n",
    "            print(f\"    - {file}: {error}\")\n",
    "    else:\n",
    "        print(\"  Success!\")\n",
    "\n",
    "\n",
    "successful = sum(1 for r in copy_results.values() if not r[\"failed\"])\n",
    "print(f\"Fully completed: {successful}/{len(copy_results)} volumes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dbac3dc-57c5-4859-95a9-53b4e79f6966",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Verify Setup\n",
    "\n",
    "Verify all environments are ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2505d2ba-d7bc-4cce-a0b9-32f77089fc7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAINING ENVIRONMENT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "for email, catalog_name in user_catalog_map.items():\n",
    "    print(f\"User: {email}\")\n",
    "    print(f\"  Catalog: {catalog_name}\")\n",
    "    print(f\"  Schemas: {BRONZE_SCHEMA}, {SILVER_SCHEMA}, {GOLD_SCHEMA}, {DEFAULT_SCHEMA}\")\n",
    "    \n",
    "    volume_path = f\"/Volumes/{catalog_name}/{DEFAULT_SCHEMA}/{VOLUME_NAME}\"\n",
    "    try:\n",
    "        # Check main folders\n",
    "        folders = dbutils.fs.ls(volume_path)\n",
    "        print(f\"  Volume root: {[f.name for f in folders]}\")\n",
    "        \n",
    "        # Check demo/main structure\n",
    "        demo_main_path = f\"{volume_path}/demo/main\"\n",
    "        try:\n",
    "            demo_main = dbutils.fs.ls(demo_main_path)\n",
    "            print(f\"    demo/main: {[f.name for f in demo_main]}\")\n",
    "        except:\n",
    "            print(f\"    demo/main: NOT FOUND\")\n",
    "        \n",
    "        # Check workshop structure\n",
    "        workshop_path = f\"{volume_path}/workshop\"\n",
    "        try:\n",
    "            workshop = dbutils.fs.ls(workshop_path)\n",
    "            print(f\"    workshop: {len(workshop)} files (AWLite)\")\n",
    "        except:\n",
    "            print(f\"    workshop: NOT FOUND\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Volume: ERROR - {e}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "\n",
    "print(\"Dataset structure in each Volume:\")\n",
    "print(\"  /demo/main/       - Basic exploration & demo\")\n",
    "print(\"  /demo/ingestion/  - Incremental load & streaming\")\n",
    "print(\"  /workshop/        - AdventureWorks Lite (star schema)\")\n",
    "print()\n",
    "print(\"Participants can now run 00_setup.ipynb to validate their environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b1bf63c-d98d-414e-b533-1788609f9bc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cleanup (After Training)\n",
    "\n",
    "Run this section to remove all training catalogs after the training is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6c4ec0c-3d1c-46d0-aeff-c58029777636",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CLEANUP - Remove all training catalogs\n",
    "# =============================================================================\n",
    "# WARNING: This will DELETE all training data!\n",
    "\n",
    "# Uncomment to run:\n",
    "# for email, catalog_name in user_catalog_map.items():\n",
    "#     try:\n",
    "#         spark.sql(f\"DROP CATALOG IF EXISTS {catalog_name} CASCADE\")\n",
    "#         print(f\"Dropped: {catalog_name}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to drop {catalog_name}: {e}\")\n",
    "# \n",
    "# print(\"\\nCleanup complete.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4873620425682545,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "00_pre_config",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
