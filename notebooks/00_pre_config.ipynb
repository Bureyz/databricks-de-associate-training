{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a762d0d9-0808-4a47-bc4a-a347f59e2be9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f28e1455-eba8-451b-a2a0-25066715d49a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training group: dp_trn_1\nCatalog prefix: ecommerce_platform\nGitHub base: https://raw.githubusercontent.com/Bureyz/DataEngineeringOne/main/dataset\nFiles to download: 27\n  - Demo main: 6 files\n  - Demo ingestion: 15 files\n  - Workshop (AWLite): 6 files\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Adjust these values\n",
    "# =============================================================================\n",
    "\n",
    "TRAINING_GROUP = \"dp_trn_1\"  # Databricks group with training participants\n",
    "CATALOG_PREFIX = \"ecommerce_platform\"  # Catalog naming: ecommerce_platform_{username}\n",
    "STORAGE_LOCATION = \"abfss://unity-catalog-storage@dbstorage3laxlzjqlvw46.dfs.core.windows.net/938068554870483\"\n",
    "# GitHub raw content base URL\n",
    "GITHUB_RAW_BASE = \"https://raw.githubusercontent.com/Bureyz/DataEngineeringOne/main/dataset\"\n",
    "\n",
    "# Files to download (remote_path, local_path in Volume)\n",
    "# Structure: demo/main (exploration), demo/ingestion (streaming), workshop (AWLite)\n",
    "DATASET_FILES = [\n",
    "    # === DEMO - MAIN (exploration, basic demo) ===\n",
    "    (\"demo/main/customers/customers.csv\", \"demo/main/customers/customers.csv\"),\n",
    "    (\"demo/main/orders/orders_batch.json\", \"demo/main/orders/orders_batch.json\"),\n",
    "    (\"demo/main/orders/stream/orders_stream_001.json\", \"demo/main/orders/stream/orders_stream_001.json\"),\n",
    "    (\"demo/main/orders/stream/orders_stream_002.json\", \"demo/main/orders/stream/orders_stream_002.json\"),\n",
    "    (\"demo/main/orders/stream/orders_stream_003.json\", \"demo/main/orders/stream/orders_stream_003.json\"),\n",
    "    (\"demo/main/products/products.csv\", \"demo/main/products/products.csv\"),\n",
    "    \n",
    "    # === DEMO - INGESTION (incremental load, streaming demo) ===\n",
    "    (\"demo/ingestion/customers/customers_extended.csv\", \"demo/ingestion/customers/customers_extended.csv\"),\n",
    "    (\"demo/ingestion/customers/customers_new.csv\", \"demo/ingestion/customers/customers_new.csv\"),\n",
    "    (\"demo/ingestion/customers/customers2.csv\", \"demo/ingestion/customers/customers2.csv\"),\n",
    "    (\"demo/ingestion/orders/stream/orders_stream_004.json\", \"demo/ingestion/orders/stream/orders_stream_004.json\"),\n",
    "    (\"demo/ingestion/orders/stream/orders_stream_005.json\", \"demo/ingestion/orders/stream/orders_stream_005.json\"),\n",
    "    (\"demo/ingestion/orders/stream/orders_stream_006.json\", \"demo/ingestion/orders/stream/orders_stream_006.json\"),\n",
    "    (\"demo/ingestion/orders/stream/orders_stream_007.json\", \"demo/ingestion/orders/stream/orders_stream_007.json\"),\n",
    "    (\"demo/ingestion/orders/stream/orders_stream_008.json\", \"demo/ingestion/orders/stream/orders_stream_008.json\"),\n",
    "    (\"demo/ingestion/orders/stream/orders_stream_009.json\", \"demo/ingestion/orders/stream/orders_stream_009.json\"),\n",
    "    (\"demo/ingestion/orders/stream/orders_stream_010.json\", \"demo/ingestion/orders/stream/orders_stream_010.json\"),\n",
    "    (\"demo/ingestion/orders/stream/orders_stream_011.json\", \"demo/ingestion/orders/stream/orders_stream_011.json\"),\n",
    "    (\"demo/ingestion/orders/stream/orders_stream_012.json\", \"demo/ingestion/orders/stream/orders_stream_012.json\"),\n",
    "    (\"demo/ingestion/orders/stream/orders_stream_013.json\", \"demo/ingestion/orders/stream/orders_stream_013.json\"),\n",
    "    (\"demo/ingestion/orders/stream/orders_stream_014.json\", \"demo/ingestion/orders/stream/orders_stream_014.json\"),\n",
    "    (\"demo/ingestion/orders/stream/orders_stream_015.json\", \"demo/ingestion/orders/stream/orders_stream_015.json\"),\n",
    "    \n",
    "    # === WORKSHOP - AdventureWorks Lite (star schema exercises) ===\n",
    "    (\"workshop/Address.csv\", \"workshop/Address.csv\"),\n",
    "    (\"workshop/Customers.csv\", \"workshop/Customers.csv\"),\n",
    "    (\"workshop/Product.csv\", \"workshop/Product.csv\"),\n",
    "    (\"workshop/ProductCategory.csv\", \"workshop/ProductCategory.csv\"),\n",
    "    (\"workshop/SalesOrderDetail.csv\", \"workshop/SalesOrderDetail.csv\"),\n",
    "    (\"workshop/SalesOrderHeader.csv\", \"workshop/SalesOrderHeader.csv\"),\n",
    "]\n",
    "\n",
    "# Schema names (Medallion architecture)\n",
    "BRONZE_SCHEMA = \"bronze\"\n",
    "SILVER_SCHEMA = \"silver\"\n",
    "GOLD_SCHEMA = \"gold\"\n",
    "DEFAULT_SCHEMA = \"default\"\n",
    "\n",
    "# Volume name for datasets\n",
    "VOLUME_NAME = \"datasets\"\n",
    "\n",
    "print(f\"Training group: {TRAINING_GROUP}\")\n",
    "print(f\"Catalog prefix: {CATALOG_PREFIX}\")\n",
    "print(f\"GitHub base: {GITHUB_RAW_BASE}\")\n",
    "print(f\"Files to download: {len(DATASET_FILES)}\")\n",
    "print(f\"  - Demo main: 6 files\")\n",
    "print(f\"  - Demo ingestion: 15 files\")\n",
    "print(f\"  - Workshop (AWLite): 6 files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a3ea19a-bf9c-41ba-a575-229e89197931",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Get Users from Training Group\n",
    "\n",
    "We use Databricks REST API to get group members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a7e1831-ca5b-411c-9889-cab9294cc300",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "def get_group_members(group_name):\n",
    "    \"\"\"\n",
    "    Get all members of a Databricks group using REST API.\n",
    "    Returns list of usernames (email addresses).\n",
    "    \"\"\"\n",
    "    context = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "    host = context.apiUrl().get()\n",
    "    token = context.apiToken().get()\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # Get group ID first\n",
    "    groups_url = f\"{host}/api/2.0/preview/scim/v2/Groups\"\n",
    "    params = {\"filter\": f'displayName eq \"{group_name}\"'}\n",
    "    \n",
    "    response = requests.get(groups_url, headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    groups = response.json().get(\"Resources\", [])\n",
    "    if not groups:\n",
    "        raise ValueError(f\"Group '{group_name}' not found\")\n",
    "    \n",
    "    group_id = groups[0][\"id\"]\n",
    "    \n",
    "    # Get group members\n",
    "    group_url = f\"{host}/api/2.0/preview/scim/v2/Groups/{group_id}\"\n",
    "    response = requests.get(group_url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    members = response.json().get(\"members\", [])\n",
    "    \n",
    "    # Get user emails\n",
    "    user_emails = []\n",
    "    for member in members:\n",
    "        if member.get(\"$ref\", \"\").startswith(\"Users/\"):\n",
    "            user_id = member[\"value\"]\n",
    "            user_url = f\"{host}/api/2.0/preview/scim/v2/Users/{user_id}\"\n",
    "            user_response = requests.get(user_url, headers=headers)\n",
    "            user_response.raise_for_status()\n",
    "            user_data = user_response.json()\n",
    "            emails = user_data.get(\"emails\", [])\n",
    "            if emails:\n",
    "                user_emails.append(emails[0].get(\"value\", \"\"))\n",
    "    \n",
    "    return user_emails\n",
    "\n",
    "def sanitize_username(email):\n",
    "    \"\"\"\n",
    "    Convert email to safe catalog name suffix.\n",
    "    Example: jan.kowalski@company.com -> jan_kowalski\n",
    "    \"\"\"\n",
    "    username = email.split('@')[0]\n",
    "    safe_name = re.sub(r'[^a-zA-Z0-9]', '_', username).lower()\n",
    "    safe_name = re.sub(r'_+', '_', safe_name)\n",
    "    safe_name = safe_name.strip('_')\n",
    "    return safe_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db7fb6d0-8de7-4d69-a245-b71f382273ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 users in group 'dp_trn_1':\n============================================================\n  bureyz@kzbdev.com -> ecommerce_platform_bureyz\n  krzysztof.burejza@outlook.com -> ecommerce_platform_training\n============================================================\nTotal: 2 catalogs to create\n"
     ]
    }
   ],
   "source": [
    "# Get users from training group\n",
    "try:\n",
    "    training_users = get_group_members(TRAINING_GROUP)\n",
    "    print(f\"Found {len(training_users)} users in group '{TRAINING_GROUP}':\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    user_catalog_map = {}\n",
    "    for email in training_users:\n",
    "        safe_name = sanitize_username(email)\n",
    "        \n",
    "        if safe_name in [\"trainer\", \"krzysztof_burejza\"]:\n",
    "            catalog_name = f\"{CATALOG_PREFIX}_training\"\n",
    "        else:\n",
    "            catalog_name = f\"{CATALOG_PREFIX}_{safe_name}\"\n",
    "        user_catalog_map[email] = catalog_name\n",
    "        print(f\"  {email} -> {catalog_name}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total: {len(user_catalog_map)} catalogs to create\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not get group members: {e}\")\n",
    "    print(f\"Possible issues:\")\n",
    "    print(f\" 1. Group '{TRAINING_GROUP}' does not exist\")\n",
    "    print(\"  2. You don't have permission to read group members\")\n",
    "    print(\"  3. Group has no members\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58a40b7a-23ce-4a59-a7d2-db63580405bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Create Catalogs and Schemas\n",
    "\n",
    "For each user, we create:\n",
    "- Catalog: `ecommerce_platform_{username}`\n",
    "- Schemas: `bronze`, `silver`, `gold`, `default`\n",
    "- Volume: `datasets` in `default` schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe16c58d-89c4-4b7b-9625-c6cc3e155a35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_user_environment(email, catalog_name,storage_location):\n",
    "    \"\"\"\n",
    "    Create catalog, schemas, and volume for a user.\n",
    "    \"\"\"\n",
    "    results = {\"catalog\": False, \"schemas\": [], \"volume\": False, \"owner\": False}\n",
    "    \n",
    "    try:\n",
    "        # Create catalog\n",
    "        spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog_name} MANAGED LOCATION '{storage_location}/{catalog_name}' \")\n",
    "        results[\"catalog\"] = True\n",
    "        \n",
    "        # Create schemas\n",
    "        for schema in [DEFAULT_SCHEMA, BRONZE_SCHEMA, SILVER_SCHEMA, GOLD_SCHEMA]:\n",
    "            spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema}\")\n",
    "            results[\"schemas\"].append(schema)\n",
    "        \n",
    "        # Create volume for datasets\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE VOLUME IF NOT EXISTS {catalog_name}.{DEFAULT_SCHEMA}.{VOLUME_NAME}\n",
    "            COMMENT 'Training datasets volume'\n",
    "        \"\"\")\n",
    "        results[\"volume\"] = True\n",
    "        \n",
    "        # Set owner to user\n",
    "        spark.sql(f\"ALTER CATALOG {catalog_name} SET OWNER TO `{email}`\")\n",
    "        results[\"owner\"] = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        results[\"error\"] = str(e)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30bf9ba2-5fda-41d8-8d33-11e5c079751d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: bureyz@kzbdev.com\n  Catalog: ecommerce_platform_bureyz\n  Schemas: default, bronze, silver, gold\n  Volume: True\n  Owner set: True\nProcessing: krzysztof.burejza@outlook.com\n  Catalog: ecommerce_platform_training\n  Schemas: default, bronze, silver, gold\n  Volume: True\n  Owner set: True\nSuccessfully created: 2/2 environments\n"
     ]
    }
   ],
   "source": [
    "# Create environments for all users\n",
    "creation_results = {}\n",
    "\n",
    "for email, catalog_name in user_catalog_map.items():\n",
    "    print(f\"Processing: {email}\")\n",
    "    result = create_user_environment(email, catalog_name,STORAGE_LOCATION)\n",
    "    creation_results[email] = result\n",
    "    \n",
    "    if \"error\" in result:\n",
    "        print(f\"  ERROR: {result['error']}\")\n",
    "    else:\n",
    "        print(f\"  Catalog: {catalog_name}\")\n",
    "        print(f\"  Schemas: {', '.join(result['schemas'])}\")\n",
    "        print(f\"  Volume: {result['volume']}\")\n",
    "        print(f\"  Owner set: {result['owner']}\")\n",
    "\n",
    "successful = sum(1 for r in creation_results.values() if \"error\" not in r)\n",
    "print(f\"Successfully created: {successful}/{len(creation_results)} environments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9f19726-7a5b-4b02-9e26-4095202cad76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Download Dataset from GitHub\n",
    "\n",
    "Download files directly from GitHub raw content to each user's Volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e24d62e6-74f9-4643-89f0-ed764092f9d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "def download_dataset_to_volume(catalog_name):\n",
    "    \"\"\"\n",
    "    Download dataset files directly from GitHub to user's Volume.\n",
    "    Creates subdirectory structure.\n",
    "    - CSV/JSON files: saved as-is (text)\n",
    "    - products.csv: converted to Parquet for demo purposes\n",
    "    \"\"\"\n",
    "    volume_path = f\"/Volumes/{catalog_name}/{DEFAULT_SCHEMA}/{VOLUME_NAME}\"\n",
    "    results = {\"success\": [], \"failed\": []}\n",
    "    \n",
    "    # Track created directories to avoid redundant checks\n",
    "    created_dirs = set()\n",
    "    \n",
    "    for remote_path, local_path in DATASET_FILES:\n",
    "        url = f\"{GITHUB_RAW_BASE}/{remote_path}\"\n",
    "        dest_path = f\"{volume_path}/{local_path}\"\n",
    "        \n",
    "        # Check if this is products.csv that should be converted to Parquet\n",
    "        convert_to_parquet = \"products/products.csv\" in remote_path and \"demo/main\" in remote_path\n",
    "        \n",
    "        try:\n",
    "            # Create parent directories if needed\n",
    "            parent_dir = \"/\".join(dest_path.split(\"/\")[:-1])\n",
    "            if parent_dir not in created_dirs:\n",
    "                try:\n",
    "                    dbutils.fs.mkdirs(parent_dir)\n",
    "                except:\n",
    "                    pass  # Directory might already exist\n",
    "                created_dirs.add(parent_dir)\n",
    "            \n",
    "            # Download file content\n",
    "            response = urllib.request.urlopen(url)\n",
    "            content = response.read().decode('utf-8')\n",
    "            \n",
    "            if convert_to_parquet:\n",
    "                # Save CSV temporarily, read with Spark, write as Parquet\n",
    "                dbutils.fs.put(dest_path, content, overwrite=True)\n",
    "                \n",
    "                # Read CSV and save as Parquet\n",
    "                parquet_path = dest_path.replace(\".csv\", \".parquet\")\n",
    "                df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(dest_path)\n",
    "                df.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "                \n",
    "                # Keep both CSV and Parquet for flexibility\n",
    "                results[\"success\"].append(local_path)\n",
    "                results[\"success\"].append(local_path.replace(\".csv\", \".parquet\"))\n",
    "            else:\n",
    "                # Save as text file (CSV, JSON)\n",
    "                dbutils.fs.put(dest_path, content, overwrite=True)\n",
    "                results[\"success\"].append(local_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[\"failed\"].append((local_path, str(e)))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c54705c6-19d7-432d-8e02-787a6a49a8ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset to user Volumes...\n============================================================\n\nDownloading to: ecommerce_platform_bureyz\n  ✗ Failed: 27 files\n    - demo/main/customers/customers.csv: HTTP Error 404: Not Found\n    - demo/main/orders/orders_batch.json: HTTP Error 404: Not Found\n    - demo/main/orders/stream/orders_stream_001.json: HTTP Error 404: Not Found\n    - demo/main/orders/stream/orders_stream_002.json: HTTP Error 404: Not Found\n    - demo/main/orders/stream/orders_stream_003.json: HTTP Error 404: Not Found\n    - demo/main/products/products.csv: HTTP Error 404: Not Found\n    - demo/ingestion/customers/customers_extended.csv: HTTP Error 404: Not Found\n    - demo/ingestion/customers/customers_new.csv: HTTP Error 404: Not Found\n    - demo/ingestion/customers/customers2.csv: HTTP Error 404: Not Found\n    - demo/ingestion/orders/stream/orders_stream_004.json: HTTP Error 404: Not Found\n    - demo/ingestion/orders/stream/orders_stream_005.json: HTTP Error 404: Not Found\n    - demo/ingestion/orders/stream/orders_stream_006.json: HTTP Error 404: Not Found\n    - demo/ingestion/orders/stream/orders_stream_007.json: HTTP Error 404: Not Found\n    - demo/ingestion/orders/stream/orders_stream_008.json: HTTP Error 404: Not Found\n    - demo/ingestion/orders/stream/orders_stream_009.json: HTTP Error 404: Not Found\n    - demo/ingestion/orders/stream/orders_stream_010.json: HTTP Error 404: Not Found\n    - demo/ingestion/orders/stream/orders_stream_011.json: HTTP Error 404: Not Found\n    - demo/ingestion/orders/stream/orders_stream_012.json: HTTP Error 404: Not Found\n    - demo/ingestion/orders/stream/orders_stream_013.json: HTTP Error 404: Not Found\n    - demo/ingestion/orders/stream/orders_stream_014.json: HTTP Error 404: Not Found\n    - demo/ingestion/orders/stream/orders_stream_015.json: HTTP Error 404: Not Found\n    - workshop/Address.csv: HTTP Error 404: Not Found\n    - workshop/Customers.csv: HTTP Error 404: Not Found\n    - workshop/Product.csv: HTTP Error 404: Not Found\n    - workshop/ProductCategory.csv: HTTP Error 404: Not Found\n    - workshop/SalesOrderDetail.csv: HTTP Error 404: Not Found\n    - workshop/SalesOrderHeader.csv: HTTP Error 404: Not Found\n\nDownloading to: ecommerce_platform_training\n  ✗ Failed: 27 files\n    - demo/main/customers/customers.csv: HTTP Error 404: Not Found\n    - demo/main/orders/orders_batch.json: HTTP Error 404: Not Found\n    - demo/main/orders/stream/orders_stream_001.json: HTTP Error 404: Not Found\n    - demo/main/orders/stream/orders_stream_002.json: HTTP Error 404: Not Found\n    - demo/main/orders/stream/orders_stream_003.json: HTTP Error 404: Not Found\n    - demo/main/products/products.csv: HTTP Error 404: Not Found\n    - demo/ingestion/customers/customers_extended.csv: HTTP Error 404: Not Found\n    - demo/ingestion/customers/customers_new.csv: HTTP Error 404: Not Found\n    - demo/ingestion/customers/customers2.csv: HTTP Error 404: Not Found\n    - demo/ingestion/orders/stream/orders_stream_004.json: HTTP Error 404: Not Found\n    - demo/ingestion/orders/stream/orders_stream_005.json: HTTP Error 404: Not Found\n    - demo/ingestion/orders/stream/orders_stream_006.json: HTTP Error 404: Not Found\n    - demo/ingestion/orders/stream/orders_stream_007.json: HTTP Error 404: Not Found\n    - demo/ingestion/orders/stream/orders_stream_008.json: HTTP Error 404: Not Found\n    - demo/ingestion/orders/stream/orders_stream_009.json: HTTP Error 404: Not Found\n    - demo/ingestion/orders/stream/orders_stream_010.json: HTTP Error 404: Not Found\n    - demo/ingestion/orders/stream/orders_stream_011.json: HTTP Error 404: Not Found\n    - demo/ingestion/orders/stream/orders_stream_012.json: HTTP Error 404: Not Found\n    - demo/ingestion/orders/stream/orders_stream_013.json: HTTP Error 404: Not Found\n    - demo/ingestion/orders/stream/orders_stream_014.json: HTTP Error 404: Not Found\n    - demo/ingestion/orders/stream/orders_stream_015.json: HTTP Error 404: Not Found\n    - workshop/Address.csv: HTTP Error 404: Not Found\n    - workshop/Customers.csv: HTTP Error 404: Not Found\n    - workshop/Product.csv: HTTP Error 404: Not Found\n    - workshop/ProductCategory.csv: HTTP Error 404: Not Found\n    - workshop/SalesOrderDetail.csv: HTTP Error 404: Not Found\n    - workshop/SalesOrderHeader.csv: HTTP Error 404: Not Found\n\n============================================================\nFully completed: 0/2 volumes\n"
     ]
    }
   ],
   "source": [
    "# Download dataset to each user's Volume\n",
    "print(\"Downloading dataset to user Volumes...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "download_results = {}\n",
    "\n",
    "for email, catalog_name in user_catalog_map.items():\n",
    "    print(f\"\\nDownloading to: {catalog_name}\")\n",
    "    result = download_dataset_to_volume(catalog_name)\n",
    "    download_results[email] = result\n",
    "    \n",
    "    if result[\"success\"]:\n",
    "        print(f\"  ✓ Downloaded: {len(result['success'])} files\")\n",
    "    if result[\"failed\"]:\n",
    "        print(f\"  ✗ Failed: {len(result['failed'])} files\")\n",
    "        for file, error in result[\"failed\"]:\n",
    "            print(f\"    - {file}: {error}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "successful = sum(1 for r in download_results.values() if not r[\"failed\"])\n",
    "print(f\"Fully completed: {successful}/{len(download_results)} volumes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dbac3dc-57c5-4859-95a9-53b4e79f6966",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Verify Setup\n",
    "\n",
    "Verify all environments are ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2505d2ba-d7bc-4cce-a0b9-32f77089fc7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\nTRAINING ENVIRONMENT SUMMARY\n============================================================\n\nUser: bureyz@kzbdev.com\n  Catalog: ecommerce_platform_bureyz\n  Schemas: bronze, silver, gold, default\n  Volume: ERROR - An error occurred while calling o513.ls.\n: com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'ecommerce_platform_bureyz'.\n\tat com.databricks.sql.managedcatalog.client.ErrorDetailsHandlerImpl.wrapServiceException(ErrorDetailsHandler.scala:119)\n\tat com.databricks.sql.managedcatalog.client.ErrorDetailsHandlerImpl.wrapServiceException$(ErrorDetailsHandler.scala:88)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:44)\n\tat com.databricks.sql.managedcatalog.client.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7912)\n\tat com.databricks.sql.managedcatalog.client.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7896)\n\tat com.databricks.sql.managedcatalog.client.ManagedCatalogClientImpl.getVolume(ManagedCatalogClientImpl.scala:8862)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.getVolume(ManagedCatalogCommon.scala:3939)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$getVolume$1(ProfiledManagedCatalog.scala:1486)\n\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:2065)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:74)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:73)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.getVolume(ProfiledManagedCatalog.scala:1486)\n\tat com.databricks.sql.acl.fs.volumes.VolumePath$.$anonfun$registerSAM$1(VolumePath.scala:271)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.acl.fs.volumes.VolumePath$.registerSAM(VolumePath.scala:259)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.register(CredentialScopeSQLHelper.scala:283)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.registerPathAccess(CredentialScopeSQLHelper.scala:1171)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.registerPathAccess(CredentialScopeSQLHelper.scala:1141)\n\tat jdk.internal.reflect.GeneratedMethodAccessor200.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat com.databricks.ucs.DBFSUtilsPermissionsChecker.$anonfun$check$2(DBFSUtilsPermissionsChecker.scala:63)\n\tat com.databricks.ucs.DBFSUtilsPermissionsChecker.$anonfun$check$2$adapted(DBFSUtilsPermissionsChecker.scala:59)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat com.databricks.ucs.DBFSUtilsPermissionsChecker.check(DBFSUtilsPermissionsChecker.scala:59)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$checkPermission$1(DBUtilsCore.scala:201)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:197)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsImpl(DBUtilsCore.scala:323)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:295)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:104)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:104)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:104)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:104)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$recordDbutilsFsOp$6(DBUtilsCore.scala:186)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:186)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:295)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n\nUser: krzysztof.burejza@outlook.com\n  Catalog: ecommerce_platform_training\n  Schemas: bronze, silver, gold, default\n  Volume root: ['demo/', 'workshop/']\n    demo/main: ['customers/', 'orders/', 'products/']\n    workshop: 0 files (AWLite)\n\n============================================================\nPRE-CONFIGURATION COMPLETE\n============================================================\n\nDataset structure in each Volume:\n  /demo/main/       - Basic exploration & demo\n  /demo/ingestion/  - Incremental load & streaming\n  /workshop/        - AdventureWorks Lite (star schema)\n\nParticipants can now run 00_setup.ipynb to validate their environment.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAINING ENVIRONMENT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "for email, catalog_name in user_catalog_map.items():\n",
    "    print(f\"User: {email}\")\n",
    "    print(f\"  Catalog: {catalog_name}\")\n",
    "    print(f\"  Schemas: {BRONZE_SCHEMA}, {SILVER_SCHEMA}, {GOLD_SCHEMA}, {DEFAULT_SCHEMA}\")\n",
    "    \n",
    "    volume_path = f\"/Volumes/{catalog_name}/{DEFAULT_SCHEMA}/{VOLUME_NAME}\"\n",
    "    try:\n",
    "        # Check main folders\n",
    "        folders = dbutils.fs.ls(volume_path)\n",
    "        print(f\"  Volume root: {[f.name for f in folders]}\")\n",
    "        \n",
    "        # Check demo/main structure\n",
    "        demo_main_path = f\"{volume_path}/demo/main\"\n",
    "        try:\n",
    "            demo_main = dbutils.fs.ls(demo_main_path)\n",
    "            print(f\"    demo/main: {[f.name for f in demo_main]}\")\n",
    "        except:\n",
    "            print(f\"    demo/main: NOT FOUND\")\n",
    "        \n",
    "        # Check workshop structure\n",
    "        workshop_path = f\"{volume_path}/workshop\"\n",
    "        try:\n",
    "            workshop = dbutils.fs.ls(workshop_path)\n",
    "            print(f\"    workshop: {len(workshop)} files (AWLite)\")\n",
    "        except:\n",
    "            print(f\"    workshop: NOT FOUND\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Volume: ERROR - {e}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PRE-CONFIGURATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Dataset structure in each Volume:\")\n",
    "print(\"  /demo/main/       - Basic exploration & demo\")\n",
    "print(\"  /demo/ingestion/  - Incremental load & streaming\")\n",
    "print(\"  /workshop/        - AdventureWorks Lite (star schema)\")\n",
    "print()\n",
    "print(\"Participants can now run 00_setup.ipynb to validate their environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b1bf63c-d98d-414e-b533-1788609f9bc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cleanup (After Training)\n",
    "\n",
    "Run this section to remove all training catalogs after the training is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6c4ec0c-3d1c-46d0-aeff-c58029777636",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CLEANUP - Remove all training catalogs\n",
    "# =============================================================================\n",
    "# WARNING: This will DELETE all training data!\n",
    "\n",
    "# Uncomment to run:\n",
    "# for email, catalog_name in user_catalog_map.items():\n",
    "#     try:\n",
    "#         spark.sql(f\"DROP CATALOG IF EXISTS {catalog_name} CASCADE\")\n",
    "#         print(f\"Dropped: {catalog_name}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to drop {catalog_name}: {e}\")\n",
    "# \n",
    "# print(\"\\nCleanup complete.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_pre_config",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}