{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd8156c0-3236-4723-b053-582f2a7f94e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Pre-Training Configuration\n",
    "\n",
    "## Purpose\n",
    "This notebook is run **once by the trainer** before the training starts.  \n",
    "It creates isolated environments for each participant:\n",
    "\n",
    "1. **Catalog** per user: `retailhub_{username}`\n",
    "2. **Schemas**: `bronze`, `silver`, `gold`, `default`\n",
    "3. **Volume**: `datasets` (managed) in the `default` schema\n",
    "4. **Dataset files**: copied from the Git repo to each user's Volume\n",
    "5. **Permissions**: full access granted to each participant on their catalog\n",
    "\n",
    "**After running this notebook**, participants can run `00_setup.ipynb` to validate their environment.\n",
    "\n",
    "---\n",
    "\n",
    "### Checklist before running:\n",
    "- [ ] Databricks workspace is ready\n",
    "- [ ] Training group exists with all participants added\n",
    "- [ ] This repo is cloned via Repos in the workspace\n",
    "- [ ] You have `CREATE CATALOG` and `MANAGE` privileges (account admin or metastore admin)\n",
    "- [ ] Storage location for managed catalogs is configured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22abc3b6-1a7e-4001-9534-02a4e962e297",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 0: Configuration\n",
    "\n",
    "Adjust the values below for your training session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3a0da4a-b8c9-43c2-9450-f10d03ccce9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION -- Adjust these values for your training\n",
    "# =============================================================================\n",
    "\n",
    "# Databricks group containing all training participants\n",
    "TRAINING_GROUP = \"admins\"\n",
    "\n",
    "# Catalog naming: retailhub_{username}\n",
    "CATALOG_PREFIX = \"retailhub\"\n",
    "\n",
    "# Managed location for catalogs (ADLS, S3, or GCS path)\n",
    "# Example Azure: \"abfss://container@account.dfs.core.windows.net/path\"\n",
    "# Example AWS:   \"s3://bucket/path\"\n",
    "# Leave empty to use the metastore default location\n",
    "STORAGE_LOCATION = \"abfss://unity-catalog-storage@dbstoragexlcs5kgkoop2g.dfs.core.windows.net/7405606614957089\"\n",
    "\n",
    "# Schema names (Medallion architecture)\n",
    "BRONZE_SCHEMA = \"bronze\"\n",
    "SILVER_SCHEMA = \"silver\"\n",
    "GOLD_SCHEMA = \"gold\"\n",
    "DEFAULT_SCHEMA = \"default\"\n",
    "\n",
    "# Volume name for datasets\n",
    "VOLUME_NAME = \"datasets\"\n",
    "\n",
    "print(f\"Training group:   {TRAINING_GROUP}\")\n",
    "print(f\"Catalog prefix:   {CATALOG_PREFIX}\")\n",
    "print(f\"Storage location: {STORAGE_LOCATION or '(metastore default)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dd67a1d-6ebb-49c6-af33-4f4caa48b5df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Get Participants from Training Group\n",
    "\n",
    "Uses the Databricks SCIM API to retrieve group members and generate catalog names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a1ede25-1f22-4617-99c7-1d87b026fb47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "def get_group_members(group_name):\n",
    "    \"\"\"\n",
    "    Get all members of a Databricks group using the SCIM REST API.\n",
    "    Returns list of usernames (email addresses).\n",
    "    \"\"\"\n",
    "    context = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "    host = context.apiUrl().get()\n",
    "    token = context.apiToken().get()\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Find the group by name\n",
    "    groups_url = f\"{host}/api/2.0/preview/scim/v2/Groups\"\n",
    "    params = {\"filter\": f'displayName eq \"{group_name}\"'}\n",
    "    response = requests.get(groups_url, headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    groups = response.json().get(\"Resources\", [])\n",
    "    if not groups:\n",
    "        raise ValueError(f\"Group '{group_name}' not found\")\n",
    "\n",
    "    group_id = groups[0][\"id\"]\n",
    "\n",
    "    # Get group details with members\n",
    "    group_url = f\"{host}/api/2.0/preview/scim/v2/Groups/{group_id}\"\n",
    "    response = requests.get(group_url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    members = response.json().get(\"members\", [])\n",
    "\n",
    "    # Resolve each member to an email\n",
    "    user_emails = []\n",
    "    for member in members:\n",
    "        if member.get(\"$ref\", \"\").startswith(\"Users/\"):\n",
    "            user_id = member[\"value\"]\n",
    "            user_url = f\"{host}/api/2.0/preview/scim/v2/Users/{user_id}\"\n",
    "            user_response = requests.get(user_url, headers=headers)\n",
    "            user_response.raise_for_status()\n",
    "            emails = user_response.json().get(\"emails\", [])\n",
    "            if emails:\n",
    "                user_emails.append(emails[0].get(\"value\", \"\"))\n",
    "\n",
    "    return user_emails\n",
    "\n",
    "\n",
    "def sanitize_username(email):\n",
    "    \"\"\"\n",
    "    Convert email to a safe catalog name suffix.\n",
    "    Example: jan.kowalski@company.com -> jan_kowalski\n",
    "    \"\"\"\n",
    "    username = email.split('@')[0]\n",
    "    safe_name = re.sub(r'[^a-zA-Z0-9]', '_', username).lower()\n",
    "    safe_name = re.sub(r'_+', '_', safe_name)\n",
    "    safe_name = re.sub(r'^[0-9_]+', '', safe_name)\n",
    "    safe_name = re.sub(r'[0-9_]+$', '', safe_name)\n",
    "    return safe_name or \"user\"\n",
    "\n",
    "\n",
    "print(\"Functions defined: get_group_members(), sanitize_username()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fb6c1c1-fbfc-4efe-a196-f717a6905d1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Get users and build catalog mapping\n",
    "# =============================================================================\n",
    "try:\n",
    "    training_users = get_group_members(TRAINING_GROUP)\n",
    "    print(f\"Found {len(training_users)} users in group '{TRAINING_GROUP}':\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    user_catalog_map = {}\n",
    "    for email in sorted(training_users):\n",
    "        safe_name = sanitize_username(email)\n",
    "\n",
    "        # Map trainer accounts to a single catalog\n",
    "        if any(sub in email.lower() for sub in [\"trainer\", \"krzysztof.burejza\"]):\n",
    "            catalog_name = f\"{CATALOG_PREFIX}_trainer\"\n",
    "        else:\n",
    "            catalog_name = f\"{CATALOG_PREFIX}_{safe_name}\"\n",
    "\n",
    "        user_catalog_map[email] = catalog_name\n",
    "        print(f\"  {email:<40} -> {catalog_name}\")\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total: {len(set(user_catalog_map.values()))} unique catalogs to create\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: {e}\")\n",
    "    print(f\"\\nPossible issues:\")\n",
    "    print(f\"  1. Group '{TRAINING_GROUP}' does not exist\")\n",
    "    print(f\"  2. You don't have permission to read group members\")\n",
    "    print(f\"  3. Group has no members\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce438b5e-a08a-498b-8613-ecb0aac97f81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Create Catalogs, Schemas, and Volumes\n",
    "\n",
    "For each participant:\n",
    "- Catalog: `retailhub_{username}`\n",
    "- Schemas: `bronze`, `silver`, `gold`, `default`\n",
    "- Volume: `datasets` in `default` schema\n",
    "- Permissions: `ALL PRIVILEGES` granted to the user and the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "945cde8f-aa8b-416f-8ef7-e758cc5eb500",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_user_environment(email, catalog_name, storage_location):\n",
    "    \"\"\"\n",
    "    Create catalog, schemas, volume and set permissions for a training participant.\n",
    "    \"\"\"\n",
    "    results = {\"catalog\": False, \"schemas\": [], \"volume\": False, \"permissions\": False}\n",
    "    trainer_email = spark.sql(\"SELECT current_user()\").first()[0]\n",
    "\n",
    "    try:\n",
    "        # Create catalog (with or without managed location)\n",
    "        if storage_location:\n",
    "            spark.sql(f\"\"\"\n",
    "                CREATE CATALOG IF NOT EXISTS {catalog_name}\n",
    "                MANAGED LOCATION '{storage_location}/{catalog_name}'\n",
    "            \"\"\")\n",
    "        else:\n",
    "            spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog_name}\")\n",
    "        results[\"catalog\"] = True\n",
    "\n",
    "        # Create Medallion schemas + default\n",
    "        for schema in [DEFAULT_SCHEMA, BRONZE_SCHEMA, SILVER_SCHEMA, GOLD_SCHEMA]:\n",
    "            spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema}\")\n",
    "            results[\"schemas\"].append(schema)\n",
    "\n",
    "        # Create managed Volume for datasets\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE VOLUME IF NOT EXISTS {catalog_name}.{DEFAULT_SCHEMA}.{VOLUME_NAME}\n",
    "            COMMENT 'Training datasets for RetailHub project'\n",
    "        \"\"\")\n",
    "        results[\"volume\"] = True\n",
    "\n",
    "        # Grant permissions\n",
    "        spark.sql(f\"GRANT ALL PRIVILEGES ON CATALOG {catalog_name} TO `{email}`\")\n",
    "        if trainer_email != email:\n",
    "            spark.sql(f\"GRANT ALL PRIVILEGES ON CATALOG {catalog_name} TO `{trainer_email}`\")\n",
    "        results[\"permissions\"] = True\n",
    "\n",
    "    except Exception as e:\n",
    "        results[\"error\"] = str(e)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fd51071-f55f-4f58-9782-0404a4dd9cad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Create environments for all participants\n",
    "# =============================================================================\n",
    "creation_results = {}\n",
    "\n",
    "for email, catalog_name in user_catalog_map.items():\n",
    "    print(f\"Processing: {email}\")\n",
    "    result = create_user_environment(email, catalog_name, STORAGE_LOCATION)\n",
    "    creation_results[email] = result\n",
    "\n",
    "    if \"error\" in result:\n",
    "        print(f\"  ERROR: {result['error']}\")\n",
    "    else:\n",
    "        print(f\"  Catalog:     {catalog_name}\")\n",
    "        print(f\"  Schemas:     {', '.join(result['schemas'])}\")\n",
    "        print(f\"  Volume:      {result['volume']}\")\n",
    "        print(f\"  Permissions: {result['permissions']}\")\n",
    "    print()\n",
    "\n",
    "successful = sum(1 for r in creation_results.values() if \"error\" not in r)\n",
    "print(f\"Result: {successful}/{len(creation_results)} environments created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5778c4e8-4bc2-4063-ace4-0a1e6eaafcdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Copy Dataset Files to Volumes\n",
    "\n",
    "Copies the entire `dataset/` folder from the Git repo to each user's Volume.  \n",
    "Uses `shutil.copytree` for a complete recursive copy.\n",
    "\n",
    "**Dataset structure in each Volume:**\n",
    "```\n",
    "/Volumes/retailhub_{user}/default/datasets/\n",
    "  customers/\n",
    "    customers.csv\n",
    "    customers_extended.csv\n",
    "    customers_new.csv\n",
    "  orders/\n",
    "    orders_batch.json\n",
    "    stream/\n",
    "      orders_stream_001.json ... 003.json\n",
    "  products/\n",
    "    products.csv\n",
    "  demo/ingestion/\n",
    "    orders/stream/\n",
    "      orders_stream_004.json ... 006.json\n",
    "  workshop/\n",
    "    Customers.csv, Product.csv, ...\n",
    "    Lakeflow/\n",
    "      Customers/Customers.csv, ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71d13d12-b7e0-48c3-8a41-08b0e88a4652",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "def copy_dataset_to_volume(catalog_name):\n",
    "    \"\"\"\n",
    "    Copy dataset files from the Git repo to a user's Volume.\n",
    "    Skips .DS_Store and other hidden files.\n",
    "    \"\"\"\n",
    "    # Source: dataset/ folder in the repo root\n",
    "    # When running from training_2026/setup/, repo root is ../../\n",
    "    repo_root = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "    source_path = os.path.join(repo_root, \"dataset\")\n",
    "\n",
    "    # Target: Volume path\n",
    "    volume_path = f\"/Volumes/{catalog_name}/{DEFAULT_SCHEMA}/{VOLUME_NAME}\"\n",
    "\n",
    "    if not os.path.exists(source_path):\n",
    "        return {\"error\": f\"Source not found: {source_path}\"}\n",
    "\n",
    "    try:\n",
    "        # Copy with ignore for hidden files\n",
    "        def ignore_hidden(directory, files):\n",
    "            return [f for f in files if f.startswith('.')]\n",
    "\n",
    "        shutil.copytree(source_path, volume_path, dirs_exist_ok=True, ignore=ignore_hidden)\n",
    "        return {\"success\": True}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aad27f4-f109-4eda-b7a4-f78de02e8f6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Copy dataset to each participant's Volume\n",
    "# =============================================================================\n",
    "print(\"Copying dataset to user Volumes...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "copy_results = {}\n",
    "\n",
    "for email, catalog_name in user_catalog_map.items():\n",
    "    print(f\"  {catalog_name}... \", end=\"\")\n",
    "    result = copy_dataset_to_volume(catalog_name)\n",
    "    copy_results[email] = result\n",
    "\n",
    "    if \"error\" in result:\n",
    "        print(f\"FAILED: {result['error']}\")\n",
    "    else:\n",
    "        print(\"OK\")\n",
    "\n",
    "successful = sum(1 for r in copy_results.values() if r.get(\"success\"))\n",
    "print(\"=\" * 60)\n",
    "print(f\"Result: {successful}/{len(copy_results)} volumes populated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43ec9d22-a356-400b-860a-e23aff5fedc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Verification\n",
    "\n",
    "Final check that all environments are ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d8dd378-e077-4b74-9904-c6c69ea118d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VERIFICATION -- Check all environments\n",
    "# =============================================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING ENVIRONMENT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "all_ok = True\n",
    "\n",
    "for email, catalog_name in user_catalog_map.items():\n",
    "    status = []\n",
    "\n",
    "    # Check catalog\n",
    "    try:\n",
    "        spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "        status.append(\"catalog: OK\")\n",
    "    except:\n",
    "        status.append(\"catalog: MISSING\")\n",
    "        all_ok = False\n",
    "\n",
    "    # Check schemas\n",
    "    try:\n",
    "        schemas = [row[0] for row in spark.sql(f\"SHOW SCHEMAS IN {catalog_name}\").collect()]\n",
    "        missing = [s for s in [BRONZE_SCHEMA, SILVER_SCHEMA, GOLD_SCHEMA] if s not in schemas]\n",
    "        if missing:\n",
    "            status.append(f\"schemas: MISSING {missing}\")\n",
    "            all_ok = False\n",
    "        else:\n",
    "            status.append(\"schemas: OK\")\n",
    "    except:\n",
    "        status.append(\"schemas: ERROR\")\n",
    "        all_ok = False\n",
    "\n",
    "    # Check Volume\n",
    "    volume_path = f\"/Volumes/{catalog_name}/{DEFAULT_SCHEMA}/{VOLUME_NAME}\"\n",
    "    try:\n",
    "        files = dbutils.fs.ls(volume_path)\n",
    "        file_count = len(files)\n",
    "        status.append(f\"volume: OK ({file_count} items)\")\n",
    "    except:\n",
    "        status.append(\"volume: MISSING\")\n",
    "        all_ok = False\n",
    "\n",
    "    print(f\"  {email}\")\n",
    "    print(f\"    Catalog: {catalog_name}\")\n",
    "    print(f\"    Status:  {' | '.join(status)}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "if all_ok:\n",
    "    print(\"ALL ENVIRONMENTS READY -- Training can begin!\")\n",
    "    print(\"Participants should run: training_2026/setup/00_setup.ipynb\")\n",
    "else:\n",
    "    print(\"WARNING: Some environments have issues. Check the details above.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "606a49a1-5327-4834-baa3-62fe7041de2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Cleanup (After Training)\n",
    "\n",
    "Run this section **after** the training to remove all participant catalogs and data.  \n",
    "**WARNING: This is a destructive operation -- all training data will be permanently deleted!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78d8cfb5-a780-4211-8ae0-bd6a1ed6771f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CLEANUP -- Remove all training catalogs (after training)\n",
    "# =============================================================================\n",
    "# Uncomment the code below to execute cleanup.\n",
    "\n",
    "# print(\"Dropping training catalogs...\")\n",
    "# for email, catalog_name in user_catalog_map.items():\n",
    "#     try:\n",
    "#         spark.sql(f\"DROP CATALOG IF EXISTS {catalog_name} CASCADE\")\n",
    "#         print(f\"  Dropped: {catalog_name}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"  Failed: {catalog_name} -- {e}\")\n",
    "# print(\"Cleanup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fba2e4b-e8ff-4474-be4c-a1e6b722a4d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ALTERNATIVE CLEANUP -- Find and remove all retailhub_* catalogs\n",
    "# =============================================================================\n",
    "# Use this if user_catalog_map is not available (e.g., new session).\n",
    "\n",
    "# catalogs_df = spark.sql(\"SHOW CATALOGS\")\n",
    "# retailhub_catalogs = [row.catalog for row in catalogs_df.collect()\n",
    "#                       if row.catalog.startswith(\"retailhub_\")]\n",
    "#\n",
    "# print(f\"Found {len(retailhub_catalogs)} catalogs to remove:\")\n",
    "# for cat in retailhub_catalogs:\n",
    "#     print(f\"  - {cat}\")\n",
    "#\n",
    "# # Uncomment to drop:\n",
    "# # for cat in retailhub_catalogs:\n",
    "# #     spark.sql(f\"DROP CATALOG IF EXISTS {cat} CASCADE\")\n",
    "# #     print(f\"Dropped: {cat}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_pre_config",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
