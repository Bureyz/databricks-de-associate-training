{
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Quiz -- Day 1\n",
    "\n",
    "**Modules:** M01 (Platform), M02 (ELT Ingestion), M03 (Delta Fundamentals)  \n",
    "**Questions:** 20 (single correct answer A-D)  \n",
    "\n",
    "---\n",
    "\n",
    "### How it works\n",
    "1. **Run Cell 2** to create answer widgets (dropdowns at the top of the notebook)\n",
    "2. Read each question and select your answer from the dropdown\n",
    "3. **Run the last cell** to check your score\n",
    "\n",
    "> All widgets appear at the top of the notebook in Databricks. Scroll up to see them after running Cell 2."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run this cell FIRST to create answer widgets\n",
    "# Dropdowns will appear at the top of the notebook\n",
    "\n",
    "dbutils.widgets.dropdown(\"Q1\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q1\")\n",
    "dbutils.widgets.dropdown(\"Q2\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q2\")\n",
    "dbutils.widgets.dropdown(\"Q3\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q3\")\n",
    "dbutils.widgets.dropdown(\"Q4\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q4\")\n",
    "dbutils.widgets.dropdown(\"Q5\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q5\")\n",
    "dbutils.widgets.dropdown(\"Q6\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q6\")\n",
    "dbutils.widgets.dropdown(\"Q7\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q7\")\n",
    "dbutils.widgets.dropdown(\"Q8\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q8\")\n",
    "dbutils.widgets.dropdown(\"Q9\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q9\")\n",
    "dbutils.widgets.dropdown(\"Q10\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q10\")\n",
    "dbutils.widgets.dropdown(\"Q11\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q11\")\n",
    "dbutils.widgets.dropdown(\"Q12\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q12\")\n",
    "dbutils.widgets.dropdown(\"Q13\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q13\")\n",
    "dbutils.widgets.dropdown(\"Q14\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q14\")\n",
    "dbutils.widgets.dropdown(\"Q15\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q15\")\n",
    "dbutils.widgets.dropdown(\"Q16\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q16\")\n",
    "dbutils.widgets.dropdown(\"Q17\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q17\")\n",
    "dbutils.widgets.dropdown(\"Q18\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q18\")\n",
    "dbutils.widgets.dropdown(\"Q19\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q19\")\n",
    "dbutils.widgets.dropdown(\"Q20\", \"-\", [\"-\", \"A\", \"B\", \"C\", \"D\"], \"Q20\")\n",
    "\n",
    "print(\"Created 20 widgets -- scroll up to see dropdowns\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "Which component of the Databricks Lakehouse Platform serves as the centralized governance layer for data, users, and permissions?\n\n- A. Databricks Runtime\n- B. Delta Lake\n- C. Unity Catalog\n- D. Photon Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "In Unity Catalog, what is the correct hierarchy of securable objects from broadest to most granular?\n\n- A. Schema > Catalog > Table\n- B. Catalog > Schema > Table\n- C. Metastore > Schema > Catalog\n- D. Workspace > Catalog > Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "What is required before you can access a table inside a Unity Catalog schema?\n\n- A. Only `SELECT` on the table\n- B. `USE CATALOG` on the catalog and `USE SCHEMA` on the schema\n- C. `ALL PRIVILEGES` on the catalog\n- D. Workspace admin role only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4\n",
    "\n",
    "Which SQL command reads data from a CSV file in a Unity Catalog Volume and creates a managed Delta table?\n\n**A.**\n```sql\nLOAD DATA INPATH '/path' INTO TABLE t\n```\n\n**B.**\n```sql\nCOPY INTO t FROM '/path'\n```\n\n**C.**\n```sql\nCREATE TABLE t AS SELECT * FROM read_files('/path', format => 'csv')\n```\n\n**D.**\n```sql\nINSERT INTO t VALUES (SELECT * FROM '/path')\n```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5\n",
    "\n",
    "Which statement about managed vs external tables in Unity Catalog is correct?\n\n- A. Managed tables store data in a user-specified cloud location\n- B. External tables have their data deleted when the table is dropped\n- C. Managed tables store data in the catalog's managed storage and data is deleted on DROP\n- D. External tables cannot be registered in Unity Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6\n",
    "\n",
    "A data engineer wants to apply SQL transformations on a DataFrame `df`. Which is the correct sequence?\n\n**A.**\n```python\ndf.createOrReplaceTempView(\"v\")\nspark.sql(\"SELECT * FROM v\")\n```\n\n**B.**\n```python\ndf.registerTable(\"v\")\nspark.sql(\"SELECT * FROM v\")\n```\n\n**C.**\n```python\ndf.saveAsTable(\"v\")\nspark.sql(\"SELECT * FROM v\")\n```\n\n**D.**\n```python\nspark.sql(\"CREATE VIEW v AS df\")\n```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7\n",
    "\n",
    "What does Delta Lake's transaction log guarantee?\n\n- A. Exactly-once semantics for batch reads only\n- B. ACID transactions, time travel, and schema enforcement\n- C. Automatic data compression only\n- D. Distributed caching of query results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8\n",
    "\n",
    "Which file format does Delta Lake use to store data?\n\n- A. JSON\n- B. ORC\n- C. Parquet\n- D. Avro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9\n",
    "\n",
    "A `DESCRIBE HISTORY` command on a Delta table shows 5 versions. What does each version represent?\n\n- A. A schema change only\n- B. A transactional operation (write, update, delete, merge, optimize, etc.)\n- C. A user session\n- D. A scheduled job execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10\n",
    "\n",
    "A data engineer runs the following query. What does it return?\n\n```sql\nSELECT * FROM my_table VERSION AS OF 3\n```\n\n- A. The current table filtered to 3 rows\n- B. The table as it existed at transaction version 3\n- C. The 3rd partition of the table\n- D. An error because time travel is not supported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q11\n",
    "\n",
    "What is the default compute type for new Databricks workspaces (as of late 2025)?\n\n- A. Classic single-node cluster\n- B. Serverless compute\n- C. GPU cluster\n- D. High-concurrency cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q12\n",
    "\n",
    "Which of the following is the recommended way to store non-tabular files (CSV, JSON, images) in Unity Catalog?\n\n- A. DBFS root\n- B. DBFS mounts\n- C. Unity Catalog Volumes\n- D. Cluster local storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q13\n",
    "\n",
    "What is the difference between ELT and ETL in the context of Databricks Lakehouse?\n\n- A. ELT loads raw data first then transforms in-place; ETL transforms data before loading\n- B. ELT is for batch only; ETL is for streaming\n- C. ELT requires external tools; ETL is native\n- D. They are the same process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q14\n",
    "\n",
    "A data engineer has a DataFrame with duplicate rows. Which PySpark method removes exact duplicates based on a subset of columns?\n\n**A.**\n```python\ndf.distinct()\n```\n\n**B.**\n```python\ndf.dropDuplicates([\"customer_id\", \"email\"])\n```\n\n**C.**\n```python\ndf.removeDuplicates([\"customer_id\"])\n```\n\n**D.**\n```python\ndf.unique([\"customer_id\"])\n```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q15\n",
    "\n",
    "What is the difference between a Temporary View and a Permanent View in Databricks?\n\n- A. Temporary views are stored in Unity Catalog; permanent views are not\n- B. Temporary views exist only within the SparkSession; permanent views are persisted in the metastore\n- C. Temporary views support SQL only; permanent views support PySpark only\n- D. There is no difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q16\n",
    "\n",
    "What does the `%run` magic command do in a Databricks notebook?\n\n- A. Runs an external Python script on the driver node\n- B. Executes another notebook in the same execution context, sharing variables and functions\n- C. Runs a SQL query and returns results\n- D. Restarts the cluster and runs all cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q17\n",
    "\n",
    "When writing a DataFrame to a Delta table, what happens if the incoming data has a column not present in the target schema?\n\nDefault behavior (schema enforcement):\n```python\ndf.write.mode(\"append\").saveAsTable(\"my_table\")\n```\n\nWith schema evolution:\n```python\ndf.write.option(\"mergeSchema\", \"true\").mode(\"append\").saveAsTable(\"my_table\")\n```\n\n- A. The new column is silently ignored\n- B. The write fails with a schema mismatch error (schema enforcement)\n- C. The new column is automatically added to the table\n- D. The entire table is recreated with the new schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q18\n",
    "\n",
    "Which `dbutils` submodule provides methods for listing, copying, and removing files?\n\n**A.**\n```python\ndbutils.fs.ls(\"/path\")\n```\n\n**B.**\n```python\ndbutils.data.list(\"/path\")\n```\n\n**C.**\n```python\ndbutils.files.browse(\"/path\")\n```\n\n**D.**\n```python\ndbutils.storage.ls(\"/path\")\n```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q19\n",
    "\n",
    "What does `RESTORE TABLE my_table TO VERSION AS OF 2` do?\n\n- A. Creates a copy of version 2 as a new table\n- B. Reverts the table to its state at version 2, creating a new version in the history\n- C. Deletes all versions after version 2\n- D. Displays the table at version 2 without modifying it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q20\n",
    "\n",
    "Which statement correctly reads a JSON file with an explicit schema in PySpark?\n\n**A.**\n```python\ndf = spark.read.json(\"/path/data.json\", schema=\"id INT, name STRING\")\n```\n\n**B.**\n```python\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType\nschema = StructType([StructField(\"id\", IntegerType()), StructField(\"name\", StringType())])\ndf = spark.read.schema(schema).json(\"/path/data.json\")\n```\n\n**C.**\n```python\ndf = spark.read.format(\"json\").option(\"schema\", \"id:int, name:string\").load(\"/path\")\n```\n\n**D.**\n```python\ndf = spark.read.json(\"/path\").cast(\"id INT, name STRING\")\n```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run this cell to check your score\n",
    "\n",
    "answers = {\n",
    "    1: ('C', 'Unity Catalog provides centralized access control, auditing, lineage, and data discovery across workspaces.'),\n",
    "    2: ('B', 'The hierarchy is: Metastore > Catalog > Schema > Table/View/Function.'),\n",
    "    3: ('B', 'Unity Catalog requires explicit `USE CATALOG` + `USE SCHEMA` grants before accessing objects within.'),\n",
    "    4: ('C', '`CREATE TABLE ... AS SELECT * FROM read_files()` (CTAS with `read_files`) is the standard pattern for loading files into managed Delta tables.'),\n",
    "    5: ('C', 'Managed tables use catalog-managed storage; dropping them deletes the underlying data. External tables reference external locations and survive DROP.'),\n",
    "    6: ('A', '`createOrReplaceTempView()` registers a DataFrame as a session-scoped temporary view queryable via `spark.sql()`.'),\n",
    "    7: ('B', 'The Delta transaction log (`_delta_log`) provides ACID guarantees, enables time travel, and enforces schema on write.'),\n",
    "    8: ('C', 'Delta Lake stores data as Parquet files with an additional JSON-based transaction log.'),\n",
    "    9: ('B', 'Each version in Delta History corresponds to a single transactional operation on the table.'),\n",
    "    10: ('B', '`VERSION AS OF` enables time travel, returning the table snapshot at the specified version number.'),\n",
    "    11: ('B', 'Since late 2025, Serverless compute is the default compute type for new Databricks workspaces.'),\n",
    "    12: ('C', 'Unity Catalog Volumes are the recommended replacement for DBFS root/mounts for storing non-tabular files.'),\n",
    "    13: ('A', 'ELT (Extract-Load-Transform) loads raw data into the lakehouse first, then transforms in-place using Spark. ETL transforms before loading. Databricks favors ELT.'),\n",
    "    14: ('B', '`dropDuplicates([\"col1\", \"col2\"])` removes duplicate rows based on a subset of columns. `distinct()` works on all columns.'),\n",
    "    15: ('B', 'Temporary views live only in the SparkSession (lost on cluster restart). Permanent views are persisted in Unity Catalog.'),\n",
    "    16: ('B', '`%run` executes another notebook in the same context, making its variables and functions available to the calling notebook.'),\n",
    "    17: ('B', 'By default, Delta enforces schema -- the write fails. Use `mergeSchema=true` to enable schema evolution.'),\n",
    "    18: ('A', '`dbutils.fs` provides file system utilities: `ls()`, `cp()`, `rm()`, `head()`, `mkdirs()`.'),\n",
    "    19: ('B', '`RESTORE TABLE` reverts the table to a previous state and creates a new version entry in the history log.'),\n",
    "    20: ('B', 'Define a `StructType` schema and pass it via `.schema(schema)` before calling `.json()`.'),\n",
    "}\n",
    "\n",
    "correct = 0\n",
    "total = len(answers)\n",
    "results = []\n",
    "\n",
    "for qnum in sorted(answers.keys()):\n",
    "    user_ans = dbutils.widgets.get(f\"Q{qnum}\")\n",
    "    correct_ans, explanation = answers[qnum]\n",
    "    if user_ans == correct_ans:\n",
    "        correct += 1\n",
    "        results.append(f\"  Q{qnum}: {user_ans} -- Correct!\")\n",
    "    elif user_ans == \"-\":\n",
    "        results.append(f\"  Q{qnum}: Not answered (correct: {correct_ans})\")\n",
    "    else:\n",
    "        results.append(f\"  Q{qnum}: {user_ans} -- Wrong. Correct: {correct_ans} | {explanation}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  QUIZ RESULTS -- Day 1\")\n",
    "print(\"=\" * 60)\n",
    "for r in results:\n",
    "    print(r)\n",
    "print(\"=\" * 60)\n",
    "pct = round(correct / total * 100)\n",
    "print(f\"\\n  Score: {correct}/{total} ({pct}%)\")\n",
    "if pct >= 90:\n",
    "    print(\"  Excellent! Exam-ready!\")\n",
    "elif pct >= 70:\n",
    "    print(\"  Good job! Review missed topics.\")\n",
    "elif pct >= 50:\n",
    "    print(\"  Keep studying -- revisit the cheatsheet.\")\n",
    "else:\n",
    "    print(\"  Re-read the demo notebooks and try again.\")\n",
    "print(\"=\" * 60)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# (Optional) Remove all widgets\n",
    "# dbutils.widgets.removeAll()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}