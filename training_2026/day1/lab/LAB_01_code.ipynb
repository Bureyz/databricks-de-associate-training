{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ac06429",
   "metadata": {},
   "source": [
    "# LAB 01: Platform & Workspace Setup\n",
    "\n",
    "**Duration:** ~35 min | **Day:** 1 | **Difficulty:** Beginner\n",
    "\n",
    "> Complete the `# TODO` cells below. Each task has an `assert` cell to verify your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0d7613",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run the cell below to initialize your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c96261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../setup/00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b517d517",
   "metadata": {},
   "source": [
    "## Task 1: Verify Catalog Context\n",
    "\n",
    "Use `spark.sql()` to run `SELECT current_catalog(), current_schema()` and display the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c63ffdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run SQL to check current catalog and schema\n",
    "# Expected: your catalog (retailhub_...) and default schema\n",
    "\n",
    "df_context = spark.sql(\"________\")\n",
    "display(df_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7e2cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "row = df_context.first()\n",
    "assert \"retailhub\" in row[0].lower(), f\"Expected catalog starting with 'retailhub', got: {row[0]}\"\n",
    "print(f\"Catalog: {row[0]}, Schema: {row[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8e27a2",
   "metadata": {},
   "source": [
    "## Task 2: List Files in Your Volume\n",
    "\n",
    "Use `dbutils.fs.ls()` to list files in the Volume you created.\n",
    "\n",
    "Volume path format: `/Volumes/{catalog}/{schema}/{volume_name}/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2137940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: List files in your datasets Volume\n",
    "# Hint: use the CATALOG variable from setup\n",
    "\n",
    "volume_path = f\"/Volumes/{CATALOG}/default/datasets/\"\n",
    "files = dbutils.fs.ls(________)\n",
    "for f in files:\n",
    "    print(f\"{f.name:40s} {f.size:>10,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbacd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "file_names = [f.name for f in files]\n",
    "assert len(files) > 0, \"No files found in Volume! Did you upload them?\"\n",
    "print(f\"Found {len(files)} items in Volume. OK!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84915f4c",
   "metadata": {},
   "source": [
    "## Task 3: Read CSV File\n",
    "\n",
    "Read the `customers.csv` file from your Volume into a DataFrame.\n",
    "\n",
    "Requirements:\n",
    "- Use `spark.read.format(\"csv\")`\n",
    "- Enable header: `.option(\"header\", True)`\n",
    "- Enable schema inference: `.option(\"inferSchema\", True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c64e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Read customers.csv from your Volume\n",
    "\n",
    "customers_path = f\"/Volumes/{CATALOG}/default/datasets/customers.csv\"\n",
    "\n",
    "df_customers = (\n",
    "    spark.read\n",
    "    .format(________)\n",
    "    .option(\"header\", ________)\n",
    "    .option(\"inferSchema\", ________)\n",
    "    .load(customers_path)\n",
    ")\n",
    "\n",
    "display(df_customers.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8a6ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert df_customers.count() > 0, \"DataFrame is empty!\"\n",
    "assert \"customer_id\" in df_customers.columns or \"id\" in df_customers.columns, \"Expected customer ID column\"\n",
    "print(f\"Loaded {df_customers.count()} customers with {len(df_customers.columns)} columns. OK!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9eaf3d",
   "metadata": {},
   "source": [
    "## Task 4: Inspect Schema\n",
    "\n",
    "Print the schema of the `df_customers` DataFrame using `.printSchema()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5912e8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print the schema\n",
    "\n",
    "df_customers.________()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcb7bcf",
   "metadata": {},
   "source": [
    "## Task 5: Explore dbutils\n",
    "\n",
    "Use `dbutils.fs.head()` to read the first 200 bytes of the customers file (raw content)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43569b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Read first 200 bytes of the raw file\n",
    "\n",
    "raw_content = dbutils.fs.head(customers_path, ________)\n",
    "print(raw_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554d40db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert len(raw_content) > 0, \"No content returned\"\n",
    "assert \",\" in raw_content, \"Expected CSV format with commas\"\n",
    "print(\"Raw CSV content displayed. OK!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4705f2",
   "metadata": {},
   "source": [
    "## Lab Complete!\n",
    "\n",
    "You have:\n",
    "- Verified your catalog context\n",
    "- Listed files in a Unity Catalog Volume\n",
    "- Read a CSV file into a Spark DataFrame\n",
    "- Inspected schema and raw file content\n",
    "\n",
    "> **Next:** LAB 01 - ELT Ingestion & Transformations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}