{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "657047e2-fd5c-4e60-9173-07d93b535bad",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "source": [
    "# M01: Platform & Workspace\n",
    "\n",
    "## 1.1. The Story: Why Are We Here?\n",
    "\n",
    "You're a **Data Engineer at a growing e-commerce company**. The business has:\n",
    "- **10,000+ customers** across multiple regions\n",
    "- **50,000+ orders** per month, growing 30% YoY\n",
    "- **500+ products** with complex pricing and inventory\n",
    "\n",
    "**The Problem:**\n",
    "- Data lives in CSV files, JSON APIs, and legacy databases\n",
    "- Data Scientists wait 2 days to get clean data\n",
    "- BI dashboards show data from yesterday (T-1 latency)\n",
    "- Nobody knows which version of \"customer data\" is correct\n",
    "- GDPR compliance is a nightmare - no audit trail\n",
    "\n",
    "**Your Mission:**\n",
    "Build a modern data platform that provides:\n",
    "- Real-time data access (minutes, not days)\n",
    "- Single source of truth (no more \"which spreadsheet is correct?\")\n",
    "- Full audit trail and governance\n",
    "- Self-service analytics for business users\n",
    "\n",
    "**The Solution:** Databricks Lakehouse with Unity Catalog\n",
    "\n",
    "---\n",
    "\n",
    "## 1.2. What You'll Learn (and Why It Matters)\n",
    "\n",
    "| Topic | Why It Matters for You |\n",
    "|-------|------------------------|\n",
    "| Lakehouse Architecture | Understand trade-offs vs. traditional DW + Lake |\n",
    "| Unity Catalog | Implement governance without slowing down |\n",
    "| Compute Options | Choose right cluster type, control costs |\n",
    "| Delta Lake | ACID transactions on files - no more corrupt data |\n",
    "\n",
    "**Target Audience:** Experienced Data Engineers who want to understand *when* and *why* to use these tools, not just *how*.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56333524-0ce0-4ecc-9fd7-22ea9d849be1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 2"
    }
   },
   "source": [
    "## 1.3. Lakehouse Architecture - The \"Why\"\n",
    "\n",
    "### 1.3.1. The Evolution of Data Architectures\n",
    "\n",
    "**Generation 1: Data Warehouse (1990s-2000s)**\n",
    "- Teradata, Oracle, SQL Server\n",
    "- Structured data only, expensive storage\n",
    "- Great for BI, terrible for ML/unstructured data\n",
    "\n",
    "**Generation 2: Data Lake (2010s)**\n",
    "- Hadoop, S3, ADLS\n",
    "- Cheap storage, any format\n",
    "- Problem: \"Data Swamp\" - no governance, no ACID, unreliable\n",
    "\n",
    "**Generation 3: Lakehouse (2020s)**\n",
    "- Delta Lake, Iceberg, Hudi\n",
    "- Best of both: cheap storage + ACID + governance\n",
    "- Single platform for BI, ML, streaming\n",
    "\n",
    "\n",
    "### 1.3.2. Cost Comparison (Rough Estimates)\n",
    "\n",
    "| Component | Traditional (DW + Lake) | Lakehouse |\n",
    "|-----------|------------------------|-----------|\n",
    "| Storage | $$$$ (2x for Lake + DW) | $$ (single copy) |\n",
    "| ETL Compute | $$$ (sync jobs) | $$ (no sync needed) |\n",
    "| Governance Tools | $$$ (separate tools) | $ (built-in) |\n",
    "| Latency | Hours (ETL sync) | Minutes (direct access) |\n",
    "| **Total TCO** | **Higher** | **30-50% lower** |\n",
    "\n",
    "*Note: Actual costs depend on workload. Run POC with your data to validate.*\n",
    "\n",
    "### 1.3.3. Alternatives to Databricks Lakehouse\n",
    "\n",
    "| Alternative | Pros | Cons | When to Choose |\n",
    "|-------------|------|------|----------------|\n",
    "| **Snowflake** | Mature, great SQL | Separate from ML, vendor lock-in | Pure SQL/BI workloads |\n",
    "| **BigQuery** | Serverless, cheap storage | GCP-only, less flexible | GCP shop, ad-hoc analytics |\n",
    "| **Spark + Iceberg on K8s** | Open source, no vendor | Complex ops, no unified governance | Strong DevOps team, cost-sensitive |\n",
    "| **Databricks** | Unified platform, strong ML/AI | Premium pricing | ML + Analytics + Streaming |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4. How Apache Spark Works — Distributed Execution & Lazy Evaluation\n",
    "\n",
    "Understanding Spark's execution model is essential before working with Databricks. Every query you run — whether PySpark or SQL — follows the same principles.\n",
    "\n",
    "---\n",
    "\n",
    "#### Driver & Executors Architecture\n",
    "\n",
    "```\n",
    "                     ┌─────────────────┐\n",
    "                     │     DRIVER      │\n",
    "                     │  (Coordinator)  │\n",
    "                     │                 │\n",
    "                     │  - SparkSession │\n",
    "                     │  - Query Plan   │\n",
    "                     │  - DAG Scheduler│\n",
    "                     │  - Task Scheduler│\n",
    "                     └────────┬────────┘\n",
    "                              │\n",
    "              ┌───────────────┼───────────────┐\n",
    "              │               │               │\n",
    "     ┌────────▼──────┐ ┌─────▼───────┐ ┌─────▼───────┐\n",
    "     │  EXECUTOR 1   │ │  EXECUTOR 2 │ │  EXECUTOR 3 │\n",
    "     │  (Worker JVM) │ │             │ │             │\n",
    "     │               │ │             │ │             │\n",
    "     │  Task  Task   │ │  Task  Task │ │  Task  Task │\n",
    "     │  ┌──┐  ┌──┐   │ │  ┌──┐  ┌──┐│ │  ┌──┐  ┌──┐│\n",
    "     │  │P1│  │P2│   │ │  │P3│  │P4││ │  │P5│  │P6││\n",
    "     │  └──┘  └──┘   │ │  └──┘  └──┘│ │  └──┘  └──┘│\n",
    "     └───────────────┘ └────────────┘ └────────────┘\n",
    "              P = Partition (chunk of data)\n",
    "```\n",
    "\n",
    "- **Driver** — single process that coordinates the entire job. Runs on the master node. Holds the `SparkSession`, builds the query plan, schedules tasks.\n",
    "- **Executors** — worker JVMs running on cluster nodes. Each executor processes multiple **partitions** in parallel.\n",
    "- **Partition** — a chunk of data (default ~128 MB). Spark processes all partitions in parallel across executors.\n",
    "\n",
    "> **Exam Tip:** Spark does NOT move data to the computation. It moves small task code to where the data resides (data locality). This is a fundamental design principle.\n",
    "\n",
    "---\n",
    "\n",
    "#### Lazy Evaluation — Nothing Happens Until You Ask for Results\n",
    "\n",
    "Spark uses **lazy evaluation**: transformations (e.g., `filter`, `select`, `groupBy`) are NOT executed immediately. They are recorded as a logical plan. Execution only starts when an **action** is called.\n",
    "\n",
    "| Type | Examples | What Happens |\n",
    "|------|----------|-------------|\n",
    "| **Transformation** (lazy) | `filter()`, `select()`, `groupBy()`, `join()`, `withColumn()` | Added to the logical plan, nothing computed |\n",
    "| **Action** (triggers execution) | `count()`, `show()`, `collect()`, `write()`, `display()`, `save()` | Triggers the entire pipeline execution |\n",
    "\n",
    "**Why lazy evaluation?** It allows Spark to **optimize the entire pipeline** before executing it. The Catalyst Optimizer can:\n",
    "- Reorder operations for efficiency (e.g., push filters before joins)\n",
    "- Combine multiple transformations into a single pass over the data\n",
    "- Prune unnecessary columns early (column pruning)\n",
    "- Choose optimal join strategies (broadcast vs. shuffle)\n",
    "\n",
    "---\n",
    "\n",
    "#### Execution Flow: From Code to Results\n",
    "\n",
    "```\n",
    "Python/SQL Code\n",
    "      │\n",
    "      ▼\n",
    "┌─────────────────┐\n",
    "│  Logical Plan    │  ← What you asked for (abstract)\n",
    "│  (Unresolved)    │\n",
    "└────────┬────────┘\n",
    "         ▼\n",
    "┌─────────────────┐\n",
    "│  Catalyst        │  ← Resolves names, applies optimizations\n",
    "│  Optimizer       │     (predicate pushdown, column pruning,\n",
    "└────────┬────────┘      join reordering)\n",
    "         ▼\n",
    "┌─────────────────┐\n",
    "│  Physical Plan   │  ← How Spark will execute (concrete)\n",
    "│  (Execution Plan)│     Choose join type, scan method, etc.\n",
    "└────────┬────────┘\n",
    "         ▼\n",
    "┌─────────────────┐\n",
    "│     DAG          │  ← Directed Acyclic Graph of stages\n",
    "│  (Stages + Tasks)│     Stage boundary = shuffle (data exchange)\n",
    "└────────┬────────┘\n",
    "         ▼\n",
    "┌─────────────────┐\n",
    "│  Task Execution  │  ← Tasks distributed to executors\n",
    "│  on Executors    │     Each task processes one partition\n",
    "└─────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Stages and Shuffles\n",
    "\n",
    "Spark divides a job into **stages**. A stage boundary occurs when data needs to be **shuffled** (redistributed across the cluster):\n",
    "\n",
    "| Operation | Shuffle? | Explanation |\n",
    "|-----------|----------|-------------|\n",
    "| `filter()`, `select()` | No (narrow) | Each partition processed independently |\n",
    "| `groupBy().agg()` | Yes (wide) | Data must be grouped by key across partitions |\n",
    "| `join()` | Yes (wide)* | Data with same key must be on same executor |\n",
    "| `repartition()` | Yes | Explicitly redistributes data |\n",
    "| `coalesce()` | No | Reduces partitions without full shuffle |\n",
    "\n",
    "*Exception: broadcast join avoids shuffle by sending the small table to all executors.\n",
    "\n",
    "> **Exam Tip:** Shuffles are the most expensive operation in Spark. Minimizing shuffles (e.g., using broadcast joins for small tables, pre-partitioning data) is key to performance. In Databricks, Adaptive Query Execution (AQE) automatically optimizes many shuffle scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Exam Concepts Recap\n",
    "\n",
    "| Concept | Definition |\n",
    "|---------|-----------|\n",
    "| **Lazy evaluation** | Transformations are recorded but not executed until an action is called |\n",
    "| **DAG (Directed Acyclic Graph)** | Execution plan showing stages and dependencies |\n",
    "| **Partition** | Unit of parallelism — one partition = one task on one core |\n",
    "| **Shuffle** | Data redistribution across cluster (expensive, causes stage boundary) |\n",
    "| **Catalyst Optimizer** | Rule-based + cost-based optimizer that rewrites logical plans |\n",
    "| **Adaptive Query Execution (AQE)** | Runtime optimization: adjusts shuffle partitions, converts joins, handles skew |\n",
    "| **Narrow transformation** | No data exchange between partitions (map, filter) |\n",
    "| **Wide transformation** | Requires shuffle (groupBy, join, distinct) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a45d94d-3139-49cf-bef1-760ae552b359",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 3"
    }
   },
   "source": [
    "## 1.4. Databricks Platform Elements\n",
    "\n",
    "### 1.4.1. Per-user Isolation\n",
    "\n",
    "Run the initialization script for per-user catalog and schema isolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f4d94df-6da8-468c-84ce-e4601a325e0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../setup/00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a822578-04ff-426c-91d0-619b23bf3f76",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 5"
    }
   },
   "source": [
    "### 1.4.2. Configuration\n",
    "\n",
    "Import libraries and set environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f74fb0b1-9b23-4d51-b1fb-b12913824fc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import re\n",
    "\n",
    "# Display user context (variables from 00_setup)\n",
    "print(\"=== User Context ===\")\n",
    "print(f\"Catalog: {CATALOG}\")\n",
    "print(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "print(f\"Schema Silver: {SILVER_SCHEMA}\")\n",
    "print(f\"Schema Gold: {GOLD_SCHEMA}\")\n",
    "print(f\"User: {raw_user}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e07cd4cc-5cd8-4d94-bc8c-b73186bd5910",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "name = \"test\"\n",
    "\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6be0ad0d-871c-49b0-ae7c-b5423e4ab8a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set catalog as default\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8883389d-dfce-4ac3-9beb-dee233de75b7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 9"
    }
   },
   "source": [
    "### 1.4.3. Lakehouse Architecture Concept\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Lakehouse is a modern data architecture that combines the benefits of Data Lake (low storage cost, support for various formats) with the benefits of Data Warehouse (reliability, SQL query performance, transaction management). A key element is Delta Lake - a metadata layer ensuring ACID transactions on Parquet files.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **ACID Transactions**: Atomicity, Consistency, Isolation, Durability - guarantees ensuring reliability of data operations\n",
    "- **Delta Lake**: Open-source storage layer ensuring transactionality on files in Data Lake\n",
    "- **Unity Catalog**: Unified system for data, metadata, and access control management\n",
    "\n",
    "**Practical Application:**\n",
    "- Elimination of data duplication between analytical and operational systems\n",
    "- Simultaneous support for BI, Data Science, and Machine Learning\n",
    "- Reduction of infrastructure and maintenance costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad256fd2-ae90-4d9c-8681-be489429f78b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 10"
    }
   },
   "source": [
    "### 1.4.4. Comparison of Traditional Architecture vs Lakehouse\n",
    "\n",
    "**Objective:** Visualize differences between traditional approach (Data Lake + Data Warehouse) and Lakehouse.\n",
    "\n",
    "**Traditional Architecture:**\n",
    "```\n",
    "Raw Data → Data Lake (S3/ADLS) → ETL Process → Data Warehouse (Snowflake/Redshift) → BI Tools\n",
    "                                ↓\n",
    "                         ML/Data Science (separate copy)\n",
    "```\n",
    "\n",
    "**Lakehouse Architecture:**\n",
    "```\n",
    "Raw Data → Delta Lake (single source of truth) → BI Tools + ML + Real-time Analytics\n",
    "```\n",
    "\n",
    "**Lakehouse Benefits:**\n",
    "- Single copy of data (single source of truth)\n",
    "- Lower storage costs\n",
    "- Elimination of synchronization latency\n",
    "- Common governance for all use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "604dbe6a-70c7-4147-8713-f922e3d88de4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 11"
    }
   },
   "source": [
    "### 1.4.5. Databricks Platform Elements\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "The Databricks platform consists of several key components that together create a complete environment for working with data in the Lakehouse architecture.\n",
    "\n",
    "**Key Components:**\n",
    "- **Workspace**: Working environment containing notebooks, experiments, folders, and resources\n",
    "- **Catalog Explorer**: Interface for managing catalogs, schemas, tables, and views\n",
    "- **Git Folders (formerly Repos)**: Git integration for versioning notebooks and code\n",
    "- **Volumes**: Management of unstructured files (images, models, artifacts)\n",
    "- **DBFS (Databricks File System)**: Virtual file system over cloud storage\n",
    "\n",
    "**Practical Application:**\n",
    "- Workspace organizes projects and team collaboration\n",
    "- Catalog Explorer enables data exploration and governance\n",
    "- Git Folders integrates development workflow with Git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95bf774f-a63e-4432-ba50-c8da3d316631",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 12"
    }
   },
   "source": [
    "### 1.4.6. Workspace Exploration\n",
    "\n",
    "#### 1.4.6.1. Example: Workspace Exploration\n",
    "\n",
    "**Objective:** Familiarize with Databricks Workspace interface\n",
    "\n",
    "**Workspace Elements:**\n",
    "1. **Sidebar** (left side):\n",
    "   - Workspace: Folders and notebooks\n",
    "   - Git Folders: Git Integration\n",
    "   - Compute: Cluster management\n",
    "   - Workflows: Databricks Jobs\n",
    "   - Catalog: Unity Catalog explorer\n",
    "\n",
    "2. **Main Panel**: Notebook editor or details view\n",
    "\n",
    "3. **Top Bar**: Quick access to compute, account, help\n",
    "\n",
    "**Navigation Instructions:**\n",
    "- Use the left menu to switch between sections\n",
    "- In the Catalog section, you can browse catalogs, schemas, and tables\n",
    "- In the Compute section, you manage Spark clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef0fa899-84b5-4b93-835c-f0afe15d4d57",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 13"
    }
   },
   "source": [
    "### 1.4.7. Catalog Explorer - Unity Catalog Structure\n",
    "\n",
    "#### 1.4.7.1. Example: Catalog Explorer\n",
    "\n",
    "**Objective:** Understand object hierarchy in Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7d181c5-b49b-4c44-bad0-d18ae5988b31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display current catalog and schema\n",
    "current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "current_schema = spark.sql(\"SELECT current_schema()\").collect()[0][0]\n",
    "\n",
    "print(f\"Current catalog: {current_catalog}\")\n",
    "print(f\"Current schema: {current_schema}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0069c11c-67d5-4a54-a1a7-fbb67f5d80e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Unity Catalog Hierarchy:**\n",
    "\n",
    "```\n",
    "Metastore\n",
    "  ├── Catalog (e.g., main, dev, prod)\n",
    "  │   ├── Schema/Database (e.g., bronze, silver, gold)\n",
    "  │   │   ├── Tables (Delta Tables)\n",
    "  │   │   ├── Views (SQL Views)\n",
    "  │   │   ├── Functions (UDFs)\n",
    "  │   │   └── Volumes (for files)\n",
    "```\n",
    "\n",
    "Unity Catalog organizes data in three levels: `catalog.schema.table`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab97de26-6e2a-4b67-87fb-7c9f012c65ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "Unity Catalog organizes data in hierarchy: Metastore → Catalog → Schema → Objects (Tables/Views/Functions). This structure enables:\n",
    "- Logical separation of environments (dev/test/prod)\n",
    "- Granular access control at each level\n",
    "- Easy management of namespaces and project isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a81256c7-e5d6-4494-af87-36671b2da3ef",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 17"
    }
   },
   "source": [
    "### 1.4.8. Browsing Catalogs and Schemas\n",
    "\n",
    "#### 1.4.8.1. Example: Browsing Catalogs and Schemas\n",
    "\n",
    "**Objective:** Programmatic listing of objects in Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b4a711e-7765-4839-87bf-be0f1cc17263",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of all catalogs available to the user\n",
    "catalogs_df = spark.sql(\"SHOW CATALOGS\")\n",
    "display(catalogs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1acf7186-a6c5-4fe8-a5e2-3f8722729d73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of schemas in the current catalog\n",
    "schemas_df = spark.sql(f\"SHOW SCHEMAS IN {CATALOG}\")\n",
    "display(schemas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b47526a4-c364-44d8-9df1-c19945d0c6eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "Commands `SHOW CATALOGS` and `SHOW SCHEMAS` allow exploring Unity Catalog structure. Each user sees only objects they have permissions for. Per-user isolation (as in our `00_setup`) ensures each training participant has their own workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b04ffdba-2d7b-4363-8c15-9312d59f107a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 21"
    }
   },
   "source": [
    "### 1.4.9. Git Folders (Repos) and Git Integration\n",
    "\n",
    "In practice, working with code in Databricks should be based on **Git Folders** (formerly Repos), not single, orphaned notebooks in Workspace.\n",
    "\n",
    "Typical workflow:\n",
    "\n",
    "1. **Create Git Folder** in Databricks: `Workspace → Git Folders → Add Repo`.\n",
    "2. **Connect to Git** (GitHub / Azure DevOps / other).\n",
    "3. Work on **feature branches** (e.g., `feature/cleaning-module`).\n",
    "4. Regularly:\n",
    "   - commit and push changes from Databricks to remote repo,\n",
    "   - create PR and merge to main/dev.\n",
    "\n",
    "Best Practices:\n",
    "\n",
    "- One repo per project/domain (e.g., `databricks-dea-training`).\n",
    "- Do not work in **Workspace root** – always in **Git Folders**.\n",
    "- Training notebooks, test data, and README can be in one repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3b97f4c-f2d7-476b-876c-03e662fc8258",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 22"
    }
   },
   "source": [
    "### 1.4.10. Volumes vs DBFS – where to store files?\n",
    "\n",
    "In new workspaces based on Unity Catalog, the preferred place for storing files are **Volumes**.\n",
    "\n",
    "- `dbfs:/` is treated as a **legacy** layer or auxiliary area.\n",
    "- `volume://catalog.schema.volume_name` is a fully managed, UC-controlled data area (permissions, audit, lineage).\n",
    "\n",
    "Volume Definition Example (SQL):\n",
    "\n",
    "```sql\n",
    "CREATE VOLUME IF NOT EXISTS ${catalog}.${schema}.training_volume\n",
    "COMMENT 'Workspace for training purposes';\n",
    "```\n",
    "\n",
    "Usage Example in PySpark:\n",
    "\n",
    "```python\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "\n",
    "volume_path = f\"volume://{catalog}.{schema}.training_volume\"\n",
    "display(dbutils.fs.ls(volume_path))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59fe7bbe-41c2-4fc0-8247-61dbd858854f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 23"
    }
   },
   "source": [
    "### 1.4.11. Serverless SQL / SQL Warehouse – when to use instead of notebook cluster?\n",
    "\n",
    "Besides notebook clusters, Databricks offers **SQL Warehouse (serverless)** – SQL query engine optimized for BI and ad-hoc analytics.\n",
    "\n",
    "When to use:\n",
    "- Reporting in Power BI / other BI tools.\n",
    "- Business analysts / power users working mainly in SQL.\n",
    "- Interactive dashboards and ad-hoc queries to **Gold** layer.\n",
    "\n",
    "Differences from all-purpose cluster:\n",
    "- Billing based on **DBU SQL** (different rates).\n",
    "- Automatic provisioning / scaling.\n",
    "- Isolation of BI workload from engineering clusters.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0269415-6dda-473d-a771-236303dbe4c8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 24"
    }
   },
   "source": [
    "## 1.5. Compute - Making Smart Choices\n",
    "\n",
    "### 1.5.1. The Real Question: How Much Will This Cost?\n",
    "\n",
    "As a Data Engineer, you'll be asked: *\"Why is our Databricks bill so high?\"*\n",
    "\n",
    "Understanding compute options is essential for cost control.\n",
    "\n",
    "### 1.5.2. Compute Options Comparison\n",
    "\n",
    "| Type | Startup Time | Cost Model | Best For |\n",
    "|------|--------------|------------|----------|\n",
    "| **All-Purpose Cluster** | 3-5 min | Per-minute (running) | Interactive development, exploration |\n",
    "| **Job Cluster** | 3-5 min | Per-minute (only during job) | Scheduled production jobs |\n",
    "| **Serverless (Preview)** | <10 sec | Per-query DBUs | Ad-hoc queries, variable workloads |\n",
    "| **SQL Warehouse** | 0 (Serverless) or 3-5 min | Per-query DBUs | BI tools, SQL analysts |\n",
    "\n",
    "### 1.5.3. Cost Optimization Strategies\n",
    "\n",
    "**1. Right-size clusters:**\n",
    "- Development: 2-4 workers, smallest instance type\n",
    "- Production: Autoscaling 2-10 workers based on workload\n",
    "\n",
    "**2. Use Spot/Preemptible instances:**\n",
    "- 60-80% cost savings for workers\n",
    "- Driver on on-demand (stability)\n",
    "- Trade-off: Job may be interrupted\n",
    "\n",
    "**3. Photon Engine:**\n",
    "- 2-3x faster for aggregations/joins\n",
    "- ~2x DBU cost, but finishes faster = often cheaper\n",
    "- Enable for: large scans, aggregations, joins\n",
    "- Skip for: simple transformations, ML training\n",
    "\n",
    "**4. Cluster policies:**\n",
    "- Enforce maximum worker count\n",
    "- Require autoscaling\n",
    "- Set auto-termination (e.g., 30 min idle)\n",
    "\n",
    "### 1.5.4. Decision Tree: Which Compute to Use?\n",
    "\n",
    "```\n",
    "Is it a scheduled production job?\n",
    "├── YES → Job Cluster (ephemeral, cost-efficient)\n",
    "└── NO → Is it interactive development?\n",
    "         ├── YES → All-Purpose Cluster (shared, always-on during work hours)\n",
    "         └── NO → Is it SQL/BI query?\n",
    "                  ├── YES → SQL Warehouse (optimized for SQL, connects to BI tools)\n",
    "                  └── NO → Serverless Notebook (instant start, pay-per-use)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "677ff10d-6ff8-4a96-b12f-5e84aba7f563",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 25"
    }
   },
   "source": [
    "### 1.5.5. Cluster Information\n",
    "\n",
    "#### 1.5.5.1. Example: Cluster Information\n",
    "\n",
    "**Objective:** Check current cluster configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91374905-ca6b-46c1-86da-c94a5549cede",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Runtime version\n",
    "dbr_version = spark.conf.get(\"spark.databricks.clusterUsageTags.sparkVersion\", \"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07ae2f31-513e-4a0b-b599-e2c652b1c64f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(dbr_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7de0918-1b85-4f28-ac88-75b62996290e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Photon enabled?\n",
    "photon_enabled = spark.conf.get(\"spark.databricks.photon.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92ddecd3-77b2-47ba-a553-c6bdefea01da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(photon_enabled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8caaae99-cbbe-475a-86d3-29e73437c53c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "This code shows basic information about the Spark cluster. The number of executors (workers) can change dynamically with autoscaling enabled. Photon Engine, if enabled, automatically accelerates SQL queries and DataFrame operations without code changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "566b74f2-66f4-4f1a-b5c9-15dd70cafa28",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 31"
    }
   },
   "source": [
    "## 1.6. Magic Commands in Notebooks\n",
    "\n",
    "### 1.6.1. Theoretical Introduction\n",
    "\n",
    "Databricks notebooks support magic commands - special commands starting with `%` that control the cell language or execute system operations. Magic commands allow mixing languages in a single notebook and interacting with the file system.\n",
    "\n",
    "**Available magic commands:**\n",
    "- **%python**: Python cell (default)\n",
    "- **%sql**: SQL cell\n",
    "- **%scala**: Scala cell\n",
    "- **%r**: R cell\n",
    "- **%md**: Markdown cell (documentation)\n",
    "- **%fs**: File system operations (DBFS)\n",
    "- **%sh**: Shell commands\n",
    "- **%run**: Run another notebook (like import)\n",
    "- **%pip**: Install Python libraries (notebook-scoped)\n",
    "- **%skip**: Skips cell execution\n",
    "\n",
    "**Practical Application:**\n",
    "- Combining SQL and Python in one workflow\n",
    "- Inline documentation with Markdown\n",
    "- File operations with %fs\n",
    "- Code modularization with %run\n",
    "- Dependency management with %pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14084b35-dd2b-4bc3-abcb-b170d0e48117",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 32"
    }
   },
   "source": [
    "### 1.6.2. Monitoring and Logging – Where to Look for Problems?\n",
    "\n",
    "When working with clusters and Jobs, it's worth knowing the basic places where we look for diagnostic information:\n",
    "\n",
    "- **Cluster → Event log** – cluster start/stop, autoscaling, infrastructure errors.\n",
    "- **Spark UI** (Jobs, SQL, Storage, Environment tabs) – execution plan, shuffling, task-level errors.\n",
    "- **Driver / Executor logs** – detailed Python/Scala stacktraces.\n",
    "- **Job Run page** – status of individual tasks, retries, execution time.\n",
    "\n",
    "Best Practices:\n",
    "- For longer pipelines, always check Spark UI (SQL/Jobs section).\n",
    "- Log critical application logs to Delta tables / storage, not just cluster logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15a320a9-fd52-454d-9a08-5c2074746fef",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 33"
    }
   },
   "source": [
    "### 1.6.3. SQL Magic Command Demonstration\n",
    "\n",
    "#### 1.6.3.1. Example: SQL Magic Command\n",
    "\n",
    "**Objective:** Execute SQL query directly in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cdd5b6e-ce7d-4a23-9056-467163b1657a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SQL magic command allows writing pure SQL without Python wrapper\n",
    "\n",
    "SELECT \n",
    "  current_catalog() as catalog,\n",
    "  current_schema() as schema,\n",
    "  current_user() as user,\n",
    "  current_timestamp() as timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72219dd5-27a3-4b77-8ed5-a3d12215b52f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "The `%sql` magic command changes the cell language to SQL. Results are automatically displayed as a table. SQL in Databricks is full Spark SQL with Delta Lake extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83c62a01-def3-4a05-9cee-251b9683b774",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 36"
    }
   },
   "source": [
    "### 1.6.4. File System Operations with fs\n",
    "\n",
    "#### 1.6.4.1. Example: File System Operations\n",
    "\n",
    "**Objective:** Explore DBFS file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "132863d8-0852-49fc-bb62-3c4d3a87e1ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List root directories in DBFS\n",
    "dbutils.fs.ls(\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "760a718c-6473-4233-83ec-526d06ec733e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "DBFS (Databricks File System) is an abstraction over cloud storage (S3, ADLS, GCS). The `%fs` command or `dbutils.fs` allows file operations. In Unity Catalog, it is recommended to use **Volumes** instead of DBFS for better governance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cda7429b-9d22-4a72-99f8-e546014c7529",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 39"
    }
   },
   "source": [
    "### 1.6.5. Mixing Languages - Python and SQL\n",
    "\n",
    "#### 1.6.5.1. Example: Mixing Languages\n",
    "\n",
    "**Objective:** Demonstrate seamless transition between Python and SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "039f22a9-d548-4c7b-9532-6ced5708a3aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Python: Raw data definition\n",
    "data = [\n",
    "    (1, \"Alice\", \"Engineering\", 95000),\n",
    "    (2, \"Bob\", \"Sales\", 75000),\n",
    "    (3, \"Charlie\", \"Engineering\", 105000),\n",
    "    (4, \"Diana\", \"Marketing\", 68000),\n",
    "    (5, \"Eve\", \"Engineering\", 98000)\n",
    "]\n",
    "\n",
    "# Schema definition\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"department\", StringType(), False),\n",
    "    StructField(\"salary\", IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "289d1ce0-479b-47f5-9e30-50c09b90c688",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e34557e1-9ac9-4f3a-88c9-3f40d3c8426f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register as temp view for SQL access\n",
    "df.createOrReplaceTempView(\"employees_temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4071b2d3-e8f0-4027-a850-c18c2fc7b71a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Temp view created: employees_temp**\n",
    "\n",
    "Temp view allows access to DataFrame from SQL cells in the same notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f59fce2a-0196-4145-8f6c-24d1f5d70b13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SQL: Aggregation on Python data\n",
    "\n",
    "SELECT \n",
    "  department,\n",
    "  COUNT(*) as employee_count,\n",
    "  AVG(salary) as avg_salary,\n",
    "  MAX(salary) as max_salary\n",
    "FROM employees_temp\n",
    "GROUP BY department\n",
    "ORDER BY avg_salary DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9ac5da9-74e5-4025-babf-c3310d15b209",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "This example shows the power of Databricks notebooks: data preparation in Python (convenient API, libraries), then analysis in SQL (declarative queries, clarity). Temp views are visible throughout the notebook regardless of cell language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ef0c1cd-1d7c-4fad-910b-8087ab6fc581",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 46"
    }
   },
   "source": [
    "### 1.6.6. Library Management (%pip)\n",
    "\n",
    "#### 1.6.6.1. Example: Library Management\n",
    "\n",
    "In Databricks, we can install notebook-scoped Python libraries using the `%pip` command. This is the recommended approach instead of global installation on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b998dba-508e-4ee7-a9a5-96f8419efe33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install emoji library\n",
    "%pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "731f59a0-93e7-4245-b94e-504b7b0c5bfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import emoji\n",
    "print(emoji.emojize('Databricks is :fire:'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6662cc7-46a9-4f65-915a-18079ed97f68",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 49"
    }
   },
   "source": [
    "### 1.6.7. Databricks Assistant (AI)\n",
    "\n",
    "In 2025, coding work is assisted by AI. Databricks has a built-in assistant (**Databricks Assistant**) that is context-aware of your data (knows table schemas in Unity Catalog!).\n",
    "\n",
    "**How to use?**\n",
    "1. Shortcut **Cmd+I** (Mac) or **Ctrl+I** (Windows) inside a cell.\n",
    "2. \"Assistant\" side panel.\n",
    "\n",
    "**What is it for?**\n",
    "- **Code Generation**: \"Write a SQL query that calculates average sales by region from the sales table\".\n",
    "- **Code Explanation**: Select a complex snippet and ask \"Explain this code\".\n",
    "- **Fixing errors**: When a cell returns an error, click \"Diagnose Error\" – the assistant will explain the cause and propose a fix.\n",
    "- **Transformation**: \"Rewrite this code from PySpark to SQL\".\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cffcf7a7-4b58-4f36-a746-71603cb32a1a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 50"
    }
   },
   "source": [
    "## 1.7. Unity Catalog vs Hive Metastore\n",
    "\n",
    "### 1.7.1. Theoretical Introduction\n",
    "\n",
    "Databricks supports two metadata systems: legacy Hive Metastore and modern Unity Catalog. Unity Catalog is recommended for all new projects due to advanced governance and security features.\n",
    "\n",
    "**Key Differences:**\n",
    "\n",
    "| Aspect | Hive Metastore | Unity Catalog |\n",
    "|--------|----------------|---------------|\n",
    "| **Governance** | Limited | Full: RBAC, masking, audit |\n",
    "| **Namespace** | 2-level (db.table) | 3-level (catalog.schema.table) |\n",
    "| **Cross-workspace** | No | Yes (shared metastore) |\n",
    "| **Lineage** | None | End-to-end lineage |\n",
    "| **Data Sharing** | Limited | Delta Sharing protocol |\n",
    "| **Isolation** | Workspace-level | Catalog-level |\n",
    "\n",
    "**Why Unity Catalog?**\n",
    "- Central access management for all workspaces\n",
    "- Automatic lineage for audit and compliance\n",
    "- Fine-grained permissions (column-level, row-level)\n",
    "- Integration with external systems (Delta Sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f41c629-f1dd-4331-a607-b16b4c7a8314",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 51"
    }
   },
   "source": [
    "### 1.7.2. Namespace - Hive vs Unity Catalog\n",
    "\n",
    "#### 1.7.2.1. Example: Namespace Comparison\n",
    "\n",
    "**Objective:** Compare table access syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ff6cc60-b5c6-4119-849f-76a3c175f19b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Hive Metastore (legacy)**\n",
    "\n",
    "- **Syntax**: `database.table`\n",
    "- **Example**: `default.sales_data`\n",
    "- **Limitations**: No fine-grained permissions, no lineage, workspace isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4da5a2ba-76c0-4230-b5ee-a5a47656beb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Unity Catalog (modern)**\n",
    "\n",
    "- **Syntax**: `catalog.schema.table`\n",
    "- **Example**: `prod.gold.sales_summary`\n",
    "\n",
    "**3-level Namespace Advantages:**\n",
    "- Environment separation (dev/test/prod catalogs)\n",
    "- Better permissions (grant at catalog level)\n",
    "- Metastore sharing between workspaces\n",
    "- End-to-end lineage\n",
    "- Fine-grained access control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11188262-35c1-49e3-8b92-38362635bd2b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 54"
    }
   },
   "source": [
    "### 1.7.3. Creating a Table in Unity Catalog\n",
    "\n",
    "#### 1.7.3.1. Example: Creating a Table\n",
    "\n",
    "**Objective:** Demonstrate full syntax with 3-level namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19c1afdc-9dba-48ea-a8fd-4f31ff2d05bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create sample table in Unity Catalog\n",
    "table_name = f\"{CATALOG}.{BRONZE_SCHEMA}.lakehouse_demo\"\n",
    "\n",
    "# Demo data\n",
    "demo_data = [\n",
    "    (1, \"Unity Catalog\", \"Enabled\", \"2024-01-15\"),\n",
    "    (2, \"Delta Lake\", \"Enabled\", \"2024-01-15\"),\n",
    "    (3, \"Photon Engine\", \"Enabled\", \"2024-01-15\"),\n",
    "    (4, \"Hive Metastore\", \"Legacy\", \"2024-01-15\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd8f2775-8274-4791-ab51-d147a1be2078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Schema definition\n",
    "demo_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"feature\", StringType(), False),\n",
    "    StructField(\"status\", StringType(), False),\n",
    "    StructField(\"date\", StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a60aa549-2b4b-4714-944c-49cd6ab52d59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "demo_df = spark.createDataFrame(demo_data, demo_schema)\n",
    "display(demo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e40fd65e-3d9b-4ed0-a807-3aa070e50685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save as Delta Table in Unity Catalog\n",
    "demo_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1506a541-232b-4b70-b15e-17fe667a9944",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification\n",
    "display(spark.table(table_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17ffb9ec-997d-4305-affd-991831d89bfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Success!** The table has been created in Unity Catalog. We can now verify its existence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "490272be-2a84-46e2-a66d-0d6d1d573ff8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task: Check in UI\n",
    "\n",
    "1. Click **Catalog** in the left sidebar.\n",
    "2. Find your catalog (name in `CATALOG` variable, e.g., `retailhub_...`).\n",
    "3. Expand the `bronze` schema (or other defined in `BRONZE_SCHEMA`).\n",
    "4. Click on the `lakehouse_demo` table.\n",
    "5. See tabs: **Sample Data** (preview) and **Lineage** (data origin)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ec8af90-b3b3-43db-a965-d0c39e0c44f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "The table was created with a full 3-level namespace. In Unity Catalog, every table automatically:\n",
    "- Is managed by the governance system\n",
    "- Has tracked lineage\n",
    "- Has permissions assigned based on catalog and schema\n",
    "- Is available in Catalog Explorer for exploration\n",
    "\n",
    "**Managed vs External Tables:**\n",
    "The table above is a **Managed Table**. Databricks manages both metadata and data files (in default catalog/schema storage). Dropping the table (`DROP TABLE`) also deletes the data.\n",
    "\n",
    "**External Table** is created when we provide `LOCATION 'path'`. Then `DROP TABLE` removes only metadata, and files remain in storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dc408bc-fa10-46b1-aadf-e21efee43fd6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 63"
    }
   },
   "source": [
    "### 1.7.4. Comparison PySpark vs SQL\n",
    "\n",
    "**DataFrame API (PySpark):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a1a2858-88a3-4425-b9ae-bbcb8dc3fd7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PySpark Approach - programmatic DataFrame API\n",
    "\n",
    "df_pyspark = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.lakehouse_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf9b31f8-ef80-4b2c-9d88-4a7106d1f7c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_pyspark = df_pyspark \\\n",
    "    .filter(F.col(\"status\") == \"Enabled\") \\\n",
    "    .select(\"feature\", \"status\", \"date\") \\\n",
    "    .orderBy(\"feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd1e2252-4c73-4961-b13c-e88b905ec60f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result_pyspark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26f26fb2-d01e-4ddd-9ba5-a4f125828150",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**SQL Equivalent:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "231818c1-6090-40c1-9837-2f3ae9d0c68f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"{CATALOG}.{BRONZE_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71130cf7-560f-4571-9ea0-93c480f9d3bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(f\"select * from {CATALOG}.{BRONZE_SCHEMA}.lakehouse_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0288f1cb-c227-4350-bace-b7d8124e4905",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96b7c9b1-a1a1-4c15-b6df-8249cd755b2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select * from \n",
    "retailhub_trainer.bronze.lakehouse_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "169a685d-24ac-4f56-b608-8a240f2cd18f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Parameterization with Databricks Widgets\n",
    "\n",
    "Below we use the **Widgets** mechanism, which allows creating interactive controls in the notebook. This allows passing parameters (e.g., table names, dates) to SQL and Python code, facilitating the building of universal reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3dff4b4-da92-40a8-9785-eb777db88424",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parameterization with Databricks Widgets\n",
    "# Set default values based on variables from 00_setup (if available)\n",
    "# This ensures SQL cells will use the same catalog as Python cells\n",
    "\n",
    "default_catalog = CATALOG if 'CATALOG' in locals() else \"retailhub_trainer\"\n",
    "default_schema = BRONZE_SCHEMA if 'BRONZE_SCHEMA' in locals() else \"bronze\"\n",
    "\n",
    "dbutils.widgets.text(\"CATALOG\", default_catalog)\n",
    "dbutils.widgets.text(\"BRONZE_SCHEMA\", default_schema)\n",
    "dbutils.widgets.text(\"BRONZE_SCHEMA_2\", default_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06523c1b-3a68-42d9-bd21-099e0ff46d49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT \n",
    "  feature,\n",
    "  status,\n",
    "  date\n",
    "FROM IDENTIFIER(:CATALOG || '.' || :BRONZE_SCHEMA || '.lakehouse_demo')\n",
    "WHERE status = 'Enabled'\n",
    "ORDER BY feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdd223e2-d3f6-42ca-ac53-acbbbea0ebbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Comparison:**\n",
    "- **Performance**: Identical - both approaches compile to the same Catalyst query plan\n",
    "- **When to use PySpark**: \n",
    "  - Complex business logic with UDFs\n",
    "  - Dynamic pipelines (parameterization, loops)\n",
    "  - Integration with Python libraries (pandas, scikit-learn)\n",
    "- **When to use SQL**: \n",
    "  - Simple transformations and aggregations\n",
    "  - Team with strong SQL skills\n",
    "  - Migration from traditional Data Warehouse\n",
    "  - Better support for business analysts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a16ec104-b474-4943-b590-30dd012a7aad",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 76"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 1.8. Summary\n",
    "\n",
    "### 1.8.1. What was achieved:\n",
    "- Learned Lakehouse concept as evolution of Data Lake + Data Warehouse\n",
    "- Explored Databricks platform elements: Workspace, Compute, Catalog\n",
    "- Understood Unity Catalog hierarchy: Metastore → Catalog → Schema → Objects\n",
    "- Practiced magic commands: %sql, %python, %fs, %pip\n",
    "- Compared Hive Metastore vs Unity Catalog\n",
    "- Created first Delta table in Unity Catalog with 3-level namespace\n",
    "\n",
    "### 1.8.2. Key Takeaways:\n",
    "1. **Lakehouse eliminates data duplication**: Single copy serves BI, ML, and real-time analytics\n",
    "2. **Unity Catalog is governance foundation**: 3-level namespace, fine-grained permissions, automatic lineage\n",
    "3. **Clusters are flexible**: Autoscaling and spot instances reduce costs, Photon accelerates queries\n",
    "4. **Notebooks are powerful**: Mixing SQL/Python, magic commands, Git integration via Git Folders\n",
    "5. **Delta Lake is default format**: ACID transactions, time travel, schema evolution\n",
    "\n",
    "### 1.8.3. Quick Reference - Key Commands:\n",
    "\n",
    "| Operation | PySpark | SQL |\n",
    "|-----------|---------|-----|\n",
    "| Set catalog | `spark.sql(f\"USE CATALOG {CATALOG}\")` | `USE CATALOG my_catalog` |\n",
    "| List catalogs | `spark.sql(\"SHOW CATALOGS\")` | `SHOW CATALOGS` |\n",
    "| List schemas | `spark.sql(\"SHOW SCHEMAS\")` | `SHOW SCHEMAS` |\n",
    "| Create table | `df.write.saveAsTable(\"cat.schema.table\")` | `CREATE TABLE cat.schema.table AS SELECT ...` |\n",
    "| Read table | `spark.table(\"cat.schema.table\")` | `SELECT * FROM cat.schema.table` |\n",
    "| Metadata | - | `SELECT * FROM system.information_schema.tables` |\n",
    "| Install lib | `%pip install package` | - |"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4873620425682744,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_platform_intro",
   "widgets": {
    "BRONZE_SCHEMA": {
     "currentValue": "bronze",
     "nuid": "3eb823de-3328-4753-a2bf-9d863378b4f1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "bronze",
      "label": null,
      "name": "BRONZE_SCHEMA",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "bronze",
      "label": null,
      "name": "BRONZE_SCHEMA",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "BRONZE_SCHEMA_2": {
     "currentValue": "bronze",
     "nuid": "55fcd747-0b42-452a-a57e-2d92169a6e4f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "bronze",
      "label": null,
      "name": "BRONZE_SCHEMA_2",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "bronze",
      "label": null,
      "name": "BRONZE_SCHEMA_2",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "CATALOG": {
     "currentValue": "retailhub_trainer",
     "nuid": "ab4cee0e-5693-4e7e-8e80-189b95567447",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "retailhub_trainer",
      "label": null,
      "name": "CATALOG",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "retailhub_trainer",
      "label": null,
      "name": "CATALOG",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
