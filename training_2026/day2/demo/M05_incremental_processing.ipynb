{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d886e96e-caab-4179-bca5-7416b87f4435",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# M05: Incremental Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07bca810-eb5e-4c51-acdd-0b8b67b7e7e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5.1. The Story: Real-Time Analytics\n",
    "\n",
    "Your e-commerce platform needs to process orders in real-time. The batch jobs are too slow (T-1 latency). Marketing needs to see campaign performance *now*, not tomorrow. You need to build a streaming pipeline that ingests data as it arrives, processes it incrementally, and handles schema changes automatically.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9267c39b-ed38-43a1-a273-b356418b368d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5.2. Per-user Isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb7ab4af-0592-4a45-8677-9c783b689c8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../setup/00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5932f8b8-d869-4108-b696-a10fce1d3935",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5.3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6ab1a78-f2d9-4dc6-8b98-9e30ba0f0620",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87cc878f-656a-49b2-abf5-f17c314b4e32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Paths and variables configuration:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e317eca-d483-430c-a87a-b2ddf7f51cad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set default catalog and schema\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "# === SOURCE DATA (REAL DATASET) ===\n",
    "SOURCE_CUSTOMERS = f\"{DATASET_PATH}/customers/customers.csv\"\n",
    "SOURCE_ORDERS = f\"{DATASET_PATH}/orders/orders_batch.json\"\n",
    "\n",
    "# === DEMO PATHS (SIMULATED ARRIVAL) ===\n",
    "DEMO_BASE_PATH = f\"{DATASET_PATH}/ingestion_demo\"\n",
    "BATCH_SOURCE_PATH = f\"{DEMO_BASE_PATH}/batch_source\"\n",
    "STREAM_SOURCE_PATH = f\"{DEMO_BASE_PATH}/stream_source\"\n",
    "\n",
    "# === TECHNICAL PATHS ===\n",
    "CHECKPOINT_BASE_PATH = f\"{DEMO_BASE_PATH}/checkpoints\"\n",
    "SCHEMA_BASE_PATH = f\"{DEMO_BASE_PATH}/schemas\"\n",
    "BAD_RECORDS_PATH = f\"{DEMO_BASE_PATH}/bad_records\"\n",
    "\n",
    "# Cleanup from previous runs\n",
    "dbutils.fs.rm(DEMO_BASE_PATH, True)\n",
    "print(f\"Demo environment prepared at: {DEMO_BASE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f973fe9-1d90-49ed-8afc-c368c131cc2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Cleanup from previous runs (for demo):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f145265-424f-4f06-b4a9-485e705f0830",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    dbutils.fs.rm(CHECKPOINT_BASE_PATH, True)\n",
    "    dbutils.fs.rm(SCHEMA_BASE_PATH, True)\n",
    "    dbutils.fs.rm(BAD_RECORDS_PATH, True)\n",
    "    dbutils.fs.rm(DEMO_BASE_PATH, True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08beba9a-ff04-4991-9a1b-cb765f8109df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Configuration verification:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "895e7ba1-07a3-4e38-9be5-b75fab0b300a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"CATALOG\", CATALOG),\n",
    "        (\"BRONZE_SCHEMA\", BRONZE_SCHEMA),\n",
    "        (\"SILVER_SCHEMA\", SILVER_SCHEMA),\n",
    "        (\"USER\", raw_user),\n",
    "        (\"CUSTOMERS_CSV\", SOURCE_CUSTOMERS),\n",
    "        (\"STREAMING_SOURCE_PATH\", STREAM_SOURCE_PATH)\n",
    "    ], [\"Variable\", \"Value\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0cbfc77-8fd1-4710-81cb-a4541cbb4e72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === DATA PREPARATION (SIMULATION) ===\n",
    "\n",
    "# 1. Prepare Batch Data (Customers)\n",
    "# We split customers into 2 days for COPY INTO demo\n",
    "df_customers = spark.read.option(\"header\", \"true\").csv(SOURCE_CUSTOMERS)\n",
    "df_batch_day1, df_batch_day2,df_batch_day3,df_batch_day4 = df_customers.randomSplit([0.25]*4, seed=42)\n",
    "\n",
    "# Save Day 1 immediately\n",
    "df_batch_day1.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{BATCH_SOURCE_PATH}/day1\")\n",
    "print(f\"Batch Data: Day 1 ready at {BATCH_SOURCE_PATH}/day1\")\n",
    "\n",
    "# 2. Prepare Streaming Data (Orders)\n",
    "# We take existing stream files, merge them, and split into 10 micro-batches for simulation\n",
    "SOURCE_STREAM_FILES = f\"{DATASET_PATH}/orders/stream/*.json\"\n",
    "df_all_orders = spark.read.json(SOURCE_STREAM_FILES)\n",
    "\n",
    "# Split into 20 parts (5% each)\n",
    "stream_batches = df_all_orders.randomSplit([0.05] * 20, seed=42)\n",
    "\n",
    "# Save Batch 1 immediately to start the stream\n",
    "stream_batches[0].write.mode(\"overwrite\").json(f\"{STREAM_SOURCE_PATH}/batch_01\")\n",
    "print(f\"Stream Data: Batch 1 ready at {STREAM_SOURCE_PATH}/batch_01\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cbd6bab-cdb3-4715-b258-edc2b7a56050",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "### 5.3.1. Databricks Data Loading Methods Overview\n",
    "\n",
    "Databricks provides three primary methods for loading data into Delta tables, each optimized for different use cases:\n",
    "\n",
    "---\n",
    "\n",
    "![alt text](../../assets/images/image_load.avif)\n",
    "\n",
    "### **1. CTAS (Create Table As Select)**\n",
    "\n",
    "**Purpose**: Quick table creation from existing data or queries\n",
    "\n",
    "**Characteristics**:\n",
    "- One-time operation (not incremental)\n",
    "- Creates table and loads data in a single statement\n",
    "- No built-in deduplication or file tracking\n",
    "- Best for data transformations and one-off loads\n",
    "\n",
    "**Use Cases**:\n",
    "- Creating derived tables from existing data\n",
    "- Data transformation and aggregation\n",
    "- Ad-hoc analysis tables\n",
    "- Quick prototyping\n",
    "\n",
    "**Limitations**:\n",
    "- No automatic tracking of processed files\n",
    "- Running twice creates duplicates or requires DROP TABLE first\n",
    "- Not suitable for incremental loads\n",
    "\n",
    "---\n",
    "\n",
    "### **2. COPY INTO**\n",
    "\n",
    "**Purpose**: Idempotent batch ingestion with automatic file tracking\n",
    "\n",
    "**Characteristics**:\n",
    "- **Idempotency**: Tracks processed files automatically\n",
    "- **Incremental**: Only loads new files on subsequent runs\n",
    "- **File Formats**: CSV, JSON, Parquet, Avro, ORC\n",
    "- **Schema Evolution**: Optional merge schema support\n",
    "- **Error Handling**: Built-in options for malformed records\n",
    "\n",
    "**Use Cases**:\n",
    "- Scheduled batch ingestion (daily, hourly)\n",
    "- Loading from cloud storage (S3, ADLS, GCS)\n",
    "- Incremental file-based loads\n",
    "- Data lake ingestion where files arrive periodically\n",
    "\n",
    "**Advantages**:\n",
    "- Safe to re-run (no duplicates)\n",
    "- Simple syntax with powerful options\n",
    "- Good for small to medium file volumes\n",
    "- Built-in file tracking via metadata\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Auto Loader (cloudFiles)**\n",
    "\n",
    "**Purpose**: Scalable streaming ingestion with automatic file discovery\n",
    "\n",
    "**Characteristics**:\n",
    "- **Streaming Architecture**: Processes files incrementally\n",
    "- **File Discovery**: Automatic detection of new files\n",
    "- **Schema Management**: Inference, evolution, and rescue modes\n",
    "- **Checkpoint Management**: Fault-tolerant processing\n",
    "- **Scalability**: Handles millions of files efficiently\n",
    "- **Cloud-Native**: Uses file notification services (optional)\n",
    "\n",
    "**Use Cases**:\n",
    "- Real-time or near-real-time ingestion\n",
    "- Large-scale file ingestion (millions of files)\n",
    "- Unknown or evolving schemas\n",
    "- Low-latency requirements\n",
    "- Continuous data arrival\n",
    "\n",
    "**Advantages**:\n",
    "- Best scalability (millions of files)\n",
    "- Automatic schema evolution\n",
    "- Rescued data for unexpected changes\n",
    "- Efficient file discovery (notification-based)\n",
    "- Exactly-once processing semantics\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison Matrix**\n",
    "\n",
    "| Feature | CTAS | COPY INTO | Auto Loader |\n",
    "|---------|------|-----------|-------------|\n",
    "| **Incremental** | [ERROR] No | [OK] Yes | [OK] Yes |\n",
    "| **Idempotent** | [ERROR] No | [OK] Yes | [OK] Yes |\n",
    "| **Schema Evolution** | [ERROR] No | [WARNING] Limited | [OK] Advanced |\n",
    "| **File Tracking** | [ERROR] No | [OK] Metadata | [OK] Checkpoint |\n",
    "| **Scalability** | Low | Medium | High |\n",
    "| **Streaming** | [ERROR] No | [ERROR] No | [OK] Yes |\n",
    "| **Error Handling** | Basic | Good | Advanced |\n",
    "| **Use Case** | One-time | Scheduled batch | Real-time/Streaming |\n",
    "\n",
    "---\n",
    "\n",
    "### **Decision Guide**\n",
    "\n",
    "**Choose CTAS when:**\n",
    "- Creating tables from existing data\n",
    "- One-time loads or transformations\n",
    "- Simple, ad-hoc operations\n",
    "\n",
    "**Choose COPY INTO when:**\n",
    "- Scheduled batch ingestion (e.g., daily files)\n",
    "- Need idempotency without streaming overhead\n",
    "- Small to medium file volumes (<100K files)\n",
    "- Simple incremental loads\n",
    "\n",
    "**Choose Auto Loader when:**\n",
    "- Real-time or near-real-time processing required\n",
    "- Large number of files (>100K)\n",
    "- Schema may change over time\n",
    "- Need advanced error handling and rescued data\n",
    "- Building production streaming pipelines\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08f69694-2300-42b9-b550-62420e42ce90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.3.2 CTAS (Create Table As Select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01cd3356-9001-40c1-b460-a4cb9a899d42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example: Load CSV from volume to customer_cts table using CTAS\n",
    "\n",
    "table_name = \"customer_cts\"\n",
    "\n",
    "display(spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {table_name} AS\n",
    "SELECT *\n",
    "FROM csv.`{SOURCE_CUSTOMERS}`\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b63d78bb-a625-47a8-b332-39d2fef620f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select * from customer_cts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4912d38e-f107-4dda-882b-0178bef6cf50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {table_name}_copy AS\n",
    "SELECT *\n",
    "FROM {table_name} \n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c910ac1c-8c3f-41d3-97ab-7957ed30115b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select * from customer_cts_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abd5d815-3650-41ee-ae24-7cb4cc75f0da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5.4. COPY INTO - Batch Loading\n",
    "\n",
    "**COPY INTO** is the recommended batch ingestion method:\n",
    "- **Idempotency**: Automatic tracking of processed files\n",
    "- **File tracking**: Only new files are loaded on re-run\n",
    "- **Format support**: CSV, JSON, Parquet, Avro, ORC\n",
    "\n",
    "\n",
    "![alt text](../../assets/images/DKXKibszdkNP36Gme2Oks.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fec72cf5-ca12-4f93-be80-926e1ec05d60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### M05_T5.1. Example: COPY INTO from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d395a42e-48b0-4533-869a-eb646b96ee8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TABLE_CUSTOMERS = f\"{BRONZE_SCHEMA}.customers_batch\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fd0dbb4-7c5d-404c-bb44-e599dfb1ceac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Creating target table:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4c46f3d-dc6a-402f-9f0d-52492c9c48b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {TABLE_CUSTOMERS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df68fd13-424c-4aca-af28-40c265795467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {TABLE_CUSTOMERS} (\n",
    "  customer_id STRING,\n",
    "  first_name STRING,\n",
    "  last_name STRING,\n",
    "  email STRING,\n",
    "  phone STRING,\n",
    "  city STRING,\n",
    "  state STRING,\n",
    "  country STRING,\n",
    "  registration_date DATE,\n",
    "  customer_segment STRING,\n",
    "  _ingestion_timestamp TIMESTAMP\n",
    ") USING DELTA\n",
    "COMMENT 'Customers data - Bronze layer'\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "492bce7c-49bc-4b56-8ea4-c8fe79989674",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Execute COPY INTO:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44e063b0-6d21-4587-9ea3-6f922721f169",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load Day 1 data\n",
    "result = spark.sql(f\"\"\"\n",
    "COPY INTO {TABLE_CUSTOMERS}\n",
    "FROM (\n",
    "  SELECT \n",
    "    customer_id,\n",
    "    first_name,\n",
    "    last_name,\n",
    "    email,\n",
    "    phone,\n",
    "    city,\n",
    "    state,\n",
    "    country,\n",
    "    TO_DATE(registration_date, 'yyyy-MM-dd') as registration_date,\n",
    "    customer_segment,\n",
    "    current_timestamp() as _ingestion_timestamp\n",
    "  FROM '{BATCH_SOURCE_PATH}/day1'\n",
    ")\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS ('header' = 'true', 'inferSchema' = 'false')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true')\n",
    "\"\"\")\n",
    "\n",
    "display(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee1bf6b4-d3cc-4ef0-97ee-1ef874bec11d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(TABLE_CUSTOMERS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fb116ae-62a4-4b81-abc3-90af3dd9d0a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### M05_T5.2. Idempotency\n",
    "\n",
    "Running COPY INTO again will **not** add duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e89be490-2f32-4a92-aefc-53611efb21f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "count_before = spark.table(TABLE_CUSTOMERS).count()\n",
    "\n",
    "# Re-run COPY INTO (same source path)\n",
    "spark.sql(f\"\"\"\n",
    "COPY INTO {TABLE_CUSTOMERS}\n",
    "FROM (\n",
    "  SELECT \n",
    "    customer_id, first_name, last_name, email, phone,\n",
    "    city, state, country,\n",
    "    TO_DATE(registration_date, 'yyyy-MM-dd') as registration_date,\n",
    "    customer_segment,\n",
    "    current_timestamp() as _ingestion_timestamp\n",
    "  FROM '{BATCH_SOURCE_PATH}/*'\n",
    ")\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS ('header' = 'true', 'inferSchema' = 'false')\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c5b861d-fdbe-4299-94c1-4300027e44eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "count_after = spark.table(TABLE_CUSTOMERS).count()\n",
    "display(count_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3937c53c-2954-49a2-9424-2bed22d0946f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Comparison:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bbb9217-d435-4acc-af8e-c3168d40a06f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"Before\", count_before),\n",
    "        (\"After\", count_after),\n",
    "        (\"Difference\", count_after - count_before)\n",
    "    ], [\"State\", \"Count\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7efe11e8-74c3-432c-b4cd-f85518fb9ae9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### M05_T5.3. Checking Results After Adding More Days\n",
    "\n",
    "Let's check what happens when we add more days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "119c2ffe-4ef2-4059-ab80-c4d59b1cc1a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load all days data\n",
    "result = spark.sql(f\"\"\"\n",
    "COPY INTO {TABLE_CUSTOMERS}\n",
    "FROM (\n",
    "  SELECT \n",
    "    customer_id,\n",
    "    first_name,\n",
    "    last_name,\n",
    "    email,\n",
    "    phone,\n",
    "    city,\n",
    "    state,\n",
    "    country,\n",
    "    TO_DATE(registration_date, 'yyyy-MM-dd') as registration_date,\n",
    "    customer_segment,\n",
    "    current_timestamp() as _ingestion_timestamp\n",
    "  FROM '{BATCH_SOURCE_PATH}/*'\n",
    ")\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS ('header' = 'true', 'inferSchema' = 'false')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true')\n",
    "\"\"\")\n",
    "\n",
    "display(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc4ffa9c-6583-4c48-8766-272032f4c554",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_batch_day2.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{BATCH_SOURCE_PATH}/day6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efda3c19-1b8a-4ae2-851c-6ee85d549cd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_batch_day3.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{BATCH_SOURCE_PATH}/day3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07e30995-0274-4e0a-959f-6daabdac4552",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_batch_day4.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{BATCH_SOURCE_PATH}/day4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be9aa213-f8a2-4c6b-9aa8-3e75cfb9210b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(TABLE_CUSTOMERS).count())\n",
    "\n",
    "display(spark.table(TABLE_CUSTOMERS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a85699d-66dd-4b83-91cc-8267b0ef312c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5.5. Auto Loader - Streaming Ingestion\n",
    "\n",
    "**Auto Loader (cloudFiles)** is a Databricks-managed streaming source:\n",
    "- Automatic file discovery (file notifications)\n",
    "- Incremental processing (only new files)\n",
    "- Schema inference & evolution\n",
    "- Checkpoint management\n",
    "- Scales to millions of files\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91de6578-4381-4007-8cb9-5197c4254ce2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.5.1. Trigger Modes & Output Modes\n",
    "\n",
    "**Trigger** determines how often streaming query executes micro-batches:\n",
    "\n",
    "| Mode | Behavior | Use Case |\n",
    "|------|----------|----------|\n",
    "| `availableNow=True` | Process everything â†’ stop | Scheduled jobs  |\n",
    "| `once=True` | Legacy (deprecated) | - |\n",
    "| `processingTime=\"10 second\"` | Every 10 seconds | Real-time |\n",
    "| `continuous=\"1 second\"` | Ultra-low latency | Experimental |\n",
    "\n",
    "\n",
    "\n",
    "![!\\[alt text\\](../../assets/images/!\\[12afdawfq2351fdawfaw.png\\](attachment:12afdawfq2351fdawfaw.png).png)](../../assets/images/12afdawfq2351fdawfaw.png)\n",
    "\n",
    "\n",
    "\n",
    "**Output Mode** determines how data is written to the sink:\n",
    "\n",
    "| Mode | Description | Use Case |\n",
    "|------|-------------|----------|\n",
    "| **Append** | Only new rows are written | Raw data ingestion (stateless) |\n",
    "| **Update** | Only updated rows are written | Aggregations (stateful) |\n",
    "| **Complete** | The entire result table is rewritten | Small aggregations (e.g. counts) |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b55bec23-5424-416b-93bc-d92f0edbe2eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    dbutils.fs.rm(CHECKPOINT_BASE_PATH, True)\n",
    "    dbutils.fs.rm(SCHEMA_BASE_PATH, True)\n",
    "    dbutils.fs.rm(BAD_RECORDS_PATH, True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ecddc1e-8381-47b1-9bb8-4bb910967534",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TARGET_TABLE_AL = f\"{BRONZE_SCHEMA}.orders_autoloader\"\n",
    "CHECKPOINT_AL = f\"{CHECKPOINT_BASE_PATH}/autoloader\"\n",
    "SCHEMA_AL = f\"{SCHEMA_BASE_PATH}/autoloader\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74068144-ba19-48f7-b56f-19e3848255f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Auto Loader readStream configuration:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11639851-8bd4-45db-b9b9-d6774dae5c1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "### Auto Loader (`cloudFiles`) Configuration Options\n",
    "\n",
    "Auto Loader options are prefixed with `cloudFiles`. Key categories:\n",
    "\n",
    "#### Common Options\n",
    "- `cloudFiles.format`: File format (`csv`, `json`, `parquet`, `avro`, `orc`, etc.)\n",
    "- `cloudFiles.schemaLocation`: Path to store inferred schema and checkpoints\n",
    "- `cloudFiles.includeExistingFiles`: `true` to process existing files on first run\n",
    "\n",
    "#### Directory Listing Options\n",
    "- `cloudFiles.useIncrementalListing`: `true` (default) for scalable file discovery\n",
    "- `cloudFiles.maxFilesPerTrigger`: Max files to process per micro-batch\n",
    "\n",
    "#### File Notification Options\n",
    "- `cloudFiles.useNotifications`: Enable file notification service (cloud-specific)\n",
    "- `cloudFiles.subscriptionId`, `cloudFiles.queueName`, etc.: Notification service configs\n",
    "\n",
    "#### File Format Options\n",
    "- `cloudFiles.inferColumnTypes`: For CSV/JSON, infer column types (`true`/`false`)\n",
    "- `cloudFiles.schemaEvolutionMode`: How schema changes are handled (`addNewColumns`, etc.)\n",
    "\n",
    "#### CSV Options\n",
    "- `cloudFiles.csv.header`: `true` if CSV files have headers\n",
    "- `cloudFiles.csv.delimiter`: Field delimiter (default: `,`)\n",
    "\n",
    "#### JSON Options\n",
    "- `cloudFiles.json.multiline`: `true` for multi-line JSON\n",
    "\n",
    "#### Parquet/Avro/ORC Options\n",
    "- Standard Spark read options apply (e.g., `mergeSchema`)\n",
    "\n",
    "#### Cloud-Specific Options\n",
    "- AWS: `cloudFiles.region`, `cloudFiles.endpoint`, etc.\n",
    "- Azure: `cloudFiles.resourceGroup`, `cloudFiles.subscriptionId`, etc.\n",
    "\n",
    "#### Other Useful Options\n",
    "- `cloudFiles.allowOverwrites`: Allow file overwrites (`true`/`false`)\n",
    "- `cloudFiles.backfillInterval`: Interval for backfilling missed files\n",
    "\n",
    "**Full list:** [Auto Loader options documentation](https://learn.microsoft.com/en-us/azure/databricks/ingestion/cloud-object-storage/auto-loader/options/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5480ace8-e9ca-40cc-8060-c2a293774fbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {TARGET_TABLE_AL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56253bc2-82c1-469c-9aa8-e0dd3372407c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_autoloader = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", SCHEMA_AL)\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")\n",
    "    .load(STREAM_SOURCE_PATH) # Reading from our simulated stream source\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88ff21ec-c900-482f-98b4-48f6b62adf10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Adding metadata columns:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b19880d3-5c2a-4f50-a42a-bcd8ef1e5e20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_enriched = (df_autoloader\n",
    "    .withColumn(\"_processing_time\", F.current_timestamp())\n",
    "    .withColumn(\"_source_file\", col(\"_metadata.file_path\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf28a8c5-299e-4e8c-b7b5-8eb2735ffea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Start streaming with `availableNow` trigger:**\n",
    "\n",
    "> `availableNow` - processes all available data and stops (batch-like streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf328d6a-3818-4c43-a390-a713d67e7eb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = (df_enriched.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_AL)\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(TARGET_TABLE_AL)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fa23eff-ac7b-4d5b-87bc-48ecb4f7c10f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(f\"{TARGET_TABLE_AL}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b625f881-ac95-4166-99c2-1f5655ce9860",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Auto Loader results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66517539-62ea-4bf1-a818-7bca51b8fee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"Records Loaded\", str(spark.table(TARGET_TABLE_AL).count())),\n",
    "        (\"Source Files\", str(spark.table(TARGET_TABLE_AL).select(\"_source_file\").distinct().count()))\n",
    "    ], [\"Metric\", \"Value\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45836a58-c68e-40bf-aa85-4fc491f43818",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "### Add new data and re-run stream with `availableNow` trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "639b741a-669b-4e73-8eaa-cb25889535a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save Batch 2 immediately to start the stream\n",
    "stream_batches[1].write.mode(\"overwrite\").json(f\"{STREAM_SOURCE_PATH}/batch_02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f553058-19d4-4a74-9d1d-b0fa804d4955",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.5.4. Example: Continuous Processing (processingTime)\n",
    "\n",
    "Now we will start a continuous stream and add data while it's running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9492c6c-cc7d-4986-b540-d7edc1059372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {TARGET_TABLE_AL}_continuous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "625aebf2-2adb-424f-9c7c-da8029669b32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Start stream in background\n",
    "query_continuous = (df_enriched.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_AL}_continuous\")\n",
    "    .trigger(processingTime=\"10 seconds\")\n",
    "    .toTable(f\"{TARGET_TABLE_AL}_continuous\")\n",
    ")\n",
    "\n",
    "print(\"Stream started... Waiting for initialization.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "077272b2-764e-4261-9df2-a443412127f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(f\"{TARGET_TABLE_AL}_continuous\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e858ff5-fb5d-4cd1-82db-c0070772137a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simulate arrival of remaining batches (2 to 10)\n",
    "print(\"Starting data simulation...\")\n",
    "\n",
    "for i in range(2, 10): # Batches 2 to 10 (indices of remaining parts)\n",
    "    batch_num = i + 1\n",
    "    print(f\"Arriving: Batch {batch_num}...\", end=\" \")\n",
    "    \n",
    "    # Write next batch\n",
    "    stream_batches[i].write.mode(\"overwrite\").json(f\"{STREAM_SOURCE_PATH}/batch_{batch_num:02d}\")\n",
    "    \n",
    "    print(\"Done. Waiting for stream...\")\n",
    "    time.sleep(4) # Wait for trigger to pick it up\n",
    "\n",
    "print(\"All batches arrived.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce174a0c-6139-4182-a4aa-6b2188aee22e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(f\"{TARGET_TABLE_AL}_continuous\").count())\n",
    "display(spark.table(f\"{TARGET_TABLE_AL}_continuous\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7eabdb50-0f4c-403b-babf-cc5a0bfe1936",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stop stream\n",
    "query_continuous.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98e5e13f-ad7b-4a2d-8e2d-1c2f2f132259",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5.6. Stream-Static Joins & Aggregations\n",
    "\n",
    "A common pattern is enriching a stream (e.g., Orders) with a static table (e.g., Customers).\n",
    "*   **Stateless**: Stream (Left) JOIN Static (Right)\n",
    "*   **Stateful**: Static (Left) JOIN Stream (Right) - *less common*\n",
    "\n",
    "**Note:** The static table is read at the start of each micro-batch.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4754aff9-6d0f-4b61-acf0-40ef81fb4948",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load Static Table\n",
    "df_static_customers = spark.table(TABLE_CUSTOMERS)\n",
    "display(df_static_customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee030957-86fe-4684-887d-a2fae0646f7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.6.1. Example: Streaming Aggregation (Update Mode)\n",
    "\n",
    "**Output Mode: Update** is used when you want to write only the rows that have changed since the last trigger. This is typical for aggregations.\n",
    "\n",
    "**Watermarking** is crucial here to handle late data and clean up the state. It tells the engine: *\"How long do we wait for late data before finalizing the window?\"*\n",
    "\n",
    "In this example, we calculate the number of orders per 30-second window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8cc4f36-e94f-4dca-bfd9-64cce0c225d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS jointed_orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cc78623-670f-4692-8e8e-d02d7a7be854",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_enriched.createOrReplaceTempView(\"enriched_orders_stream\")\n",
    "df_static_customers.createOrReplaceTempView(\"static_customers\")\n",
    "\n",
    "df_joined = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  o.order_id,\n",
    "  o.total_amount,\n",
    "  c.first_name,\n",
    "  c.last_name,\n",
    "  c.email,\n",
    "  o._processing_time\n",
    "FROM enriched_orders_stream o\n",
    "LEFT JOIN static_customers c\n",
    "  ON o.customer_id = c.customer_id\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b4b4e1b-ec1d-429c-929b-d0ad62611750",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write enriched stream\n",
    "query_join = (df_joined.writeStream\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"enriched_orders\")\n",
    "    .outputMode(\"append\") # Joins (Left Stream-Static) are append-only\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_BASE_PATH}/join_demo_3\")\n",
    "    .trigger(processingTime=\"5 seconds\")\n",
    "    .option(\"includeExistingFiles\", \"true\")\n",
    "    .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d0daa85-eacd-42f4-a024-8455f59f7a46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simulate arrival of remaining batches (2 to 10)\n",
    "print(\"Starting data simulation...\")\n",
    "\n",
    "for i in range(11, 13): # Batches 2 to 10 (indices of remaining parts)\n",
    "    batch_num = i + 1\n",
    "    print(f\"Arriving: Batch {batch_num}...\", end=\" \")\n",
    "    \n",
    "    # Write next batch\n",
    "    stream_batches[i].write.mode(\"overwrite\").json(f\"{STREAM_SOURCE_PATH}/batch_{batch_num:02d}\")\n",
    "    \n",
    "    print(\"Done. Waiting for stream...\")\n",
    "    time.sleep(4) # Wait for trigger to pick it up\n",
    "\n",
    "print(\"All batches arrived.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aa77038-530d-4721-8389-b2c79ac31025",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(\"enriched_orders\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f29721c3-3626-4ea7-a376-133ee30ba8fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query_join.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f338534-f921-45bf-9d80-759af0e73290",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"SELECT count(1) FROM enriched_orders \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fabf8e5d-1388-4910-af46-25aa7a116c7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.6.2. Define Streaming Aggregation (Orders per 30 seconds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7537cfcb-1342-49dd-86ed-3c02db5862e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Define Streaming Aggregation (Orders per 30 seconds)\n",
    "# We use the previously defined df_enriched (from Auto Loader)\n",
    "\n",
    "windowed_counts = (df_enriched\n",
    "    .withWatermark(\"_processing_time\", \"1 minutes\") # Allow 1 mins late data\n",
    "    .groupBy(\n",
    "        F.window(\"_processing_time\", \"30 seconds\"),\n",
    "        \"customer_id\"\n",
    "    )\n",
    "    .count()\n",
    ")\n",
    "\n",
    "# 2. Write Stream with UPDATE mode\n",
    "# Update mode is efficient for aggregations - it emits only changed windows\n",
    "query_agg = (windowed_counts.writeStream\n",
    "    .format(\"console\") \n",
    "    .queryName(\"orders_counts\")\n",
    "    .outputMode(\"update\")\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_BASE_PATH}/agg_demo\")\n",
    "    .trigger(processingTime=\"5 seconds\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "print(\"Aggregation stream started...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8da7c99-1342-405e-b9b1-d1ee8ccc0a4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simulate arrival of remaining batches (2 to 10)\n",
    "print(\"Starting data simulation...\")\n",
    "\n",
    "for i in range(14, 16): # Batches 2 to 10 (indices of remaining parts)\n",
    "    batch_num = i + 1\n",
    "    print(f\"Arriving: Batch {batch_num}...\", end=\" \")\n",
    "    \n",
    "    # Write next batch\n",
    "    stream_batches[i].write.mode(\"overwrite\").json(f\"{STREAM_SOURCE_PATH}/batch_{batch_num:02d}\")\n",
    "    \n",
    "    print(\"Done. Waiting for stream...\")\n",
    "    time.sleep(4) # Wait for trigger to pick it up\n",
    "\n",
    "print(\"All batches arrived.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2f1b4d5-1794-4bdf-9859-fbf78fb24efb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(windowed_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74faaba0-5370-47ec-ba9e-bf985ba1ea2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stop for demo purposes\n",
    "query_agg.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc0aff7d-41af-4785-9845-619e41087a3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5.7. Error Handling\n",
    "\n",
    "**Error handling strategies in COPY INTO:**\n",
    "\n",
    "| Mode | Behavior |\n",
    "|------|----------|\n",
    "| `PERMISSIVE` | Parses what it can, errors â†’ `_corrupt_record` |\n",
    "| `DROPMALFORMED` | Removes malformed records |\n",
    "| `FAILFAST` | Stops on first error |\n",
    "\n",
    "**`badRecordsPath`** - saves bad records to a folder for later analysis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ec4262c-1c19-4828-85d4-0da5e73dbd28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.7.1 Schema Evolution & Rescued Data\n",
    "\n",
    "**Rescued Data Column** is an Auto Loader mechanism for handling unexpected data:\n",
    "\n",
    "| Scenario | Behavior |\n",
    "|----------|----------|\n",
    "| New columns | Saved in `_rescued_data` |\n",
    "| Type mismatches | Saved in `_rescued_data` |\n",
    "| Malformed records | Saved in `_rescued_data` |\n",
    "\n",
    "**Configuration:**\n",
    "```python\n",
    ".option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\n",
    "```\n",
    "\n",
    "**schemaEvolutionMode options:**\n",
    "- `addNewColumns`: Automatically adds new columns\n",
    "- `rescue`: New columns â†’ `_rescued_data` JSON\n",
    "- `failOnNewColumns`: Fail if schema changes\n",
    "- `none`: Ignores new columns (risky!)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49cd1f2c-f354-4709-9a86-18b0958d87fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TARGET_TABLE_RESCUE = f\"{BRONZE_SCHEMA}.orders_rescued\"\n",
    "CHECKPOINT_RESCUE = f\"{CHECKPOINT_BASE_PATH}/rescue\"\n",
    "SCHEMA_RESCUE = f\"{SCHEMA_BASE_PATH}/rescue\"\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {TARGET_TABLE_RESCUE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64130422-fd79-4be4-87c7-67431fdc0b0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Define explicit schema (partial):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b434ee06-b8f5-411a-807c-f65161e0598f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Deliberately define only some columns - rest will go to _rescued_data\n",
    "partial_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7699b09-f861-45a6-908f-226ac3fb36ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Auto Loader with rescue mode:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d03ef69-63fc-4dd2-bc41-7a0008fb7171",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create BAD data (Extra column + Type mismatch)\n",
    "bad_data = [{\"order_id\": 99999, \"total_amount\": \"INVALID_NUMBER\", \"new_col\": \"surprise\"}]\n",
    "spark.createDataFrame(bad_data).write.mode(\"overwrite\").json(f\"{STREAM_SOURCE_PATH}/bad_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eab793fd-bc8b-4e97-91d1-ee3ae0e9b775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_rescue = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", SCHEMA_RESCUE)\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")  # Rescue mode!\n",
    "    .schema(partial_schema)  # Partial schema\n",
    "    .load(STREAM_SOURCE_PATH)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c256190d-5dff-4d30-9400-90befefbc6f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Start stream:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21169df2-704e-4a1b-a08a-77dba6a3c709",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query_rescue = (df_rescue.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_RESCUE)\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(TARGET_TABLE_RESCUE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b164065-749d-4097-8c89-e3ca5ae8cca1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(TARGET_TABLE_RESCUE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9866b65d-1af3-4b9f-b7d4-f772e7dc6cfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Schema with `_rescued_data` column:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b9facae-c8e1-4373-a46c-787d3ff30d2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.table(TARGET_TABLE_RESCUE).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba9dad62-adc2-4bbf-a625-d4a238095cd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Data with rescued columns:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43ad1a6e-69e5-4ff0-8804-02bca0e5c7f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.table(TARGET_TABLE_RESCUE)\n",
    "    .limit(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "172d65d9-53c1-4a75-a153-b7a0286c657d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.7.2. Example: Error handling with badRecordsPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd46e30d-a729-4de9-b6e7-d517a0ed2ba0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TABLE_ERRORS = f\"{BRONZE_SCHEMA}.customers_with_validation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67871578-4110-4c64-90c4-5e7a849bcdfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Creating table with `_corrupt_record`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cfe2bd9-018c-48ae-a953-1c7a8e9f86f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {TABLE_ERRORS}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {TABLE_ERRORS} (\n",
    "  customer_id STRING,\n",
    "  first_name STRING,\n",
    "  last_name STRING,\n",
    "  email STRING,\n",
    "  phone STRING,\n",
    "  city STRING,\n",
    "  state STRING,\n",
    "  country STRING,\n",
    "  registration_date DATE,\n",
    "  customer_segment STRING,\n",
    "  _corrupt_record STRING,\n",
    "  _ingestion_timestamp TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbfbceac-fe6b-4d94-bb24-91f9ab851737",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Loading with error handling:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17c085d1-af69-4b48-8227-94264ca514b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a CSV with a bad row\n",
    "bad_csv_data = [\n",
    "    (999, \"Eve\", \"2023-01-03\"),\n",
    "    (888, \"Frank\", \"NOT_A_DATE\") # This will fail date parsing\n",
    "]\n",
    "spark.createDataFrame(bad_csv_data, [\"customer_id\", \"first_name\", \"registration_date\"]).write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{BATCH_SOURCE_PATH}/bad_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f78243b-e754-4759-ba96-f6eabf7450a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_errors = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
    "    .option(\"badRecordsPath\", BAD_RECORDS_PATH)\n",
    "    .schema(\"\"\"\n",
    "        customer_id STRING,\n",
    "        first_name STRING,\n",
    "        registration_date DATE,\n",
    "        _corrupt_record STRING\n",
    "    \"\"\")\n",
    "    .load(f\"{BATCH_SOURCE_PATH}/bad_csv\")\n",
    "    .withColumn(\"_ingestion_timestamp\", F.current_timestamp())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed60242f-c983-4348-8d61-4309eaea2ad6",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764071348400}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_with_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "626f1086-464e-49f5-99ea-b0b5c80c3f82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Bad records statistics:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "812f16e8-3425-4c18-b449-029845a536b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "files = [f.path for f in dbutils.fs.ls(BAD_RECORDS_PATH)]\n",
    "files = [f + \"*\" for f in files]\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f4614c6-31dd-4307-8e69-08825d729d0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "bad_record_schema = StructType([\n",
    "    StructField(\"path\", StringType(), True),\n",
    "    StructField(\"record\", StringType(), True),\n",
    "    StructField(\"reason\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "df_bad_records = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .schema(bad_record_schema)\n",
    "    .load(files)\n",
    ")\n",
    "display(df_bad_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06ca148f-f9e7-422e-a290-87bb1b34e8b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5.8. Lakeflow Connect (Informational)\n",
    "\n",
    "**Lakeflow Connect** is managed SaaS integration without writing code:\n",
    "\n",
    "### Supported sources:\n",
    "- Salesforce\n",
    "- Workday\n",
    "- Google Analytics\n",
    "- HubSpot\n",
    "- Stripe\n",
    "- SAP\n",
    "- Netsuite\n",
    "- ServiceNow\n",
    "\n",
    "### Key features:\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Zero-code** | Configuration via UI |\n",
    "| **Managed** | Automatic scaling |\n",
    "| **CDC Support** | Change Data Capture |\n",
    "| **Schema Evolution** | Automatic updates |\n",
    "| **Unity Catalog** | Full integration |\n",
    "\n",
    "### How to start:\n",
    "1. Workspace â†’ **Data** â†’ **Lakeflow** â†’ **Connect**\n",
    "2. Choose connector (e.g. Salesforce)\n",
    "3. Provide credentials\n",
    "4. Select objects to synchronize\n",
    "5. Set schedule\n",
    "\n",
    "### Ingestion methods comparison:\n",
    "\n",
    "| Method | Use Case |\n",
    "|--------|----------|\n",
    "| **COPY INTO** | Files in cloud storage (batch) |\n",
    "| **Auto Loader** | Files in cloud storage (streaming) |\n",
    "| **Lakeflow Connect** | Data from SaaS systems |\n",
    "| **Lakeflow Pipelines** | Transformations Bronze â†’ Silver â†’ Gold |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b8901d4-859d-4920-bc1b-4a40199da6bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5.9. Cleanup (Optional)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62712302-8e7c-4539-83c1-f10c0beae485",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of created tables\n",
    "created_tables = [\n",
    "    \"customers_batch\",\n",
    "    \"orders_autoloader\",\n",
    "    \"orders_rescued\",\n",
    "    \"customers_with_validation\",\n",
    "    \"orders_trigger_test\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df386671-3c5d-4cdd-8a06-3292819bbe12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5.10. Final Summary\n",
    "\n",
    "### What was achieved:\n",
    "- Built a robust streaming pipeline using Auto Loader\n",
    "- Implemented incremental processing with Structured Streaming\n",
    "- Handled schema evolution with `cloudFiles.schemaEvolutionMode`\n",
    "- Enriched streaming data with static dimension tables (Stream-Static Join)\n",
    "- Managed stateful aggregations with Watermarking\n",
    "- Connected the pipeline to Lakeflow for orchestration\n",
    "\n",
    "### Key Takeaways:\n",
    "1. **Auto Loader (cloudFiles)** is the standard for ingestion - handles state, schema, and files automatically.\n",
    "2. **Structured Streaming** unifies batch and streaming APIs.\n",
    "3. **Watermarking** is critical for state cleanup in aggregations.\n",
    "4. **Stream-Static Joins** allow enriching streams with reference data (but require careful caching/reloading).\n",
    "5. **Schema Evolution** prevents pipeline failures when data changes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaeb682f-faa3-4046-bb90-cc0436a9ac1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Created resources verification:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40962ec5-3478-449d-865b-7ec5e756d9f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for table in created_tables:\n",
    "    full_table = f\"{CATALOG}.{BRONZE_SCHEMA}.{table}\"\n",
    "    try:\n",
    "        if spark.catalog.tableExists(full_table):\n",
    "            count = spark.table(full_table).count()\n",
    "            results.append((table, \"EXISTS\", str(count)))\n",
    "        else:\n",
    "            results.append((table, \"NOT FOUND\", \"-\"))\n",
    "    except Exception as e:\n",
    "        results.append((table, \"ERROR\", str(e)[:30]))\n",
    "\n",
    "display(spark.createDataFrame(results, [\"Table\", \"Status\", \"Records\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cec9d84a-4418-440c-8776-07a70a45762f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Flaga cleanup\n",
    "CLEANUP_ENABLED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "765c9b92-0528-4384-b9f6-2651d59021f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Execute cleanup (if enabled):**\n",
    "> [TIP] *Recommended: Keep data for subsequent notebooks!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e69e637-a538-42f6-b3c6-20ead9495071",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if CLEANUP_ENABLED:\n",
    "    results = []\n",
    "    for table in created_tables:\n",
    "        full_table = f\"{CATALOG}.{BRONZE_SCHEMA}.{table}\"\n",
    "        try:\n",
    "            spark.sql(f\"DROP TABLE IF EXISTS {full_table}\")\n",
    "            results.append((table, \"DROPPED\"))\n",
    "        except Exception as e:\n",
    "            results.append((table, f\"ERROR: {str(e)[:30]}\"))\n",
    "    \n",
    "    # Cleanup checkpoints\n",
    "    try:\n",
    "        dbutils.fs.rm(CHECKPOINT_BASE_PATH, True)\n",
    "        results.append((\"checkpoints\", \"REMOVED\"))\n",
    "    except:\n",
    "        results.append((\"checkpoints\", \"NOT FOUND\"))\n",
    "    \n",
    "    display(spark.createDataFrame(results, [\"Resource\", \"Status\"]))\n",
    "else:\n",
    "    display(spark.createDataFrame([\n",
    "        (\"CLEANUP_ENABLED\", \"False\"),\n",
    "        (\"Action\", \"Change to True to delete resources\")\n",
    "    ], [\"Setting\", \"Value\"]))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4873620425681583,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "04_streaming_incremental",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}