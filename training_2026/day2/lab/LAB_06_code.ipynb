{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41147a89",
   "metadata": {},
   "source": [
    "# LAB 06: Advanced Transforms -- PySpark & SQL\n",
    "\n",
    "**Duration:** ~40 min | **Day:** 2 | **Difficulty:** Intermediate-Advanced\n",
    "\n",
    "> *\"Build analytical reports using window functions, CTEs, explode, and CTAS.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89c90b0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e696fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../setup/00_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933a7d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum, count, desc, row_number, rank, dense_rank, lag, lead, explode, from_json\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Load base data\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{GOLD_SCHEMA}\")\n",
    "\n",
    "df_orders = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.orders\")\n",
    "df_customers = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers\")\n",
    "df_products = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.products\")\n",
    "\n",
    "# Register as temp views for SQL tasks\n",
    "df_orders.createOrReplaceTempView(\"orders\")\n",
    "df_customers.createOrReplaceTempView(\"customers\")\n",
    "df_products.createOrReplaceTempView(\"products\")\n",
    "\n",
    "print(f\"Data loaded: {df_orders.count()} orders, {df_customers.count()} customers, {df_products.count()} products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b9f86d",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1: Window Function -- Rank Products by Revenue (PySpark)\n",
    "\n",
    "For each product, calculate total revenue. Then rank products using `row_number()`.\n",
    "\n",
    "Hint: Use `Window.orderBy(desc(\"total_revenue\"))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddedea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate total revenue per product and rank them\n",
    "df_product_revenue = (\n",
    "    df_orders\n",
    "    .groupBy(\"product_id\")\n",
    "    .agg(sum(\"total_price\").alias(\"total_revenue\"))\n",
    ")\n",
    "\n",
    "window_spec = Window.orderBy(________(\"total_revenue\"))\n",
    "\n",
    "df_ranked = (\n",
    "    df_product_revenue\n",
    "    .withColumn(\"rank\", ________(________))\n",
    ")\n",
    "\n",
    "display(df_ranked.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cef489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert \"rank\" in df_ranked.columns, \"Missing 'rank' column\"\n",
    "first = df_ranked.orderBy(\"rank\").first()\n",
    "assert first[\"rank\"] == 1, \"First row should have rank 1\"\n",
    "print(f\"Task 1 OK: Top product has revenue {first['total_revenue']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625942a1",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Running Total (SQL)\n",
    "\n",
    "Write a SQL query to compute a cumulative running total per customer ordered by order_date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae601329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the SQL window function\n",
    "df_running = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        order_date,\n",
    "        total_price,\n",
    "        SUM(total_price) OVER (\n",
    "            PARTITION BY ________\n",
    "            ORDER BY ________\n",
    "            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
    "        ) AS running_total\n",
    "    FROM orders\n",
    "    ORDER BY customer_id, order_date\n",
    "\"\"\")\n",
    "\n",
    "display(df_running.limit(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d048c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert \"running_total\" in df_running.columns, \"Missing 'running_total' column\"\n",
    "print(f\"Task 2 OK: Running totals computed for {df_running.select('customer_id').distinct().count()} customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b395b57",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3: Multi-step CTE\n",
    "\n",
    "Write a SQL query with two CTEs to find the top 5 days by total revenue:\n",
    "1. `daily_sales` -- total revenue per day\n",
    "2. `ranked_days` -- rank days by revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fa7e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the CTE query\n",
    "df_top_days = spark.sql(\"\"\"\n",
    "    WITH daily_sales AS (\n",
    "        SELECT \n",
    "            order_date,\n",
    "            ________(total_price) AS daily_revenue,\n",
    "            ________(*)          AS order_count\n",
    "        FROM orders\n",
    "        GROUP BY order_date\n",
    "    ),\n",
    "    ranked_days AS (\n",
    "        SELECT *,\n",
    "            ROW_NUMBER() OVER (ORDER BY daily_revenue ________) AS day_rank\n",
    "        FROM daily_sales\n",
    "    )\n",
    "    SELECT * FROM ranked_days\n",
    "    WHERE day_rank <= 5\n",
    "    ORDER BY day_rank\n",
    "\"\"\")\n",
    "\n",
    "display(df_top_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fcdb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert df_top_days.count() <= 5, \"Should return at most 5 rows\"\n",
    "assert df_top_days.first()[\"day_rank\"] == 1, \"First row should be rank 1\"\n",
    "print(f\"Task 3 OK: Top {df_top_days.count()} days by revenue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1cb829",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 4: Correlated Subquery\n",
    "\n",
    "Find customers whose total spending is above the overall average spending per customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c685d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write SQL with subquery\n",
    "df_high_spenders = spark.sql(\"\"\"\n",
    "    SELECT customer_id, SUM(total_price) AS total_spent\n",
    "    FROM orders\n",
    "    GROUP BY customer_id\n",
    "    HAVING SUM(total_price) > (\n",
    "        SELECT ________(total_spent) FROM (\n",
    "            SELECT customer_id, SUM(total_price) AS total_spent\n",
    "            FROM orders\n",
    "            GROUP BY customer_id\n",
    "        )\n",
    "    )\n",
    "    ORDER BY total_spent DESC\n",
    "\"\"\")\n",
    "\n",
    "display(df_high_spenders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f13e850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert df_high_spenders.count() > 0, \"Should find at least some high spenders\"\n",
    "print(f\"Task 4 OK: {df_high_spenders.count()} customers above average spending\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b582a90c",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 5: Explode Array Column\n",
    "\n",
    "Create a sample DataFrame with an array column and use `explode()` to flatten it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e002906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array, lit, explode\n",
    "\n",
    "# Sample data with array column\n",
    "df_with_array = spark.createDataFrame([\n",
    "    (1, [\"Electronics\", \"Books\", \"Clothing\"]),\n",
    "    (2, [\"Food\", \"Electronics\"]),\n",
    "    (3, [\"Books\"])\n",
    "], [\"customer_id\", \"categories\"])\n",
    "\n",
    "# TODO: Explode the categories array into individual rows\n",
    "df_exploded = df_with_array.select(\n",
    "    \"customer_id\",\n",
    "    ________(col(\"categories\")).alias(\"category\")\n",
    ")\n",
    "\n",
    "display(df_exploded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c314787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert df_exploded.count() == 6, f\"Expected 6 rows after explode, got {df_exploded.count()}\"\n",
    "assert \"category\" in df_exploded.columns, \"Missing 'category' column\"\n",
    "print(f\"Task 5 OK: Exploded {df_with_array.count()} rows into {df_exploded.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0a735e",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 6: CTAS -- Create Gold Tables\n",
    "\n",
    "Use `CREATE TABLE AS SELECT` to persist the top products analysis as a Gold table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18515769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create gold table using CTAS\n",
    "gold_table = f\"{CATALOG}.{GOLD_SCHEMA}.top_products\"\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {gold_table}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    ________ {gold_table} AS\n",
    "    SELECT \n",
    "        product_id,\n",
    "        SUM(total_price) AS total_revenue,\n",
    "        COUNT(*) AS order_count\n",
    "    FROM {CATALOG}.{BRONZE_SCHEMA}.orders\n",
    "    GROUP BY product_id\n",
    "    ORDER BY total_revenue DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "display(spark.table(gold_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597e7166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "gold_count = spark.table(gold_table).count()\n",
    "assert gold_count > 0 and gold_count <= 10, f\"Expected 1-10 rows, got {gold_count}\"\n",
    "detail = spark.sql(f\"DESCRIBE DETAIL {gold_table}\").first()\n",
    "assert detail[\"format\"] == \"delta\", \"CTAS should create a Delta table\"\n",
    "print(f\"Task 6 OK: Gold table '{gold_table}' created with {gold_count} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2e9c0d",
   "metadata": {},
   "source": [
    "---\n",
    "## Lab Complete!\n",
    "\n",
    "You have:\n",
    "- Applied window functions (row_number, running SUM) in PySpark and SQL\n",
    "- Written multi-step CTEs for complex analytics\n",
    "- Used subqueries to filter by aggregate conditions\n",
    "- Flattened arrays with explode()\n",
    "- Created Gold tables using CTAS\n",
    "\n",
    "> **Exam Tip:** Know the difference: `ROW_NUMBER()` always gives unique sequential numbers. `RANK()` gives the same number for ties (with gaps). `DENSE_RANK()` gives same number for ties (no gaps).\n",
    "\n",
    "> **Next:** LAB 07 - Build a Medallion Pipeline in Lakeflow"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
