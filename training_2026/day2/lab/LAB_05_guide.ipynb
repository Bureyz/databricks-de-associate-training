{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 05: Streaming & Auto Loader\n\n**Duration:** ~35 min  \n**Day:** 2  \n**After module:** M05: Incremental Data Processing  \n**Difficulty:** Intermediate-Advanced\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario\n\n> *\"RetailHub launched a new sales channel -- orders now arrive as JSON files every 5 minutes in a landing zone. Your task: configure Auto Loader for continuous ingestion, process the stream, and land the data in the Bronze Delta table with exactly-once guarantees.\"*\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n\nAfter completing this lab you will be able to:\n- Configure Auto Loader (`cloudFiles`) for streaming file ingestion\n- Set up `readStream` and `writeStream` pipelines\n- Use `trigger(availableNow=True)` for incremental batch processing\n- Manage checkpoints for exactly-once guarantees\n- Monitor active streams\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: COPY INTO (Batch) (~10 min)\n\n### Task 1: Load Files with COPY INTO\n\nUse `COPY INTO` to load the first batch of JSON files. Observe its idempotent behavior.\n\n> **Exam Tip:** `COPY INTO` tracks files already loaded. Re-running it on the same files is a no-op. Useful for simple, scheduled batch ingestion.\n\n<screen = COPY INTO command result showing rows loaded and files processed count>\n\n### Task 2: Re-run COPY INTO\n\nRun the same `COPY INTO` again. Observe that 0 new rows are loaded (idempotent).\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Auto Loader (~15 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Configure Auto Loader Stream\n\nSet up a `readStream` with Auto Loader:\n- Format: `cloudFiles`\n- Source format: JSON\n- Schema location for schema inference\n- Checkpoint location for exactly-once processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Write Stream to Bronze\n\nWrite the Auto Loader stream to a Bronze Delta table using `trigger(availableNow=True)`.\n\n<screen = Streaming query progress showing numInputRows, numOutputRows, and processing time>\n\n> **Exam Tip:** `trigger(availableNow=True)` processes all available files and then stops. It replaces the deprecated `trigger(once=True)` with better performance (multiple micro-batches)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Simulate New Arrivals\n\nCopy new JSON files to the landing zone, then re-run the stream. Only new files should be processed.\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Stream Monitoring (~10 min)\n\n### Task 6: Check Active Streams\n\nUse `spark.streams.active` to list running streams.\n\n### Task 7: Inspect Checkpoint\n\nExamine the checkpoint directory to understand the state management.\n\n> **Exam Tip:** Checkpoints store: (1) what files have been processed (source offsets), (2) what data has been committed (sink commitlog). Never delete or modify checkpoints in production.\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n\nIn this lab you:\n- Used COPY INTO for idempotent batch ingestion\n- Configured Auto Loader for streaming file ingestion\n- Used trigger(availableNow=True) for incremental processing\n- Verified exactly-once guarantees via checkpoints\n\n> **What's next:** Day 3 starts with LAB 06 - Advanced Transforms using PySpark and SQL."
   ]
  }
 ]
}