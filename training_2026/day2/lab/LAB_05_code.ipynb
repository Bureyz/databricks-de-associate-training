{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8e7f0d1",
   "metadata": {},
   "source": [
    "# LAB 05: Streaming & Auto Loader\n",
    "\n",
    "**Duration:** ~35 min | **Day:** 2 | **Difficulty:** Intermediate-Advanced\n",
    "\n",
    "> *\"Set up Auto Loader for streaming JSON ingestion into the Bronze layer with exactly-once guarantees.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fbfc80",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d95d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../setup/00_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0046ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare landing zone and checkpoint paths\n",
    "landing_path = f\"{DATASET_PATH}/orders/stream\"\n",
    "checkpoint_path = f\"/tmp/{CATALOG}/lab05/checkpoint\"\n",
    "schema_path = f\"/tmp/{CATALOG}/lab05/schema\"\n",
    "target_table = f\"{CATALOG}.{BRONZE_SCHEMA}.orders_stream\"\n",
    "\n",
    "# Clean up from previous runs\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {target_table}\")\n",
    "dbutils.fs.rm(checkpoint_path, True)\n",
    "dbutils.fs.rm(schema_path, True)\n",
    "\n",
    "print(f\"Landing path: {landing_path}\")\n",
    "print(f\"Target table: {target_table}\")\n",
    "print(f\"Files available: {[f.name for f in dbutils.fs.ls(landing_path)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da740c00",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1: COPY INTO (Batch Ingestion)\n",
    "\n",
    "Use `COPY INTO` to load the first file from the landing zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd9cd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create the target table\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {target_table}\n",
    "    (order_id STRING, customer_id STRING, product_id STRING, \n",
    "     quantity INT, total_price DOUBLE, order_date STRING, \n",
    "     payment_method STRING, store_id STRING)\n",
    "\"\"\")\n",
    "\n",
    "# TODO: Use COPY INTO to load JSON files\n",
    "spark.sql(f\"\"\"\n",
    "    COPY INTO {target_table}\n",
    "    FROM '{landing_path}'\n",
    "    FILEFORMAT = ________\n",
    "    FORMAT_OPTIONS ('mergeSchema' = 'true')\n",
    "\"\"\")\n",
    "\n",
    "count_after_copy = spark.table(target_table).count()\n",
    "print(f\"Rows after COPY INTO: {count_after_copy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd6b49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert count_after_copy > 0, \"COPY INTO should have loaded data\"\n",
    "print(f\"Task 1 OK: {count_after_copy} rows loaded via COPY INTO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a96f94d",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Verify COPY INTO Idempotency\n",
    "\n",
    "Run COPY INTO again on the same files. How many new rows are loaded?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666136ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Re-run the same COPY INTO\n",
    "spark.sql(f\"\"\"\n",
    "    COPY INTO {target_table}\n",
    "    FROM '{landing_path}'\n",
    "    FILEFORMAT = JSON\n",
    "    FORMAT_OPTIONS ('mergeSchema' = 'true')\n",
    "\"\"\")\n",
    "\n",
    "count_after_rerun = spark.table(target_table).count()\n",
    "print(f\"Rows after re-run: {count_after_rerun} (was {count_after_copy})\")\n",
    "print(f\"New rows loaded: {count_after_rerun - count_after_copy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140e77ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert count_after_rerun == count_after_copy, \"COPY INTO should be idempotent - no new rows!\"\n",
    "print(\"Task 2 OK: COPY INTO is idempotent. 0 new rows on re-run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ed5a6b",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3: Auto Loader - Configure Stream\n",
    "\n",
    "Set up Auto Loader (`cloudFiles`) to read JSON files from the landing zone.\n",
    "\n",
    "Key options:\n",
    "- `cloudFiles.format` = json\n",
    "- `cloudFiles.schemaLocation` = path for inferred schema\n",
    "- `cloudFiles.inferColumnTypes` = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df6640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset target for Auto Loader test\n",
    "al_target = f\"{CATALOG}.{BRONZE_SCHEMA}.orders_autoloader\"\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {al_target}\")\n",
    "dbutils.fs.rm(checkpoint_path, True)\n",
    "dbutils.fs.rm(schema_path, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eff1728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Configure Auto Loader readStream\n",
    "df_stream = (\n",
    "    spark.readStream\n",
    "    .format(________)\n",
    "    .option(\"cloudFiles.format\", ________)\n",
    "    .option(\"cloudFiles.schemaLocation\", schema_path)\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .load(landing_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5fdf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert df_stream.isStreaming, \"Should be a streaming DataFrame\"\n",
    "print(f\"Task 3 OK: Streaming DataFrame configured with schema: {df_stream.schema.fieldNames()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c7522d",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 4: Write Stream with trigger(availableNow=True)\n",
    "\n",
    "Write the stream to a Delta table using `trigger(availableNow=True)`.\n",
    "\n",
    "This processes all available files and stops automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5397a84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write stream to Delta table\n",
    "query = (\n",
    "    df_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .trigger(________=________)\n",
    "    .toTable(al_target)\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "print(f\"Stream completed. Rows loaded: {spark.table(al_target).count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66c4e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "al_count = spark.table(al_target).count()\n",
    "assert al_count > 0, \"Auto Loader should have loaded data\"\n",
    "print(f\"Task 4 OK: {al_count} rows loaded via Auto Loader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fc942a",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 5: Incremental Processing\n",
    "\n",
    "Re-run the stream. Since no new files arrived, 0 new rows should be processed.\n",
    "\n",
    "This proves the checkpoint tracks processed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c1910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run the stream (same checkpoint = incremental)\n",
    "df_stream2 = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", schema_path)\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .load(landing_path)\n",
    ")\n",
    "\n",
    "query2 = (\n",
    "    df_stream2\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(al_target)\n",
    ")\n",
    "\n",
    "query2.awaitTermination()\n",
    "al_count2 = spark.table(al_target).count()\n",
    "print(f\"Rows after re-run: {al_count2} (was {al_count})\")\n",
    "print(f\"New rows: {al_count2 - al_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c67741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Validation --\n",
    "assert al_count2 == al_count, f\"Should be 0 new rows, but got {al_count2 - al_count}\"\n",
    "print(\"Task 5 OK: Incremental processing verified. 0 new rows on re-run (checkpoint works!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68360d55",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c420707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop any active streams\n",
    "for s in spark.streams.active:\n",
    "    s.stop()\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {target_table}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {al_target}\")\n",
    "dbutils.fs.rm(checkpoint_path, True)\n",
    "dbutils.fs.rm(schema_path, True)\n",
    "print(\"Lab cleanup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094389d9",
   "metadata": {},
   "source": [
    "---\n",
    "## Lab Complete!\n",
    "\n",
    "You have:\n",
    "- Used COPY INTO for idempotent batch loading\n",
    "- Configured Auto Loader (cloudFiles) for streaming ingestion\n",
    "- Used trigger(availableNow=True) for incremental processing\n",
    "- Verified checkpoint-based exactly-once guarantees\n",
    "\n",
    "> **Exam Tip:** Auto Loader uses `cloudFiles` format. COPY INTO is simpler but Auto Loader scales better (file notification mode for millions of files). Both are idempotent.\n",
    "\n",
    "| Feature | COPY INTO | Auto Loader |\n",
    "|---------|-----------|-------------|\n",
    "| Format | SQL command | readStream/writeStream |\n",
    "| Scalability | Thousands of files | Millions of files |\n",
    "| Schema evolution | Manual | Automatic (rescue column) |\n",
    "| File tracking | SQL state | Checkpoint directory |\n",
    "\n",
    "> **Next:** LAB 06 - Advanced Transforms "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
