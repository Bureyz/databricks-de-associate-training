{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Adjust these values\n",
    "# =============================================================================\n",
    "\n",
    "TRAINING_GROUP = \"dp_trn_1\"  # Databricks group with training participants\n",
    "CATALOG_PREFIX = \"ecommerce_platform\"  # Catalog naming: ecommerce_platform_{username}\n",
    "\n",
    "# GitHub raw content base URL\n",
    "GITHUB_RAW_BASE = \"https://raw.githubusercontent.com/Bureyz/DataEngineeringOne/main/dataset\"\n",
    "\n",
    "# Files to download (remote_path, local_name)\n",
    "DATASET_FILES = [\n",
    "    (\"customers/customers.csv\", \"customers/customers.csv\"),\n",
    "    (\"customers/customers2.csv\", \"customers/customers2.csv\"),\n",
    "    (\"customers/customers_extended.csv\", \"customers/customers_extended.csv\"),\n",
    "    (\"customers/customers_new.csv\", \"customers/customers_new.csv\"),\n",
    "    (\"products/csv/products.csv\", \"products/products.csv\"),\n",
    "    (\"products/csv/products_new.csv\", \"products/products_new.csv\"),\n",
    "    (\"products/csv/products_premium.csv\", \"products/products_premium.csv\"),\n",
    "    (\"products/csv/products_updated.csv\", \"products/products_updated.csv\"),\n",
    "    (\"orders/orders_batch.json\", \"orders/orders_batch.json\")\n",
    "]\n",
    "\n",
    "# Schema names (Medallion architecture)\n",
    "BRONZE_SCHEMA = \"bronze\"\n",
    "SILVER_SCHEMA = \"silver\"\n",
    "GOLD_SCHEMA = \"gold\"\n",
    "DEFAULT_SCHEMA = \"default\"\n",
    "\n",
    "# Volume name for datasets\n",
    "VOLUME_NAME = \"datasets\"\n",
    "\n",
    "print(f\"Training group: {TRAINING_GROUP}\")\n",
    "print(f\"Catalog prefix: {CATALOG_PREFIX}\")\n",
    "print(f\"GitHub base: {GITHUB_RAW_BASE}\")\n",
    "print(f\"Files to download: {len(DATASET_FILES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Get Users from Training Group\n",
    "\n",
    "We use Databricks REST API to get group members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "def get_group_members(group_name):\n",
    "    \"\"\"\n",
    "    Get all members of a Databricks group using REST API.\n",
    "    Returns list of usernames (email addresses).\n",
    "    \"\"\"\n",
    "    context = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "    host = context.apiUrl().get()\n",
    "    token = context.apiToken().get()\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # Get group ID first\n",
    "    groups_url = f\"{host}/api/2.0/preview/scim/v2/Groups\"\n",
    "    params = {\"filter\": f'displayName eq \"{group_name}\"'}\n",
    "    \n",
    "    response = requests.get(groups_url, headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    groups = response.json().get(\"Resources\", [])\n",
    "    if not groups:\n",
    "        raise ValueError(f\"Group '{group_name}' not found\")\n",
    "    \n",
    "    group_id = groups[0][\"id\"]\n",
    "    \n",
    "    # Get group members\n",
    "    group_url = f\"{host}/api/2.0/preview/scim/v2/Groups/{group_id}\"\n",
    "    response = requests.get(group_url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    members = response.json().get(\"members\", [])\n",
    "    \n",
    "    # Get user emails\n",
    "    user_emails = []\n",
    "    for member in members:\n",
    "        if member.get(\"$ref\", \"\").startswith(\"Users/\"):\n",
    "            user_id = member[\"value\"]\n",
    "            user_url = f\"{host}/api/2.0/preview/scim/v2/Users/{user_id}\"\n",
    "            user_response = requests.get(user_url, headers=headers)\n",
    "            user_response.raise_for_status()\n",
    "            user_data = user_response.json()\n",
    "            emails = user_data.get(\"emails\", [])\n",
    "            if emails:\n",
    "                user_emails.append(emails[0].get(\"value\", \"\"))\n",
    "    \n",
    "    return user_emails\n",
    "\n",
    "def sanitize_username(email):\n",
    "    \"\"\"\n",
    "    Convert email to safe catalog name suffix.\n",
    "    Example: jan.kowalski@company.com -> jan_kowalski\n",
    "    \"\"\"\n",
    "    username = email.split('@')[0]\n",
    "    safe_name = re.sub(r'[^a-zA-Z0-9]', '_', username).lower()\n",
    "    safe_name = re.sub(r'_+', '_', safe_name)\n",
    "    safe_name = safe_name.strip('_')\n",
    "    return safe_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get users from training group\n",
    "try:\n",
    "    training_users = get_group_members(TRAINING_GROUP)\n",
    "    print(f\"Found {len(training_users)} users in group '{TRAINING_GROUP}':\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    user_catalog_map = {}\n",
    "    for email in training_users:\n",
    "        safe_name = sanitize_username(email)\n",
    "        catalog_name = f\"{CATALOG_PREFIX}_{safe_name}\"\n",
    "        user_catalog_map[email] = catalog_name\n",
    "        print(f\"  {email} -> {catalog_name}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total: {len(user_catalog_map)} catalogs to create\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not get group members: {e}\")\n",
    "    print(f\"\\nPossible issues:\")\n",
    "    print(f\"  1. Group '{TRAINING_GROUP}' does not exist\")\n",
    "    print(\"  2. You don't have permission to read group members\")\n",
    "    print(\"  3. Group has no members\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Catalogs and Schemas\n",
    "\n",
    "For each user, we create:\n",
    "- Catalog: `ecommerce_platform_{username}`\n",
    "- Schemas: `bronze`, `silver`, `gold`, `default`\n",
    "- Volume: `datasets` in `default` schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_environment(email, catalog_name):\n",
    "    \"\"\"\n",
    "    Create catalog, schemas, and volume for a user.\n",
    "    \"\"\"\n",
    "    results = {\"catalog\": False, \"schemas\": [], \"volume\": False, \"owner\": False}\n",
    "    \n",
    "    try:\n",
    "        # Create catalog\n",
    "        spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog_name}\")\n",
    "        results[\"catalog\"] = True\n",
    "        \n",
    "        # Create schemas\n",
    "        for schema in [DEFAULT_SCHEMA, BRONZE_SCHEMA, SILVER_SCHEMA, GOLD_SCHEMA]:\n",
    "            spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema}\")\n",
    "            results[\"schemas\"].append(schema)\n",
    "        \n",
    "        # Create volume for datasets\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE VOLUME IF NOT EXISTS {catalog_name}.{DEFAULT_SCHEMA}.{VOLUME_NAME}\n",
    "            COMMENT 'Training datasets volume'\n",
    "        \"\"\")\n",
    "        results[\"volume\"] = True\n",
    "        \n",
    "        # Set owner to user\n",
    "        spark.sql(f\"ALTER CATALOG {catalog_name} SET OWNER TO \\`{email}\\`\")\n",
    "        results[\"owner\"] = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        results[\"error\"] = str(e)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environments for all users\n",
    "print(\"Creating user environments...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "creation_results = {}\n",
    "\n",
    "for email, catalog_name in user_catalog_map.items():\n",
    "    print(f\"\\nProcessing: {email}\")\n",
    "    result = create_user_environment(email, catalog_name)\n",
    "    creation_results[email] = result\n",
    "    \n",
    "    if \"error\" in result:\n",
    "        print(f\"  ERROR: {result['error']}\")\n",
    "    else:\n",
    "        print(f\"  Catalog: {catalog_name}\")\n",
    "        print(f\"  Schemas: {', '.join(result['schemas'])}\")\n",
    "        print(f\"  Volume: {result['volume']}\")\n",
    "        print(f\"  Owner set: {result['owner']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "successful = sum(1 for r in creation_results.values() if \"error\" not in r)\n",
    "print(f\"Successfully created: {successful}/{len(creation_results)} environments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Download Dataset from GitHub\n",
    "\n",
    "Download files directly from GitHub raw content to each user's Volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "def download_dataset_to_volume(catalog_name):\n",
    "    \"\"\"\n",
    "    Download dataset files directly from GitHub to user's Volume.\n",
    "    Uses simple urllib + dbutils.fs.put approach.\n",
    "    \"\"\"\n",
    "    volume_path = f\"/Volumes/{catalog_name}/{DEFAULT_SCHEMA}/{VOLUME_NAME}\"\n",
    "    results = {\"success\": [], \"failed\": []}\n",
    "    \n",
    "    for remote_path, local_path in DATASET_FILES:\n",
    "        url = f\"{GITHUB_RAW_BASE}/{remote_path}\"\n",
    "        dest_path = f\"{volume_path}/{local_path}\"\n",
    "        \n",
    "        try:\n",
    "            # Download file content\n",
    "            response = urllib.request.urlopen(url)\n",
    "            content = response.read()\n",
    "            \n",
    "            # Write to volume using dbutils\n",
    "            dbutils.fs.put(dest_path, content.decode('utf-8'), overwrite=True)\n",
    "            results[\"success\"].append(local_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[\"failed\"].append((local_path, str(e)))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset to each user's Volume\n",
    "print(\"Downloading dataset to user Volumes...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "download_results = {}\n",
    "\n",
    "for email, catalog_name in user_catalog_map.items():\n",
    "    print(f\"\\nDownloading to: {catalog_name}\")\n",
    "    result = download_dataset_to_volume(catalog_name)\n",
    "    download_results[email] = result\n",
    "    \n",
    "    if result[\"success\"]:\n",
    "        print(f\"  ✓ Downloaded: {len(result['success'])} files\")\n",
    "    if result[\"failed\"]:\n",
    "        print(f\"  ✗ Failed: {len(result['failed'])} files\")\n",
    "        for file, error in result[\"failed\"]:\n",
    "            print(f\"    - {file}: {error}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "successful = sum(1 for r in download_results.values() if not r[\"failed\"])\n",
    "print(f\"Fully completed: {successful}/{len(download_results)} volumes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Verify Setup\n",
    "\n",
    "Verify all environments are ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAINING ENVIRONMENT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "for email, catalog_name in user_catalog_map.items():\n",
    "    print(f\"User: {email}\")\n",
    "    print(f\"  Catalog: {catalog_name}\")\n",
    "    print(f\"  Schemas: {BRONZE_SCHEMA}, {SILVER_SCHEMA}, {GOLD_SCHEMA}, {DEFAULT_SCHEMA}\")\n",
    "    \n",
    "    volume_path = f\"/Volumes/{catalog_name}/{DEFAULT_SCHEMA}/{VOLUME_NAME}\"\n",
    "    try:\n",
    "        files = dbutils.fs.ls(volume_path)\n",
    "        print(f\"  Volume folders: {[f.name for f in files]}\")\n",
    "    except:\n",
    "        print(f\"  Volume: ERROR - cannot access\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PRE-CONFIGURATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Participants can now run 00_setup.ipynb to validate their environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (After Training)\n",
    "\n",
    "Run this section to remove all training catalogs after the training is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CLEANUP - Remove all training catalogs\n",
    "# =============================================================================\n",
    "# WARNING: This will DELETE all training data!\n",
    "\n",
    "# Uncomment to run:\n",
    "# for email, catalog_name in user_catalog_map.items():\n",
    "#     try:\n",
    "#         spark.sql(f\"DROP CATALOG IF EXISTS {catalog_name} CASCADE\")\n",
    "#         print(f\"Dropped: {catalog_name}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to drop {catalog_name}: {e}\")\n",
    "# \n",
    "# print(\"\\nCleanup complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
